[
  {
    "id": 38477259,
    "title": "Generating Optical Illusions with Pretrained Diffusion Models",
    "originLink": "https://dangeng.github.io/visual_anagrams/",
    "originBody": "Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models Daniel Geng, Inbum Park, Andrew Owens, University of Michigan Correspondence to: dgeng@umich.edu Paper arXiv Code Colab Print a Jigsaw! tl;dr: We use pretrained diffusion models to make optical illusions Overview We present a simple, zero-shot method to generate multi-view optical illusions. These are images that look like one thing, but change appearance or identity when transformed. We show in theory and practice that our method supports a broad range of transformations including rotations, flips, color inversions, skews, jigsaw rearrangements, and random permutations. We show some examples below. Jigsaw Permutations Flips and 180¬∞ Rotations 90¬∞ Rotations Color Inversions Miscellaneous Permutations Random Patch Permutations Method Our method is conceptually simple. We take an off-the-shelf diffusion model and use it to estimate the noise in different views or transformations, ùë£ ùëñ , of an image. The noise estimates are then aligned by applying the inverse view, ùë£ ùëñ ‚àí 1 , and averaged together. This averaged noise estimate is then used to take a diffusion step. Conditions on Views We find that not every view function works with the above method. Of course, ùë£ ùëñ must be invertible, but we discuss two additional constraints. Linearity A diffusion model is trained to estimate the noise in noisy data ùë• ùë° conditioned on time step ùë° . The noisy data ùë• ùë° is expected to have the form ùë• ùë° = ùë§ ùë° signal ùë• 0 ‚èü signal + ùë§ ùë° noise ùúñ ‚èü noise . That is, ùë• ùë° is a weighted average of pure signal ùë• 0 and pure noise ùúñ , specifically with weights ùë§ ùë° signal and ùë§ ùë° noise . Therefore, our view, ùë£ must maintain this weighting between signal and noise. This can be achieved by making ùë£ linear, which we represent by the square matrix ùê¥ . By linearity ùë£ ( ùë• ùë° ) = ùê¥ ( ùë§ ùë° signal ùë• 0 + ùë§ ùë° noise ùúñ ) = ùë§ ùë° signal ùê¥ ùë• 0 ‚èü new signal + ùë§ ùë° noise ùê¥ ùúñ ‚èü new noise . Effectively, ùë£ acts on the signal and the noise independently, and combines the result with the correct weighting. Statistical Consistency Diffusion models are trained with the assumption that the noise is drawn iid from a standard normal. Therefore we must ensure that the transformed noise also follows these statistics. That is, we need ùê¥ ùúñ ‚àº ùëÅ ( 0 , ùêº ) . For linear transformations, this is equivalent to the condition that ùê¥ is orthogonal. Intuitively, orthogonal matrices respect the spherical symmetry of the standard multivariate Gaussian distribution. Therefore, for a transformation to work with our method, it is sufficient for it to be orthogonal. Orthogonal Transformations Most orthogonal transformations on images are meaningless, visually. For example, we transform the image below with a randomly sampled orthogonal matrix. However, permutations matrices are a subset of orthogonal matrices, and are quite interpretable. They are just rearrangements of pixels in an image. This is where the idea of a visual anagram comes from. The majority of illusions here can be interpreted this way‚Äîas specific rearrangements of pixels‚Äîsuch as rotations, flips, skews, \"inner rotations,\" jigsaw rearrangements, and patch permutations. Finally, color inversions are not permutations, but are orthogonal as they are a negation of pixel values. Video (Coming Soon!) Related Links This project is inspired by previous work in this area, including: Diffusion Illusions, by Ryan Burgert et al., which produces multi-view illusions, along with other visual effects, through score distillation sampling. This colab notebook by Matthew Tancik, which introduces a similar idea to ours. We improve upon it significantly in terms of quality of illusions, range of transformations, and theoretical analysis. Recent work by a pseudonymous artist, Ugleh, uses a Stable Diffusion model finetuned for generating QR codes to produce images whose global structure subtly matches a given template image. BibTeX @article{geng2023visualanagrams, title = {Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models}, author = {Geng, Daniel and Park, Inbum and Owens, Andrew}, journal = {arXiv:2311.17919}, year = {2023}, month = {Novemeber}, abbr = {Preprint}, url = {https://arxiv.org/abs/2311.17919}, } This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This means you are free to borrow the source code of this website, we just ask that you link back to this page in the footer. Please remember to remove the analytics code included in the header of the website which you do not want on your website.",
    "commentLink": "https://news.ycombinator.com/item?id=38477259",
    "commentBody": "Visual Anagrams: Generating optical illusions with diffusion modelsHacker NewspastloginVisual Anagrams: Generating optical illusions with diffusion models (dangeng.github.io) 632 points by beefman 15 hours ago| hidepastfavorite48 comments jamilton 14 hours agoI really like the man&#x2F;woman inversion.I wonder how many permutations could legibly be generated in a single image with an extended version of the same technique. I don&#x27;t understand the math, but would two orthogonal transformations in sequence still be an orthogonal transformation and thus work? reply kurthr 2 hours agoparentThe mosaics of a duck and a rabbit, however, was hilarious. reply hombre_fatal 10 hours agoparentprevThe man&#x2F;woman one stuck out to me as well. I probably watched it ten times. Probably because it seems so forlorn. reply xanderlewis 13 hours agoparentprevI‚Äôm not sure whether ‚Äòorthogonal transformations‚Äô in this context refers to the usual orthogonal linear transformations (&#x2F;matrices), but if so then yes. reply mkl 3 hours agorootparentThe article explicitly specifies orthogonal matrices. reply mg 14 hours agoprevI had a similar idea early last year and also dabbled with a checkerboard approach.Here a cat is made from 9 paintings of cats in the style of popular painters:https:&#x2F;&#x2F;twitter.com&#x2F;marekgibney&#x2F;status&#x2F;1521500594577584141You might have to squint your eyes to see it.I made a few of them and then somehow lost interest. reply rob74 2 hours agoparentThat looks more like a cat-aclysm to me TBH. Probably the model was overwhelmed by the conflicting requirements, so that neither the individual images nor the composite image are particularly good. But, as you wrote, maybe they will get better at this eventually... reply hammock 12 hours agoparentprevThat&#x27;s really cool. Can you do 3x3x3? As in, 9x9 with 81 1-cell cats, 9 9-cell cats and 1 81-cell cat? reply mg 3 hours agorootparentThat could be interesting. A recursive cat, so to say.The problem would be this: In the picture at hand, the big cat is rather simple. Just a portrait of a smiling cat. While the 9 smaller cats are doing all kinds of poses to adjust to the form of the big cat portrait. So the subcats are more complex than the main cat.When doing the recursive cat, it would be hard to make a subcat from 9 subsubcats because the subcat is already a complex image that is not as easy to recognise as the main cat. reply mdonahoe 10 hours agoprevThe man&#x2F;woman color inversion one was the most impressive to me. On the rotations, I can rotate in my mind and see the other view‚Ä¶ but I find it very hard to color invert mentally reply gitgud 7 hours agoparentThat is amazing, here&#x27;s the link for anyone interested (there&#x27;s a lot of images on that page)https:&#x2F;&#x2F;dangeng.github.io&#x2F;visual_anagrams&#x2F;static&#x2F;videos&#x2F;grid... reply minimaxir 15 hours agoprevNote that this technique and its results are unrelated to the infamous \"spiral\" ControlNet images a couple months back: https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;09&#x2F;dream...Per the code, the technique is based off of DeepFloyd-IF, which is not as easy to run as a Stable Diffusion variant. reply tpudlik 5 hours agoparentDid you mean to say it&#x27;s _related_? The original \"spiral\" image by Ugleh is explicitly credited in the \"Related Links\" section. reply minimaxir 4 hours agorootparentIt‚Äôs a similar topic which is why they credit it but the mechanism is much different. reply SamBam 14 hours agoparentprevI missed it, what was infamous about it? reply minimaxir 14 hours agorootparentIt created a backlash because a) it was too popular with AI people hyping \"THIS CHANGES EVERYTHING!\" and people were posting low-effort transformations to the point that it got saturated and b) non-AI people were \"tricked\" into thinking it was a clever trick with real art since ControlNet is not ubiquitous outside the AI-sphere, and they got mad. reply andybak 12 hours agorootparentI rather liked it and actually didn&#x27;t get to see as many examples as I wanted to.Is there a good repository anywhere or is it just \"wade through twitter\"? reply swyx 10 hours agorootparentnot a repository as such but i linked to some good examples in my sept recaphttps:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;sep-2023https:&#x2F;&#x2F;github.com&#x2F;swyxio&#x2F;ai-notes&#x2F;blob&#x2F;main&#x2F;Monthly%20Notes... reply yreg 4 hours agorootparentprevIt is real art. reply Der_Einzige 14 hours agoparentprevI always thought it was weird that this idea took off with that particular controlnet model. Many other controlnet models when combined with those same images produce excellent and striking results.The ecosystem around Stable Diffusion in general is so massive. reply minimaxir 14 hours agorootparentOther ControlNet adapters either preserve the high-level shape not enough or preserve it too well, IMO. Canny&#x2F;Depth ControlNet generations are less of an illusion. reply ShamelessC 14 hours agoparentprev> Per the code, the technique is based off of DeepFloyd-IF, which is not as easy to run as a Stable Diffusion variant.I haven&#x27;t dug in yet, but it _should_ be possible to use their ideas in other diffusion networks? It may be a non-trivial change to the code provided though. Happy to be corrected of course. reply minimaxir 14 hours agorootparentI suspect the trick only works because DeepFloyd-IF operates in pixel space while other diffusion models operate in the latent space.> Our method uses DeepFloyd IF, a pixel-based diffusion model. We do not use Stable Diffusion because latent diffusion models cause artifacts in illusions (see our paper for more details). reply guybedo 7 hours agoprevthe explosion in creativity brought by generative AI truly is incredible. reply Nition 11 hours agoprevThe duck&#x2F;rabbit that rearranges would be really cool to use on one of those sliding puzzles. Two valid solutions! reply kurthr 2 hours agoparentWith that many rearrangeable elements, you could make so many different \"valid\" solutions, indistinguishable without a photograph, that it would become art rather than a puzzle. reply hammock 12 hours agoprevDo real-life jigsaw puzzles like the ones shown here, exist for purchase? reply mkl 3 hours agoparentThis research uses DeepFloyd IF, which forbids commercial use. They&#x27;d need to find&#x2F;train another suitable image generator. reply shanedrgn 6 hours agoparentprevYou could always make it yourself! Not sure how well the method above would scale up though https:&#x2F;&#x2F;www.createjigsawpuzzles.com&#x2F; reply rob74 1 hour agoprevAs usual with AI-generated artwork: looks nice at first sight, but if you look closer, you can&#x27;t help but notice the flaws. E.g. the ambigrams: in the \"happy\"&#x2F;\"holiday\" one, the second word is actually missing the \"i\", and the two \"blessing\"s are really hard to read. Also, the \"campfire man\"&#x27;s face seems to be melting in a very disconcerting way... reply belugacat 1 hour agoparentI&#x27;m a photographer, and for years I&#x27;ve been pixel peeping at photos taken on phones with \"portrait mode\"; many years after the first introduction of the feature, regardless of the implementation, results still look crummy to my eye.Looking at fine elements like hairs (nevermind curly hair) is a disaster, especially when you&#x27;re used to fine classic german&#x2F;japanese optics that accurately reproduce every subtle detail of a subject while having extremely aesthetically pleasing sharpness falloff&#x2F;bokeh.I&#x27;ve had to swallow the pill though: No one (end users; pros are another story) cares about those details. People just want something that vaguely looks good in the immediate moment, and then it&#x27;s on to the next thing.I suspect it&#x27;ll remain the same for AI generated visuals; a sharp eye will always be able to tell, but it won&#x27;t really matter for consumption by the masses (where the money is). reply chrisweekly 10 hours agoprevI really enjoy these. Great post. reply IIAOPSW 9 hours agoprevI feel like a neural network is probably overkill for this task and a suboptimal substitute for a theoretical understanding of optical illusions, but can&#x27;t argue with results. reply dwighttk 9 hours agoprevEvery single one of the examples is like \"yeah... I mean, I guess... sorta\"the penguin&#x2F;giraffe is probably the best one. The old lady&#x2F;dress barely looks like either. reply moritzwarhier 12 hours agoprevThis is wonderful. reply cwkoss 10 hours agoprevWould be cool to make some of these that look like different things under red&#x2F;blue light. reply kevinwang 7 hours agoprevWow, these examples are amazing reply cloudyporpoise 11 hours agoprevThis may be one of the cooler things i&#x27;ve ever seen reply adkaplan 10 hours agoparentsome of these style illusions I&#x27;ve seen drawn by hand before, but the lithopane ones are new to me. I&#x27;m sure the 3d printing lithopane community will love them reply DonHopkins 10 hours agoprevFrom the HN \"Boustrophedon\" discussion:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15539373https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Boustrophedonhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15547162DonHopkins on Oct 25, 2017prevnext [‚Äì]Scott Kim has a wonderful talent at designing \"ambigrams\". Check out his classic book \"Inversions\" and his gallery of more recent work!http:&#x2F;&#x2F;www.scottkim.com.previewc40.carrierzone.com&#x2F;inversion...An inversion is a word or name written so it reads in more than one way. For instance, the word Inversions above is my name upside down. Douglas Hofstadter coined ambigram as the generic word for inversions. I drew my first inversion in 1975 in an art class, wrote a book called Inversions in 1981, and am now doing animated inversions.A Scott Kim Ambigram for \"George Hart\":https:&#x2F;&#x2F;www.georgehart.com&#x2F;scott-kim.htmlJohn Maeda&#x27;s Blog: Scott Kim‚Äôs Ambigramshttps:&#x2F;&#x2F;maeda.pm&#x2F;2017&#x2F;12&#x2F;17&#x2F;scott-kims-ambigrams&#x2F;The Inversions of Scott Kim:https:&#x2F;&#x2F;www.anopticalillusion.com&#x2F;2012&#x2F;04&#x2F;the-inversions-of-...Channel: An Optical Illusion ¬ª scott kim:https:&#x2F;&#x2F;optical397.rssing.com&#x2F;chan-26600952&#x2F;index-latest.phpScott Kim‚Äôs symmetrical alphabet:https:&#x2F;&#x2F;stancarey.wordpress.com&#x2F;2012&#x2F;10&#x2F;18&#x2F;scott-kims-symmet...Typography Two Ways: Calligraphy With a Twisthttps:&#x2F;&#x2F;www.wired.com&#x2F;2009&#x2F;05&#x2F;pl-arts-6&#x2F; reply willsmith72 12 hours agoprev> This colab notebook requires a high RAM and V100 GPU runtime, available through Colab Pro.That&#x27;s sad, I would&#x27;ve loved to try it. reply nomel 11 hours agoparentI completely disagree. It&#x27;s fantastic that we can get access to this hardware for so cheap. A used V100 is $1300. You could pay for Colab Pro for 10 years with that, which will get you faster and faster hardware through the years. Where I am, a month is the cost of two bags of chips. reply andybak 12 hours agoparentprevwell - chuck $10 at it and spend the rest of your month trying other things.(Back in Disco Diffusion days I was happy to spend money on Colab Pro. It was fun) reply DonHopkins 10 hours agoparentprevHave you never put quarters into a PacMan machine? reply willsmith72 9 hours agorootparenti take cars for test drives before buying them reply DonHopkins 5 hours agorootparentDo you hang out a GameStop all day and test drive cars in GTA instead of renting a game about stealing them?Aren&#x27;t you sad they don&#x27;t just let you shoplift it for free? reply aunwick 7 hours agoprev [‚Äì] So, Im grad school I had access to an sgi onyx and basically did this but didn&#x27;t toot my horn about it because. 1. I didn&#x27;t think it was particularly amazing 2. We didn&#x27;t have social platforms yet.Congratulations! reply mkl 3 hours agoparent [‚Äì] An SGI Onyx has a tiny fraction of the computing power needed to run text-to-image generative models like this. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The authors propose a method for generating multi-view optical illusions using pretrained diffusion models.",
      "The method supports various transformations such as rotations, flips, color inversions, skews, jigsaw rearrangements, and random permutations.",
      "The method involves aligning and averaging noise estimates from different views or transformations of an image to take a diffusion step.",
      "The authors discuss the conditions that views must meet for the method to work, including linearity and statistical consistency.",
      "Use of orthogonal transformations, particularly permutation matrices, can generate interpretable and visually meaningful optical illusions.",
      "The paper provides examples and references related works in the field."
    ],
    "commentSummary": [
      "The article explores the application of diffusion models and generative AI in producing visual anagrams, optical illusions, and image illusions.",
      "There is a debate surrounding the use of AI-generated artwork and the potential of generative AI in creating puzzles with multiple solutions.",
      "The comments touch on various topics, including admiration for the examples provided, interest in different types of illusions, and a discussion about the cost of accessing high RAM and GPU runtime. The conversation also briefly mentions playing video games and test driving cars."
    ],
    "points": 632,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1701369587
  },
  {
    "id": 38471744,
    "title": "Amazon HR Insider Exposes Disturbing Reality of PIP Plans, Claims Company Focused on Eliminating Underperforming Employees",
    "originLink": "https://www.businessinsider.com/amazon-hr-performance-improvement-plans-pip-pivot-had-to-quit-2023-11",
    "originBody": "An Amazon human-resources staffer (not pictured) worked on the company's performance-review system and then got put on a PIP. David Ryder/Getty Images Redeem now A former HR staffer at Amazon put employees on a performance-improvement plan known as Pivot. Then, the HR staffer, who says they developed PTSD from the work, was put on their own PIP. An Amazon spokesperson said the account contained inaccuracies about the company's process. Advertisement This as-told-to essay is based on a conversation with a former Amazon human-resources worker who was put into the company's performance-management program known as Pivot. This person spoke on condition of anonymity to avoid jeopardizing their career. Business Insider has verified their identity and employment at the company. The conversation has been edited for length and clarity. I worked at Amazon in HR for several years. Not only did I administer Pivots, but it was eventually brought to my attention that I was going to be going through one. They made a mistake by doing that with me. There wasn't a lot of information to justify a poor performance. The Pivot goal was a straight, across-the-board 6% number. And as an HR person, that is a hefty figure. Advertisement And it was driven hard by the HR VPs to show the metrics ‚Äî daily, weekly ‚Äî to make sure we knew who was in the pipeline. Not to improve, but who was in the pipeline to get out. There wasn't a lot of interest in improving people. You might be cutting some prime choice with the fat. And they were OK with that. They wanted that number. The managers who had to implement it and tell their people they were on Pivot ‚Äî I would say a majority of them hated it. Because, one, they didn't have the skills to be able to manage performance that soon out of the gate. A lot of our managers were brand new. The first thing you had to do was work with a Pivot consultant. So, that was somebody in HR besides the manager's business partner. And you'd talk about if it was the right time or if it was the wrong time to Pivot someone. I would say 80% of my time ended up being focused one way or another on Pivot. Either the Pivot appeal or the Pivot work that workers' managers had to do. And look, I'm not going to say you're going to ever find this somewhere, locked down in words. But the idea is, if you're putting somebody in Pivot, you make that so damn hard that they don't get out. Advertisement Almost always, unless there was some really unique set of situations where it came out during the appeal, the success rate of that was virtually none. When I wasn't working on Pivots, working in HR was great. We were supposed to be doing coaching and focusing on strengths and moving people through the organization in a positive way. Later, when Pivot came back, we had to stack rank all of our employees. The way we broke it down, we called it top tier, which was, you know, maybe 15-20% by the time it worked out. And then you had the middle. And then you have the bottom tier. The bottom tier was about 20-25%, maybe even up to 30%. The guidelines that they expressed publicly may be different because we always worked to make sure we had more than that because some went bad ‚Äî or went off the rails, and we couldn't exit them for whatever reason. We were way over how many people were actually underperforming or detrimental to the business. Maybe around 1% or 1.5% to 2% were actually not performing well. Advertisement I have PTSD I was disgusted at what I was seeing with the Pivot process. This process alone has given me post-traumatic stress disorder. It impacted me so much as a person that I had to get out of there. When it was justified, it was easier to push someone out. If it's deserved, there's no problem. But when it wasn't deserved, you had people crying and begging, and they couldn't understand. You had visa-sponsored employees who, once we Pivoted them and moved them out, no longer were authorized to work in the United States. So, they had to make immediate plans to get out of the country. And it's a long process to get sponsored by another group. In the years I was there, I never, ever, ever had any performance issue given to me ‚Äî not even anything close to being serious. I had no worries because I asked for feedback all the time. I'm like, \"What can I do? How can I do better?\" I didn't ever want to be blindsided by Pivot myself. And what a lot of people did ‚Äî if they got the indication that they were going down that track ‚Äî was they would transfer jobs right away. Some people were successful. A lot of people weren't. Advertisement Normally, with a performance-improvement program, as an HR person, you're following progressive discipline. Are you seeing notes that this person is having trouble? Are you seeing coaching conversations that are taking place? So for it to actually just ‚Äî boom ‚Äî be there is really problematic. It was my turn During my performance evaluation, when it was clear I was on a PIP, my manager shared criticisms that I'd never heard before. I said, \"I've never had any of these comments come to me ever.\" Essentially, it was a lot of made-up stuff. I mean, you could put some truth to it. I'd been late on a few assignments. But everybody's got some element of things they can improve on in their work. My manager just chose to bring those out. Amazon broke down people into three categories. You were either top-tier, middle of the pack, or at least effective. Normally, they won't tell you what they rated you, and I'm like, \"Come on. I know this stuff just as much as you do. I know the wording. You didn't put me in the medium category. Would you just admit that you put me in the least-effective category?\" And I got my manager to admit that. Advertisement I wasn't put on Pivot. My manager wanted to work with me a little bit to see if I was going to commit to the job. So they sat me down and said I could go on Pivot and leave right away, or they would work with me. Well, obviously not having any job opportunities, I said, \"Look, I'm in it. Let's try to get better and go from there.\" So my manager took away all my direct reports, and shoved me into a small box, and said you could do this and try to work yourself out of it. Right after that, I started putting in the full push to get another job. And so I started interviewing. I actually had a headhunter that reached out to me. Originally, I told her no, but then some of this stuff happened. And I'm like, \"OK, let's revisit it.\" I got to the point where they offered me a job, and I was going to quit. But I had a huge stock investment coming up. So there was no way I was going to rock the boat in any way, shape, or form just trying to get to this date. If you walked away during the Pivot or anytime before you had your investment before it was there for you, you would lose it all. And I'm not talking a little bit of money. I'm talking: I had a couple hundred thousand dollars coming to me. I played along, and I'm good at playing along when I have to be. So then the money is in my account. That next day, I called my manager and I told them I was resigning. They blew a gasket ‚Äî absolutely blew a gasket because I had told them that I was in it for the long run. I said, \"Look, you gave me no choice. You put this threat against me. I'm not just gonna sit there and wait for it to be dependent on you. You get to make the call whether I make it or not.\" My manager was super mad and asked me when I was leaving. I said two weeks. They were incredulous that I wasn't giving them more respect. Advertisement The biggest thing ‚Äî and I'm gonna say this goes for many, many people that were put on Pivot ‚Äî is there were no warning signs. There was no trail of communication saying, \"You are underperforming.\" I mean, even if it's something as simple as, \"Hey, can you do better on this next time?\" I know, certainly, I got zero negative feedback. I got the feedback that I was rocking it. And then all of a sudden, to be in this place, it's like, \"Huh.\" I still wonder about what happened to all the people that went through that process. How did it impact their life? I think it leads to a lot of mental-health issues. Margaret Callahan, an Amazon spokesperson, told BI via email: \"Like most companies, we have a performance management process that helps our managers identify who on their teams are performing well and who may need more support. For the small number of employees who are underperforming, we use performance management programs to help them improve, and many employees do just that. Sometimes the programs result in employees leaving the company. Business Insider declined to share the information needed to verify this individual's account, but it contains a number of inaccuracies about our performance management process. An unverified, anonymous anecdote in a Business Insider 'As told to' essay does not represent the experience of the vast majority of our employees.\" Do you have something to share about what you're seeing in your workplace? Insider would like to hear from you. Email our workplace team from a nonwork device at thegrind@businessinsider.com with your story or to ask for one of our reporter's Signal numbers. Or check out Business Insider's source guide for tips on sharing information securely. Sign up for notifications from Insider! Stay up to date with what you want to know. Subscribe to push notifications Read next Watch: Marketing leaders from TikTok, Roku, Mastercard and more tell Insider how consumer behavior has changed across industries Careers Amazon Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=38471744",
    "commentBody": "I worked in Amazon HR and was disgusted at what I was seeing with PIP plansHacker NewspastloginI worked in Amazon HR and was disgusted at what I was seeing with PIP plans (businessinsider.com) 552 points by cebert 23 hours ago| hidepastfavorite567 comments steveBK123 20 hours agoOne underlying problem with these PIP type programs at FAANG seems to be that they have very high barriers to entry in the interview process, and then act like 30% of the company is underperforming and subject to an annual 6% cull.There are industries & companies that have grown fat & lazy and could use a few annual 6% culls, but you eventually run out of fat. If you have a very competitive interview process and high compensation to attract the best talent, it is unlikely you have so many underperformers lying about to cull annually.So really it&#x27;s overhiring BS that is then getting taken out on employees. Given that, I think as has been pointed out by another commenter - the old Wall St model of doing one cut in one afternoon, calling people into an office and giving a severance is far more humane. Everyone understood it was about the numbers not about your performance, generally. Seems better than year round psychological torture of being at risk of a PIP, and then if being put on one knowing the most likely outcome is being fired. So you feel dragged through the mud and then having doubly failed (put on PIP & failed the PIP).I knew a guy who moved from Wall St to Amazon and described the performance management &#x2F; compensation system to be pretty rough and had explicitly described the compensation cliff and how a lot of people in the good years were proactively leaving, cooling off, and then coming back to reset the compensation instead of going over the cliff. reply giantg2 19 hours agoparentOne thing about fat that I think is overlooked is how fat today might be muscle tomorrow, and how little companies try to convert it.Maybe a person is in a bad role, on a bad team, or simply doesn&#x27;t have experience or skills fitting the specific requirements of that single point in time. I&#x27;ve been a high performer in the past. Now I&#x27;m a low performer (I&#x27;m slow) after the company has changed the way it treats teams&#x2F;work and the technology shifted. My disability isn&#x27;t suited to the new working style&#x2F;process (or lack of). However, I do excellent work outside of my regular day to day responsibilities, like my secondary role as an ASC. My manager even said that I&#x27;d I could just speed up my regular work I would easily get the highest rating and a promotion. Hearing him say that really sucks because I unable to just speed up with my disability in the given work environment. The company&#x2F;managers have made no effort in helping me find a role that works better with my skills or is impacted less by my disability. It seems there may not be any of the traditional roles&#x2F;teams left. So now I fill midlevel roles and move team to team trying to find somewhere that I fit in. If I can find a place I fit in like before, then I would easily excel.Another example was a dev on another team that I worked with a few years ago was sloppy, slow, and just all around seemed like a poor performer - and even our tech lead and manager talked poorly about him. He was even considerably slower than me. About 3 years after that he&#x27;s now the head of data and analytics for our international operations. reply steveBK123 19 hours agorootparentAgreed that what is left unsaid in a lot of performance conversations is the person-role match. Someone isn&#x27;t a high&#x2F;low performer globally, just at that time in that role.Wall St firms I have worked at typically try to find different roles for people before marking them as at risk of a layoff. Not sure FAANG behaves the same, but from all these PIP discussions it seems not. reply closeparen 17 hours agorootparentLeadership was asked why not find new roles for people getting laid off due to project cancellations at a time when we were still hiring like crazy in the immediate run-up and aftermath. The response was ‚Äúwe need to move fast.‚ÄùThey encouraged those laid off to apply for open roles. But of course it read to employees as a monumental ‚Äúfuck you.‚Äù reply Arelius 17 hours agorootparentWell, but to mention that once, at least at amazon, you enter the pip processs. You are no longer able to move teams. And if you get removed. Not able to apply to return. reply rescbr 13 hours agorootparent> And if you get removed. Not able to apply to return.Having been past the PIP&#x2F;Pivot process, certainly I don&#x27;t want to return. reply scarface_74 9 hours agorootparentprevWhy the hell would I ever want to return? That‚Äôs like going back to an ex thinking it ‚Äúwill be different this time‚Äù reply j45 15 hours agorootparentprevIt&#x27;s surprising companies don&#x27;t seem interested in understanding their employees well enough to know that the cost of re-hiring and re-training vs trying to find a better fit in the org is not a good idea.Some industries coach the employee into a different role, or help them exit more openly. reply giantg2 13 hours agorootparentI&#x27;ve even seen articles about how Amazon chews through so many people that they&#x27;re having trouble finding \"virgin\" hires. Everyone in their pool is already fed up with them or were PIP&#x27;d. reply pixl97 9 hours agorootparent\"We need more HB1&#x27;s.... we already burned the local population\" reply aitchnyu 4 hours agorootparentprevIIRC isnt this about warehouse workers? reply j45 6 hours agorootparentprevInteresting.Explains the cyclical visits from Amazon recruiters on LinkedIn. replyjacobyoder 18 hours agorootparentprevI left a company years ago because I didn&#x27;t feel I was a good fit for the team&#x2F;project - skills&#x2F;goals&#x2F;etc. But wasn&#x27;t allowed to leave because it would hurt the image of the team to be seen to have people leaving it for other teams. But... apparently, leaving the company altogether doesn&#x27;t affect the image (and yes... I&#x27;ve left the team, and people internally knew why, so... how does it really save face?) reply Terr_ 17 hours agorootparentPerhaps if the employee leaves the company entirely, all parts of the company get a chance to pretend the deficiency was in the employee. [0]Like some weird inverse of the \"You can&#x27;t fire me, I quit!\" trope.[0] P.S.: Even if the employee left for a more prestigious role in another company, the social-circles in the old company aren&#x27;t forced to acknowledge that the same way as if it were an internal move: HR will just say \"they left to pursue other opportunities\", and that permits the assumption that that the worker--their incompetence discovered--has fallen into ignominious chronic unemployment. reply SpaceNoodled 16 hours agorootparent\"You can&#x27;t quit, I fire you!\" reply giantg2 16 hours agorootparentGood, gets the unemployment and makes any legal case a little easier. reply scarface_74 9 hours agorootparentYes that whopping $387&#x2F;week on average. That‚Äôll show themhttps:&#x2F;&#x2F;www.forbes.com&#x2F;advisor&#x2F;personal-finance&#x2F;best-and-wor... reply giantg2 8 hours agorootparentBetter than 0 reply scarface_74 7 hours agorootparentNot much better. In my state it is $275 max a week replyaway271828 18 hours agorootparentprevI&#x27;ve absoluttely been in a position where the ground moved under me. I went from doing (based on evaluations, RSUs, etc.) from doing a very good job to bouncing around a bunch of managers and groups in roles that I wasn&#x27;t really suited for and didn&#x27;t have much real interest in. The only saving grace was that I was pretty close to semi-retiring anyway. reply giantg2 18 hours agorootparentI&#x27;ve been bouncing for about 5 years. Have about another 20 to go. Not sure how I&#x27;m going to handle that. reply rad_gruchalski 13 hours agorootparentConsider a different job, and pick up some responsibilities. reply giantg2 13 hours agorootparentWhat are you talking about?I can&#x27;t change jobs now due to home life responsibilities and having specialized in dead tech.I have picked up responsibilities. In the past I&#x27;ve filled the role of a senior dev for a year and a tech lead for another. I volunteered to be an ASC. I volunteered for all sorts of other responsibilities.Perhaps the world doesn&#x27;t work in the way that you think. It&#x27;s not possible for everyone to successfully job hop, especially with a disability and home responsibilities. Just picking up responsibilities doesn&#x27;t mean you&#x27;ll get rewarded. reply rad_gruchalski 11 hours agorootparent> Just picking up responsibilities doesn&#x27;t mean you&#x27;ll get rewarded.I&#x27;m sorry, I did not mean to attack, or otherwise offend you. I was attempting to say that one can look for an opportunity to work in an environment where one can feel appreciated. I&#x27;d employ a person with such experience as long as they&#x27;d know some basics. What&#x27;s TLS, how does it work, roughly. What&#x27;s DNS. TCP vs UDP. At least understanding of what problems are solved (or created, if you are in that camp) by Kubernetes. Docker basics. Read open source code.Those are interesting subjects. I&#x27;m sure there is a ton of hypothetical past problems one could attempt to modernize and have fun while doing so.It&#x27;s just a free opinion of someone on the internet! reply scarface_74 9 hours agorootparentprev> I can&#x27;t change jobs now due ‚Ä¶having specialized in dead tech.Then don‚Äôt do that?I‚Äôve been working professionally for 27 years and have been aggressive about keeping pace with technology. I saw what happened first hand when I didn‚Äôt between around 2002-2008 and I said it would never happen again.For reference: I didn‚Äôt know ‚Äúcloud‚Äù from a hole in the wall at 44 years old and I got my first and hopefully last job in BigTech at 46 working at AWS in the ProServe department (where I got PIPed see my top level comment)It is completely irresponsible for anyone in tech especially with a family to not always keep themselves employable and depend on the whims of thier current employer to support their families addiction to food, clothing and shelter.And how does a disability preclude you from getting a remote job?And if you ‚Äúcan‚Äôt job hop‚Äù, would you not look for another job if you got laid off or fired or would you just give up and be on the street - homeless and hungry?> Just picking up responsibilities doesn&#x27;t mean you&#x27;ll get rewarded.I‚Äôm always rewarded for picking up additional responsibilities - if not by the current company by the next. It‚Äôs what allows me to aggressively job hop with a family and not be stuck with dead technologies on my resume. reply giantg2 8 hours agorootparentI have responsibilities and problems at home that greatly limit my non-work hour freedom. My disability is not a physical one, but a neurodivergent one, so remote would only hurt me more. I would look for another job if I lost this one, but it would probably pay less - again, no real time to upskill. Sure, I&#x27;ve gotten new certs and other BS over the years and started working with new tech. That hasn&#x27;t been helpful. I&#x27;ve worked roles above my official position and not been rewarded. In fact, I haven&#x27;t seen any of my hard work or extra responsibilities pay off after my first promotion about 3 years in. I don&#x27;t expect another promotion for the rest of my career either. That&#x27;s just the way it is for me.Not everyone&#x27;s life is as easy as you make your&#x27;s out to be. reply scarface_74 7 hours agorootparentAgain if you lost your job you would do what it took to get another one wouldn‚Äôt you? So what‚Äôs the difference between changing jobs when you do have a choice and waiting until you don‚Äôt have a choice?If you don‚Äôt make time now to upskill, what do you think is going to happen if&#x2F;when you don‚Äôt have a choice but to look for a job and be stuck with in your words ‚Äúdead tech‚Äù. replykjkjadksj 17 hours agorootparentprevThis is just how companies operate today. Burn furniture to heat the place for a quarter. Sell your property to a for profit entity you lease from instead to save a line item today and no doubt get screwed when its time to renegotiate the lease (long after whoever made this decision has left the company I‚Äôm sure). Fire staff and rehire instead of retrain. Its all myopic thinking that you‚Äôd find in any MBA program sold to you as the best practices in modern business. reply j45 15 hours agorootparentModernizing MBA&#x27;s will have to happen at some point. reply anvil-on-my-toe 16 hours agorootparentprevI think managers in tech companies are very often just really untalented as leaders. Good leaders create more leaders. Growing talent and getting the best out of people should be the goal for any leader, and I&#x27;ve seen so little of that from the ones I worked with.After a point it just becomes part of the landscape, a pervasive culture of not recognizing what good leadership looks like, and people filtering out of the industry because they&#x27;re over it. reply giantg2 16 hours agorootparentI&#x27;m actually in a financial company, but I assume it&#x27;s all pretty similar at this point. reply no_wizard 19 hours agorootparentprevYou should file for disability accommodations. That is completely legal to ask. Sometimes you need to back it up with documentation, but i suspect that won&#x27;t be a problem.They can&#x27;t discriminate against you for this. reply giantg2 19 hours agorootparentI have documentation. They don&#x27;t care. HR said to just work with my manager on accommodations. My main problem is dealing with ambiguity and context switching. That&#x27;s something they can&#x27;t change given their chosen work model.The old model of working was that you supported 1 main app and maybe a smaller 1 or 2 that were related. These were usually in 1 stack and well documented. The new model is that teams are typical supporting 6-10 apps in multiple stacks. I&#x27;m slow when switching between apps and stacks, and without consistent exposure to a single app&#x2F;stack, I won&#x27;t get faster.It&#x27;s funny though - they talk a big game and even hire in Auticon contractors, but then refuse to do anything for existing employees. I assume a least some of those contractors have similar difficulties as me. reply itsboring 18 hours agorootparentI don‚Äôt have any (diagnosed) neurological disability but I would do poorly in that situation too. Excessive context switching is a major source of stress: adrenaline, fast heartbeat, high blood pressure. Makes it almost impossible to stay focused and motivated. reply scarface_74 18 hours agorootparentprevThe way that Amazon‚Äôs PIP works, you are highly incentivized not to fight it.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37963423The risk&#x2F;reward isn‚Äôt worth it unless you‚Äôre close to a vesting period. reply babyshake 18 hours agorootparentprevThey can discriminate against you. They just need to make sure it&#x27;s not obviously discrimination. reply giantg2 18 hours agorootparentYeah, that was a risk I was willing to take by disclosing my disability. So far it doesn&#x27;t seem like it made anything worse, but it hasn&#x27;t made anything better either. reply theGnuMe 15 hours agorootparentI&#x27;d speak to an employment lawyer and see what they advise. A couple hundred bucks for the piece of mind might be worth it. You&#x27;d get an opinion on what they can and can&#x27;t do and what you can do to proactively manage the situation. reply scarface_74 9 hours agorootparentSo exactly what do you think he‚Äôs going to accomplish spending years working with an employment lawyer? A reinstatement of his job at a toxic company? Some monetary compensation that probably won‚Äôt be worth the headache?That energy can be better spent looking for a new job. My granddad use to tell me to never chase after old money or old girlfriends. reply giantg2 13 hours agorootparentprevThanks but I don&#x27;t think I will. The thing about disabilities is that they have to provide reasonable accommodations. What is reasonable is wildly subjective. There&#x27;s really not much to be done. reply pharrington 17 hours agorootparentprevUnfortunately, there&#x27;s a HUMONGOUS difference between \"not allowed to\" and \"can&#x27;t\" when a company optimizes for profit at the expense of human well-being. reply pavel_lishin 19 hours agorootparentprevWhat&#x27;s an ASC? reply giantg2 18 hours agorootparentApplication security champion. Basically the Sec part of DevSecOps for the team. It sounds a lot cooler than it is. It&#x27;s mostly paperwork. reply didntknowya 9 hours agorootparentprevcould&#x27;ve picked a better analogy that don&#x27;t reinforce the fitness myth that fat \"turns into\" muscle.fat is fat. muscle is muscle. one does not convert into another. it can be broken down into energy to be used but ultimately each cell starts from scratch. reply pyrale 18 hours agoparentprevYou&#x27;re mistaken in that you believe that the goal of culls is to cut underperformers. Stack ranking doesn&#x27;t assess performance, it checks whether you&#x27;re willing to do a little more to be ahead of your peers.The real goal of culls is to motivate people to work more, to be more accessible to their managers&#x27; requests, etc. so they make it \"to the next round\". It&#x27;s a pressure tool, not a performance control tool. reply JohnFen 18 hours agorootparentThis sort of thing is one of the big reasons why I would never work for a FAANG company. I prefer to work at a place that encourages good work rather than a weird cutthroat arena combat thing. reply giantg2 17 hours agorootparentI&#x27;d still work there. For those comps I&#x27;d do a lot of things. My current job is already torture and doesn&#x27;t pay nearly as well. But I guess that really a fictional problem for me since I&#x27;d never pass the interview. reply scarface_74 9 hours agorootparentAs someone who has been there done that - read my other replies - I can say both that I don‚Äôt regret doing it and that I would not sacrifice my life now to ever go back to any large tech company or for the money.I very much made two decisions this year after being Amazoned that prioritized my stress level over money.1. Deciding not to put my shingle up and go independent. I have the network, the credentials and the knowledge to make more than I was making at Amazon going that route2. Turning down a position that was going to be created for me by a former coworjer now director to be responsible for the cloud migration and infrastructure for a $7 billion public non tech company that would have also paid more in cash than my total comp at Amazon.No, I never struck it rich in tech, I was a journeymen CRUD developer&#x2F;architect when I got hired with two years of AWS experience and above average soft skills. reply IKantRead 18 hours agorootparentprevI feel the same way and would also never work at a FAANG.That said, I suspect this is would be viewed as a benefit of the process since I&#x27;m fairly confident Amazon and the like don&#x27;t want people who feel this way working for them regardless of the talent of those individual people. reply moate 18 hours agorootparentExactly this. ‚ÄúAre you willing to kill another human? No? Sorry, there‚Äôs no place for you in the infantry‚Äù reply Terr_ 16 hours agorootparentI feel a better analogy would involve (A) a demand that is outside the official job-description or duties and (B) isn&#x27;t a fixed-bar but instead makes multiple employees fight each other in an indefinite race to the bottom.Ex: \"Are you willing to spend your free time becoming the General&#x27;s #1 personal servant? No? Sorry, there&#x27;s no place for you in the infantry.\" reply geodel 10 hours agorootparentI don&#x27;t find better but more vague perhaps in attempt to sound nuanced.If one goes by job description Amazon is acting strictly within those parameters. And that race to the bottom is not really hunger games.On the other hand if one goes by unsaid rules of games, everyone including management, HR, employee know FAANG salary is multiple times of equivalent IT workers. And perhaps 95% of FAANG employees are not really developing new algorithms, hauling terabytes of data on their shoulders or being the force of good whatever that means. Just keep doing made-up work and wait until reach x million dollars goal. After that leave quietly or loudly by writing a nasty blog post lambasting management.So mostly people understand they have to keep up the facade because talking about what&#x27;s really happening will make it rather awkward for everyone. reply ClumsyPilot 12 hours agorootparentprevThe question should be the other way round, is there a place for them in civilised society.Maybe their executives should stick to peeing in bottles. reply mattgreenrocks 16 hours agorootparentprevI did a DiSC (corporate Myers-Briggs-esque personality test) assessment and dominance was listed as one of the core motivators for people. I&#x27;m not sure I actually believe that; it feels like that was retconned into truth so it&#x27;d get past the decision-makers who may actually have that tendency. But maybe it is!However, on the bright side, it is kind of nice that FAANG attracts the type of people who actually think this sort of constant stack ranking arena fighting is okay. Think about it: if they really hated it, they&#x27;d leave. Obviously the pay is a huge motivator, but if it was as distasteful as it was to many others, they wouldn&#x27;t hang around. reply JohnFen 16 hours agorootparentI do appreciate that FAANG companies are a bit like fly traps, collecting those sorts of people somewhere else so I don&#x27;t have to work with them personally.But understand, I am not asserting that FAANG companies are places nobody wants to work. I&#x27;m asserting that they are places I am not willing to work at. It&#x27;s because I would be utterly miserable in that sort of working environment, and there is no amount of compensation that could make that tolerable to me. reply wideroots 13 hours agorootparentprevThis is happening not only in FAANG companies but also a lot (if not, all) of companies. Unless you are in a hyper-growth company, you are more likely to see this kind of performance management process to continuously review and cut. reply coin 17 hours agorootparentprevNot all FAANG are like this. Google and Apple are known for being easy going. This of course depends on the team. reply JohnFen 16 hours agorootparent> Google and Apple are known for being easy going.The engineers I know who work at those places wouldn&#x27;t agree. But how true that is depends a lot on what sorts of things tend to bother you. reply englishspot 17 hours agorootparentprevI&#x27;m surprised it hasn&#x27;t spilled over to the rest of the industry, unlike their hiring practices. reply callalex 16 hours agorootparentIt has. I went through this exact same PIP process with subjective improvement goals at Twilio. It was extremely stressful and I met all the goals but got fired anyway. In hindsight it was just a layoff. reply JohnFen 16 hours agorootparentI&#x27;ve never been through a PIP, but based on what I&#x27;ve seen with people who have, if I were put on a PIP that would be a strong signal that I&#x27;m not wanted there, so I&#x27;d just quit and save everyone the aggravation. reply verteu 18 hours agorootparentprevAdd the fact that \"fired\" means \"deported\" for H1B workers, and it&#x27;s the perfect way to extract maximum labor from a sizable proportion of your workforce. reply tomcar288 17 hours agorootparentprevI think you&#x27;re right on the money. and it&#x27;s completely Toxic. i would never work at a company like that.if FAANGs can get away with treating their employees like dog s*t then there must be vast over supply of talent. reply ThrowawayR2 17 hours agorootparentprevMeh. Going to a FAANG is like joining an Olympic sports team; it&#x27;s not a place you&#x27;d want to be if you aren&#x27;t hell bent on going for the gold. The irony is that the pressure cooker works; an unhealthy degree of ambition, competitiveness, and willingness to work their tails off are all assets in the business world. I know a, at least to me, surprising number of people who successfully navigated these environments who later became founders&#x2F;CEOs and high level executives. reply electrondood 17 hours agorootparent> Going to a FAANG is like joining an Olympic sports team; it&#x27;s not a place you&#x27;d want to be if you aren&#x27;t hell bent on going for the gold.Not according to everyone I know working at a FAANG company. This is mythology. You&#x27;re at a FAANG because you did the LeetCode grind and passed the interview. reply steveBK123 15 hours agorootparentprevTts almost the opposite. Olympics has a bunch of objective measures of performance. For individual sports its winning, for team sports theres various scoring&#x2F;assist&#x2F;defense&#x2F;etc metrics that can be used.. plus if your team wins.In tech, good luck. It&#x27;s all vibes. reply pyrale 15 hours agorootparentprev> Going to a FAANG is like joining an Olympic sports teamI guess, if the discipline is corruption. My company doesn&#x27;t feel the need to tell me how to avoid discussing sensible topics with my teammates on group chat.Yeah, because technical merit isn&#x27;t the only way you get ranked in the stack. For instance, would you say no to an ethically questionable order if you&#x27;re on a PIP? reply Cheezewheel 15 hours agorootparentprevIt is naive to even imply that being a hard worker is going to insulate you from getting culled or laid off. reply mattgreenrocks 16 hours agorootparentprevThis feels like the tech version of the temporarily embarrassed millionaire meme: it&#x27;s something people need to be true so they can justify their ambitions of making it. So they repeat it, and say it enough to believe it.FAANG employees are no different from you and I in the aggregate. That&#x27;s not a dig, that&#x27;s just statistics. reply ClumsyPilot 12 hours agorootparentprev> It&#x27;s a pressure tool, not a performance control tool.This, exactly. The world is full of lies, lies people honestly believe.For example, have you seen a situation when parents feed a child, and try to get the kid to eat a bit more even when the child is full? What do they tell the kid - eat some more, it&#x27;s good for you. Why do they really do it?Because kids get hungry more often than adults, it&#x27;s annoying &#x2F; sometimes not possible &#x2F; practical to stop to feed them as often. So we lie to them. reply commandlinefan 18 hours agoparentprevMy ancestors were coal miners in Tennessee. They tried to reason with their employers for a long time to establish reasonable working conditions. When it became clear that their employers were not going to listen to reason, they found other ways to establish reasonable working conditions. reply pierat 16 hours agorootparentWell, yes. And lets expand that.The coal miners&#x27; solution was to get a bunch of guns and&#x2F;or gasoline, and either pump bullets in the bosses&#x27; house, or to torch it with gasoline. Usually both.That was responded to by bringing in the Pinkertons (private military), along with the US military. These got horrifically bloody.And it got to a point, that by the 1920&#x27;s, the USA had a strong representation of US Communist Party. 1927 really cemented that they were right in the long run. However, FDR sold the Communists out with \"labor reforms\" that paled in comparison to what the Communists wanted.Now, FDR&#x27;s reforms lasted until the late 60&#x27;s and 70&#x27;s. Capital was able to get enough property and money, that they ended up *buying* the governmental reforms one politician at a time.Well, perhaps heads will have to roll again, to reset Capital. They&#x27;re not going to willingly cede property&#x2F;money&#x2F;power. And this time, they bought government. And it&#x27;s obvious that the NLRB and the DoL is going to do diddly. But whatever it is, I hope it&#x27;s peaceful. But knowing labor history through the ages and US specifically, it will be anything but. reply landemva 15 hours agorootparentThere was serious labor abuse in past years, and capital doesn&#x27;t have as much power in the new info-WFH-open source economy. Mineral rights and office space and expensive yearly software support contracts are gone. Stop waiting on government regulators for action on white collar issues. reply ClumsyPilot 12 hours agorootparent> capital doesn&#x27;t have as much power in the new info-WFH-open source economySeriously? 30% of the population believes whateber chatgt-bot tells them is true on twitter> Stop waiting on government regulators for actionI think the key is that there is likely to be violence. Look at how clinate protesters are treated. reply landemva 9 hours agorootparent> Look at how clinate protesters are treated.How does that relate to capital and jobs? Unlike physical mining and physical manufacturing, capital is not a gating feature of bits and bytes tech startups.We saw violent destruction during covid lockdowns by BLM. What did that accomplish? reply ClumsyPilot 1 hour agorootparent> gating feature of bits and bytes tech startups.Does &#x27;tech&#x27; just mean software to you? like what about 95% of the economy, you need capital even for ebikes> We saw violent destruction during covid lockdowns by BLM. What did that accomplish?I am not spokesperson for BLM, and not from US.But UK had similar riots when the Government made a pledge not to raise univeristy fees, and in the first 3 months they raised them, and then killed a student at a protest. So protests turned into riots.Iam not sure whats new here, we knew for thousands of years - You fuck around and piss off like a million people, eventually you are gonna find out. replyphilipwhiuk 20 hours agoparentprev> If you have a very competitive interview process and high compensation to attract the best talentIt&#x27;s a big assumption that the competitive interview process is sufficient to identify only the top talent. Every other week News@YC has posts complaining that every type of interview process doesn&#x27;t identify talent well. reply steveBK123 19 hours agorootparentSure, some people interview better than they work, and vice versa. But do a huge flood of bad employees really make it through these very difficult gates and merit 30% of employees being marked underperformers every year? reply leetcode5ucks07 18 hours agorootparentI can tell you Indian new graduates interview very well. They grind years on these DSA puzzles. But have very little engineering knowledge or skill in general.It&#x27;s all about gaming the system here.I have literally seen people justifying lying on the resume, by feeding the job description to ChatGPT and let it come up with a resume.Yet, there&#x27;s not much influential software coming out of India, where leetcode hards for graduates are the norm, but these graduates don&#x27;t understand what a virtual machine is.On the other hand I have seen prolific open source contributors in the same age bracket (they contributed not to some web design project, but very much used cloud &#x2F; system software) struggling to get a job, because they didn&#x27;t do enough \"DSA\".I am not saying people of my country are dumb. Quite the contrary.I am saying when a measure becomes a target, it ceases to be good measure.That&#x27;s the case with leetcode problems.It selects for people who can grind through it. Not the people who have other interesting things to do. reply ClumsyPilot 12 hours agorootparent> have literally seen people justifying lying on the resume> prolific open source contributors...struggling to get a jobThe market selects the best, if it&#x27;s best at lying, who are we to tell the invisible hand that it&#x27;s wrong? &#x2F;sI&#x27;ve seen environments where a certain amount of manipulating the truth is expected. Individuals eventually figure out what&#x27;s expected of them.You know it&#x27;s not the truth, they know it&#x27;s not the truth, and you know they know. reply scarface_74 19 hours agorootparentprevDo you realize how relatively easy it is for a CS grad with time and motivation to ‚Äúgrind leetCode‚Äù enough to pass an interview at Amazon?I didn‚Äôt go into AWS as a developer. I worked there in Professional Services. I shadowed a few coding interviews and conducted a few system design interviews of software devs while there. reply Firmwarrior 18 hours agorootparentThe problem is that forced attrition doesn&#x27;t create a dynamic, competitive environment full of go-getters. It creates an environment full of treacherous bums who work as little as possible, and spend all of their efforts making sure they look good and don&#x27;t take the blame for any failures.You can see the results of this in Amazon&#x27;s consistently poor companywide performance in every arena. They&#x27;re even managing to lose ground to Microsoft in cloud computing. It&#x27;s pathetic. reply scarface_74 9 hours agorootparentThey are losing ground to Microsoft because MS is already in bed with the ‚Äúenterprise‚Äù and big enterprise was late adopters to cloud.Source: I worked at AWS ProServe for a little over 3 years and saw how hard it was to detangle BigEnterprise from Microsoft reply scarface_74 18 hours agorootparentprevWhen you have the scale of Amazon, you don‚Äôt need a lot of go getters. Just a relatively few and a lot of grunts. One CS leetCode grinder at the L5 level is replaceable with another. reply geodel 15 hours agorootparentI think this is really important point. Somehow lot of folks are missing it. As if each engineer is responsible for launching a new Amazon service every year. I have heard and met many current Amazon software engineer. Most of their job description sound rather crappy routine work even for me earning 1&#x2F;3rd salary of Amazon.I surmise large number of engineers there are just collecting bounty on Amazon monopoly on cloud infra. Their skill and work is not more challenging or interesting than hundreds of thousand engineers at 20-30% income of Amazon in boring enterprise IT shop. reply scarface_74 12 hours agorootparentThat‚Äôs not just true of engineers. It took years for a friend who worked there as an L6 in the finance department for Amazon Retail to match his compensation after he left and it took him becoming a director at a manufacturing company.Another former coworker who was a project manager (engagement manager) got maybe 60% of their compensation and is doing more work.Myself personally, I was an L5 at AWS and now I‚Äôm doing at least L6 level work and I‚Äôm still making 15% less than I was making at Amazon.It‚Äôs going to take a couple of years at the company I‚Äôm at now and building out an entire specialty practice - L7 level work - to get to my L5 compensation.I have a roadmap I designed with my manager to do it and it‚Äôs a straightforward process, it‚Äôs just going to take time. I‚Äôm excited about doing it. It‚Äôs the scope&#x2F;impact level of work I did to get into AWS on a much smaller scale at a startup. reply steveBK123 11 hours agorootparentprevYeah, to be fair.. the people I know from Wall St tech that left for Amazon were not the best tech people I knew. The best tech people would have found more well compensated roles within Wall St tech.Generally a mix of people who exhausted other paths to higher compensation by their mid 30s - mid 40s (senior management track &#x2F; trader track) and this was the next big thing. reply roncesvalles 18 hours agorootparentprev>CS grad>time and motivation to grind>pass an interviewLet&#x27;s be honest, what more are we looking for? reply closeparen 16 hours agorootparentAbility to deliver the projects? Be a pleasant, reliable collaborator? Teach, learn, critique, take feedback, adapt? reply scarface_74 16 hours agorootparentI agree with you and my interviewers that I conduct are always more behavioral. Most of the positions I interview candidates for are for bog standard CRUD jobs that anyone can do technically. The deciding factor are the traits you mentioned. reply closeparen 15 hours agorootparentTechnical skills can very much be the barrier to delivering CRUD projects, but it is usually more like troubleshooting, debugging, unscrewing your dev environment, understanding the behavior of your framework and database engine, leaving yourself good log messages, etc. replyArainach 17 hours agorootparentprevObviously this depends on the company, but after 15 years at multiple FAANG, I&#x27;m leaning towards no. It might have been true at 2010-era Microsoft but the painful transition where they fired all the SDETs got rid of most of the people who \"made it through\" and after that I never felt this to be the case.Google, for instance, has a slow tedious hiring process with a lot of false negatives, but in my experience those who made it through were absolutely qualified and great to work with. There certainly wasn&#x27;t 30% to cull. In my opinion there wasn&#x27;t even enough people to fill the percentage quota that need to be given ratings lower than satisfactory - before the layoffs. After them, the fact that they&#x27;re still enforcing these quotas on teams is absurd cruelty that kills morale. reply scarface_74 14 hours agorootparentI mean looking at Googles abysmal track record compared to its peers when it comes to creating and sustaining new products and product categories over the last two decades is not exactly an argument for their hiring process. reply Arainach 14 hours agorootparentThe decision to sustain products is at the executive level, not the ground level. reply scarface_74 11 hours agorootparentIt‚Äôs also part of the promotion culture where no one shows ‚Äúimpact‚Äù by sustaining an existing project. reply IKantRead 18 hours agorootparentprevOf all the ex-FAANG people I&#x27;ve worked with, I&#x27;ve found the ex-Amazon ones to be, by a wide margin, the least impressive group.I&#x27;m not huge fan of Google, but will readily admit that, on average, the ex-Googlers I&#x27;ve worked with have tended to be great engineers and very sharp people.So, at least from that anecdote, there does seem to be a difference in each of these hiring filters. reply mrbgty 19 hours agorootparentprevYes. Difficult doesn&#x27;t mean good. reply astura 19 hours agorootparentprevSure, I&#x27;ve seen plenty of very capable people put very little to no effort into their work. reply steveBK123 19 hours agorootparentIf Amazon&#x27;s only management stick is to mark 30% underperforms & cull 6% every year, then they need to send their managers to manager school. reply marcus0x62 18 hours agorootparentTheir stated goal is for each person they hire on a team to be better than 50% of their peers. For a company that professes to think a lot about the long term, I&#x27;d suggest they need to go to math school before they go to manager school. reply geodel 15 hours agorootparentWell you are right. But reality here is company is printing boat load of money with their cloud infra monopoly. So what to do? Blabber non sensical bullshit that no one will question because again everyone is making money and if they are so smart to challenge leadership dictates why wouldn&#x27;t they just leave and apply their ideas some place else that needs them. reply groby_b 18 hours agorootparentprevThis is not on the managers. This is HR gone wild. Most managers close to the frontline understand perfectly well what an incredibly stupid idea this is. reply NoMoreNicksLeft 16 hours agorootparentprevThat&#x27;s an interesting question camouflaged as a boring question.The assumption is that though interview processes are acknowledged as imperfect, they&#x27;re largely considered to measure something like productivity or worker value.Under that assumption, then it should be exceedingly difficult to \"cheat\" the interviews such that a person could do great at an interview, and be of low \"worker value\". Not impossible, but difficult. And I&#x27;m just pulling stuff out of my ass here, but if we were to do a survey here on HN, the most likely consensus on how difficult that is would be something like \"it&#x27;s actually more difficult to fake it in the interview than to just be good enough at what you say you&#x27;re good at\".Imagine if you were one of the fastest sprinters in the world, but to be \"hired on\" to the Olympic team they decided to interview you for it. Sure, they&#x27;d probably ask in the interview what your latest 100M times were, stuff like that, but after the interview they&#x27;d all talk privately about whether or not you were a good \"culture fit\", and whether your smile was sincere or fake, whether you were too nervous or smooth and a \"real go-getter\". In such an environment, the Olympic team might well have 30% of its members that could be safely culled. The only jobs interviewing is even a good process for are those jobs where the actual work itself resembles interviewing... sales positions, PR, being the \"public face\" of an organization (news anchors, I dunno). The reason we use interviewing for technology jobs is because that&#x27;s what the big bosses know works for their jobs, a CEO is the public face of the organization. reply Ferret7446 33 minutes agoparentprevDevil&#x27;s advocate: no matter how \"high barrier\" your interview process is, you are 100% going to hire duds, and that&#x27;s only going to be apparent after a couple months at least. \"Ideally\", the interview process would be those first couple months, but that is illegal and&#x2F;or not socially accepted. reply JackFr 18 hours agoparentprevI have posted this before, an argument in defense of stack ranking I had heard from a friend&#x27;s father who was a long-time manager for a large industrial conglomerate from the 60&#x27;s through the 80&#x27;s. Simply put it was that they were far more willing to hire non-standard candidates on gut and hunches, knowing that if it didn&#x27;t work out, it didn&#x27;t work out they would be gone in 1-1.5 years and it would not be a ding against the hiring manager. Further, at least in that day, it was an environment where everyone was going in with their eyes open -- people were not blindsided by it, they expected it. But as you point out, the FAANG&#x27;s are doing the opposite of this. They&#x27;ve got extremely difficult hiring processes and are pretending that it&#x27;s not stack ranking. reply ep103 18 hours agorootparentStack Ranking was pioneered by Jack Welch who was operating in an environment where labor was the dominant party in the labor-capital relationship, and he was taking over companies that had had decades of labor growth and power while capital was stagflating nationwide. Introducing stack ranking was a way of trimming excess fat from decades of build up within companies, while reasserting capital in the relationship within the company.Long term, it has been a disaster in every company. Because eventually you run out of fat, and then what you describe as \"far more willing to hire non-standard candidates on gut and hunches, knowing that if it didn&#x27;t work out, it didn&#x27;t work out they would be gone\" becomes \"I need to hire people I don&#x27;t want to work with, so I have someone to fire in the next stack rank\". And that&#x27;s its own form of fat.In short, stack ranking makes some sense if you have a fat organization, not unlike restricting calories to lose fat when working out. But once you&#x27;ve lost fat, continuing to restrict your calories results in losing more muscle than fat. reply psunavy03 18 hours agorootparent> \"I need to hire people I don&#x27;t want to work with, so I have someone to fire in the next stack rank\". And that&#x27;s its own form of fat.There&#x27;s a great long-form article on this called \"Microsoft&#x27;s Lost Decade.\" reply Terr_ 16 hours agorootparentRef: https:&#x2F;&#x2F;www.vanityfair.com&#x2F;news&#x2F;business&#x2F;2012&#x2F;08&#x2F;microsoft-l... reply steveBK123 16 hours agorootparentprevJack Welch also mostly made his mark by juicing earnings via both financializing the whole company going into the GFC, and by doing various forms of (probably illegal then, definitely now) accounting tricks to smooth earnings to always make his number.GE was run into the ground by his tenure. reply twiddling 13 hours agorootparentThis. GE Financial was an earnings laundering operation reply psunavy03 18 hours agorootparentprevHaving been a Navy officer for 20 years active and reserve, one of the other overlooked flaws of the stack rank is that it&#x27;s a vehicle for cultivating egos. When the only way up is through many levels of being \"1 of X, Early Promote,\" it&#x27;s fascinating to watch a bright-eyed humble flight student turn into a condescending jerk of an instructor over the years. Turning the performance review system into \"winners\" and \"losers\" and telling a subset of people they&#x27;re the winners tends to break some people&#x27;s brains.Because why should they have to listen to anyone who hasn&#x27;t gotten as good performance reviews as them? Obviously, they care more and those other slackers just couldn&#x27;t cut it. reply giantg2 17 hours agorootparentI have a friend who is getting out (technically going reserve). It sounds like there are lot of factors impacting retention and most of them are being ignored. Why should the leaders focus on things they are a part of and admit fault if they can simply blame stuff on civilian cultural issues or subordinate \"weakness\"?I heard some of the cyber warfare units are proposing to bring in people in as high as O5. No way that causes any related problems... reply psunavy03 17 hours agorootparentIMHO there&#x27;s a huge blind spot in leadership exactly due to the stack rank, because how do you recommend changes to the system to someone who succeeded and owes their whole career and professional reputation to that system? The answer&#x27;s going to be \"well it worked for me, so it must not be that bad.\"The other part of the problem is yes, there are times when the military has every right to expect folks to endure hardship. I mean, the whole point is to send folks into combat if needed, and combat sucks. Being in the field or underway for months at a time sucks, but they&#x27;re necessary. But because of this, it&#x27;s easy to slip into \"just suck it up\" as a response to a whole bunch of hardship, stupidity, or inefficiency that isn&#x27;t necessary.The reserves have their own breed of stupidity revolving around reserve center staff enforcing Kafkaesque bureaucratic \"readiness\" requirements on the drilling reservists. At some point, taking 1&#x2F;4 of your weekends off to come in and be told you&#x27;re delinquent on something you turned in three times already, or having to flail to complete some late-breaking tasker gets old. I loved supporting my gaining active duty commands. I retired because I got sick of the hoops I had to jump through to keep doing it.Cyber is having to bring folks in at a high level to get the experience base they desperately need. This isn&#x27;t totally unusual though. Surgeons have come in at that level when the military has needed their expertise. And in WWII, FDR brought in an automotive executive as a general officer to supervise wartime production. reply giantg2 16 hours agorootparentI don&#x27;t, even for the surgeon thing, they probably shouldn&#x27;t be skipping ranks. If the MOS needs a specific level of pay, they should address it by changes to that duty pay and revamping the retirement system to look at that pay too. The automotive executive is a little different since you do what them to oversee everything since they are an industry expert, not just a individual contributor or midlevel manager.Funny thing about the surgeon part. I know a surgeon who served and had a unique specialty with aerospace occupational preventative medicine. They didn&#x27;t fight too hard to keep them other than offering a promotion. Even at O6, they could have been making 3x the money if they were a civilian. Bringing people in at a higher rank isn&#x27;t going fix the pay issue and isn&#x27;t necessarily going to place them in the right level of authority. They need to make some changes to how that&#x27;s managed to ensure the right people are at the right levels. I&#x27;m sure there are career guys who would not be happy to have someone with limited experience come in over their rank and pay if they&#x27;re doing the same job. reply psunavy03 16 hours agorootparentDocs in the military have a bonus structure based on their specialty area that&#x27;s supposed to at least try to keep their pay from being ridiculously behind. And the idea behind bringing a surgeon in as an O-5 is that a surgeon has that level of seniority as a medical practitioner, so it aligns with their field. To bring an O-5 cyber person in, they would be expected to have enough experience as a cybersecurity professional to rate that rank. If they were entry-level but DOD needed entry-level cybersecurity folks badly enough, they would add a bonus but keep the rank lower. reply giantg2 13 hours agorootparent\"Docs in the military have a bonus structure based on their specialty area that&#x27;s supposed to at least try to keep their pay from being ridiculously behind.\"Tries is the key word there. Most of the time it&#x27;s not even close unless you&#x27;re a family practicioner or other lower paid specialty. On the other hand, there is a retirement program (although that&#x27;s based on base pay and not as great for roles like this) and they don&#x27;t need mal-practice. At least to me, it seems the biggest way they get docs is to pay for their med school if they commit to 8-12 years or whatever it is for them now.\"To bring an O-5 cyber person in, they would be expected to have enough experience as a cybersecurity professional to rate that rank.\"I mean, that makes sense in a general sense, but doesn&#x27;t really make sense the way it works now. There are plenty of people with experience that if they were external would come in at a higher rank than they are now. I&#x27;m not sure how you would design a good system for an industry where people jump companies every 2-3 years, are expecting high pay (at least O5 with 10 years), are used to rapid promotions, and are used to lots of perks. I mean, you could have someone with 5 years of internal experience with a great record be something like O3 and have someone external be called a senior analyst&#x2F;dev or lead in 3-5 years and come in as an O5 with basically no difference in duties or performance.Although I&#x27;m not sure how you would even measure performance in that role. The industry doesn&#x27;t have clue how to measure it - just look at the article or all the stuff about interviews and leetcode. The military might set some more objective standards, but they&#x27;re not going to remotely apply to external candidates.I don&#x27;t know, I guess in general I would imagine the best thing for stability, readiness, and even morale, would be to train and promote from within while focusing on retention. It sounds like they&#x27;re already providing bonuses for all levels and saying they&#x27;re focusing on retention, but clearly they have other issues. I doubt bringing people in at a higher rank will fix that. replydiracs_stache 17 hours agorootparentprevAs a flight attrite who ended up running a division on something grey, the Navy knocked me down more pegs than I was thought possible. Appreciate the forced opportunities to grow, but there&#x27;s got to to be some better ways. Sounds like you&#x27;re doing some good Sir. reply psunavy03 17 hours agorootparentI retired at 20 to make my way in my civilian tech career. I looked at the amount of effort you&#x27;d need to put into make Captain or above, and the knock-on effects on your civilian career, and said \"nope.\" Tried to do the best I could. One of the best officers I worked with in the reserve dropped on request from helo training due to family issues, and ended up making a career as a support guy for the reserve Naval Special Warfare folks. reply fatnoah 18 hours agoparentprev> One underlying problem with these PIP type programs at FAANG seems to be that they have very high barriers to entry in the interview process, and then act like 30% of the company is underperforming and subject to an annual 6% cull.FWIW, when I was a manager at a FAANG, the numbers were closer to: Underperforming: ~12% Meeting or Exceeding: ~80% Better than exceeding: ~8%As for my own anecdata, for the roughly 100 person cohort where I was part of performance management, there would be 1-3 people each cycle that were in the \"cull\" bracket. That said, people with a couple consecutive cycles of not meeting expectations were likely to be put on a PIP. I managed 4 people in that group, and all were able to successfully exit the PIP.All that said, every company and even org within a company have their own goals, targets, processes, and flows, and my experience was pre-COVID, so I&#x27;m also sure that any needle movement has been in the direction of harsher evaluations. reply linster 18 hours agorootparentAs a Canadian, ‚Äúbetter than Exceeding‚Äù sounds like an American construction.I‚Äôd imagine the scale to be: 1. Not Meeting Expectations 2. Meeting Expectations 3. Exceeding expectations reply hinkley 16 hours agorootparent‚ÄúMeeting or exceeding‚Äù is the American construction. If I‚Äôm exceeding I expect a big raise, or at least talk of a promotion (maybe next year). Starting a phrase with ‚Äúmeeting‚Äù puts a iscount on it.Of course we are talking about companies that are paying too much in the first place, so maybe they‚Äôll be forgiven for trying to bring the slope down a bit. reply belval 17 hours agorootparentprevIt is probably a way to give workers fake kudos without actually giving them anything.A lot of north american modern management theory seems to think that praising your employees is the best way to get them to perform. By telling someone who&#x27;s good but not great a \"You got exceed expectations!!\" they&#x27;ll feel like they are valued. reply fatnoah 17 hours agorootparent> It is probably a way to give workers fake kudos without actually giving them anything.It&#x27;s actually the opposite. The ratings themselves are associated with multipliers for your annual bonus and stock grants. IIRC, the full spectrum of ratings and multipliers was:Meets None (0.0) -> Meets Some (0.5) -> Meets Most (0.85) -> Meets (1.0) -> Exceeds (1.25) -> Greatly Exceeds (2.0) -> and Redefines (3.0)Ignoring the company multiplier, which was always a value slightly greater than 1.0 when I was there (that may not be true now), for a Senior Engineer (L5) with a $175k salary, 15% bonus target, and annual stock grant of $110k vesting over 4 years , the value of the rating to them at each exceeds level would be: Exceeds: ~$12k &#x2F; year Greatly Exceeds: ~$53k &#x2F; year Redefines: ~$107k &#x2F; yearThe top 2 buckets only account for about 8% or so of employees, but the number for Exceeds was around 20% of employees during my time. reply benlivengood 17 hours agorootparentprevIt&#x27;s a way to correct under-leveling rapidly. Google at least aimed to hire at the level of fully demonstrated and sustainable competency; if there was any doubt you&#x27;d be hired at the lower level. That can be corrected in a ~year with greatly exceeds or redefines ratings. There was a bit of step function in yearly raises depending on the rating as well but promotion was a substantial raise. reply resolutebat 17 hours agorootparentprevQuite the opposite: getting into the top brackets is directly tied to large pay increases, larger bonuses and stock grants, and puts you in line for promotion. reply jiveturkey 15 hours agorootparentprevit&#x27;s a bit logarithmic. the upper end needs to be 1% 0.1% 0.01%, for the same kind of reasons we went from \"MTS\" to staff, sr staff, principal, distinguised, fellow. reply Aurornis 17 hours agoparentprev> here are industries & companies that have grown fat & lazy and could use a few annual 6% culls, but you eventually run out of fat.I&#x27;ve worked on many high-performing teams where you couldn&#x27;t find 6%, or 1-in-18 of people who shouldn&#x27;t be there.However, I&#x27;ve never worked for an entire fast growing company where I couldn&#x27;t readily pick out 1-in-18 people who were clearly underperforming and everyone knew it.Hiring is far from perfect and these companies are growing quickly. Combined with turnover, you&#x27;re bringing in a large number of new developers every year. Some people are really good at Leetcode and interviewing, but don&#x27;t actually like to do work at jobs, for example.IMO, the problem comes when these arbitrary thresholds are applied at low levels like individual teams. If you take a group of 18 people and declare that 1 of them must be fired every year no matter what, it&#x27;s terrible. On the other hand, if you were to look at a large department of 1000 people and went through everyone&#x27;s performance closely, I don&#x27;t think it would be hard to identify 60 people who weren&#x27;t working out.The other problem is that some people have become really good at gaming this system. Every time I&#x27;ve worked at a company that does arbitrary culling of employees, there was always a contingent of old timers who had mastered the system years ago and never found themselves up for consideration to PIP. Either they were friends with management, had some arcane knowledge that they refused to let anyone else access, or they were really good at buttering up their manager come performance review time. I can think of one guy who was always fighting with everyone and refusing to do work, but would become the sweetest, most helpful hero a couple times a year when some high profile issue came up. He&#x27;d work 16 hour days for a couple weeks, emergent triumphantly with some solution, then disappear again for another 6 months. Untouchable, yet clearly the lowest net performer on the team. reply tanjtanjtanj 16 hours agorootparentIf you work for a company that lays off 6% of the \"fat\" every year and then still continues to have 6+% of the company considered \"fat\" then the issue isn&#x27;t the 6% of the people, it&#x27;s the structure, culture, and practices of the company. reply Aurornis 13 hours agorootparent> If you work for a company that lays off 6% of the \"fat\" every year and then still continues to have 6+% of the company considered \"fat\"Most tech companies I&#x27;ve worked for in the past years have had about 30-40% new people every year through a combination of natural turnover and growth.You&#x27;re right that if a company never grows and nobody ever leaves yet they still try to isolate 6% to fire every year, that&#x27;s a bad practice.However, nobody gets hiring perfectly right. When you&#x27;re hiring hundreds of people every year into a department, you&#x27;re going to get a couple people who don&#x27;t work out.I think people on HN get nervous around these discussions because they imagine companies as static collections of individuals, but at a high growth company you actually have a lot of new people coming through all the time.It&#x27;s also bad when companies never fire anyone. If you just let everyone stay until they quit, you accumulate a lot of people who aren&#x27;t working out while their high performing coworkers get frustrated and leave. I&#x27;ve seen this situation too and it&#x27;s not good. reply hinkley 16 hours agorootparentprevLet‚Äôs call a spade a spade. It‚Äôs not an ‚Äúissue‚Äù it‚Äôs abuse. This is abusive behavior. reply steveBK123 16 hours agorootparentprevThe problem is I&#x27;ve been at both fast growing companies & big dumb banks, and the \"1-in-18\" are never evenly distributed.It is often clear that some teams are about 50% underperform while others are close to 0%. reply hinkley 16 hours agorootparentprevIt‚Äôs only 1 in 18 if you have the attention span of a goldfish.If your hiring quality is uniform, there‚Äôs only about a 5.6% chance the new guy gets fired next year. Instead they‚Äôll go after someone else. For an average tenure of 3 years that‚Äôs closer to 1&#x2F;6 chance of ending up on a PIP. Higher if they hand out extra PIPs in order to hit the quota. reply zhdc1 19 hours agoparentprev> There are industries & companies that have grown fat & lazy and could use a few annual 6% culls, but you eventually run out of fat.Agreed, but there&#x27;s also an unaddressed issue of whether many (most?) positions at large tech companies provide an actual economic benefit to the company or whether they exist simply because of organizational inefficiencies (Parkinson&#x27;s Law).To put it another way, a hire may have been the best candidate out a very selective interview process, but if his position exists simply because of internal silo building, forced ranking - as bad of a system as it is - may work.Is there any economically valid reason for Meta to have ~75K employees? reply steveBK123 19 hours agorootparentForced ranking in your use case doesn&#x27;t work because it&#x27;s generally firm wide cascade. Is 6% of every team in every department in every org unnecessary?No, in fact.. there&#x27;s teams that should be 2x as big and teams that should be completely let go.Management actually needs to do what they get the big bucks for and make strategic decisions about what business lines do&#x2F;don&#x27;t need to be staffed rather than culling arbitrary %s everywhere.Think - Google Bard vs Search vs Ads vs Youtube vs their 27 different chat&#x2F;video apps vs .. etc. reply mjr00 19 hours agorootparent> Management actually needs to do what they get the big bucks for and make strategic decisions about what business lines do&#x2F;don&#x27;t need to be staffed rather than culling arbitrary %s everywhere.The performance management process and 4-6% \"unregretted attrition\" (to use the technical term) target at Amazon is totally independent from figuring out project resourcing and headcounts. An employee who is fired for performance reasons doesn&#x27;t change the headcount on your team. reply steveBK123 17 hours agorootparentYes, that is my point. Past a few iterations, there is not a lot of value in doing this company wide over and over.. versus making hard big picture decisions on resourcing departments properly. reply mjr00 16 hours agorootparentI don&#x27;t understand what you mean. The 4-6% URA target is for continually managing out low performing employees. It may not be the most effective way, but that is the intent. It has nothing to do with resourcing departments properly; that&#x27;s a totally different process and conversation.Again, I think you are confusing layoffs, which are a reduction in headcount for a department&#x2F;team, with attrition, where the people themselves are let go but the headcount remains so you can hire to replace them. reply marcosdumay 17 hours agorootparentprevDo hairless employees in a vacuum interfere with each commutativelly or anti-commutativelly? reply jfindley 19 hours agoparentprevHigh barriers to entry on the interview process don&#x27;t mean as much as you may think. Even with the best interview process in the world, you&#x27;re only going to have a small number of hours to try to evaluate a lot of complex factors about a human you know nothing about. You&#x27;re going to hire people you shouldn&#x27;t - and lots of them. You&#x27;re also going to miss hiring people you should. It sucks, but that&#x27;s life.With that in mind I do think your conclusion&#x27;s a little suspect - there really will be a good amount of underperforming people you really do want to part ways with. Maybe not 6% - I don&#x27;t work in HR, so I don&#x27;t see those sorts of metrics - but I definitely have encountered lots of people who got through the interview process but nevertheless had no ability to do the job adequately.I&#x27;m sure a bunch of people will jump on this to then complain about the arduous interview process - but NO interview process is perfect. Having a tough process is a reasonable way to reduce the number of people you end up not keeping on, and expecting any process involving humans to be anything close to perfect is wildly unrealistic. reply HDThoreaun 14 hours agorootparentRight. FAANG interviews generally aren&#x27;t even trying to figure out if someone will be good at their job. Leetcode tests for IQ and being willing to sink tons of hours into bs to get the job. FAANG companies have decided those are important qualities needed to succeed, but they clearly aren&#x27;t the only ones. reply ghaff 18 hours agorootparentprevAnd for roles at companies with very quantifiable outputs--like sales for example--the approach at a lot of companies is not to sweat the hiring process too much and just let go people who don&#x27;t make their numbers (whether it&#x27;s really their fault or not). Someone I knew&#x27;s shorthand for this was that sales managers have no trouble firing people. reply patorick002 17 hours agoparentprevAmazon eng manager here, at least for one more day. I think Amazon is easier to get into than the other FAANGs (I&#x27;ve worked at two of them and was very close to getting into another). This is partially because it&#x27;s easy&#x2F;required to manage people out, and partially because Amazon has a rotten reputation in the industry.It even feels like some teams hire low performers specifically to feed to the pip machine. Supposedly the idea of having a bar raiser in the interview dissuades that, but it certainly feels this way at times. reply steveBK123 17 hours agorootparentDatapoint - as a Wall St tech guy, AMZN & META are the only FAANGs who has reached out to me.. repeatedly. Same for my coworkers. reply chadash 17 hours agoparentprevI&#x27;m gonna play devil&#x27;s advocate even though I agree with your point.Firing people sucks. As a manager, it&#x27;s a terrible feeling to tell someone they are out of a job. Not only that, but often times my own pay might be tied to how many people I manage, so in some cases, it&#x27;s better for me to have people around doing nothing than to let them go. By having a forced ranking&#x2F;culling, you somewhat alleviate the tendency of people not wanting to fire non-performing staff.Companies should have a culture where this isn&#x27;t the case. It should be encouraged to let people go with nice severance. You should be judged on your teams&#x27; output, not on how many people you manage. You should be rewarded for doing more with fewer staff. But enforcing this is very hard in a big company while enforcing a 6% cull is easy, so that&#x27;s what happens. reply tcgv 20 hours agoparentprevTheir interview process, while thorough, isn&#x27;t failproof. Additionally, it primarily assesses candidates at entry, overlooking potential changes in performance that may emerge after a couple of years within the company. reply Kon-Peki 19 hours agorootparentIf 6-30% are failing year after year after year, you can&#x27;t blame the interview process. You could blame the leadership failure that didn&#x27;t fix the interview process, though. reply steveBK123 19 hours agorootparentReally it&#x27;s indicative of Amazon treating their employees as disposable. They have enough employment market power still to attract enough great talent to burn through individuals in 2-3 years. Eventually they run out of meat or they soften the process.Of all the FAANG, I almost admire Amazon for chewing up & disposing of their white collar work force the same way they do their blue collar.The other guys are all pretty bad to work for at the bottom non-SWE tiers, but have cushy SWE adult daycare office environments. At least Amazon treats all their employees as disposable? reply selectodude 17 hours agorootparentBezos cut his chops in finance at DE Shaw. That‚Äôs how every bank treats their employees. reply twiddling 12 hours agorootparentprev they have very high barriers to entry in the interview process, and then act like 30% of the company is underperforming and subject to an annual 6% cull.There&#x27;s no contradiction. No interview process can avoid low performers. There will always be unmotivated people, people who are difficult to work with and so on...But yes, this isn&#x27;t cool, but at least, people working there have very high compensation and know it&#x27;s a competitive environment. reply throwrhpip 17 hours agoparentprevI was put on a PIP at Red Hat that was quite farcical and very much about personality disconnects between myself and my manager (and made much more annoying by the fact that he had told me before the PIP that multiple other managers had let him know that they would be interested in me being on their team if I was looking for something different - instead he went the PIP route).There was room for improvement (isn&#x27;t there always), and on the face of it, to read the PIP \"Objectives for success\", they all seemed reasonable, and were very ... objective. Specifically to craft some extended documents around a potential product, to capture and present some of the research around that product to a group, etc. About five items.I completed them. All of them.My manager and I met for our regular 1:1s where he expressed that things were looking good (well, he no-showed for one, and my skip-level came for another).I show up to our end-of-PIP review and immediately know the outcome because HR is there with my manager. Part of me was pragmatic. However...What really ground my gears?\"I have been reviewing the documents and material you created as part of your objectives for the PIP and I feel that they are not of the standard that we need.\"I expressed confusion. \"When we discussed this in our meetings, you expressed no concern\". \"They&#x27;re just not the standard of what we need.\"And then literally during the call, I pulled up the documents and discovered&#x2F;remembered GDocs access list.I started screen sharing in front of him and HR.Document 1: Manager - Last Viewed: NeverDocument 2: Manager - Last Viewed: NeverAbout this point he turned his camera off.Rinse and repeat. Of about 5 documents, presentations, spreadsheets, he&#x27;d only ever looked at one of them, at that was months before the PIP.He mentioned that he had given me feedback more in Google Chat than in our 1:1s. I pulled up our chat history, simple to review, since he&#x27;d actually 100% ghosted me for the duration of the PIP. Sitting there in front of HR, with me firing off about a dozen questions, updates, etc., and there&#x27;s just no responses from manager.HR had at least the decency to look rather embarrassed about it all. My manager said nothing for the rest of the call.A few days later someone higher in HR acknowledged that they‚Äôd looked at the same things and confirmed my perspective and said that the managers handling of the PIP was not how it should have been but that their decision was final (which was fine, I never expected it to change anyone‚Äôs mind). reply tempnow987 12 hours agorootparentI&#x27;m often confused by folks going to great lengths to \"prove\" stuff to HR. Why? HR works for the company &#x2F; manager.Focus on yourself. Your greatest power is not in arguing with a boss who doesn&#x27;t like you. You can do things like switch teams internally - just put in for a switch. Start interviewing and work for another company and find someone who does like working with you. Maybe even go work for yourself.Even when working for yourself, this actually still holds. When it&#x27;s miserable working with a client, don&#x27;t \"PROVE\" to them they are idiots. Do you really want to keep working with someone you&#x27;ve had to do this with? Again, move on.I&#x27;ve had this happen a few times where the previous boss &#x2F; client remembers me fondly - and I don&#x27;t hate them either at the end of it. reply throwrhpip 11 hours agorootparent> I&#x27;m often confused by folks going to great lengths to \"prove\" stuff to HR. Why? HR works for the company &#x2F; manager.In this case, if they had fought unemployment, there was factual and objective evidence against their case. \"On review, his work output was unsatisfactory\". \"According to Google, you never reviewed said work output.\"> You can do things like switch teams internally - just put in for a switch.This pissed me off - I could easily have switched teams, even per my manager. Until he announced that I was now on a PIP. No manager is going to approve a transfer to their team, then.Like I said, I had no expectations that any of this would change anything with respect to my termination. I&#x27;d moved on, and wouldn&#x27;t want to keep being there.But worst case scenario: \"The employee&#x27;s plan said that they could do X, Y and Z. You stated that they were terminated because they didn&#x27;t do X, Y and Z.\" \"Yes.\" \"How did you determine that?\" \"By reviewing those things.\" \"But it can be demonstrated that you conducted no such review.\"In my state, that would mean they&#x27;d have to demonstrate that I was actually fired for misconduct, not performance, to make me ineligible for benefits. reply Terr_ 16 hours agorootparentprevOof. I wonder if the outcome could have been different if someone from legal had been on the line. Hostile work environment, constructive dismissal, etc. reply Melchizedek 14 hours agorootparentprevHow did we get to this? WTF happened? To treat another human being like this. The pathetic little weasels cannot even straight out tell someone they‚Äôre fired. They have to play this humiliating game of pretending you can affect the outcome and that there‚Äôs something wrong with you. Satanic. reply throwrhpip 12 hours agorootparent100% - I would have taken a simple \"This isn&#x27;t working out for us. We appreciate your efforts. Today will be your last day.\" over all this.For insult on injury, they declined severance \"as we do not offer severance for performance-related termination\", after acknowledging that it had been shown that my manager had done nothing to actually assess my performance in the PIP (I suppose their claim will be that the performance issues led to the PIP). reply pierat 16 hours agorootparentprevThat&#x27;s what I never get. Management ALWAYS sides with managers, no matter how toxic or illegal they act.Whereas a terrible manager is worth their weight in radioactive drum of sludge. They will poison products, projects, AND people.It&#x27;s my belief that only when management IS the labor, will this be settled (worker cooperatives). Unions are only a band-aid to what amounts to arterial spray and broken bones. reply awill 18 hours agoparentprevTo play devil&#x27;s advocate, the flaw in your logic is assuming that because someone passed the interview at a FAANG, they must be good.No interview process is perfect. Companies, no matter how &#x27;high the entry barrier&#x27; make mistakes when hiring. This is how they&#x27;ve decided to correct it. reply goalonetwo 17 hours agoparentprev>> they have very high barriers to entry in the interview processYou mean ... Leetcode?I would categorize it as an artificial high barrier to entry that is based on how much you study and a bit of luck. If anything, it is designed to make employees believe they are top-tier. reply wickedwiesel 19 hours agoparentprevI know you are sharing some interesting observations, but can we all stop talking about employees as the \"fat\" of companies and firing of employees as \"culls\".> underperformers lying about to cullis not just bad wording. By definition, there will always be underperformers. A profitable company only has \"too many\" employees if you think it has to pay a larger dividend to its owners &#x2F; shareholders or that future growth is mandatory.Why is it not fine when a company is more or less breaking even but paying good salaries to its employees? reply pc86 18 hours agorootparent> Why is it not fine when a company is more or less breaking even but paying good salaries to its employees?Because this ignores reality. Especially at FAANG companies, growth is expected and the majority of compensation is stock. If you stop growing, and stop having a profit, your stock tanks, and future compensation - as well as compensation from years prior that is still in the form of company stock - becomes worth much less. You can&#x27;t hire people who are as good because you&#x27;re \"in decline.\" Your products falter because talent leaves and you can&#x27;t find new talent. Even if all the above is imagined (it&#x27;s not) you now have real impacts in terms of declining application quality and shrinking user bases. reply wickedwiesel 14 hours agorootparentI was trying to point out that not every company ownership model is a publicly traded stock company. Your point is totally valid in the context you give. reply ThrowawayR2 17 hours agorootparentprev> \"Why is it not fine when a company is more or less breaking even but paying good salaries to its employees?\"Do you have a retirement plan that contains stock or get any options&#x2F;stock grants as part of your compensation? Those shares are your ownership stake in corporations and your retirement quite directly depends on those shares increasing in price faster than inflation. reply wickedwiesel 14 hours agorootparentYour point is valid but also quite obvious. As soon as you own stock you will want them to increase in valuation. My point is a different one. Publicly traded stock companies have very specific (growth) and often bad (growth, disregarding other impact) incentives. Other forms of ownership can provide a stake for talented employees and safeguard their financial Fortune. I believe a cooperative may better align everyone‚Äôs incentives for example. reply Taikonerd 19 hours agoparentprev> ... had explicitly described the compensation cliff and how a lot of people in the good years were proactively leaving, cooling off, and then coming back to reset the compensation instead of going over the cliff.Could you explain this a bit more? I thought that it was the opposite -- you have stock that&#x27;s going to vest in X years, so you really want to stay. reply steveBK123 19 hours agorootparentThere was something of a cliff at certain levels if you didn&#x27;t get promoted around the third year. Something about the offer package would subsidize your base pay for a limited number of years until your stock started to vest, to smooth out your pay. However on the third year that subsidy would go away, and depending on the stock performance, your future stock grants could adjust down as well. In practice because the stock had been running up so much pre-2022, by the time the stock vested it was up a good amount.So if your stock grants & stock price performance after the third year weren&#x27;t \"just so\" you could see your TC slip because the salary subsidy went away. reply izacus 19 hours agorootparentprevYeah, but once X passes, your comp drops significantly - \"down a cliff\". reply fatherzine 17 hours agoparentprevThe point is to instill fear through humiliation. Bonus if you quit on your own, so they don&#x27;t have to pay severance. reply golergka 17 hours agoparentprev> If you have a very competitive interview process and high compensation to attract the best talent, it is unlikely you have so many underperformers lying about to cull annually.Interview process doesn‚Äôt test for everyday laziness. A lot of very smart people who easily solve leetcode are just coasting at their jobs. reply badpun 18 hours agoparentprevThere‚Äôs plenty of people who lose motivation to work hard and want to just coast while taking very high salary. Happens in professional sports teams, happens in tech. reply xkekjrktllss 20 hours agoparentprev>the old Wall St model of doing one cut in one afternoon, calling people into an office and giving a severance is far more humane.Since when did being \"humane\" contribute to profit?>Everyone understood it was about the numbers not about your performanceThe idea is literally to make the remaining employees think it&#x27;s about performance. The company doesn&#x27;t care what ex-employees think. reply brianmcc 19 hours agorootparentThere&#x27;s something to be said, after an initial \"reduction in force\" happens, for the remaining employees to look ahead and go: OK so I and my team may be at X% risk of layoffs, but with decent-to-good-to-sometimes-excellent severance pay and at least no prospect of dehumanizing PIPs&#x2F;whatever, I am happy with those odds.Better than a pervasive dread amongst the whole workforce. reply A4ET8a8uTh0 18 hours agorootparentprevEven vampires knew that to feed another day, they need to let victim rest a little. Just sayin&#x27; But I had a huge stock investment coming up. So there was no way I was going to rock the boat in any way, shape, or form just trying to get to this date.So the employer has a financial inventive program to encourage people to stay in the organization long term, and some hyper-rational VP repurposes that reward as a kind of tenure cliff forcing people out just ahead of it? All the pieces are in the article, just waiting for folks to put them together.If you&#x27;re someone considering moving to a company that aggressively uses \"performance management\" like this ... the target of this system is you, not because you&#x27;re bad at you&#x27;re job but because you&#x27;re new. The human toll of people in positions of trust essentially gaslighting their colleagues about their performance to confiscate special comp or satisfy the gods of analytics.... Deeply misanthropic. reply miningape 22 hours agoparentI was thinking the same thing reading this article. In this case it seems it was pretty clearly meant to push this employee out before their stocks vest because its probably a lot cheaper to spend $100,000 hiring a new person than lose several 100&#x27;s of thousands to a decent worker who has just been at the company for a while.While atrocious its very clear math, but I hope employees keep leaving after they vest. reply rawling 22 hours agorootparentI read through the article expecting this, but... if this is the case, it doesn&#x27;t feel like they tried very hard?> I wasn&#x27;t put on Pivot. My manager wanted to work with me a little bit to see if I was going to commit to the job. So they sat me down and said I could go on Pivot and leave right away, or they would work with me.> ...> I played along, and I&#x27;m good at playing along when I have to be. So then the money is in my account. That next day, I called my manager and I told them I was resigning.They let him stay long enough to find a new job and for his options to vest, and then he quit. reply kudokatz 21 hours agorootparent> if this is the case, it doesn&#x27;t feel like they tried very hard?I don&#x27;t think you get it. The way this stuff is done is just as described - totally blindsiding, and potentially flat-out gaslighting. There&#x27;s no actual metrics or data until they fabricate a paper trail and force you out.You don&#x27;t win these things. Rather, you take your money and go elsewhere. Then turn down the subsequent job offers you get from them, using them only as leverage to juice your pay elsewhere with a competing offer ;-) reply ballenf 21 hours agorootparentBut it sounds like he won? He got his $200k and found a new job before any PIP was implemented? Why would his manager have been upset if they were actually trying to force him out?Surely HR knows how long it takes to exit someone and if they wanted to keep the $200k they would have started the process earlier? reply anyoneamous 21 hours agorootparentThe manager wasn&#x27;t the one doing the forcing - the overall stack-ranking system was, and the manager was just employing the required level of doublethink to convince themselves the employee was both suitable for firing and also worth trying to retain. reply cornholio 19 hours agorootparentI disagree, this is not some junior manager drinking the company cool aid. They are a middle manager with hundreds of indirect reports - in HR, no less - fully aware of what they&#x27;re doing and with a metric to hit.So they lined up potential candidates, made them the same deal and tested their level of docility and loyalty to himself. Some of the candidates demurred and they got the boot, the author played the loyalty game and then \"backstabbed\" his superior and screwed the stats. reply hobs 17 hours agorootparentSo, good? The employee recognizes that the employer is just screwing around with everyone, seems like a FAFO situation; I hope Amazon has this happen endlessly. reply okaramian 18 hours agorootparentprevTo the point here: I think people think these managers are in a position of real power. They are not. They are cogs in the wheel as are their subordinates. It&#x27;s entirely possible this manager wasn&#x27;t even the one doing the direct ranking, sometimes this roles up to levels beyond where the manager can give real input. Someone has to get pipped as the system demands it, it happened to land on the person in the article. The manager is then trying to get them out of it because they believe they don&#x27;t actually require the pip.So this is both a failure of the manager (it is their job to navigate the system and boost their reports during stack ranking), and also a failure of the system as a whole (this person probably shouldn&#x27;t have been pipped).I don&#x27;t think it&#x27;s so much doublethink as it is this manager is trying to balance competing interests in their very immediate sphere. reply Obscurity4340 19 hours agorootparentprevHow can they be maintaining these completely antithetitical interests, like wjy do they want to arbitrarily fire the person at all if they have already evaluated that person to be worthy of retention? Why is this even on the table in the absence of any failure to meet whatever metrics?Is it just a big power play that keeps rolling in the expectation of bottomless&#x2F;infinite talenent and pepetual inflow&#x2F;attrition? reply nerdbert 15 hours agorootparentI assume there are different competing priorities at play that converge in particularly dumb ways sometimes.Some stakeholders latched onto the idea of churning some % of staff each year in an effort to, I guess, eventually filter the entire human population for the best possible employees.Some people want to make it look like their HR team is doing a lot of useful stuff.Some people want to boost their own department&#x27;s metrics.Some people want to work with a team to achieve actual business goals. reply anyoneamous 17 hours agorootparentprev> Is it just a big power play that keeps rolling in the expectation of bottomless&#x2F;infinite talenent and pepetual inflow&#x2F;attrition?This has always been my interpretation. The \"everybody is replaceable\" mindset comes from Amazon retail warehouses, and bled into the rest of the company. reply Obscurity4340 17 hours agorootparentEven Bezos? reply anyoneamous 13 hours agorootparent\"Everybody\" meaning \"the normie plebs who work for a living\". reply Obscurity4340 13 hours agorootparentI think I did read about there having been extensive succession plans and efforts established already reply1000100_1000101 19 hours agorootparentprev> Why would his manager have been upset if they were actually trying to force him out?The manager made up reasons to place this person in the lowest rating, threatening w&#x2F; a PIP. Why? Perhaps to have a warning on file. Perhaps to try to persuade an even higher level of performance than what they&#x27;d previously considered excellent... who knows.The manager being upset shows the PIP threat wasn&#x27;t justified, as you noted. You don&#x27;t get upset when a non-performer chooses to leave. Why would anyone continue to work under a manager that unjustly threatens them, or tries to motivate performance w&#x2F; fear? No, leaving was the right choice, even if they \"won\".To me, it sounds like the manager is the one who really needs to go. Perhaps this really isn&#x27;t Amazon policy at all, and this manager is being overly tyrannical and training his staff to be the same. We&#x27;ll never know. On the other hand, Amazon never said what the \"number of inaccuracies\" were. It could very well be this tale is actually very close to policy too. Heck, it could be the actual policy exactly... zero IS a number after all. reply dtmooreiv 18 hours agorootparentprevHis manager was likely upset because the guy left before he was put on the PIP and so wouldn&#x27;t count towards the HR VP&#x27;s (and by extension his manager&#x27;s) 6% goal. reply DanAtC 10 hours agorootparentprevThe manager was upset because leaving before PIP is regretted attrition which is a metric the manager themselves could get PIPed for. reply JohnFen 18 hours agorootparentprevThat doesn&#x27;t sound like he won to me. It sounds like he managed to get a good consolation prize, though. reply ghaff 17 hours agorootparentMost people are probably going to eventually leave their company anyway. If they can do so more or less on their own terms, that counts as more of a win than a loss. reply rawling 20 hours agorootparentprevI&#x27;m specifically addressing the parent comment> In this case it seems it was pretty clearly meant to push this employee out before their stocks vestwhich if so it singularly failed at. reply cyanydeez 21 hours agorootparentprevat some point, I assume they meta game the PIP expecting employees to leave willingly. reply NBJack 20 hours agorootparentBingo. Remember, one of the key points of a PIP is to gather documentation. When they sit you down for The Meeting, the &#x27;evidence&#x27; is already on record, reducing your leverage should you decide to pursue them legally later for something like wrongful termination. Then, they often dangle a pittance in the form of severance pay if you go ahead and exit right away, of your own free will and volition. This gets even worse if the person is on a H1B.It&#x27;s a machine. reply bsenftner 22 hours agorootparentprevHow Amazon&#x27;s behavior here, clearly mandated as official corporate policy, is not an instant class action lawsuit demonstrates how far we are in our \"late stage capitalism\" and economic slaves, not free, not by half. reply ballenf 21 hours agorootparentIn earlier times, they&#x27;d just fire you without any kind of PIP process.Are you saying that PIP vs. immediate firing is worse? reply jzb 19 hours agorootparentGenerally yes. Being PIP&#x27;ed or otherwise softly managed out is worse than being immediately fired.Being fired: Happens quickly, they either give you a reason or tell you \"your position has been eliminated.\" Sucks, but rips the Band Aid off and done is done.PIP: I&#x27;ve seen this happen to good and bad employees. Bad employees - it just drags out the process and provides a lot of friction and anguish, for the manager and maybe the employee. (Depends on whether they are actually even aware they&#x27;re a bad fit or not, and whether they&#x27;re well-intentioned or not.)Good employees? It&#x27;s literally gaslighting people. One day they&#x27;re pulled aside and told they&#x27;re not performing well enough and given a PIP. The PIP usually isn&#x27;t designed to help them get performance up - it&#x27;s creating a paper trail to manage them out.Suddenly (as described in the parent article) they&#x27;re being hit with complaints they&#x27;ve never heard before. Maybe there&#x27;s substance, maybe not. It&#x27;s a major blow to self-esteem and it&#x27;s a dragged out process. They&#x27;re not even given the satisfaction of a quick firing where they can be angry and done. It&#x27;s a long suffering.And to make the managers do this stuff - it&#x27;s awful. Having to lay somebody off when a company RIFs people sucks. Having to fire people usually sucks unless they&#x27;ve really earned it. But telling a manager they have to PIP a percentage of their team arbitrarily and participate in this gaslighting process is simply evil. reply ghostoftiber 19 hours agorootparent> Good employees? It&#x27;s literally gaslighting people. One day they&#x27;re pulled aside and told they&#x27;re not performing well enough and given a PIP. The PIP usually isn&#x27;t designed to help them get performance up - it&#x27;s creating a paper trail to manage them out.Exactly this. The PIP process also protects the company from wrongful termination suits and oftentimes serves as evidence against paying out unemployment claims. Rightly or wrongly applied, telling employees that they&#x27;re underperforming solely serves as creating a papertrail to push someone out. reply scarface_74 18 hours agorootparentprevI went through the process there. The process gave me months to prepare for another job and by the time the official PIP happened, I was more or less waiting on it to get my severance.I could have quit anytime during the process and had other opportunities waiting for me. I was offered another job before my 10 days of paid time off elapsed let alone my 3+ months severance and I survived through a vesting event.At almost 50 years old and Amazon being my 8th job out of now nine, why would I wrap my esteem on my job? It was just my 8th time to exchange labor for money to support my addiction to food and shelter. reply euix 19 hours agorootparentprevI have a colleague at Amazon who mentioned this last point as the explicit reason he planning to leave Amazon. He was (is) a people manager and every year, having to cut off a member of his team for no reason was not something he wanted to do on an ethical level. He described the process with me and in the end his solution was to give the IC a tip off that a PIP was coming and in that case that person was able to move to another team quickly enough to avoid the chopping block. reply maccard 19 hours agorootparentprevMy dad had a government job where the move was to enforce all contractual obligations rigorously, remove any benefits provided, and give them absolutely 0 work to do.He had a disagreement with his manager (let&#x27;s leave it at that, heh), and over the next week, his arrangement to work from a different office (the rest of his team were based in this office too, just his department&#x27;s office was technically somewhere else) was removed, his flex was removed, he was taken off all projects, and told that using his computer for non-work purposes was a disciplinary offence.He had a 90 minute each way commute, was required to be clocked in between 9:30 and 4:30 every day (not 9:31, 9:30), required to work 37.5 hours, requests for using TOIL due to commute or other issues were immediately denied, no work to do, and not being allowed to use the computer in his office. The resolution was he found someone in a different department in the city we lived in that had the exact same thing happen to them, and they negotiated a swap with each other and things got better again.==Point being, these processes are a combination of ass-covering, parallel construction, and outright abuse. Nobody deserves to be treated like he was (or like people are being treated in these PIP programs), it would be more humane for everyone if we did something different. reply JohnFen 21 hours agorootparentprevIf the PIP is intended to get the employee to quit, then yes, I think the PIP is worse. reply apwell23 20 hours agorootparentprevi was given the option to take 1 month PIP or take 1 month severance. I took severance but not quite sure why they offered me pip. I am guessing something do with unemployment benefits? reply ghostoftiber 19 hours agorootparentYou did the correct thing. The PIP is to cover their butt that you really were a bad employee and you were not wrongfully terminated. The laws vary wildly state-to-state but generally if you&#x27;re PIP&#x27;ed out you can&#x27;t claim unemployment or wrongful termination. reply throwrhpip 16 hours agorootparentAt least in Washington state, that&#x27;s not the case. When I was PIPed out of Red Hat and claimed unemployment, benefits were approved.\"You or your employer said you were fired.We decided:You were fired, but there was no misconduct. You gave your best effort, but you were unable to do your job as expected.When you are fired but there is no misconduct, you are eligible for benefits.\" reply danaris 21 hours agorootparentprevIn some ways, yes.Firing you immediately for bullshit reasons is something you can potentially take to court, and also make a solid case for unemployment payments due to unfair termination.If they put you on a PIP, and then claim that you&#x27;re failing to meet it, even if they&#x27;re lying through their teeth, that makes it much harder to prove that their actions were unfair or illegal. Suddenly it&#x27;s no longer a \"person said, faceless company said;\" it&#x27;s \"person said, faceless company had months of documented (if falsified) evidence to back up their position\". Unless you are savvy enough to start documenting everything, which is a) hard to know you need to do, b) hard to know how to do effectively, and c) kind of exhausting even if you can get it right.To a large extent, the PIP process is there to cover Amazon&#x27;s ass and make sure that your firing sticks.And all that is if you don&#x27;t just get frustrated and fed up with being constantly told you&#x27;re failing to meet expectations that either you are, in fact, meeting, or no actual human could ever meet. reply NBJack 20 hours agorootparentThis reminds me of the big piece years ago talking about employees crying at their desks from the pressure. People joked about it, and to be fair, it wasn&#x27;t that common.Reality is, you wanted to find a conference room or the bathroom for that.Let&#x27;s not forget that in order to use said documentation legally later, you would potentially run the risk of violating company policy on confidential information disclosure. reply cool_dude85 21 hours agorootparentprevIn many jobs \"in earlier times\" there were for-cause clauses negotiated into contracts and you could not be fired without management proving that you had done something wrong, even after you had been through a negotiated disciplinary process. reply sumtechguy 20 hours agorootparentprevLate stage? This is SOP for many large companies going back decades. The tech companies are just sort of new to it and gave it a new name. Many companies in the 70s&#x2F;80s&#x2F;90s famously had their 10% culling every year. Microsoft called it stack ranking. This is hardly new and is fairly commonly done in many industries. The book name escapes me but the CEOs loved it about how it touted this very thing to do. Is it the &#x27;right thing&#x27;, no. But is something you should expect from companies. They are not your friend or buddy or partner, no matter what lies they tell you. They are the people who pay you to do work.In the past 15-20 years the tech world has been sheltered from it. If you could code you could make decent money. That part is starting to stabilize as more people can do it. The supply is catching up with the demand. &#x27;Late stage capitalism&#x27; is a catch all term for &#x27;the parts of the economic world I think are bad&#x27;. If you ignore the curves the curves will remin",
    "originSummary": [
      "A former HR staffer at Amazon shares their traumatic experience with the company's performance-improvement plan, Pivot, which led them to develop PTSD.",
      "Pivot's focus was on eliminating employees who did not meet performance metrics, rather than improving their performance.",
      "The process had a negative impact on employees, including visa-sponsored workers who were forced to leave the country.",
      "Amazon disputes the account, claiming it contains inaccuracies and does not reflect the majority of employees' experiences.",
      "The individual eventually resigned from Amazon but secured a stock investment before leaving."
    ],
    "commentSummary": [
      "The conversation covers criticisms of Amazon's Performance Improvement Plans (PIP), challenges faced by employees with disabilities, job hopping, career growth issues, FAANG company culture, and military and civilian tech careers.",
      "Participants express frustrations with performance management practices, interview processes, and management decisions.",
      "The conversation also discusses layoffs, the role of HR in terminations, and concerns about the treatment of underperforming employees, highlighting the need for strategic decision-making in staffing and alternative compensation structures in the tech industry."
    ],
    "points": 552,
    "commentCount": 567,
    "retryCount": 0,
    "time": 1701338987
  },
  {
    "id": 38477100,
    "title": "The Animated GIF that Crashed Chrome: A Challenging Debugging Journey",
    "originLink": "https://engineering.gusto.com/the-weirdest-bug-ive-seen-yet/",
    "originBody": "29 November 2023 7 min read The Weirdest Bug I‚Äôve Seen Yet Engineering Collaboration On-call How digging into an on-call issue led to an unlikely culprit Photo by Mariusz Dabrowski / Unsplash What in tarnation? During one of my on-call rotations for our internal tools team, we got a report that Chrome was crashing for users of Gusto‚Äôs internal software. This was causing all sorts of interruptions to our normal customer service. Gusto employees in the middle of answering customer emails or phone calls might suddenly find themselves without visibility into customers‚Äô accounts necessary to do their jobs. Chrome tab crash This was fairly far outside the usual scope of our on-call issues. Our team is generally well-insulated by other teams from issues like browser compatibility, so I didn‚Äôt know the first thing about browser debugging. Where would I even begin? I leaned on a more experienced teammate, our product infrastructure team, and our IT team. The first clues We started by trying to find out what the affected users had in common. We learned rather quickly that: Not all Gusto employee users were affected. Our customer-facing software did not appear to be affected. Other internal software webpages appeared to be fine. Crashing did not occur consistently. Users who experienced it could reload the same page multiple times, and sometimes it would crash and sometimes it would not. Not all of our internal pages crashed, but more than one of them did, including our most heavily-trafficked pages. The issue was not occurring with Safari or Firefox. Hunch #1: A bad Chrome version Our first hunch was that maybe it was the Chrome version. We had one affected user update their Chrome version, and early signs looked promising that the issue was resolved. However, we soon learned that although installing the new Chrome version decreased the frequency of crashing, it didn‚Äôt eliminate the issue. We added the following new clues to our list: The specific Chrome version had already been released for a while. We had affected users on different versions of Chrome. We had affected users and unaffected users on the same version of Chrome. Upgrading/downgrading the Chrome version for affected users did not fix the issue. Hunch #2: A bad Chrome extension Okay, maybe it was a Chrome extension? We thought the crashing stopped when one of our users disabled three core extensions. When we tried to reproduce the fix with a guest profile (without extensions present), we still saw crashing. Back to the drawing board, then. Trouble reproducing the bug Our Infrastructure team put out a call to all of engineering to ask if anyone could reproduce the issue. Frustratingly, although many of our customer service representatives were affected, none of our engineering teams reported any crashes at all except for two engineers in Turkey. With precious little overlap in available pairing time due to time zone differences, we slowly learned the following over several weeks: For security reasons, we do not have Chrome crash reporting enabled. Checking out code from several weeks prior did not fix the issue, which indicated that the cause was not a recent change. Loading a static html version of a crashing page did not cause crashing. Using open-source Chromium instead of Chrome did not cause crashes, so we couldn‚Äôt see what Chrome code was failing either. Several different internal applications were causing Chrome to crash, not just one. Removing all of the page-specific content from the page did not fix the issue. Disabling our in-house font did not fix the issue. As urgency waned because our users were using other browsers as a workaround, progress on this bug slowed to make way for other priorities. We didn‚Äôt have much left to go on without being able to reproduce the bug. However, we wanted to resolve it since users had bookmarks/settings/preferences in Chrome. We believed that we shouldn‚Äôt have to ask our users to avoid the world‚Äôs most popular browser, and we were also still getting periodic pings from various users asking whether we had made any progress on this bug. A stroke of good luck One day out of the blue, one of our Denver engineers reported being newly affected. The only change she had made was downloading the Grammarly desktop app. Wait, really? We had to see if we could reproduce the bug. I downloaded the Grammarly desktop app too. Instant reproduction of the issue (at last!). I deleted Grammarly. The issue didn‚Äôt go away. I restarted my computer and the issue went away again. I reinstalled Grammarly. Chrome started crashing again. We also confirmed with many of our affected users that they had Grammarly installed on their computers. Now we were cooking with fire! Making headway With our ability to debug now greatly improved, we started making tedious headway: make a change, reload the page ten times or until it crashes. Our main internal application is built on ActiveAdmin, but newer parts of it use React without the same ActiveAdmin framework as the rest of the application. The pages that are built in React did not crash. Hmm, so maybe our ActiveAdmin code was causing the crashes? We learned earlier that removing all page-specific content did not fix the issue, so we started looking at parts of the code that were common to multiple pages, like the main navigation header and sidebar. Notably, our React pages do not use the same navigation header. The code for our main navigation bar has a fair amount of metaprogramming, and chasing down threads here was often more confusing than not. We eventually figured out how to comment out pieces of the navigation bar, until we pinpointed one line that stopped crashing Chrome when it was commented out: dropdown 'My History', [], turbo: true, src: '/navigation/my_history' HTML This section is called ‚ÄúMy History‚Äù and it differs from the rest of the main navigation in that instead of being more-or-less the same for all users, it is customized for each user, displaying the handful of pages that each user has visited most recently. We discovered that even when the page loaded successfully, hovering over the ‚ÄúMy History‚Äù section could cause Chrome to immediately crash. So close we can smell it Hunch #3: Turbo Then we looked at turbo: true. Could that be causing the issue? Turbo is a gem we added to speed up our Rails application, but it turned out to be a red herring: it was only introduced after the bug had already been reported, and we learned that the engineer who introduced it had actually been experiencing these Chrome crashes for months prior to Turbo being introduced, and months prior to the bug being escalated to us. Okay, so where was the dropdown being defined? We use a framework called Arbre to metaprogram html from this type of method. To navigate the internal plumbing, I turned to one of our engineers with deep Rails knowledge. In this case, the relevant code (once we finally found it) looked like this: ul(class: 'dropdown-menu') do li(style: 'text-align: center;') { frontend_image_pack 'loaders/loader-spinner.gif' } if has_spinner menu_items.each do |item| li(id: \"#{attributes[:id]}_#{item.id}\") do text_node link_to item.name, item.url end end end Ruby Rails code for drop down menu This code generates html that looked something like this:Totally Awesome Company A Even More Awesome Company B HTML HTML for drop down menu We replaced the call to dropdown with the generated html, removing pieces of the new html until we zeroed in on the culprit. Eureka! When I removed loader-spinner.gif, the placeholder we display while the menu options load, the page stopped crashing. Eureka! It‚Äôs the gif! We swapped in a different gif and the page did not crash. We downloaded the image file and dragged the file into the browser window. With Shakespearean melodrama, the page immediately crashed. My pair and I both audibly gasped. We also found out that: Opening the file in Safari did not cause a crash. After uninstalling Grammarly and restarting the computer, the gif loaded in Chrome without crashing. At this point, we notified our Design Systems team of the very peculiar fact that this gif was causing Chrome to crash, and they promptly replaced it with a new one. Pull request for swapping the problematic GIF Why did this particular gif crash Chrome when Grammarly was installed? Unfortunately, with access to neither the Chrome source code nor the Grammarly source code, we can only guess. In the time since we replaced the gif, either Grammarly or Chrome or both have fixed this issue, because the original gif no longer causes Chrome to crash. Epilogue I would never, ever have guessed that the treasure at the end of the debugging rainbow was an animated gif. Even though the priority of this bug changed over time as we found workarounds, relentless curiosity won out in the end. No single one of us had all of the necessary knowledge to solve this bug on their own, but with persistence and collaboration, we were able to figure it out together. If you also enjoy collaborating with relentlessly curious people, we are hiring! Hats off to the many people who collaborated to investigate and fix this issue: Iain McGinniss, Lucy Fox, Gregor MacDougall, Oguzhan Ince, Can Gen√ßler, Daniel Flynn, Eric Nagy, Lijie Zhou, Harry Seeber, Nathaniel Strauss, and Steve Konves. Amy Lai See all posts by Amy Lai",
    "commentLink": "https://news.ycombinator.com/item?id=38477100",
    "commentBody": "The weirdest bug I&#x27;ve seen yetHacker NewspastloginThe weirdest bug I&#x27;ve seen yet (gusto.com) 488 points by jevans 15 hours ago| hidepastfavorite278 comments gelatocar 11 hours agoAs I was reading this I was thinking to myself \"I wonder if it is grammarly related\" because I experienced a bug some time ago that presented itself in a similar way. It was impossible to reproduce but affecting lots of people internally within certain departments. Eventually we figured out the thing they had in common was that they had the Grammarly extension installed.The other key thing was that the bug only appeared on our staging preview urls, not on the live website. It turned out it was because of a bad regex in the grammarly extension that caused the page to hang if the domain name was more than about 100 characters. Our staging domains were pretty long, I think they contained a few subdomains and had a job number or something in there.This one is more crazy though if it is really caused by the desktop app - that&#x27;s pretty scary! reply karmakaze 9 hours agoparentI was so disappointed that the story ended with we can&#x27;t look inside Grammerly or Chrome to know why the gif decode&#x2F;rendering causes it to crash. This isn&#x27;t interesting at all. Many problems get narrowed down to some combination but not knowing really why is unsatisfying. reply chatmasta 5 hours agorootparentIt would be nice if the author would at least publish the .gif file. I want to dig into this. reply jsunderland323 5 hours agorootparentprevAgreed. I was on the edge of my seat. I wouldn‚Äôt do a write up for something like this. I feel no-soap radio‚Äôd. reply jolux 4 hours agorootparentprevThe Chromium source code is also available, not sure why they said they couldn&#x27;t look at it. If it replicates with Chromium you can dig right in. reply rhaps0dy 4 hours agorootparentIf you read carefully you will see that they said the bug didn‚Äôt replicate with Chromium reply rezonant 2 hours agorootparentThey did yeah, but did they try Chromium with Grammarly? reply codethief 10 hours agoparentprev> It turned out it was because of a bad regex in the grammarly extension that caused the page to hang if the domain name was more than about 100 characters.Just today I debugged a regex that would DoS our backend whenever the user enters the wrong thing in a form.Now I&#x27;m reading up on regex engines: https:&#x2F;&#x2F;swtch.com&#x2F;%7Ersc&#x2F;regexp&#x2F;regexp1.html reply dev_slash_null 8 hours agorootparentJust in case you haven&#x27;t seen the postmortem of the Cloudflare outage which also was caused by a regex based DoS: https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;details-of-the-cloudflare-outage... reply radiojosh 6 hours agorootparentThat was a great read, but there was one thing I didn&#x27;t understand: Why would the regex string have \".\" twice in a row? What does \"..\" find that \".\" doesn&#x27;t find? Does that just mean \"at least two characters\"? reply rjbwork 5 hours agorootparentIt means specifically 2 characters, and is equivalent to .{2}..+ or ...* are ways of writing \"at least two characters\". reply beaugunderson 2 hours agorootparentprevI had a ReDoS issue at a prior company many years ago; at least they&#x27;re lintable now: https:&#x2F;&#x2F;ota-meshi.github.io&#x2F;eslint-plugin-regexp&#x2F;rules&#x2F;no-super-linear-move.html https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;eslint-plugin-redos&#x2F;v&#x2F;1.2.0 reply bongodongobob 6 hours agoparentprevHoly shit. I had a similar thing happen with some web based video surveillance software maybe 5 years ago.A manager of some sort had his aging laptop replaced due to a company wide Windows 10 upgrade project. Super friendly older guy, probably in sales. IT went through all the procedures of inventorying software and network needs, backing up user profile and docs, etc. Great processes in place. I remember this because I saw the device assessment and it was like a 10 year old Thinkpad with 4G of RAM and a note saying he had to keep it plugged in at all times or it would shut down. Who puts up with that? Patience of a saint. Anyway.Laptop was deployed by onsite IT to verify everything was gravy. All checked out except for Grammarly. License didn&#x27;t get transferred properly or something so they had to put in a request to get his licensing working.Fast forward a week and he gets his license key and Grammarly is tested good to go. He&#x27;s checked off the list.Later that day we get a call about not being able to see security cameras because the web page is crashing. Helpdesk tries the basics, reboot, clear cache, reinstall browser, rebuild profile, etc., nothing works and it gets escalated to me. I check the network, firewall logs, log into another PC, onsite, off-site, etc. All working for me, no one else having issues.I tell him \"I&#x27;m completely baffled here, have there been any changes lately? In your office? With your PC?\" He jokingly says \"Well yeah they installed Grammarly today maybe that&#x27;s it?\" We both laugh and I say well, I&#x27;m literally out of ideas, fuck it let&#x27;s try it.I remote in and uninstall Grammarly. \"Ok go ahead and try the cameras lol\". I then watch him open up Outlook, go to a folder named \"Cameras\", and open an email with a link to his cameras \"home page\". It fuckin worked. I turned Grammarly back on and clicked the link and sure enough it failed.I made him a browser shortcut, moved his \"email shortcuts\" into his browser, blew his mind, and closed the ticket, but it definitely bugged me.This tracks because it was some very dated camera software (you&#x27;ll know what I mean if you&#x27;ve seen it) and the link was to his customized homepage with a super long php (or something) generated url. He was the only one at the site with Grammarly as well so it was the only time we saw the problem.Thank you, I can finally close this cold case out in my brain. reply leptons 23 minutes agoparentprevIf a website bug is not easily solved, first order of troubleshooting is to disable all extensions. Devs don&#x27;t often think an extension could be causing the problem, but extensions can do wild things to a webpage. I&#x27;ve caught a few bugs caused by extensions this way. reply toddmorey 12 hours agoprevOnce my college professor was working on her research paper and told me she was struggling get text to stay underlined. Assuming a simple user error, I expected to help her out in 5 minutes.Over three hours later, we discovered that the combo of her specific video card driver version along with her specific printer driver version would keep text from printing out underlined. reply issung 11 hours agoparentHow does one discover something that niche in ~3 hours? reply toddmorey 11 hours agorootparentLots of internet searching and even a few calls to HP support. To be honest, we dismissed some of the earlier suggestions to upgrade the other drivers from other vendors... so maybe most of the time was us getting past ourselves and our disbelief. reply 29athrowaway 9 hours agoparentprevMuch better than the Xerox bug that caused numbers in scanned documents to get changed.https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;xerox-scanners-alter-numbers-i...Ouch reply b3lvedere 2 hours agorootparentI kind of remember this one. It wasn&#x27;t really a bug. I think Xerox used software that was known to not be 100% trustworthy to recognize numbers when used at a certain compression level. It was even in the manual if i&#x27;m not mistaken. reply calessian 2 hours agorootparentThe manual pointed out this can happen at higher compression levels, but they were able to reproduce it at all levels. reply generationP 11 hours agoparentprevHuh? How does a video card affect printing? reply to11mtm 11 hours agorootparentOne of the ways to print things (especially on windows) is Via GDI. [0]Basically using the OS&#x27;s rendering to make a raster that is then sent to the printer. The main thing the printer&#x27;s driver does in this case is know how to take the bitmap and tell the printer to print the bitmap (i.e. chunking data and&#x2F;or sliding the right commands into the bitmap stream)Contrast to, say, PostScript which allow for more compact and better scaling definition of what to print. This obviously works better for quality, however for a long time the issue was you then had to have sufficient processing power on the printer itself to handle it.[0] - Search for &#x27;GDI Printer&#x27; for a little more info. reply generationP 8 hours agorootparentInteresting! So that&#x27;s why I used to get crappily rasterized printouts of PDFs in Chrome a few years ago.I had thought printers could be trusted with their own rendering, but of course that is another can of worms... reply userbinator 8 hours agorootparentprevMore relevant to the bug is the fact that GDI can do its own rendering, or send commands to a driver, usually for GPU hardware acceleration, but the same applies to printers. reply shever73 11 hours agorootparentprevIt often did, particularly on older versions of Windows. I helped uncover a bug in Epson printer drivers ~20 years ago that was caused by a specific graphics card. reply shermantanktop 11 hours agorootparent20+ years ago I was in tech support and had to help someone figure out an issue where her document wouldn&#x27;t print on a Brother printer. Turns out a section divider line would block the entire doc from printing (by crashing the app) if the line&#x27;s end-cap style was set to square rather than rounded. reply jfoutz 11 hours agorootparentprevA lot of rendering will go through the video card if available, like the jvm does this as an optimization. reply toddmorey 11 hours agorootparentAh interesting. That makes sense. I had no idea. reply jspaetzel 12 minutes agoprevI&#x27;ve run into something similar to this a few years ago where chrome randomly introduced a rendering bug that resulted in a similar crash and only affected our application in seemingly random fashion. I don&#x27;t remember the full context but I think it was something involving layers & transparency.We finally figured it out when a coworker couldn&#x27;t replicate and we noticed they were a version of chrome behind. We were able to track down the specific commits in chrome that broke & fixed it in our case which was pretty cool to see at the time.If you have any more time I recommend reading through the commit log and see if you can find the changes that broke&#x2F;fixed this for you. I&#x27;d bet on another rendering bug. reply cristeigabriel 4 minutes agoprevIf Chrome is able to generate a crash-report, nothing would prevent you to intercept it.> Unfortunately, with access to neither the Chrome source code [...]I mean, it&#x27;s still very possible to debug (especially with the fact in mind that Chromium is open-source, and for me has been a very useful source when reverse-engineering and debugging Chrome), but I understand why web developers would not be trained in reverse-engineering techniques. reply aquafox 11 hours agoprevI randomly had the issue that after booting up Linux, I didn&#x27;t have any sound. Turns out it was related to my Windows dual-boot setup!When restarting from Windows, Windows doesn&#x27;t shut down my realtek audio device, but only puts it to sleep and Linux fails to start it. Only solution is to always do a shutdown from Windows and then hitting the power button. The issue is still there: https:&#x2F;&#x2F;askubuntu.com&#x2F;questions&#x2F;1032543&#x2F;no-sound-in-ubuntu-1... reply nightfly 10 hours agoparentI&#x27;ve seen the opposite-ish: Someone who was dual booting windows and linux only had working wifi if they booted into windows and rebooted into linux. The linux install didn&#x27;t have the firmware package for their wifi card installed, so when rebooting from windows into linux it was still all primed and ready to go but not when cold booting into linux directly. reply sodality2 5 hours agorootparentThis is my exact problem with a HP envy X360. HP refuses to fix the bug in BIOS. :&#x2F;https:&#x2F;&#x2F;bugzilla.redhat.com&#x2F;show_bug.cgi?id=2107845 reply 6510 46 minutes agorootparentEvery other day I read some fascinating HP adventure. I&#x27;ve done some questing myself, cant really tell if I did well, are they new multiple issues or the good old same?Your comment had me wonder if a kickstarter for an rpg game called HP printers would take off. The potential fan base is HUGE, I imagine even people who would never play a game would want to see it.Let the GUI be like little computer people (only an office)https:&#x2F;&#x2F;youtu.be&#x2F;SkTgX1mGmDg?t=17Then the player has to go though rituals depending on the HP printer type, swap old for new then new for old then old for new ink cartridges after finding the instant ink package in storage, if any spawned of course.If you try to go to fast the hero starts destroying the office, starting with the printer. Then a new different model is delivered. Ready for setup.... or maybe not?In some laps of consciousness I purchase one more some years ago. The hours of configuring it made me feel dumber than the wrath of the HP dungeon master.The ISP has a wifi router fixed on channel 11, half the neighborhood sits on channel 11, range is about 4 meters, the printer doesn&#x27;t reconnect unless unplugged first. But there is wifi direct, this doesn&#x27;t have 50 cm of range, it wants to use channel 11.in the game our hero at least at first doesn&#x27;t know about channels. The next printer arrives: Something seems off, there is no usb cable in the box? Do you want to A) order a cable B) we will use wifi! Of course if you order the cable the boss will complaint that it takes to long to set up the printer but when you eventually get the cable the boss wants to print from his phone...https:&#x2F;&#x2F;freesound.org&#x2F;people&#x2F;InspectorJ&#x2F;sounds&#x2F;402095&#x2F; reply genpfault 10 hours agoparentprevHad the same thing going the other way: rebooting from Linux caused Windows 10 to bluescreen during boot. reply trinsic2 6 hours agoparentprevturning off fast user switching didnt fix it? reply j1mmie 15 hours agoprevWhat an interesting conflux of tech to create this bug. That&#x27;s the web in 2023. I would love to know if it was a Chromium bug that got resolved, but navigating this is tough: https:&#x2F;&#x2F;bugs.chromium.org&#x2F;p&#x2F;chromium&#x2F;issues&#x2F;list?can=1&q=gif...Also, I am fully here for Gusto posting this to say \"wasn&#x27;t our fault\" and to throw some shade at Grammarly in the process reply thenoblesunfish 14 hours agoparentSeems like they&#x27;re posting it because it was a fun story, and it&#x27;s free advertising - this wasn&#x27;t externally visible so I don&#x27;t see where fault comes into it. reply nonethewiser 11 hours agoparentprevThey should definitely publish the gif reply j16sdiz 6 hours agoparentprevSomething like this https:&#x2F;&#x2F;bugs.chromium.org&#x2F;p&#x2F;chromium&#x2F;issues&#x2F;detail?id=129770... reply DaleCurtis 7 hours agoprevEven if you have crash reporting disabled there should be a .dmp generated somewhere in the user profile directory. Manually uploading that to a bug at https:&#x2F;&#x2F;crbug.com&#x2F;new would allow a Chrome developer to debug it.If you can&#x27;t share the dump for similar reasons to why you have crash reporting disabled, you can build minidump_stackwalk from Chromium and use it to generate an unsymbolized stack trace that you can post to the bug. A Chrome developer can then symbolize it.https:&#x2F;&#x2F;www.chromium.org&#x2F;developers&#x2F;decoding-crash-dumps&#x2F; has some more details. reply aaaronic 12 hours agoprevThis is _so_ familiar!I have seen accessibility tools in Chrome lead to this kind of issue in the past with a dropdown menu -- to the point where it could be replicated with a miniscule amount of HTML. The particular bug I hit 2 years ago was in Chromium-Edge, but the symptoms and cause were very similar.Grammarly almost certainly leans on some of the accessibility tools in Chrome. These tools are somewhat different in the various Chromium flavors (Edge, Brave, Chrome, etc.). reply nonethewiser 11 hours agoparentSo the theory would be that grammarly desktop sees the gif (what? How?) and calls some browser accessibility function on it (or?) which chrome cant handle and it crashes? reply saagarjha 11 hours agorootparentPerhaps it has an extension that it installs that does this? reply codethief 10 hours agorootparentThey did try disabling browser extensions, according to the blog post. reply cyco130 14 hours agoprevThe weirdest one I saw was this: User claims that the wording of the info they enter into a certain form changes when they save. At first I suspected someone else editing the same form at the same time unbeknownst to each other but it wasn&#x27;t the case according to the logs. And I saw the correct wording on my own computer.Then I noticed in their screenshots that some of the menus had weird wording too. Turns out they had Chrome&#x27;s \"Translate this page\" option on. Problem went away when we showed them how to properly switch languages in the app. reply lifthrasiir 30 minutes agoparentThis is really annoying. I often rely on Google Translate to read Japanese websites (I can read Japanese but only very slowly), and it breaks every website using React [1] because both React and Google Translate try to update DOM nodes without knowing each other. I even seriously looked at Google Translate to see how they are implemented, so that maybe I can recreate the web widget without this issue in the future.[1] https:&#x2F;&#x2F;bugs.chromium.org&#x2F;p&#x2F;chromium&#x2F;issues&#x2F;detail?id=872770 reply robocat 13 hours agoparentprevI added to all pages in a single-page-web-app after discovering some bug or other with Chrome screwing up the page.Apparently the new incantation to fix an app (can be applied to an element) is (ugly: I presume it isn&#x27;t CSS to avoid supporting dynamically changing it): https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;HTML&#x2F;Global_att...Every now and then I would look at the meta tags for a major single page app and discover some new horror when searching for the reason for the meta tag! reply jonathrg 12 hours agorootparentnext [‚Äì]I first read this as \"translate to Norwegian\" reply elygre 1 hour agorootparentIn Norway, the word ‚Äúsubject‚Äù translates to ‚Äúfag‚Äù. Back in the Usenet days there was a Norwegian group or hierarchy named ‚Äúno.fag‚Äù, which of course regularly got misunderstood. reply anitil 10 hours agorootparentprevIt&#x27;s the reverse-Norway problem - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26671136 reply cyco130 11 hours agorootparentprevI wouldn&#x27;t want to disallow translation, but in this case it was unnecessary anyway. reply michaelcampbell 13 hours agorootparentprevI know what you mean, but this caused me a second of \"wait, what?\"> all pages in a single-page-web-app reply robocat 12 hours agorootparentGood point. This was the Elizabethan days when computers ran on coal: IE when we were explorers of The Internet.We were bleeders, but there still existed a vestigial login page, and some other evil cthulic pages (I know whence they were begat for I was their father). reply callalex 10 hours agorootparentIn the USA, 20% of computers still run on coal!https:&#x2F;&#x2F;www.eia.gov&#x2F;energyexplained&#x2F;electricity&#x2F;electricity-... reply bee_rider 14 hours agoparentprevWhat was translating from English to American or something? reply cyco130 11 hours agorootparentFrom English to Turkish but Google won&#x27;t just leave the parts that are already in Turkish alone and subtly reword them instead for some reason. reply sfink 11 hours agoprevNext time this happens, I recommend just letting people use a different browser. Firefox in particular has gotten much better at importing bookmarks, passwords, etc. from Chrome.It was a Sign from the universe that it was time to make the switch. Who are we to reject Signs?(Full disclosure: I&#x27;m an engineer on Firefox. But that has nothing whatsoever to do with my advice here, no siree Bob, not in the least.) reply OsrsNeedsf2P 11 hours agoparentAs an engineer, yes Firefox is a good solutionAs a PM, we spent 4 months making the onboard easier, and now you want people to install a new browser? reply noizejoy 8 hours agorootparentHaving multiple browsers as a standard installation and employee training is an investment in business continuity.And business continuity investment is like insurance: A waste until you need it.Too much monoculture is short sighted and long term expensive. reply hirsin 5 hours agorootparentDo you ship them extra computers too? Dual boot? There&#x27;s gotta be a limit somewhere, and browser choice is a fair one.For engineers, absolutely, two browsers because some are liable to hit (self or team induced) bugs. But for sales? Support? Nah. reply langsoul-com 4 hours agoparentprevChrome is over half the market. Not a good sign for a browser based product to not properly work on the most used browser on the world... reply mplewis 10 hours agoparentprevThis is explicitly stated as a workaround in the linked article. reply socketcluster 3 hours agoprevThe weirdest bug I&#x27;ve seen yet was a concurrency race condition which led to &#x27;spooky action at a distance&#x27;. Every time I tried to console.log() to observe some value to see the cause of the bug, the bug would go away.But alas, I hadn&#x27;t discovered some new quantum effect... It turns out to have been because the race condition was close within maybe 1 millisecond and adding the console.log() statement there meant that one part of the code would take a bit longer to execute and so the race condition would not occur. reply ghosty141 1 hour agoparentIn Qt QML using print can lead to different behavior very very quickly since their engine creates bindings on whatever it comes by so using two variables in a print statement can lead to different behavior than just using one. QML in general is just awful. reply KevinMS 2 hours agoparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Heisenbug reply bluesmoon 12 hours agoprevReminds me of the time back in 2010 when a piece of CSS on the Yahoo Search page would cause a complete desktop crash on Red Hat Linux: https:&#x2F;&#x2F;tech.bluesmoon.info&#x2F;2010&#x2F;04&#x2F;can-website-crash-your-r...To the author, did you ever consider contacting the Chrome dev team about this? They&#x27;re pretty responsive to bug reports. reply resonious 9 hours agoprevI gotta say, the ending was pretty disappointing. I was so hyped to learn why Grammarly crashes when Chrome loads a particular gif. What about the gif was different? What part of Grammarly made that happen? I hate to say it but this is modern web dev in a nutshell. Remove stuff until you find where the bug is, tweak it randomly until it&#x27;s fixed, done. No root cause analysis or anything deep.This is a great post anyway. Well written and still quite intriguing right up until the end. And it seems lots of comments on here seem to also know about this problem, so I think I can still satisfy my curiosity. reply eyelidlessness 8 hours agoparent> I hate to say it but this is modern web dev in a nutshell. Remove stuff until you find where the bug is, tweak it randomly until it&#x27;s fixed, done.I‚Äôve alternated between frontend, ‚Äúfull stack‚Äù, and backend roles for over 20 years. It‚Äôs my experience that what you‚Äôre describing is ‚Äúdev in a nutshell‚Äù‚Äîneither ‚Äúmodern‚Äù nor ‚Äúweb‚Äù in the sense you seem to mean. And in any case it‚Äôs highly situational and variable depending on the dev and their team. reply flippinfloppin 17 minutes agoparentprevAnd when you consider they end with the pitch \"If you also enjoy collaborating with relentlessly curious people, we are hiring!\"What is effectively guessing which is what this article entails - without knowing the actual cause - hardly qualifies one as \"relentlessly curious\".Personally I deeply dislike the random walk towards insanity that modern dev takes with the constant churn and layers of fixes upon fixes - react state is no good! use redux! use this! use that! And before you know it knowing what is actually going on becomes nearly impossible! reply karim79 8 hours agoparentprev> When I removed loader-spinner.gif, the placeholder we display while the menu options load, the page stopped crashing. Eureka! It‚Äôs the gif! We swapped in a different gif and the page did not crash.I would love to have the original and the un-crashy gifs mentioned. It&#x27;s super easy, generally, and even without an extensive knowledge of image formats to get a grasp of what might be going on and then going down some really exciting rabbit-hole of image encoding&#x2F;decoding issues.Just take the two gifs and run them (one by one) through ImageMagick or GraphicsMagick to print out the details of what&#x27;s in them, and look for differences.Assuming ImageMagick is installed (or GraphicsMagick) installed, something like:#imagemagick$ diffNo root cause analysis or anything deep.Spending time to create a test case and sending it to the browser bug team gets the bug fixed? Riiiiiiiiight.Actually I did that for a while and the Chromium team would occasionally fix some fairly subtle issues: assuming I could make a demo of the problem and took the time to write up a good bug report. Maybe they just liked me! The Chromium team also wrote fantastic public followup to bug reports (whether fixed or not).Certainly I never had any luck getting even extremely serious browser bugs fixed by anyone else (Apple, Mozilla, Microsoft) regardless of how much time I wasted trying to give good informative bug reports. And you never found out anything further - talking to walls is more productive.It feels good trying to help the world be a better place, but it wasn&#x27;t worth it.Don&#x27;t waste your time fighting windmills. Find a workaround, document it with a comment, forget about it. Do something that makes your business successful instead. reply Sxubas 6 hours agoparentprev> I hate to say it but this is modern web dev in a nutshellI&#x27;d rather call it scientific method: observe, form a hypothesis, experiment and analyze results.I agree it is anticlimatic to not know the root cause, but the rant about the current state of web dev seems out of place. We dont even know if it was because a web technology.Some folks were mentioning issues from printers being caused by graphic cards drivers. One would love to blame printers, but it turned out they were not the culprit. reply alpb 7 hours agoparentprevAgreed.> Unfortunately, with access to neither the Chrome source code nor the Grammarly source code, we can only guess.Chromium source code is open. They could also certainly try different versions of Chrome to bisect when the issue has started to happen. Isn&#x27;t there a chance perhaps this crash might actually be disguising a buffer overflow vulnerability as well? Typically user inputs aren&#x27;t supposed to crash browser&#x2F;renderer processes. reply alexeldeib 7 hours agorootparentThey allegedly tried chromium without luck. reply computerfriend 15 hours agoprevIf they can figure it out, they&#x27;re sitting on potentially a very valuable exploit. reply chatmasta 12 hours agoparentYes, I too would like to read more details about this. It&#x27;s a great writeup from an engineer who got stuck debugging this. But I hope some experts in security or reverse-engineering can replicate it and take a closer look. There&#x27;s definitely a more interesting story here, probably regarding the localhost bridge between Grammarly extension and desktop.(Grammarly has a bug bounty btw... and their chrome extension has quite a large surface area...)If OP is here: can you provide the raw .gif file? (And if you&#x27;re feeling generous, maybe even a minimal ruby example that replicates that templating setup, although it sounds like that wasn&#x27;t required to reproduce it in the end.)P.S. \"For security reasons, we do not have Chrome crash reporting enabled\" - maybe consider disabling Grammarly extension for the same reasons ;) reply saulpw 12 hours agorootparentIt wasn&#x27;t the Grammarly extension, it was the desktop app. reply chatmasta 11 hours agorootparentI guess I just assumed the extension was installed too, and communicating with the desktop app. But now I see the post doesn&#x27;t mention the extension. If it was triggered even without the presence of the extension then that&#x27;s quite strange, and even more suspicious - is that gif triggering a call to a localhost endpoint? Is the grammarly desktop app interacting with browser elements without using the extension? (IIRC the grammarly app uses some accessibility privileges to inject into textareas across all apps)Grammarly is honestly insane, I can&#x27;t believe corporations allow it to run on employee machines. reply nonethewiser 10 hours agorootparent> Is the grammarly desktop app interacting with browser elements without using the extension? (IIRC the grammarly app uses some accessibility privileges to inject into textareas across all apps)It seems like that must be the case. If we have the details right about desktop app only (which seemed pretty clear). reply Sophira 11 hours agorootparentprevI&#x27;d also be interesting in seeing the raw .gif file - as a hobbyist wannabe researcher myself, I&#x27;d love to investigate this. reply jdminhbg 14 hours agoparentprevOne that&#x27;s been patched already, though, as they say that in current versions of Chrome and Grammarly it doesn&#x27;t crash. reply slavboj 6 hours agoprevGrammarly is an ipso facto security hole that is extremely likely to be pwned or controlled outright by a number of state security services. It should be treated like malware. reply jtokoph 4 hours agoparentIt‚Äôs hard blocked on any corporate devices where I work. reply fijiaarone 3 hours agoparentprevAh, Tsutomo, my learned disciple! reply CGamesPlay 10 hours agoprevI love the corporate security policy that disables Chrome crash reporting for security reasons, but allows employees to install Grammarly. reply userbinator 8 hours agoprevI agree with the others here about the ending being a total letdown.Unfortunately, with access to neither the Chrome source code nor the Grammarly source code, we can only guess.Is this what the \"open source\" movement has created --- developers who are totally lost without source code and refuse to dig deeper? Of course the corporate interests who don&#x27;t want us to know the truth, because their profit depends on it, would absolutely adore such propaganda...I still remember a time when a lot of people would disassemble, understand, and patch programs without source --- and many of them weren&#x27;t even career developers; it was just a way to get software to do what one wanted, and driven by that motivation, one would naturally learn enough to do so.The article also touches on another point worth mentioning: the amount of complexity in the whole stack is insane. Seeing all the frameworks upon frameworks being name-dropped, I can&#x27;t help but feel like a lot of this is self-inflicted.When I removed loader-spinner.gif, the placeholder we display while the menu options load, the page stopped crashing.Do menu options take long enough to load that they need a loading animation? reply hmry 53 minutes agoparentPresumably they make a network request to get the options. So it makes sense to have a loading spinner just in case it takes a long time, even if it&#x27;s usually near-instant. reply romanhn 13 hours agoprevAnother favorite bug investigation of you&#x27;re into this sort of thing: https:&#x2F;&#x2F;www.pagerduty.com&#x2F;blog&#x2F;the-discovery-of-apache-zooke... reply whirlwin 14 hours agoprevShared library like libgif used by both Chrome and Grammarly but different versions? reply Night_Thastus 13 hours agoprevI&#x27;ve actually seen something like this in the wild myself. For awhile there were some GIFs that if placed in Discord, would cause it to crash for everyone who was looking at the chat.Admins had a fun day when that was found! reply LASR 7 hours agoprevI discovered a bug like this with GIFs too. It turns out some gifs can be encoded such that it is animated and a single frame long.Can‚Äôt remember the exact event name now, but the browser fires a play event when gifs loop.This particular gif was issuing too many of these play events that made the app super slow and freeze since it was doing some work in those handlers.We had a bunch of crash reports and bug reports. None of those showed the actual gifs our customers were using. When we asked for the actual gif, we immediately spotted the problem. reply Modified3019 15 hours agoprevThe true bug in the photo is a \"candy-striped leafhopper\", Graphocephala coccinea, which is tiny but has very striking coloration.The Larvae of leafhoppers are commonly known as spittlebugs, which create protective bubble nests while feeding on plant stems reply jrockway 14 hours agoprevDid they open a bug against Chrome with the image file? I feel like any crash on user-provided data is a big deal, always a correctness problem, but potentially a security problem. \"We deleted the image so the problem is fixed for us\" is OK (I wouldn&#x27;t waste time writing a blog post about it personally), but I think that Chrome needs to fix this bug. reply tedivm 14 hours agoparentWas it actually a Chrome bug though? It only happened when the Grammerly desktop app was installed. My guess is grammerly is doing something sketchy. reply bayindirh 14 hours agorootparentMaybe adding Grammarly created enough of a lag causing the GIF file to be shown? reply nonethewiser 10 hours agorootparentThat doesnt hold when you consider they opened the gif in the browser with and without grammarly and it only crashed with. So its not simply a bad gif and chrome. reply AnimalMuppet 14 hours agorootparentprevDoes Grammerly hook something in Chrome? If not, then it&#x27;s still probably a Chrome bug, even if some second-order effect of Grammerly is necessary to trigger it. reply majormunky 14 hours agorootparentIt looks like the desktop Grammerly app hooks into all sorts of things, \"An all-in-one writing assistant that works on your desktop and in your browser. Use it in apps, word processors, email clients, and more.\" reply MattDaEskimo 13 hours agorootparentprevI&#x27;m thinking the same thing. It could be that Grammerly injects it&#x27;s own loading spinner with the same filename into the HTML.I wish they tried to simply rename the file instead of remove it. reply meandmycode 11 hours agorootparentThe pr seems to suggest it&#x27;s not the filename though given the new file was named the same and didn&#x27;t crash.I would guess grammarly is hooking chrome and potentially trying to read metadata about images, and the particular gif had metadata in a format they hadn&#x27;t expected. reply nonethewiser 10 hours agorootparentBut only in Chrome. Not necessarily inconsistent with what you‚Äôre saying, just an interesting wrinkle. replytru3_power 15 hours agoprevIs there a copy of the gif available? That‚Äôs interesting reply digging 12 hours agoparentI wouldn&#x27;t even know how to look for something unusual in a gif&#x27;s source code but I also feel this is the most compelling part. I wish they&#x27;d uploaded it. reply guessmyname 11 hours agorootparent> I wouldn&#x27;t even know how to look for something unusual in a gif&#x27;s source code but I also feel this is the most compelling part. I wish they&#x27;d uploaded it.GIF stands out as a widely understood file format [1][2].To kick things off, delve into the GIF file using a hexadecimal editor. HexFiend [3], for instance, offers a template for visualizing GIF file structures [4]. Another excellent option is Synalyze It! [5], which comes pre-loaded with an extensive list of file formats, encompassing GIF among others.These visualizations serve as a guide to pinpoint any irregular byte clusters that might pose issues when loading the file into an application with an image reader lacking support for that specific byte group or its arrangement. Once you&#x27;ve identified such a cluster, consider it the bug.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GIF#Example_GIF_file[2] https:&#x2F;&#x2F;www.w3.org&#x2F;Graphics&#x2F;GIF&#x2F;spec-gif89a.txt[3] http:&#x2F;&#x2F;hexfiend.com[4] https:&#x2F;&#x2F;github.com&#x2F;HexFiend&#x2F;HexFiend&#x2F;blob&#x2F;master&#x2F;templates&#x2F;I...[5] https:&#x2F;&#x2F;www.synalysis.net reply spuz 10 hours agorootparentAnother good option is ImHex which is an open source hex editor that supports file patterns. The gif pattern is one of the patterns already available:https:&#x2F;&#x2F;github.com&#x2F;WerWolv&#x2F;ImHex reply hilux 14 hours agoprevWhat a cool mystery-solving post! I wish all technology writing were this clear and explanatory.For another fun debugging tale, google: Mazda radio Seattle NPR bug reply EdwardDiego 8 hours agoparentHoly hell lol. reply JohnMakin 14 hours agoprevI have a saying that isn&#x27;t perfectly true but often will apply to \"fixes\" like this -If you don&#x27;t know why the fix worked, you may not have actually fixed it. reply gwbas1c 13 hours agoparentBut they couldn&#x27;t fix the bug: The bug was in another product that they couldn&#x27;t access source code or submit patches.The best they could do was work around it.Sometimes workarounds are the best you can do until your vendor provides a real fix. reply JohnMakin 12 hours agorootparentI didn&#x27;t mean literally fix the underlying bug. They also don&#x27;t really know why their workaround worked, which means it could not really be fixed at all. reply tetha 13 hours agoparentprevSomeone once said, there is accidental function, and deliberate function.If your system doesn&#x27;t work, and you just plonk around at values, until, very surprisingly, the system starts behaving well and you the call it working... well it might be working now. But it&#x27;s just accidental correctness. As soon as something causes the system to bank left, something&#x27;s gonna break and no one knows how to fix it - and you&#x27;re back to square one.On the other hand, as hard as it is, if you can clearly tell why your fix will restore function to the system without even applying it, you have deliberate correctness and function. If done right, it is very boring, because exactly and only the expected thing will happen. You should know about the unknowns and plan around those as well, so even if an unknown bites you, it&#x27;s a known and handled unknown. This can be exhausting to make happen, because it is much harder, but those systems will just work.But this is a fight I have with some development teams probably forever. \"But we poked at the values, and that stopped the flames. It is fixed!\" \"but why?\" \"Dunny. But no fire anymore. All good.\" And then 2 weeks pass, and there is more fire and everyone is like \"Oh but why would this happen? How should we have known for this to happen again\" reply saulpw 12 hours agorootparentOn the other hand, I&#x27;ve spent weeks with a team looking for a bug, and by the time we found something that appeared to fix it, we were way behind on everything else that really needed to get done. How long would it take to find the root cause? We tried. It wasn&#x27;t worth weeks or months of effort, to anyone. This isn&#x27;t JPL and human lives weren&#x27;t on the line. We just needed it not to crash so we could all get on with the \"real\" task of shipping useful and profitable software. reply tetha 11 hours agorootparentYeah, that is why software engineering and system operations is hard.For example, the article doesn&#x27;t get to a root cause in an absolute way. There is no absolute SEGFAULT of the OS causing the misbehavior. However, they nail down the crash to a gif, and if the gif is in, it crashes, and if the gif is out it doesn&#x27;t. If the gif is loaded otherwise it crashes, too. At that level, to me, that would be enough, because we&#x27;re users of the browser&#x27;s rendering there.Finding a solid cause that can demonstrate and reproduce a problem, and basing a workaround around that at a boundary you&#x27;re unwilling to cross can be fine. If it&#x27;s within the company, it absolutely is fine as long as you escalate beyond that boundary.However, I have enough teams who are like \"Oh, we set all values to 25 one by one and when we arrived at flum-value at 25 it stopped crashing. Fixed.\" Why 25? Who knows. Why flum? Who knows. Maybe the other value changed at the same time fixed it? Who knows. Do we use 26 once it starts crashing again? Fuck knows. Maybe 24 is better?We have no explanation for 25, so why would 25 be a good fix? reply derefr 11 hours agoparentprevAnd in fact, I don&#x27;t think they have fixed it. I&#x27;ve seen \"Error 5\" plenty of times in Chrome. It seemingly occurs whenever I have a lot (100+) of tabs open for any site where each page allocates at least one accelerated drawing canvas (a literal , or a , or a .gif .) I&#x27;ve seen it happen on Reddit (but only new reddit, not old reddit!) and on a number of other sites.I hypothesize that Chrome simply has a global (i.e. cross-tab) per-toplevel-origin limit to the number of allocated accelerated drawing canvases it&#x27;s willing to allow; and that when you go over it, Chrome forcibly de-allocates all the existing drawing canvases used by other tabs that have that toplevel origin loaded, thereby causing them to crash. It&#x27;s probably a measure designed to prevent a site from from DoSing your computer by just allocating an infinite number of canvases. reply nonethewiser 10 hours agorootparentThey said it also crashes when they just load the gif. It seems hard to believe they always had tons of tabs open for these tests. reply nonethewiser 10 hours agoparentprevI agree 100% and the observe this all the time with things that ‚Äúmagically fix themselves.‚Äù But Im not sure it applies here since they seemed to have actually quarantined it.I suppose since they dont know the root cause it‚Äôs impossible to say. But I think the saying would fit better if they kept the gif but made some change that seemed to fix it without knowing why. reply JohnMakin 8 hours agorootparentMaybe, but they don‚Äôt know another GIF couldn‚Äôt eventually cause the same issue again. reply bicijay 13 hours agoparentprevBut you may have reply lbhdc 13 hours agorootparentAhhh, Schrodinger&#x27;s patch. reply HPsquared 13 hours agorootparentSchr√∂dinger&#x27;s bug? reply hrtk 2 hours agoprevCan you share the `loader-spinner.gif` ? Preferably over a service that does not recompress it. reply sethammons 12 hours agoprev> The code for our main navigation bar has a fair amount of metaprogramming, and chasing down threads here was often more confusing than not.One more point for Don&#x27;t Be Clever. As Brian Kernighan put it: ‚ÄúDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.‚Äù reply NautilusWave 10 hours agoprevThis reminds me of an old Chrome bug we ran up against involving the auto fill feature. All day, our software users are entering patient data, including address information. Chrome would grind to a halt on certain pages trying to provide all their previous entries as auto fill options to the form fields. Our sysadmins disabled the Chrome feature for all of our internal users and the issue disappeared. reply user3939382 14 hours agoprevReminds me of the story I read where the guy&#x27;s car wouldn&#x27;t start depending on what flavor of ice cream he picked and when investigated he was right. Some kind of evaporation&#x2F;vacuum leak or something that was dependent on time and some flavors were farther away in the store and took more time to buy. reply superfrank 13 hours agoparentUp there with the \"I can&#x27;t send an email over 500 miles\" storyhttps:&#x2F;&#x2F;web.mit.edu&#x2F;jemorris&#x2F;humor&#x2F;500-miles reply suzzer99 5 hours agorootparentI love this one so much. reply nudgeee 14 hours agoparentprevVapor lock. Legend here: https:&#x2F;&#x2F;www.snopes.com&#x2F;fact-check&#x2F;cone-of-silence&#x2F; reply RajT88 13 hours agorootparentI heard a similar tale in high school.A friend of mine had an aunt who passed away, and so he ended up inheriting her car. The car came with a petrified apple pie in the back. He was insistent that the car would not start without the pie in the back window.Several of his friends who he played in a punk band with confirmed this, that they had tested it. Take the pie out, car won&#x27;t start. Put the pie back in, the car starts.I don&#x27;t think anyone ever figured out what was going on, I graduated a couple of months after hearing the story, and fell out of touch. But - timing and vapor lock makes sense, if they were always testing it by first starting the car, removing the pie, and then putting the pie back in.As an aside, the aunt who had passed away was one Aunt Martha (after which the car was also named), which in honor of the strange car and its strange pie was what their garage punk band was named after. There&#x27;s some totally unrelated band now called Aunt Martha - any evidence of their band is not on the internet. reply gostsamo 13 hours agoparentprevThis one is a legend. I love it, but you can find the debunking on the fact checking sites. reply trehalose 13 hours agorootparentThe Snopes page doesn&#x27;t really seem to debunk it, but merely points out that the legend&#x27;s been retold with many variations and contradictory explanations. Suspicious, definitely, but it doesn&#x27;t seem clear that none of the variations could ever have happened? reply IshKebab 13 hours agorootparentIt&#x27;s a just-so story. The null hypothesis is that it&#x27;s not true. reply sfink 11 hours agorootparentThat is the definition of null hypothesis, yes.Not to be blunt, but you might get a closer shave with Occam&#x27;s Razor. reply IshKebab 2 hours agorootparentThe null hypothesis isn&#x27;t \"it&#x27;s not true\" it&#x27;s \"what&#x27;s has the highest prior probability?\".If my wife said \"I&#x27;m going to the shops\" I wouldn&#x27;t assume she was lying until proven otherwise. replyazlev 15 hours agoprevHash collision? reply andershaig 8 hours agoprevI can&#x27;t remember the details anymore but the craziest bug I ever found was one that would crash the page whenever Chrome&#x27;s Developer Tools were opened. I ended up having to use an embedded JS-based dev tool to figure out what was going on (I was a pretty junior developer and didn&#x27;t have any better ideas at the time). reply neilv 14 hours agoprev> Using open-source Chromium instead of Chrome did not cause crashes, so we couldn‚Äôt see what Chrome code was failing either.They don&#x27;t address why they didn&#x27;t just run Chromium. Or Firefox.(This is potentially better than the &#x27;solution&#x27; they much later ended up with, in which they probably only relieved a symptom of an underlying problem that can exhibit again, and in the meantime is a zero-day exploit waiting to happen. At least, with a different browser, there&#x27;s a chance that the vulnerability doesn&#x27;t actually exist, when it&#x27;s known to exist in their Chrome configuration.) reply gwbas1c 13 hours agoparent> They don&#x27;t address why they didn&#x27;t just run Chromium. Or Firefox.The article implies that Gusto&#x27;s employees can whatever browser they want.And, honestly, telling your employees to run a browser that only techies have heard of sounds like a really dumb idea. reply neilv 11 hours agorootparent> The article implies that Gusto&#x27;s employees can whatever browser they want.For users of their security-sensitive internal software?> And, honestly, telling your employees to run a browser that only techies have heard of sounds like a really dumb idea.Sounds like they&#x27;re using this for internal tools, as a kind of thin-client layer. They could recommend or mandate a particular browser, and people would just use it. (\"Click this icon, and a window opens with our internal tool. It&#x27;s pretty much the same as any other browser, as far as you care.\") reply saagarjha 11 hours agorootparentAs an employee I would be really upset if you forced me to use a specific browser to do my work. reply laurent_du 6 hours agorootparentI have been forced to use IE for some internal apps when I was working in investment banking. It&#x27;s not that bad. reply Etheryte 13 hours agoparentprevWhat makes this doubly frustrating is that they also didn&#x27;t report the bug to Chrome. It&#x27;s super easy to do, plus they&#x27;re very responsive if you have a repro case which in this case they do. I think I&#x27;m now up to three or four Chrome bugs reported that their team has subsequently fixed. reply ncann 13 hours agorootparentThey said it wasn&#x27;t reproducible anymore though. So if they make a bug report now and say \"this used to cause a crash in an old version of Chrome while also having an old version of another software installed, but is no longer reproducible in latest builds\", most people would probably just ignore it. reply masto 13 hours agorootparentIt was reproducible at the time they found it, and trivially so: install Grammarly, drag this GIF into Chrome, and it crashes. I understood everything up to the point where they just changed the GIF and moved on without ever telling the Chrome or Grammarly folks about it. reply kube-system 13 hours agoparentprev> They don&#x27;t address why they didn&#x27;t just run Chromium. Or Firefox.Probably:1. Because it is reasonable to expect the application to work in Chrome.2. Chromium isn&#x27;t intended for production use cases.Back when IE and Chrome had about equal market share, I worked somewhere that had one team insisting that all employees must use IE for one of their applications, and another team insisting that all employees use Chrome for their application. 50%+ of support calls were employees confusing the two browsers. reply MattDaEskimo 13 hours agorootparentI don&#x27;t think the post you quoted is implying that they should&#x27;ve closed their eyes.It makes much more sense to try a different browser first and see if the problem persists. Instead of test versions and extensions. reply hbn 12 hours agorootparent> As urgency waned because our users were using other browsers as a workaround, progress on this bug slowed to make way for other priorities. We didn‚Äôt have much left to go on without being able to reproduce the bug. However, we wanted to resolve it since users had bookmarks&#x2F;settings&#x2F;preferences in Chrome. We believed that we shouldn‚Äôt have to ask our users to avoid the world‚Äôs most popular browser, and we were also still getting periodic pings from various users asking whether we had made any progress on this bug. reply kube-system 13 hours agorootparentprevThe post I quoted itself quoted the article saying they did test in Chromium. The article also says they tested Firefox and Safari. reply nabakin 11 hours agoparentprevThey say in the article that the bug became a much lower priority because their employees simply switched browsers reply mplewis 13 hours agoparentprevYou really want to change an entire company‚Äôs mandated browser every time a bug causes a problem with an extension? reply lobf 13 hours agoparentprevYou must not have read the article because he literally addresses this. reply mplewis 13 hours agorootparent*She ‚Äì the author‚Äôs name is Amy reply realmike33 14 hours agoprevThis reminds me of similar issues I&#x27;ve encountered as a software engineer. I first ran into this issue about a decade ago, albeit not because of Grammerly,but due to some specific gif causing web app to crash. Both times the gifs were animated. Happened years apart and at different companies.I see some comments highlighting RAM, which could totally have been the issue. Totally looking forward to a follow up to this later down the road, I am sure this isn&#x27;t going to be the last time we hear of this. reply crazygringo 12 hours agoprevtl;dr: a certain GIF would crash a Chrome tab, but only when the desktop version of Grammarly was installed. (Not a Chrome extension.)That&#x27;s insane!Can anyone think by what possible mechanism the installation of Grammarly could affect whether a .gif file would crash Chrome?The company seems to be on Macs since they report that the problem doesn&#x27;t surface in Safari.Is there some kind of dynamically-linked GIF decoding library used by macOS that Chrome relies upon, and Grammarly somehow installs one that takes precedence for all applications? I didn&#x27;t think this would be possible -- I thought image decoding was done natively in the browser and not outsourced to the OS, for security reasons. reply masswerk 8 hours agoparentMaybe, the GIF contains a text comment and Grammarly not only accesses this, but also alters it (for whatever reason), thus overwriting the file in a way in memory that offsets in the GIF file or its in-memory representation are violated? reply saagarjha 11 hours agoparentprevThis would be strange since macOS should not let Chrome load libraries that aren‚Äôt signed by Google. reply ja5087 12 hours agoprevWe used to develop software that used the Windows Accessibility APIs (UI Automation). On certain versions of Excel with some files it would crash the client application with a null pointer exception once you try to read the window name&#x2F;class. It would be interesting to see the cause of the crash e.g. a core dump&#x2F;user-mode dump&#x2F;event viewer log. reply mgaunard 14 hours agoprevThe main reason for tabs to crash is running out of RAM.Never do you see the guy investigating memory usage, which is weird. reply marcellus23 14 hours agoparentYeah, that would have been a good first step, but he does admit to not really knowing much about browsers:> This was fairly far outside the usual scope of our on-call issues. Our team is generally well-insulated by other teams from issues like browser compatibility, so I didn‚Äôt know the first thing about browser debugging. reply gwbas1c 13 hours agoparentprevIf that were the case, I think the bug would be much easier to reproduce; and be a lot more widespread. reply nialv7 12 hours agoprevI feel the author might have missed out on a multi-thousand dollar bug bounty. reply 6510 1 hour agoprevnext [‚Äì]alert(banana++)Should lock things up and not continue until OK is clicked.If it is just you and your application you can just spray paint from the hip the alerts from top to bottom. After each line of html you can have one more alert(banana++), in the middle of your css alert(banana++) etcIf there are uhh thousands(?) of people actively using the page you put just one alert some place in the middle.Everyone will have to click on OK, the page crashes. You ask the crashee: Did it crash before or after the \"make America great again\" alert message. (call it something they would remember)Now you know the issue is in the top or bottom half. You move the alert to half way the half with the issue in it. If you can get some sort of reasonably consistent crashing you will find it pretty quick even in production.Hope this helps, or maybe it is a stupid idea and you could explain why. reply jongjong 6 hours agoprevGrammarly is awful. I remember adding custom grammarly attributes to sensitive password fields so that Grammarly would not read and store our users&#x27; passwords inside their service. So nasty.This company would sure make a great asset for the FBI, NSA and CIA given that they&#x27;re so interested in snooping in on foreign language speakers; which happen to be grammarly&#x27;s main demographic. The thing is malware. reply rootsudo 12 hours agoprevFor security reasons, your organization disables Chrome crash reports but allows the use of Grammarly, an app that essentially functions as a keylogger. Consented or not, it‚Äôs a keylogger.https:&#x2F;&#x2F;support.grammarly.com&#x2F;hc&#x2F;en-us&#x2F;articles&#x2F;360003816032... reply jiveturkey 13 hours agoprevI&#x27;m guessing this is the webp bug.The auto conversion to webp on the backend, signaled by chrome, resulted in a bad image that crashes the browser due to grammarly parsing of said bad webp.Safari doesn&#x27;t tell the server it does webp and so it downloads the actual gif, and doesn&#x27;t crash. reply saagarjha 11 hours agoparentGIFs don‚Äôt get automatically converted to webp files. reply jiveturkey 10 hours agorootparentsays who? any web server (nginx, whatever) can be told to do this reply saagarjha 8 hours agorootparentYes, but why would it? reply jiveturkey 3 hours agorootparentbecause it&#x27;s super convenient, and improves performance? replyhcrean 12 hours agoprevI wonder if they checked for exploit code in the image the they likely originally found somewhere on Google. reply lantry 10 hours agoprev> Turbo is a gem we added to speed up our Rails application, but it turned out to be a red herring: it was only introduced after the bug had already been reported, and we learned that the engineer who introduced it had actually been experiencing these Chrome crashes for months prior to Turbo being introduced, and months prior to the bug being escalated to us.lolol I can just imagine the engineer working on this: \"huh, it keeps crashing, that&#x27;s weird. Oh well, it was like that when I got here. ship it\" reply lloydatkinson 15 hours agoprevDiscord suffered (suffers?) from a similar thing with gifs. It is or was common for people to post specifically crafted gifs in channels, anyone viewing the channel immediately had their client crash.Discord client uses Electron, which is in turn Chromium. reply rvanlaar 12 hours agoprevAh, Chrome and slow spinners.Python tests were taking ages on VSCode due to an SVG spinner:https:&#x2F;&#x2F;bugs.chromium.org&#x2F;p&#x2F;chromium&#x2F;issues&#x2F;detail?id=103626...https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;vscode-python&#x2F;issues&#x2F;9216 reply andrewfromx 13 hours agoprevoh and there&#x27;s this one: https:&#x2F;&#x2F;web.mit.edu&#x2F;jemorris&#x2F;humor&#x2F;500-miles reply 1letterunixname 7 hours agoprevSpoiler alert But if you read the article... ;@)Improper animated GIF decoding is potentially a bug and potentially a security vulnerability. The reality is this leads to duck-tape workarounds and greater tech debt on the production side of the web. Sigh. There are standards, there are expectations, there quirks across N implementations, and MxN layers of duck tape in M consuming implementations. Just one implementation intentionally or unintentionally being different causes M headaches.Also fun Chromium-derived browserisms:- Updating while open is allowed and leads to silent and not-so-silent breakage.- MSFT MDE causes unexpected breakage in fun ways every now and then, including the cause of a crash while dragging a tab on Windows.- Some flavors of Chromium browsers are broken with IPv6 enabled, leading to an ERR_NETWORK_CHANGED on every nth page visit. reply simion314 15 hours agoprevI am using chromium to print web pages to pdf, and I have some images that will crash chrome&#x27;s to pdf process, I found nothing wrong with this images, the metadata is fine (nothing weird in it). The other bad thing it does not reproduce n my dev machine only on the production server , so nothing I can do, in rare cases an image will always crash crhomium, I find it, open it and re-export it and then it works. reply LgWoodenBadger 14 hours agoparentThis sounds more like a hardware fault than something wrong with the software, especially since it doesn&#x27;t seem to be deterministic.But stranger things have happened, and given the enormous surface area of a modern computer (hardware, software, drivers, state, etc.) can anything truly be deterministic? reply saagarjha 11 hours agorootparentMost bugs are software issues, though. reply simion314 13 hours agorootparentprevIt happens with that image no matter what. I can have a html with 100 images and one bad image, I make one new html only with that image and it still has the problem. My guess is that probably a bug in a low level image decoder. My local machine has different kernel, different libs, plus I have different cpu,gpu and X11 on top so too much difference and I do not have the expertese to do aremote debug(or local) reply saagarjha 11 hours agorootparentIf you can grab a crash log I am sure the Chrome team would be happy to look at it. reply hartator 13 hours agoprevMaybe the name was odd `loader-spinner.gif` reply boringuser2 12 hours agoprevThe blink debug logs would probably be pretty useful for the engineers involved... reply ecshafer 13 hours agoprevI don&#x27;t think I would believe myself if I found this being a specific gif. This is a great amount of coincidences in code to cause this.Grammarly is an application that I don&#x27;t get. The fact that people are installing, basically spyware, on their computers just to get grammar suggestions to make their writing more boring and add a spellchecker (which is already inside web browsers) is pretty astounding to me. The fact companies allow employees to have it, despite obvious security issues of sending everything one types to a saas, is even more wild. reply thaumaturgy 12 hours agoparentPeople that are comfortable with text-based forums may not realize the extent of illiteracy and semiliteracy in the US. Decades ago, a small company was able to convince most of the education system (public and sometimes private) to use a teaching method based on junk science. The end result is that there are many millions of adults in the current workforce who can barely read, and many of them work in office settings. Some of those would install anything that would help them through text-based communications.[1]: \"How a flawed idea is teaching millions of kids to be poor readers\" https:&#x2F;&#x2F;www.apmreports.org&#x2F;episode&#x2F;2019&#x2F;08&#x2F;22&#x2F;whats-wrong-ho...[2]: \"Sold a Story: How Teaching Kids to Read Went So Wrong\" https:&#x2F;&#x2F;features.apmreports.org&#x2F;sold-a-story&#x2F;[3]: \"According to the U.S. Department of Education, 54% of U.S. adults lack proficiency in literacy, reading below the equivalent of a sixth-grade level\" https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;todayilearned&#x2F;comments&#x2F;rqulik&#x2F;til_t... reply akira2501 12 hours agorootparentIt&#x27;s odd to read the story of an adult who believes they&#x27;re a poor reader, still to this day apparently, because of what happened to them 30 years ago. Odder still that the article leaves itself the only conclusion of going all the way back to grade school and trying an entirely different strategy and hoping that just \"works out\" in the end.The lack of \"continuing education\" in the era of the internet is baffling to me. reply thaumaturgy 11 hours agorootparentI think about this a lot, too. My academic interests are pretty broad, and I could improve in every subject, so why don&#x27;t I? I think there are two reasons: a lack of focused effort, and the steadily increasing demands of adulthood.I do reasonably okay at self-guided education when I want to, but there&#x27;s definitely a difference vs. a structured secondary education environment, where there is accountability and other people to guide each other through the process. And, that&#x27;s coming in to those subjects with already a better-than-average literacy and numeracy; I have to expect that for people who struggle with grade school reading comprehension or math, trying to bootstrap those abilities alone would be daunting.Also, there&#x27;s just less room for pursuing those now. Lots of people are getting squeezed by concerns that aren&#x27;t part of most childrens&#x27; awareness -- housing costs, bureaucracy, the treadmill of maintaining all the machines that get us through daily life. Those add enormous pressure to dedicate more time towards professional development and \"getting ahead\", or at least not falling further behind, and that has been eroding all of the unstructured time that I would spend working my way through a textbook (or online class). People with poor literacy are probably more likely to have lower-paying positions, so all of those demands are even more severe.Not that it&#x27;s impossible. Lots of people do manage to self-educate their way out of poorer circumstances, and certainly the internet has made that far more accessible than it was before the turn of the century. But, let&#x27;s not underestimate how challenging it is, either. reply WarOnPrivacy 10 hours agorootparent> My academic interests are pretty broad, and I could improve in every subject, so why don&#x27;t I?My suspected culprits:1) The massively increased complexity of ordinary living is overconsumining our personal resources. and2) For post-GenX and later, the erasure of childhood (free-roaming & peer-only hours) sabotages[1] the reward systems (joy) that supercharge early learning.The less joy there is to facilitate learning, the more effort is required (from otherwise overly depleted resources).[1] similar to what abuse and neglect do reply djhn 9 hours agorootparentIsn&#x27;t the opposite true with regards to complexity of ordinary living? We&#x27;ve specialised so far that most people aren&#x27;t required to or even capable of sewing their own clothes, hunting&#x2F;growing&#x2F;foraging their own food, building their own shelter and furniture. Something our great grandparents would find unbelievable.Modern living is so monotonously boring and devoid of any challenge that people are find more and more creative ways to try and get an ounce of that physical and mental stimulation back in their lives (hobbies, exercise, gaming, etc). reply robocat 10 hours agorootparentprevTo reinforce your point: I have dyslexic friends and family that have learnt to write over time. Very difficult, but they have learnt because they had to for high paying jobs.One friend literally couldn&#x27;t read. He took himself through adult reading courses. He ain&#x27;t no Shakespeare, but I can now txt him and get a written reply.Tech is helping, but the underlying reason for the change is their own initiative.I would judge that none of the friends or family illiteracy was actually caused by our schooling system in New Zealand. Some people just struggle and our pedagogy will always be imperfect. Certainly I can see some failures in my own schooling that still exist and I would like to see fixed (mostly get rid of 99% of the deep crap). reply pixl97 11 hours agorootparentprev>The lack of \"continuing education\" in the era of the internet is baffling to me.It&#x27;s all about incentives. That is companies are incentivised to give continuing entertainment for ad clicks, rather than building a world of the educated that may have a better all outcome for society (but probably not the ad companies at all). reply melagonster 9 hours agorootparentprevshe had known thant she has disease, so she can&#x27;t read well when childhood. but the main point is why her normal daughter was taught same strategy by school. reply mardifoufs 12 hours agorootparentprevI&#x27;m not sure that&#x27;s specific to the US, and I don&#x27;t even think that particular teaching method has been used here in Quebec, yet we still see broadly similar literacy rates and levels.Last I checked US students rank well and are near the top in most education global rankings, so I think bad education is more of a global problem than Americans think it is. Maybe that&#x27;s outdated though, I&#x27;ll do my research. reply gumby 12 hours agorootparent> Last I checked US students rank well and are near the top in most education global rankings, so I think bad education is more of a global problem than Americans think it is.US is at the bottom of the OECD PISA rankings (as it is with life expectency too), though on a global basis you&#x27;re right (better than Morocco or Indonesia on both criteria).Shockingly Australia has fallen quite a bit from the initial PISA study where it was ranked #4, now almost as bad as USA.https:&#x2F;&#x2F;www.datapandas.org&#x2F;ranking&#x2F;pisa-scores-by-country reply mardifoufs 11 hours agorootparentHonestly what surprised me the most from your very informative link was that France is lower than the US! I&#x27;m probably biased but I&#x27;ve always considered the French education system to be quite rigorous and well rounded, with a few different education paths to fit different student profiles from a pretty early stage. Especially compared to canada, which in my experience has a rather weak and rigid curriculum.(Though I dislike the way french and European higher education in general works. You&#x27;re basically boxed in to your specific domain or degree that you often don&#x27;t even really choose and changing or switching careers is almost impossible. The choices you make in high school basically define what you can even study in, and thus what you can do for the rest of your life. I think that&#x27;s one thing the US does super well, even more so considering that degrees are less important there in the first place.) reply jacquesm 10 hours agorootparent> You&#x27;re basically boxed in to your specific domain or degree that you often don&#x27;t even really choose and changing or switching careers is almost impossible.This was true 50 years ago but hasn&#x27;t been true since the 90&#x27;s or so. France may well be the exception in this, but then again, France is an exception in many ways. reply mardifoufs 10 hours agorootparentIsn&#x27;t it still true in Germany? With the different high school tiers that can even make it impossible to enroll for a university degree? Though you are right that I shouldn&#x27;t say that Europe as a whole is like Germany or France even if it&#x27;s sometime easy to assume so haha. reply jacquesm 8 hours agorootparentGermany tends to be more focused on paperwork, there isn&#x27;t a German that is even moderately active in business that I know that doesn&#x27;t have a &#x27;steuerberater&#x27;, it&#x27;s overly complicated and the paper tends to be in the lead. Germany has fewer free professions than other EU countries as far as I know, lots of things are regulated and it can definitely be harder to switch. But it isn&#x27;t impossible and I know more than one German who successfully switched careers, even between regulated industries and academic &#x2F; business careers.In France, from what I know there is a fairly strong culture of secondary education that creates an &#x27;in-group&#x27;, not unlike what you see in the UK or the USA with their top-tier universities, and you are either &#x27;on the plan&#x27; or you won&#x27;t be able to get in unless you are of exceptional abilities and that rarely happens later in life, so I think that alone is sufficient to explain the discrepancy.In NL you can enroll in higher education basically whenever you want, quota permitting and with the intense competition for such spots from abroad by very qualified young people this too can be tricky, depending on the field. But in NL a university degree isn&#x27;t a pre-requisite for many jobs outside of academia (and teaching) itself.Anything to do with technology tends to be more merit based, and achievement there tends to trump formal education, and by the time you are 40+ that formal education tends to be weighted far less than when you are say 25 and just out of school.Other countries would add more to the pattern of variability, there is a huge difference between say Poland or Romania or the Nordics or the Baltics, further reflected in the weight that which a diploma or degree from such institution would carry, especially abroad. For instance, right now in the Baltics there is something of a brain drain happening with the younger generation moving West in droves and so as an older person it is stupidly easy to enroll in a university program. But that degree isn&#x27;t going to help you much unless you remain in the local economy and the degrees from a decade or more ago are given more weight than the ones that you get there right now because they are fairly desperate for students just to keep the departments up and running. reply Jakob 9 hours agorootparentprevThere are different tiers of highschools and different tiers of higher education.The first tiers are more targetted towards craftsmanship (e.g. arithmetic and trigonometry you can quickly do in your head, you start working earlier in life, as early as 16) while the latter tiers are more universal and abstract (e.g. math concepts that have better use for computer science, you start working much later, around 25 years old).You can switch between tiers or fetch later. reply mardifoufs 7 hours agorootparentThanks for the details! If you wanted to switch from craftsmanship to say, a more abstract field. Would you have to do the entire 10 years (ish?) of \"missed\" education?I know that here in Quebec, you can enroll in university no matter what as long as you are 21 years old and finished high school. Does that happen in Germany? Or is it rare to actually be able to switch between \"paths\"? replyAerbil313 12 hours agorootparentprevThat‚Äôs definitely outdated. Literacy rate of my ‚Äúthird world‚Äù country is %16 higher than US atm. reply Retric 12 hours agorootparentBe careful trying to compare countries or even historical numbers when standards vary. The US has a 99% literacy rate based on some metrics, but as often happens when metrics become useless the people tracking them raise the bar.Thus the US‚Äôs ‚ÄúLevel 1‚Äù literacy rate, which represents being able to follow basic written instructions, was 92% in 2014. But in 2020 the standard changed yet again to: ‚Äú54% of adults in the United States have English prose literacy below the 6th-grade level.‚Äù Noticeably being literate in a non English language suddenly doesn‚Äôt count, the prose at 6th grade level is also higher than it‚Äôs been in the past.Or as Wikipedia puts it: In many nations, the ability to read a simple sentence suffices as literacy, and was the previous standard for the U.S. The definition of literacy has changed greatly; the term is presently defined as the ability to use printed and written information to function in society, to achieve one&#x27;s goals, and to develop one&#x27;s knowledge and potential.[3] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Literacy_in_the_United_States reply mardifoufs 11 hours agorootparentprevWhat&#x27;s the functional literacy rate though? reply NautilusWave 10 hours agorootparentprevBetween the whole language approach to literacy and undiagnosed ADHD, I was very slow at learning how to read. Fortunately, my parents were able to force my grade school to put me into special ed for a couple of years and I rapidly acquired the skill. I have no idea how the teaching methods differed though, beyond classes being half the size and the special ed teacher being accompanied by an aide reply mckn1ght 10 hours agorootparentprevI didn‚Äôt see a direct mention of it in the links, but is the junk method referred to called Reading Recovery? First I‚Äôve ever heard of it, at least by name. Found it by googling the name Marie Clay that I did see mentioned in one of these links. (Annoying how hard they work to bury the lede.) reply thaumaturgy 9 hours agorootparentIf I remember right (it has been a while since I dived into this topic), Reading Recovery was one of the programs mentioned. It was supposed to be a supporting approach to teaching reading to kids who were struggling otherwise.The mainstream approach that really made a hash of things though was \"Whole Language\" learning, largely commercialized by Fountas and Pinnell, which eventually provided the program and associated materials to classrooms around the world. This program relied, in part, on guessing: if a child got stuck on a word, they would be asked to guess what word might fit in the sentence. Sometimes they were given context clues, like a picture on the page. Any efforts to associate individual letters or letter combinations with sounds was abandoned.Wikipedia has something of a watered-down overview of this at https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Whole_language, but I really recommend listening to the podcast in the second link in my parent comment (https:&#x2F;&#x2F;features.apmreports.org&#x2F;sold-a-story&#x2F;), APM Reports put together a compelling examination of what happened, and it&#x27;s explained well. (I do wish it was also available in a more typical article format.) reply natbennett 13 hours agoparentprevThere are a lot of people whose professional outcomes are meaningfully constrained by their ability produce clear business English.I know a guy who used to get inexplicable feedback about his communication that boiled down to ‚Äúwrite better.‚Äù This limited his ability to get promoted. He runs all his comms through ChatGPT and asks it to ‚Äúmake this more professional‚Äù and doesn‚Äôt get that feedback anymore. reply ianlevesque 12 hours agorootparentI get that people don&#x27;t care or understand this, but that&#x27;s also saying he cc&#x27;s OpenAI, and therefore probably Microsoft, and therefore almost definitely the NSA, on all his business communications. What a world. reply bongodongobob 9 hours agorootparentMan, I don&#x27;t know where most of the people here work, but the shit most people write about in their jobs is inconsequential and not super secret. It&#x27;s performance reviews and meeting notes. 99.99% of the workforce isn&#x27;t writing about the company&#x27;s secret sauce... Ever. Maybe director level and up. Also, most companies make physical things, not software, so there&#x27;s no secret sauce to share through email anyway. reply Georgelemental 9 hours agorootparent> most companies make physical things, not software, so there&#x27;s no secret sauce to share through email anyway.The conclusion does not follow from the premises. reply bongodongobob 9 hours agorootparentSure it does. There&#x27;s no secret to making boxes, or installing glass windows, or making frozen pizzas, or ordering gravel, or installing breakers, or paving sidewalks. Very very few companies make something that actually sets them apart via a trade secret. And sure, there might be some tricks in those examples but it&#x27;s not putting anyone out of business via being accidentally found a year from now in accidentally leaked partial training data.My last job was Director of IT and I&#x27;m absolutely sick and tired of security fear mongering. Turn on MFA for everything. Boom. You are now secured from 99.9999% of attacks. Chat GPT is absolutely the least of my concerns. reply gnulinux 11 hours agorootparentprevPeople won&#x27;t care until something major happens and after that they&#x27;ll implement some draconian half-measure that doesn&#x27;t fix anything like snooping on office WiFi. reply exikyut 10 hours agorootparentHow&#x27;s that even supposed to work, given that the average corporate ecosystem is so vastly cloud-based now that the majority of services use desktop and mobile apps that pin HTTPS certificates? reply almostnormal 11 hours agorootparentprev> [...] and therefore probably Microsoft [...]Where it will go through teams, outlook&#x2F;exchange, or O365.Not leaking data is no longer as easy as it used to be. Just some forms are more accepted than others. reply pixl97 11 hours agorootparentprevAs good as the average corporate IT security is that I&#x27;ve witnessed via my work, passing said data to NSA&#x2F;OpenAI is the least of their issues. Far less scrupulous hackers are running amok as it is. reply zztop44 11 hours agorootparentprevAbout 0.1% (0.001%??) of business communication might have adverse consequences for you&#x2F;your company if forwarded to Microsoft or OpenAI or the NSA. The rest is absolutely fine. And you‚Äôre probably already using Gmail or Android or Chrome or Exchange365 or iOS or *something* that could theoretically forward your comms to a tech company (and the security state).Compared to the alternative of having your colleagues think you‚Äôre a bit stupid just because you were raised speaking a language other than English, or your parents weren‚Äôt middle class‚Ä¶ using Grammarly or ChatGPT is a no brainer. I‚Äôd support anyone using whatever tools they can to overcome discrimination and thrive.The alternatives are:1. Educate everyone in the company to stop discriminating against people based on language ability (impossible??)2. Provide a local self-hosted version of the tools (although as a worker at RandomCorp, I would probably prefer to forward all my comms to Microsoft than to management!)3. Tell people facing discrimination to just shut up and deal with it. reply jeremyjh 10 hours agorootparentGiven the context of an engineer with such poor writing skills that they can&#x27;t be promoted I&#x27;d say the odds anything they write mattering to Microsoft or the NSA are quite a lot closer to 0%. In the only circumstances that it did matter, I&#x27;d guess you&#x27;re not unlikely to be better off with them in the loop anyway since we&#x27;re talking deep cover industrial espionage. reply natbennett 3 hours agorootparentprevI‚Äôm not sure how to explain this but Microsoft doesn‚Äôt need OpenAI to get access to business emails. reply beebeepka 11 hours agorootparentprevI&#x27;ve seen people do this on the same week as mandatory trainings featuring this exact scenario. At multiple companies reply Muromec 12 hours agorootparentprevI suspect it all started with two Ukrainian who got tired of checking how much of \"a\" and \"the\" they forgot to sprinkle into their texts. reply pavel_lishin 11 hours agorootparentI read comments online, and in my experience the most difficult writing to parse isn&#x27;t from foreign speakers who drop articles or mis-conjugate things - it&#x27;s from people whose writing is just, for the lack of a better term, bad. This is very common on places like Nextdoor or Facebook.It&#x27;s things like:- total stream-of-consciousness gibberish that could probably be assembled into a coherent statement if the writer would re-read what they wrote and edit it- A complete lack of punctuation, or even understanding of sentence and paragraph structure; at a glance, it looks like what I described above, but it&#x27;s different because there&#x27;s definitely a topic and a point they&#x27;re looking to make, but they can&#x27;t put the words together correctly.- spelling so bad, that even with context, it&#x27;s unclear what word they&#x27;re intending to use.- A wild misunderstanding of how to start and stop conversations online. (One recent example is me asking someone on Facebook if I could stop by to check out a garage sale, and a clarifying question about a term they used, only to get the response \"ok.\" Note that in their post, they didn&#x27;t specify an address beyond the name of the town they live in.)You can definitely point out flaws in the way I grew up - somewhat solitary, spending a lot of time alone in my room on a computer connected to the internet - but I think that it at least taught me how to make myself understood in written form. reply jvanderbot 11 hours agorootparent> - total stream-of-consciousness gibberish that could probably be assembled into a coherent statement if the writer would re-read what they wrote and edit itThis drives me nuts. Did anyone see this [1] on HN the other day? People in comments were springing up to defend this atrocious writing style.Make a paragraph. Make a point.1. https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38275905 reply pavel_lishin 11 hours agorootparentI&#x27;m not a huge fan of that, but it looks like poetry, and what&#x27;s more, it looks intentional. The author was going for something, and is probably aware that some folks won&#x27;t like it.That&#x27;s a whole different beast from an email I&#x27;ll get from a coworker&#x2F;neighbor where I cannot parse what&#x27;s even being asked of me, and where the writing is so confusing I don&#x27;t even know how to ask them to clarify their statement other than to tell them to start over, possibly all the way from kindergarten. reply mattnewton 10 hours agorootparentprevI think that writing style has a point in _realtime_ text chat, where you are racing to get your comment in and turn in the conversation. It‚Äôs like a way of streaming your thoughts, not unlike voice conversation - many of the defenses seemed to be talking about that. Not defending it in non-realtime scenarios though. reply pavel_lishin 10 hours agorootparentOh yeah, I didn&#x27;t even think about context switching for different communication mediums - maybe my complaints are mostly about people who are used to texting people a series of text messages, in a more conversational way, that doesn&#x27;t really translate to something like an email or a Facebook&#x2F;forum post? reply notpachet 12 hours agorootparentprevI have far more understanding and patience for non-native English speakers making those sorts of mistakes than I do for native speakers. reply Szpadel 10 hours agoparentprevthat.especially what is puzzling me is:> For security reasons, we do not have Chrome crash reporting enabled.so we do not want to have stack traces or whatever else this includes for security reasons, but installing basically keylogger that does spell check is okthere are companies that forbid using chatgpt for even html development because this could leak company secrets, but grammarly on confluence&#x2F;jira is just fine reply bongodongobob 9 hours agorootparentThe vast majority of companies don&#x27;t write any code ever. Not everyone is developing \"apps\".If you&#x27;re going to generalize \"everyone\" you need to understand your business type is a tiny tiny minority of what most people do. reply eichin 13 hours agoparentprevI worked with someone who really needed it, but we had the usual \"keep sales users as far from the actual product as possible\" organizational isolation so it worked out in practice. (For engineering, it was on the \"don&#x27;t install this in particular\" list.) reply CobrastanJorji 12 hours agoparentprevI have to wonder whether Grammarly&#x27;s \"Enterprise\" tier and its underspecified \"advanced security features\" involve installing it on-site and offering am \"all of your company&#x27;s words don&#x27;t get sent across the Internet to another company\" feature. reply _jal 13 hours agoparentprev> Grammarly is an application that I don&#x27;t get.You write like a native speaker, so I&#x27;m not surprised. But imagine having a few years of school-German, and then taking a German language job. I&#x27;d bet there would be times you&#x27;d want a writing assistant, too.There are also plenty of native English speakers who for whatever reason got a crappy education, and didn&#x27;t get a lot of writing feedback.As far as corporate security goes, you are correct, and we ban it. But I get why people want it. reply ryandrake 13 hours agorootparent> As far as corporate security goes, you are correct, and we ban it. But I get why people want it.That is what stuck out to me: Installing rando applications on your corporate computer that has access to internal stuff... Whoooaaaa Baby! That&#x27;s just a security disaster waiting to happen. It&#x27;s stuff like this that eventually leads to draconian and crappy \"Nobody gets admin access to their machines\" corporate policies coming down.Most TechCorp places I worked, if someone installed something like that on their corporate device, they&#x27;d get at least a stern talking-to and probably sent back to security training. reply generationP 11 hours agorootparentprevLearning a language at school, you will soon be better than natives at grammar. It&#x27;s the vocabulary, idioms and implicatures that will be tripping you up. Does grammarly really help with those? reply willsmith72 10 hours agorootparenthah better than a German at German grammar? don&#x27;t think so reply ozr 10 hours agoparentprevI really like Grammarly as a product, but I exclusively use their web editor. I wonder what their web vs desktop usage is. reply denton-scratch 11 hours agoprevAwww. I was really enjoying that; I like detective stories and rabbit-holes.Then we get to the punchline: \"Uh, we fixed the bug, but sorry folks; we didn&#x27;t solve the puzzle\". So I guess we&#x27;ll never know why that particular anigif crashed Crome but only Chrome, and only if Grammarly was installed (or had been installed during the same session).I hope Amy Lai lets us know if the story ever gets an ending! reply 12_throw_away 15 hours agoprevWell, this is a fascinating murder mystery that establishes 3 compelling suspects - Grammarly, Chrome, and a gif - and then just ... ends, right before the big reveal. reply fifafu 14 hours agoparentMaybe it‚Äôs because Grammarly enables full accessibility support in Chrome to be able to access all elements in the browser (similar to a screen reader). This has caused me various issues in the past (e.g. https:&#x2F;&#x2F;bugs.chromium.org&#x2F;p&#x2F;chromium&#x2F;issues&#x2F;detail?id=136448... ). However it‚Äôs probably good that the Accessibility functions get more exposure due to this. reply chrismorgan 14 hours agoparentprevThe GIF cannot be responsible: as untrusted web content, if it can trigger a crash, the responsibility lies with the local software stack. So you have only two suspects: Chrome and Grammarly. The GIF is at most an accomplice. reply sfink 11 hours agorootparentmore like a murder weapon reply chrismorgan 2 hours agorootparentThanks, that‚Äôs much better. I wasn‚Äôt at all happy with ‚Äúaccomplice‚Äù but my mind was blanking on what it should be. reply 0xNotMyAccount 15 hours agoparentprevI have a friend who worked at Gusto, and my wife tried using Gusto for her small business (they handle payroll for small business, got a big boost from the pandemic). The lack of technical resolution here is so Gusto, it hurts. reply robocat 14 hours agorootparentThat&#x27;s unfair: isn&#x27;t this is exactly how most strange bugs get \"fixed\" by most companies?It is an abnormal developer and an even more abnormal business that actually spends enough time to find the root cause of outr√© glitches. Especially when you start having to debug complex third party systems to debug them properly - requires skills and motivation plus a company that will encourage a developer to do that.The story is not specific to Gusto - it is the story of every developers life. I have chased down bugs in my OS and my browser - it is rarely well rewarded! Fixing a compiler bug should be on my bucket list! A long time ago I worked around a compiler bug by inserting a label: (I think the label prevented certain optimisations where the label was put). reply tclancy 14 hours agorootparentprevI mean, this story is a hell of a rundown of debugging. The fact they don&#x27;t have insight into the ways Chrome or Grammarly work isn&#x27;t something to apologize for. reply Andrex 15 hours agorootparentprevThey nuked my account after trying to charge an expired card three times.The customer response team was extremely quick and responsive telling me their hands were tied.Fuck Gusto. reply Kognito 14 hours agorootparentHate that.‚ÄúSorry, the system says no‚ÄùHad similar situations with PayPal and Uber recently where their support have absolutely no information or ability to take a decision.Support essentially becomes a glorified text-to-speech system. reply Maxion 15 hours agoparentprevI feel so unsatisfied reply neilv 14 hours agoprevWhy would they disable crash reporting for security reasons (which might actually help solve the root cause of their availability problem, which they never did solve)... yet run Grammarly (which I&#x27;d guess, security-wise, is less trustworthy than Google, in how they secure data themselves once they&#x27;ve inevitably stolen it from the customer)? reply Zetobal 14 hours agoparentMaybe they have the enterprise licence with grammarlys pinky swear that they won&#x27;t train on your data. reply dblitt 15 hours agoprev> For security reasons, we do not have Chrome crash reporting enabled.> We also confirmed with many of our affected users that they had Grammarly installed on their computers.Ironic. reply roozbeh18 15 hours agoparenthaha, that was questionable for me as well. It&#x27;s ok for Grammarly to read your stuff, but crash metadata is a no no. reply JohnMakin 14 hours agorootparentWelcome to security in 2023 :) reply 26 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A team encountered a bug that caused Chrome to crash when using Gusto's internal software, sparking a debugging journey to identify and fix the issue.",
      "By investigating different possibilities, the team found that a particular animated GIF was responsible for the crash when loaded on the webpage.",
      "Despite the challenges and lack of expertise, the team's determination and collaboration ultimately led to successfully resolving the problem."
    ],
    "commentSummary": [
      "The conversation covers a range of software bug topics including issues caused by the Grammarly extension, problems with regex, printing and rendering problems, translation errors, browser crashes, and security concerns related to Grammarly.",
      "Education systems, literacy struggles, and challenges of self-education are also discussed.",
      "Debates about writing styles, the use of writing assistance tools like Grammarly, and the importance of security in software applications are explored."
    ],
    "points": 488,
    "commentCount": 278,
    "retryCount": 0,
    "time": 1701368941
  },
  {
    "id": 38472198,
    "title": "European Parliament Rejects Mass Scanning of Private Messages, Protecting Digital Human Rights",
    "originLink": "https://edri.org/our-work/csar-european-parliament-rejects-mass-scanning-of-private-messages/",
    "originBody": "Home ¬ª ¬´ Resources ¬ª CSAR: European Parliament rejects mass scanning of private messages. Here is why CSAR: European Parliament rejects mass scanning of private messages. Here is why On 22 November, the European Parliament officially adopted its position on the draft ‚ÄòRegulation laying down rules to prevent and combat child sexual abuse‚Äô (CSAR). With strong support for this position from all seven European political groups, this marks a positive development for human rights in one of the most controversial draft European Union (EU) laws in recent memory. By EDRi ¬∑ November 30, 2023 On 22 November, the European Parliament officially adopted its position on the draft ‚ÄòRegulation laying down rules to prevent and combat child sexual abuse‚Äô (CSAR). With strong support for this position from all seven European political groups, this marks a positive development for human rights in one of the most controversial draft European Union (EU) laws in recent memory. EDRi has long advocated against the CSAR‚Äôs mass scanning and encryption-breaking measures proposed in 2022 by the EU executive‚Äôs unit for home affairs. We are reassured, therefore, that the Parliament has listened to the evidence and the rule of law. At the same time, we are still far from the end of the legislative process. This means that we must stay alert to how the other two law-making institutions ‚Äì the Council of EU Member States and the European Commission ‚Äì respond. Will they agree with the Parliament that new EU laws need to respect fundamental rights? Or will they double down on ‚ÄòChat Control‚Äô? The position of the European Parliament As we explained when this position was provisionally agreed by the Parliament‚Äôs Civil Liberties Committee on 14 November, it is a clear political statement that even the most important societal aims do not justify measures at any cost. EU fundamental rights law requires that limitations on people‚Äôs rights are necessary for the aim they seek to achieve ‚Äì including being objectively effective and the least intrusive possible ‚Äì and proportionate. That means that their broader impact must be reasonable. On this basis, the Parliament firmly rejected rules which would force companies to scan huge volumes of people‚Äôs private messages ‚Äì instead now requiring there to be reasonable suspicion. Lawyers for the Council of EU Member States had previously made an unprecedented warning that the original proposal would violate the essence of the right to privacy. In EU-speak, this is a damning assessment, because case law from the Court of Justice of the EU has always upheld that while rights can be limited for justifiable reasons, the ‚Äúessential core‚Äù of any human right must never be violated. The Parliament has clearly listened to this warning. EDRi‚Äôs work and coalition Since before the home affairs unit (DG HOME) first put forward this law, EDRi has been on the front lines urging the EU to ensure that measures to tackle the serious crime of child sexual abuse are in line with human rights rules. Yet our ‚Äò10 principles to defend children in the digital age‚Äô were ignored in the original legislative proposal (so too were concerns from the Commission‚Äôs own review board). Thanks to the EDRi-led Stop Scanning Me campaign, thousands of people across Europe have since sounded the alarm about the draft measures. Scientists and researchers across the world have been unambiguous that as proposed, the measures would undermine encryption, putting everyone‚Äôs digital information at risk of harm. And other stakeholders such as journalists, youth activists, lawyers and survivors‚Äô associations have warned how they could be put at risk by the proposal. What‚Äôs next? As their position has been officially adopted, the European Parliament is ready to enter ‚Äútrilogues‚Äù. This is closed-door negotiations between lead Parliamentarians and the Council of EU Member State governments. However, in this case, the Council does not currently have a negotiating mandate with which to enter the trilogues. In fact, EU Member State governments have been divided on the issue, with some countries refusing to listen to technological and legal reality. Fortunately, many others have stood up against their colleagues, rightly warning that the EU cannot give a carte blanche to the destruction of digital security, privacy and anonymity. Back in July, EDRi urged Member State governments not to agree to a position which would usher in the mass surveillance of everyone‚Äôs digital private lives. People subsequently took to the streets in Germany, Sweden and several other locations to urge their governments not to accept ‚ÄòChat Control‚Äô. These efforts paid off, with the governments of Germany, Austria, Poland, Estonia and Slovenia reportedly taking a firm stance against the misguided proposal, and France subsequently raising major concerns. Without a Council position, the legislative process for the CSAR is currently in limbo. The Spanish Presidency of the Council is reportedly attempting to push through a position before the end of their mandate (end December 2023). But at the time of writing, they do not have a text on the table ‚Äì let alone political agreement from a sufficient number of Member States. Even if the Council is able to agree their position soon, it is highly unlikely that they would be able to pass the law during this political mandate. That‚Äôs because we are approaching a once-in-five-years event: European elections. In June 2024, a new Parliament will be elected, and subsequently, a new set of European Commissioners will be appointed. According to leaked documents, this means that any negotiations between the legislative institutions must wrap up by early February 2024. Whilst trilogue negotiations on even simple laws can take months, it would be unprecedented for such a complex and sensitive file as CSAR ‚Äì with so much at stake ‚Äì to be pushed through in such a short time. This current limbo also means that the political figurehead for the CSAR, controversial Swedish Commissioner Ylva Johansson, is unlikely to remain in post for this proposal‚Äôs life-cycle. EDRi has consistently advocated that laws to tackle CSA online must be in line with fundamental rights law and with objective evidence of effectiveness. With this latest step, the European Parliament has firmly rebutted DG HOME‚Äôs attempt to pass a law which bitterly fails on both of these counts. Whilst the next steps for this law are not clear, this is nevertheless a huge milestone for the protection of digital human rights, and we are counting on the Council not to go backwards. Join EDRi‚Äôs supporter list & become an active changemaker Ahead of 2024 EU elections, you can shape EU digital policy that will make the internet a more secure space for all.",
    "commentLink": "https://news.ycombinator.com/item?id=38472198",
    "commentBody": "CSAR: European Parliament rejects mass scanning of private messagesHacker NewspastloginCSAR: European Parliament rejects mass scanning of private messages (edri.org) 451 points by pera 22 hours ago| hidepastfavorite84 comments bad_alloc 21 hours agoFinally. As a next step, we need mechanisms to prevent the constant rehashing of these attempts to break security. Otherwise, it will get through during a crisis or via fatigue. reply mhitza 21 hours agoparentPer the article> At the same time, we are still far from the end of the legislative process. This means that we must stay alert to how the other two law-making institutions ‚Äì the Council of EU Member States and the European Commission ‚Äì respondTo be able to fight these ludicrous attempts at privacy, we must put a spotlight on those behind these proposals (lobbyists). Coincidence or not, it wasn&#x27;t transparent, but at least some journalists investigated https:&#x2F;&#x2F;privatecitizen.press&#x2F;episode&#x2F;160&#x2F; reply miohtama 17 hours agorootparentThe engineers behind the scan lawshttps:&#x2F;&#x2F;twitter.com&#x2F;echo_pbreyer&#x2F;status&#x2F;1721558597769818496Inc. people from Google. Deserve to called out.The shady politics and the corruptive US software companies that pushed for this:https:&#x2F;&#x2F;balkaninsight.com&#x2F;2023&#x2F;09&#x2F;25&#x2F;who-benefits-inside-the... reply matthewdgreen 15 hours agorootparentYour first link is a set of experts that the EU Commission consulted while developing their regulations. It does not mean those folks were necessarily \"behind\" the regulations, so I would not call out anyone on that list.Some of the folks on that list are certainly pro-scanning: it&#x27;s an absurdly biased list. But to me that&#x27;s reflective of the EU Commission having a desired policy from the start, then mainly seeking out experts who could help them achieve their goal. reply vlovich123 15 hours agorootparentprevI didn‚Äôt check all of the people but picked 2 names at random and they were policy people not engineers. reply raxxorraxor 18 hours agorootparentprevAlso, while the most egregious part might be cancelled, these type of bills often still bring along their slightly less bad, but still fairly ugly brothers.So private message scanning is off the table, we now just save meta data and build a communication graph for every citizen for the last 10 years.No idea if this bill includes such laws, but that is usually the strategy to get people distracted. reply jstarfish 11 hours agorootparentIt&#x27;s the sucker-punch of legal maneuvers. reply Angostura 20 hours agorootparentprevThe Parliament is very much the junior member compared with the Council and Commission reply toyg 20 hours agorootparentIt still has ultimate veto powers. reply Angostura 19 hours agorootparentI&#x27;ve been unable to find an instance of the European Parliament vetoing legislation reply toyg 18 hours agorootparentPlenty. Some recent examples:https:&#x2F;&#x2F;www.reuters.com&#x2F;world&#x2F;europe&#x2F;european-parliament-scr...https:&#x2F;&#x2F;agenceurope.eu&#x2F;en&#x2F;bulletin&#x2F;article&#x2F;13274&#x2F;20https:&#x2F;&#x2F;www.thejournal.ie&#x2F;emissions-trade-system-fit-for-55-...(Somewhat unsurprisingly, being currently dominated by right-wing parties, it happens often on \"green\" legislation...)It doesn&#x27;t happen every day simply because 1) MEPs typically don&#x27;t want to be seen as \"Mr. No\", and 2) plenary votes are the end of a long legislative process, involving several steps; the Commission will typically not bring legislation to the floor if it understands, in previous committees, that it will likely be voted down.The process is roughly this: EU Council (i.e. national governments) agree that \"we should really do something about X\"; the Commission drafts legislation to that effect, and brings it to Parliamentary committees; MEPs provide feedback and instructions on how to change things; Commission decides if the changes are acceptable, and if not they go back to Council asking \"is this still ok if we do it in XY way?\"; and back and forth they go, until the Commission decides to either withdraw it or put it to a plenary vote (in which case it&#x27;s typically in a shape acceptable to Parliament, because nobody likes losing). reply soco 15 hours agorootparentThe entire irony of this is that all rightwingers are winning local elections by blaming the EU for wokeism, while they do have majority in all the EU organizations so whatever gets through against their tastes it&#x27;s only through their own failures. But who cares about the truth, if the truth doesn&#x27;t get you votes at home. It&#x27;s just so disappointing that the regular Joe Voter, even though very loud about \"doing their own research\", never actually DO their own research, just swallow whatever they&#x27;re told in their bubble. reply Angostura 18 hours agorootparentprevBut if you look at the first case, for example that was a rejection a first reading - not a definitive killing off.It&#x27;s not quite clear what happens next - the Council of ministers may apparently decide to continue working on the legislation regardless of the Parliament&#x27;s vote.In other words - it is not evidence of an \"ultimate veto power\" reply toyg 13 hours agorootparentCouncil and Commission can work on whatever they want - if it&#x27;s not ultimately approved by an EP plenary, it&#x27;s not a Directive. Occasionally some governments will go ahead and introduce laws that they tried and failed to go past the EP, but that&#x27;s just national politics in action. reply tormeh 16 hours agorootparentprevThey can continue to work on it, but without the parliament&#x27;s approval it cannot become law. reply agent327 15 hours agorootparentprevHere&#x27;s an overview of the political composition of the European Parliament: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;European_Parliament#ElectionsHow do you reckon it is \"dominated by right-wing parties\"? Those parties make up about 20% of the parliament, whereas left-wing parties make up some 35% (with the rest being centrists and &#x27;other&#x27;). reply jltsiren 11 hours agorootparentYou have a weird definition of left and right.If you think that Social democrats (S&D; center-left) are \"left\", then Christian democrats and conservatives (EPP; center-right) are \"right\". Those two are the traditional mainstream left-wing and right-wing groups in Europe. With these, we have 141 seats for the left and 178 seats for the right.Then we have more radical parties with a clear position on the left-right axis. The inconveniently named The Left in the European Parliament have 37 seats, while their right-wing counterparts are ECR (66 seats) and ID (60 seats). This brings the total to 178 seats for the left and 304 seats for the right.There are also two centrist-groups: Greens&#x2F;EFA (72 seats) and ALDE (102 seats). The former is a weird amalgamation of greens, regional parties, independents, and pirates ranging from left to center. The latter consists of center to center-right parties that usually have some connection to the liberal tradition. But in some cases, the party in ALDE is more conservative and less liberal than their national counterpart in EPP. If we include these centrist groups in the calculations, the balance shifts further to the right.Finally there are 49 MEPs outside the major parties, bringing the total to 705. reply toyg 14 hours agorootparentprevI guess your definition of \"right-wing\" is a bit different from mine. \"Centrists\" in post-WW2 Europe are largely conservative: fundamentally religious, pro-business, anti-immigration. That, to me, is right-wing - respectable, not touting nazi tattoos (mostly), but still fundamentally reactionary in nature. Those blocs are usually allied with \"liberal\" parties, a term which in Europe carries right-wing connotations because \"liberalism\" is meant in the original economic sense: free-trade, unbridled capitalism, etc. Occasionally they ally even with ultra-right parties, which often include real neofascist &#x2F; neonazis.If you consider them like that, traditionally-conservative parties account for over 65% of current MEPs. reply agent327 12 hours agorootparentPredictably, someone comes along to argue that anyone who is not fully in the left must therefore be right-wing. Don&#x27;t you see that the word &#x27;centrist&#x27; indicates people who are in the center, and therefore by definition not right-wing?Oh, and that &#x27;mostly&#x27;? Take it down a notch. There are no actual nazis in the European Parliament. reply toyg 11 hours agorootparentNeonazi maybe not (yet), but neofascists for sure: https:&#x2F;&#x2F;www.tandfonline.com&#x2F;doi&#x2F;pdf&#x2F;10.1080&#x2F;23248823.2023.22...The \"centrism\" framing, btw, is fundamentally useless. In postwar Europe, PSE parties are left-wing and PPE parties are right-wing; other parties are fundamentally defined by their primary relationship with one of these two. The \"centrism\" mantra is reactionary twaddle to justify one&#x27;s ideological vacuum. reply nvoeiah 11 hours agorootparentIf only EPP parties were really right-wing. I say this as an actual conservative. EPP parties generally are \"our position is whatever the left espoused ten years ago\". Which is, basically, a very progressive position. reply jltsiren 8 hours agorootparentTraditional European parties have been converging towards the center. That&#x27;s largely thanks to the EU, which is a centrist project founded on ideas such as social liberalism and pro-market policies. Social democratic parties have also become pretty right-wing by their traditional standards, largely due to Third Way politics that have been dominant since the late 90s. replyorwin 13 hours agorootparentprevEPP is a right-wing party. Its basically a mix of christian democrats (basically catholics), conservative (Les Republicains, amny others) and some liberals-conservatives (pro free-trade, anti union, pro-immigration if it makes labor cheaper, but also really conservative on according right to those migrants). It is also pro EU, in a weird way (Forza Italia is a member).ALDE-PACE is basically Emmanuel Macron&#x27;s party, so more socially liberal, and by that i mean he does accept that gay people do exist and can do whatever they want, if they want (the bar is low). They also are very pro-immigration in sectors that boost economies, but accept that immigrant workers can have equal rights. Extremely pro-Europe. I&#x27;d call them right-wing, but to be fair, only its leader is, most party members are pretty much center, center-right (they would be liberal-democrat in the US), and they push a lot of the legislation the greens want to pass, for multiple reasons (the green are seen as an \"acceptable compromise\", citing an EPP member i ate with).I would not call the current Green left-wing either, its a torn party. I guess after the Covid and last summer, the wars and the resulting immigration, a lot of young people joined, and politically active young people are more left-wing, but the leaders are more center, center-left. But they hold major power on the left and can work with the other center party, and sometimes even the EPP. They are also on point (and have&#x2F;propose good formations) with privacy and civil liberties, which might seems left-wing if you&#x27;re in the US, but to me it&#x27;s basically to political proposition of the old french party \"les radicaux\" which was so much in the center they split in two 30 years ago). reply uxp8u61q 13 hours agorootparentprev\"Centrists\" are right wing in European politics. reply zajio1am 17 hours agorootparentprevOnce i read an analysis comparing EP legislative action to action of national parliaments and it said that EP has much higher rate of rejecting legislation.It makes sense - in parliamentary democracy, the coalition in government has majority in parliament and government members are often party leaders (or other important people in parties), so legislature could be pushed through parliament by party lines.In EP there is much weaker connection between government (EU Commission) and EP, which makes EP more independent. reply xvector 20 hours agorootparentprevFinally, some good name and shame. I hope someone well-funded exposes these people for who they are and turns public opinion against them en masse. reply bradley13 17 hours agoparentprevAbsolutely this. They just keep trying, one angle or another. They&#x27;ll be back for another try in a year or three, with new arguments, and having purchased a few more politicians. In the worst case, they&#x27;ll do something like the USA: institute secret programs that do whatever the heck they want, with no oversight.Part of the problem is that there are no negative consequences. Again, look at the US: Snowden reveals massive, illegal surveillance. Consequences to politicians and government officials: zero. reply freedomben 14 hours agoparentprevYep, the standard playbook on this stuff is to table it for 6 months, at which point you reintroduce. Repeat ad infinitum until it passes. If at any point there&#x27;s a crisis that can be used, reintroduce immediately. reply prox 21 hours agoparentprevSo it needs be enshrined in a constitution I wager. reply phtrivier 20 hours agorootparentWell, the trick is, a surefire way to make voters angry in large part of the continents (or, well, large part of France anyway) it so put \"Constitution\" and \"Europe\" in the same sentence - so there is not much of a place to enshrine that at the EU level.Besides, every member state&#x27;s constitution probably already has a variant of \"privacy is a fundamental right except in cases defined by law\".I will argue that we _definitely do_ want cases where privacy is not 100% respected (sadly, \"investigating crime\" is not always a red herring, newspeak, lobbying propaganda, etc...People really do that for a living, and in the common interest.)In the end, it will always be a policymaker&#x27;s job to draw the lines.What I would love to enshrine in a constitution is that \"People shall choose policymakers wisely.\". But I&#x27;m not sure of how to enforce that :&#x2F; reply dannyw 20 hours agorootparentI think the 4th amendment did a good job and is quite specific. The courts rewrote it judicially.> The right of the people to be secure in their persons, houses, papers, and effects,[a] against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized reply waterheater 18 hours agorootparentThe Fourth Amendment protects the physical, yet other amendments also address privacy in different ways. The First Amendment protects the mind and arguably one&#x27;s spirit, the Third Amendment is a specific type of physical protection, the Fifth Amendment also protects the mind, the Ninth Amendment protects the existence of privacy, and the Tenth Amendment lets states implement greater privacy protections as they see fit.Given privacy is fundamentally related to the expression of free will, it&#x27;s not surprising so much touches it. reply smolder 17 hours agorootparent> The Fourth Amendment protects the physical,That&#x27;s a narrow reading of it. \"Papers and effects\" ought to extend to our data as well, something the authors could not have called out more explicitly at the time of its writing. Call it inconvenient or impractical, whatever, but it&#x27;s ridiculous to conclude their intent was that government can spy on presumed innocents as long as they don&#x27;t make a physical mess.I think third party doctrine is also a pile of crap... and that data brokers shouldn&#x27;t have a square inch of legal ground to stand on. GDPR sets a good example in that regard. reply waterheater 16 hours agorootparentI completely agree, and computing has flipped the table on our understanding of the Fourth Amendment. In fact, our technological development and privacy concerns are proportional, which says something quite profound. Are you familiar with Kyllo v. United States and Carpenter v. United States? If not, you&#x27;ll probably find them intriguing.Third-party doctrine is indeed a pile of crap. Still, the fact remains that zero-cost (okay, effectively zero-cost) digital information breaks virtually all historical ownership models which legal systems protect. GDPR is okay, but compliance with it is so burdensome to small businesses that corporate-driven cloud infrastructure is the only way to survive. reply smolder 12 hours agorootparentI&#x27;ll look into those cases, thanks! reply wuiheerfoj 14 hours agorootparentprevWhile I agree it does a good job, ‚Äòunreasonable‚Äô is a term that‚Äôs up to the implementer reply eigenket 20 hours agorootparentprevIt essentially is. The right to privacy is part of the Charter of Fundamental Rights of the European Union, which is enshrined into law as part of the Treaty of Lisbon. reply gpderetta 19 hours agorootparentYes, the Treaty of Lisbon plus the Charter act as the de-facto EU constitution. reply logicchains 20 hours agorootparentprevIt&#x27;s already enshrined in the constitution of at least one European country, the right to privacy. reply bonzini 20 hours agorootparentIt is in Italy&#x27;s (\"Freedom and secrecy of correspondence and any other communication cannot be violated\"), but I didn&#x27;t think this was particularly unique?(Note that this article of the constitution doesn&#x27;t include E2E encryption because there&#x27;s a carve out for the judiciary to limit this freedom). reply fullspectrumdev 18 hours agorootparentWhat‚Äôs deeply funny is how Italy is also where a lot of police spyware industry got its start in Europe (HackingTeam and fellow travellers). reply AdrianB1 18 hours agorootparentprevNo rights enshrined in a constitution is guaranteed or respected, anywhere in the world. First, in come countries legislation breaching constitutional rights appear all the time and the burden is on the people to fight that legislation that is presumed valid. Second, people believe that some rights are no longer aligned with the modern times (see USA second amendment) and are willing to remove it. Third, constitutional rights that are not completely removed are brutally \"regulated\", with the same effect, and most people agree with that or actively support it.In summary, there are no rights that are guaranteed even if they are in a constitution. reply varispeed 20 hours agorootparentprevIt should be illegal to even propose this and it should be equalled to hate crime.People who think this is okay, to the point that they want to enact this in law should be cast out of the society. reply JW_00000 16 hours agorootparentIt seems dangerous to make proposing laws in a parliament illegal, don&#x27;t you think? Anything should go in a parliament. I&#x27;m also not sure hate speech could be illegal when a member of parliament speaks in parliament, typically parliamentary debate is more widely protected. reply chopin 12 hours agorootparentMy favorite solution: Those who vote for legislation found to be anti constitutional by the respective courts lose their passive voting rights and must leave Parliament immediately. reply ratg13 20 hours agoparentprevThe mechanisms are in place .. literally nobody was paying attention.They spent millions on campaigns advertising this stuff and asking for feedback only to getForbid politicians to lie to and deceive the public, that&#x27;s it.Forbidding something is never ‚Äúit‚Äù. People do prohibited things all the time.‚ÄúProhibit‚Äù is not ‚Äúmagically prevent‚Äù. reply g-b-r 17 hours agorootparentTo be clear this would not prevent every instance of lying, but the current state is in most cases manifest shameless lying or deceits that surface some years later, so it would be a stratospheric improvement, in my opinion. reply g-b-r 17 hours agorootparentprevOf course forbid with very strong penalties and no statute of limitations... reply dragonwriter 13 hours agorootparentBy whom are laws enforced, and by whom are those people appointed and to whom do they answer?Or, to take another angle, why don&#x27;t the prohibitions in FISA effectively stop the government from abusing foreign intelligence apparatus for domestic spying? reply g-b-r 12 hours agorootparentBecause the US are messed up on so many levels.Of course the judicial, legislative and executive branches should be independent and they&#x27;re not that much right now.In any case, even in such a system the proposal might have more positive than negative effects, and maybe lead to gradual improvements to everything else. reply HideousKojima 11 hours agorootparentOr it might make things even worse. After all,the FISA court system itself was created in response to abuses by the CIA and FBI (and others) as a way to check their power. Instead, it became a (secret and opaque) rubber stamp that approves over 99% of all warrant applications. reply g-b-r 2 hours agorootparentNo one is proposing anything secret and opaque replyaugustulus 16 hours agorootparentprevbesides the fact that you‚Äôre essentially running on a platform of ‚Äúmake crime illegal‚Äù, you run into the obvious problem of who decides what a lie is? who decides whether a lie has taken place? reply g-b-r 16 hours agorootparentLying is currently usually protected for politicians, not illegal.Lies are obvious in most cases, and I think there are established judicial systems to assess if a crime has been committed or not...Of course investigations and indictments have to occur only with sufficient elements to suspect a malfeasance, we&#x27;re not arguing for wiring politicians to mind readers reply Roark66 19 hours agoprevAnd that exactly is why we absolutely have to keep the veto power of member states in the EU. The only reason why they backed down is because few countries said \"there is no chance in hell we&#x27;re agreeing to this\". If we were making laws based on simple majority few biggest countries plus a couple others forced&#x2F;bought into submission could override everyone else. reply sofixa 19 hours agoparentNot simple, but qualified majority, would be better than a single country being able to stop any process. The Polish Lithuanian Commonwealth learned this the hard way, where every Sejm member could veto any legislation, so the country stagnated until it was picked apart by neighbourds. reply SiempreViernes 19 hours agoparentprevAre you saying you think the EP voted to reject the mass scanning provisions because a member state promised to vote this? reply HideousKojima 7 hours agoparentprevYet you&#x27;ll often see the exact opposite argument made against the Electoral College and the Senate in US politics reply nurple 12 hours agoprevMass surveillance is, like most absolute power, cancer of the soul. I have read your email, and your IMs; I know first-hand how it corrupts a person. I can, without hesitation, guarantee that the situation is like that in the movie Elysium[0]: \"They will hunt you to the edge of the earth for this [capability].\"The most salient point, I think, is that it is worthless, from a LEO perspective, to tap into communication systems used by the masses--whether through provider taps or client-side scanning like Apple&#x27;s purported CSAM AI--unless what you&#x27;re really after is a way to monitor the general public at large.There&#x27;s no way in hell that a nefarious player with technical resources, or chops themselves, would use one of these public systems to communicate with their compatriots. There are infinite and myriad bespoke channels of covert communications that these laws would never be able to touch which are much more likely to be the hubs of serious malfeasance.[0] https:&#x2F;&#x2F;youtu.be&#x2F;qUQQerrs52w?t=54 reply xiphias2 21 hours agoprevAs shown by recent wiretappings, politicians have the most to lose if they give their chat logs away to a smaller, unelected group of people. It&#x27;s a sure way of losing their power and becoming a puppet. reply almostnormal 19 hours agoparent> As shown by recent wiretappings, politicians have the most to lose if they give their chat logs away to a smaller, unelected group of people.Surely the creators of the policy will not forget to exclude themselves from being affected. reply logicchains 13 hours agoparentprev>It&#x27;s a sure way of losing their power and becoming a puppetThey&#x27;re already blackmailed puppets to the spy agencies, that&#x27;s why they keep pushing this stuff. There&#x27;s a reason none of the people Epstein was accused of trafficking young girls to went to jail. reply xkcd1963 20 hours agoparentprevUnless they team up of course reply TrackerFF 11 hours agoprevHere in Norway we (unfortunately) passed a law which allows our intelligence service to read all meta-data of traffic which crosses our borders.Which makes no fucking sense, as pretty much all data crosses borders now. When you use facebook&#x2F;twitter&#x2F;tiktok&#x2F;gmail&#x2F;whatever, you have zero knowledge what (geolocation) server instance the owners of those products are using.And even if one service uses one \"local\" (as in within borders) server, many of the others could very well not.This is of course in the name of fighting terrorism, which makes up for such a small percentage of all data traffic, that it might as well be ZERO.Terrorism and CP, the two things that will usher in lots of overreaching laws. reply RecycledEle 19 hours agoprevThat&#x27;s half the battle.Now the real fight begins. We need every message sent or received by the politicians who supported this absurd proposal to be public record. The public&#x27;s need to stop abuses of power outweighs privacy rights they do not value. reply goalonetwo 11 hours agoprevThat&#x27;s why we need to continue developing apps that are cryptographically open and secure.Then it doesn&#x27;t matter what the government wants, they will have simply no ways to read my texts.The issue is that only the technically and privacy savy will be able to continue to encrypt their message. The masses will happily comply and continue using facebook messenger&#x2F;whatsapp or whatever new bigtech \"cool app of the day\" full of backdoor. reply hollow-moe 19 hours agoprevAwesome day for privacy ! They&#x27;ll try again with another name in three months and it&#x27;ll pass tho reply dsnr 18 hours agoprevFor now. See you at the next attempt. reply metalrain 17 hours agoprevI mean, even if messages are encrypted in the wire, surely most platform holders will comply to law enforcement requests. Exposing your messages from databases, logs, analytics, installing keyloggers..Most of people use smartphones and there isn&#x27;t secure smartphone platform. Even on PC, all you can do is to hope nothing in the chain leaks your messages. reply techwizrd 15 hours agoprevI think the public outcry made this outcome inevitable, and this is good to see! reply ReptileMan 19 hours agoprevThere is no such thing as mass scanning. Because the cost of scaling anything digital is almost 0, every ability to break encryption or scan is mass by nature. reply tavavex 15 hours agoparentNo one would need to control every single bit that passes through the internet. The governments would just need to force most major social media and chat platforms to let them get a peek at user data, and that&#x27;d be more than enough to get information on most people. reply superjan 16 hours agoprevI regret that the discussion about message encryption is so black and white. Encryption is being used to evade prosecution for crimes such revenge porn, CSAM and criminal conspiracy. For one, I would not mind if encryption was forbidden for group chats bigger than 10 people, or that large groups are forbidden to share encrypted images. reply tavavex 15 hours agoparentHow&#x27;s that justifiable in any way but \"I feel like it&#x27;d be nice\"? Like, what&#x27;s the logical process that dictates that being in a group of more than specifically 10 people in likely to cause illegal activity? Why can I talk privately with 9 people, but not 10?My point is that all this legislation is pushed as anti-criminal because it&#x27;s the best spotlight to put mass data surveillance under. In reality, the powers that governments will reap from this ability stretch much further. Would you be okay with the exact same measures in real life? Should large gatherings of people require everyone to wear a wiretap, lest they conspire to commit something illegal? Should we mandate inspection of every postal package, just in case there are drugs or other illegal contents? reply aunty_helen 16 hours agoparentprevWould that put a stop to these types of criminal activity? (No) reply anthk 14 hours agorootparentThey would just set groups of 5 people creating rings with token connected to outside networks. Did it solve anything as you said? No. On encryption, with base64 and rotations over text you get nonsense and still plain text. reply spurgu 14 hours agoparentprevOnce you have it in place there&#x27;s suddenly a good argument for \"if 10, why not 9?\" -> \"if 9, why not 8?\" etc etc. --> 5. ---> 2. reply anthk 14 hours agoparentprevThen make everyone group >10 people go outside nude to any event&#x2F;concert, just to be sure. reply yttribium 17 hours agoprevEU governance structure is set up so that a EU parliament resolution has basically the status of a People&#x27;s Choice Award. reply cabirum 19 hours agoprev [‚Äì] In some sense, a constant pressure from the lawmakers to compromise privacy is vital for encryption to evolve.No law should be able to break into private messages, and so I think CSAR should be passed, because as a consequence, new encryption schemes would be developed to counter it. reply LtWorf 14 hours agoparent [‚Äì] I think you don&#x27;t understand what encryption is. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The European Parliament has rejected the proposal to mass scan private messages in the draft 'Regulation laying down rules to prevent and combat child sexual abuse' (CSAR).",
      "This decision is a positive development for human rights and a crucial step in the legislative process.",
      "The rejection of mass scanning is a significant milestone in protecting digital human rights and ensuring the privacy of private messages."
    ],
    "commentSummary": [
      "The European Parliament has rejected a proposal for mass scanning of private messages, indicating concerns about privacy and potential abuse of surveillance measures.",
      "Less intrusive methods may still be considered, raising concerns about the preservation of privacy rights.",
      "The discussion highlights the role and authority of the European Parliament in rejecting legislation and touches on political ideologies, the significance of encryption, and the importance of transparency and accountability in government decision-making."
    ],
    "points": 450,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1701343506
  },
  {
    "id": 38475545,
    "title": "Stanis≈Çaw Lem's 'The Invincible': Unveiling Artificial Life's Prophetic Vision",
    "originLink": "https://thereader.mitpress.mit.edu/stanislaw-lems-prescient-vision-of-artificial-life/",
    "originBody": "Stanis≈Çaw Lem's Prescient Vision of Artificial Life As with the best science fiction, Lem‚Äôs novel ‚ÄúThe Invincible‚Äù has as much to teach us about our present situations as any futures we may face. By: N. Katherine Hayles In the grand tradition of H. G. Wells and Jules Verne, Stanis≈Çaw Lem‚Äôs ‚ÄúThe Invincible‚Äù tells the story of a space cruiser sent to an obscure planet to determine the fate of a sister spaceship whose communication with Earth has abruptly ceased. Landing on the planet Regis III, navigator Rohan and his crew discover a form of life that has apparently evolved from autonomous, self-replicating machines ‚Äî perhaps the survivors of a ‚Äúrobot war.‚Äù Rohan and his men are forced to confront the classic quandary: What course of action can humanity take once it has reached the limits of its knowledge? In ‚ÄúThe Invincible,‚Äù Lem has his characters confront the inexplicable and the bizarre: the problem that lies just beyond analytical reach. The following is literary critic and theorist N. Katherine Hayles‚Äô foreword to the 2020 edition of Lem‚Äôs classic novel, which was originally published in Polish in 1964. Science fiction has famously predicted many of the important technologies of the 20th century: space travel, satellites, the atomic bomb, television, the internet, and virtual reality, to name a few. In ‚ÄúThe Invincible,‚Äù Stanis≈Çaw Lem predicts another: artificial life. Although speculations about self-reproducing artificial systems date from the 1940s, the scientific field received its name from Christopher Langton only in 1986, more than two decades after the original publication of ‚ÄúThe Invincible‚Äù (1964). One of the central controversies in artificial life is whether evolutionary programs and devices are actually alive (the strong version), or whether they merely simulate life (the weak version). Researchers who follow the strong version argue that the processes embedded in software programs such as genetic algorithms are as ‚Äúnatural‚Äù as life itself; what is artificial is the medium in which these processes take place. This foreword first appeared in the 2020 English edition of Stanis≈Çaw Lem‚Äôs novel ‚ÄúThe Invincible.‚Äù The issue prompted Robert Rosen, among others, to speculate about the essential characteristics of ‚Äúlife itself,‚Äù not only as it evolved on Earth in carbon-based life forms but also about the possibility of life-as-it-could-be in exoplanetary systems, arguing that silicon-based artificial life forms may provide insight into these theoretical speculations. ‚ÄúThe Invincible‚Äù presents a fascinating hybridization of these different views. Dr. Lauda‚Äôs hypothesis proposes that a space ship from the Lycran system landed on Regis III millions of years ago; while the biological visitors perished, the automata did not. There then followed an evolutionary struggle between the automata and the planet‚Äôs indigenous life forms, on the one hand, and between the different kinds of automata, on the other. Such a scenario requires that the ‚Äúsurvive and reproduce‚Äù mandate that governs life on Earth could also operate on this planet. Lem minimally fulfills the requirement by postulating that the automata could manufacture themselves with modifications dictated by evolutionary processes. Clearly his interest is not in filling out how this might take place (John von Neumann, encountering a similar problem, imagined metal parts floating on a lake that could self-assemble). Rather, Lem‚Äôs focus is on envisioning an artificial life form that won the evolutionary competition on Regis III for profoundly different reasons than did Homo sapiens on Earth. The effect is achieved by introducing a significant factor that has a monumental impact on evolutionary trajectories: rather than fulfilling their energy needs through ingesting food, the automata on Regis III evolve to use solar power. The smaller the artificial organism, the less energy it needs. Hence the evolutionary driver is toward smaller forms, which overcome not through superior intellect but through swarm intelligence. Lem added to this the ability of the swarm of ‚Äúflies‚Äù to generate immensely powerful electromagnetic fields, which meant that the tiny automata are not only the evolutionary winners on their planet but a powerful force against the invading humans. Their tiny size notwithstanding, their awesome potential illuminates the profound ambiguity of the work‚Äôs title, which can be taken to refer either to the spaceship‚Äôs proud name or to the swarms of alien automata that threaten it. ‚ÄúFrom a broader cosmic perspective, the best of human science, engineering, and weaponry may reveal humans to be completely out of our depth, mere kindergarteners bidding for a place in the universe‚Äôs adult civilizations.‚Äù Contemporary research in artificial life has validated Lem‚Äôs insight that swarms of artificial beings require only a few simple rules to manifest complex behaviors and hence each member needs to carry only a little cognitive power onboard. Computer simulations that have accurately depicted swarm behaviors in fish, birds, bees, and other biota demonstrate that each individual responds only to the four or five closest to it, with rule sets that take up only a few lines of code. For example, a school of fish swimming to evade a predator is guided by the fish closest to the predator. The direction this most imperiled individual follows determines how the entire school will run as it flashes back and forth, a simple strategy that makes excellent sense, since the fish that has the most to lose will try hardest to escape. Although each fish‚Äôs behaviors are simple, the collective result nevertheless generates swarm intelligence of considerable complexity. Decades before these ideas became disseminated within the scientific community, Lem intuited that different environmental constraints might lead to radically different evolutionary results in automata compared to biological life forms. Although on Earth the most intelligent species (i.e., humans) has tended to fare the best, their superior intelligence comes with considerable costs: a long period of maturation; a lot of resources invested in each individual; socialization patterns that emphasize pair bonding and community support; and a premium on individual achievement. But these are not cosmic universals, and different planetary histories might result in the triumph of very different kinds of qualities. The contrasts between humans and the automata swarm are brought out most poignantly in the scene between Captain Horpach and First Officer Rohan, in which the captain delegates to Rohan the decision whether to put another crew member in grave danger to determine if the missing four men have indeed perished, as seems all but certain, or whether one or more might still be alive. The assumptions that make this gamble even remotely worth taking are revealing: human life is precious; human solidarity depends on the crew‚Äôs belief that everything possible will be done to save them if they are in peril; and every human is unique and therefore uniquely valuable. None of these, of course, holds true for the swarm, whose individual members are virtually identical to one another, with each tiny automaton easily replaced and therefore disposable. Consequently, none is valuable in itself; only the swarm has evolutionary survival value. The contest, then, is not only between different life forms but also between the different values that have resulted from the divergent evolutionary pathways of humans on Earth and the ‚Äúflies‚Äù on this strange planet. As with ‚ÄúSolaris,‚Äù Lem suggests that assumptions born and bred of Earth may appear hopelessly provincial in light of human encounters with radically different life forms. From a broader cosmic perspective, the best of human science, engineering, and weaponry may reveal humans to be completely out of our depth, mere kindergarteners bidding for a place in the universe‚Äôs adult civilizations. The reduction of crew members to infancy when attacked by the ‚Äúflies‚Äù may be a metaphor for this realization. Of all the human characters, Rohan has the strongest claim to have encountered the planet on its own terms. He has traversed its terrain with his own feet; he has mixed his sweat with its crevices, valleys, and hills; he has breathed its native atmosphere into his lungs. The insight he gains from his heroic trek therefore commands our respect. When he concludes that ‚Äúnot everything everywhere is for us [humans],‚Äù we are right to hear in this pronouncement Lem‚Äôs own challenge to the anthropocentric assumptions that continue to dominate human ethical frameworks as well as human exploitations of planet Earth. As with the best science fiction, ‚ÄúThe Invincible‚Äù has as much to teach us about our present situations as any futures we may face. N. Katherine Hayles is Distinguished Research Professor of English at the University of California, Los Angeles.",
    "commentLink": "https://news.ycombinator.com/item?id=38475545",
    "commentBody": "Stanis≈Çaw Lem&#x27;s vision of artificial lifeHacker NewspastloginStanis≈Çaw Lem&#x27;s vision of artificial life (mitpress.mit.edu) 397 points by axiomdata316 17 hours ago| hidepastfavorite141 comments ajuc 16 hours agoLem has a book about almost any subject related to science&#x2F;philosophy you can imagine.Here he writes about ChatGPT :)https:&#x2F;&#x2F;mwichary.medium.com&#x2F;one-hundred-and-thirty-seven-sec...Here as well: https:&#x2F;&#x2F;electricliterature.com&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;11&#x2F;Tr...And here about Ebooks and Audiobooks:http:&#x2F;&#x2F;www.technovelgy.com&#x2F;ct&#x2F;content.asp?Bnum=1024 reply cubefox 14 hours agoparentHe was actually more explicit elsewhere. Lem&#x27;s book Imaginary Magnitude consists of a collection of prefaces to nonexistent books. One of them is for \"Juan Rambellais et al., A History of Bitic Literature, Volume I\". The \"bitic\" literature consists of novels written by, well, language models. You would feed in the combined work of Tolstoy, and out comes a new \"Tolstoy\" novel.When I first read this \"preface\" twenty years ago, the idea seemed implausible to me: How could a system write novels only by being fed other novels, and without simultaneously being a general intelligence? Surely novel writing is AGI-complete! reply prithee 13 hours agorootparentThis quote from \"Non Serviam\" section of \"A Perfect Vacuum\" by Lem also hints at future stochastic parrots argument.The machine will employ, as the need arises, the pro- noun \"I\" and all its grammatical inflections. This, however, is a hoax! The machine will still be closer to a billion chattering parrots‚Äîhowsoever brilliantly trained the parrots be‚Äîthan to the simplest, most stupid man. It mimics the behavior of a man on the purely linguistic plane and nothing more. Nothing will amuse such a machine, or surprise it, or confuse it, or alarm it, or distress it, because it is psychologically and individually No One. It is a Voice giving utterance to matters, supplying an- swers to questions; it is a Logic capable of defeating the best chess player; it is‚Äîor, rather, it can become‚Äîa consummate imitator of everything, an actor, if you will, brought to the pinnacle of perfection, performing any programmed role‚Äîbut an actor and an imitator that is, within, completely empty. One cannot count on its sympathy, or on its antipathy. It works toward no self-set goal; to a degree eternally beyond the con- ception of any man it \"doesn&#x27;t care,\" for as a person it simply does not exist.... It is a wondrously efficient combinatorial mechanism, nothing more. reply r618 10 hours agorootparentit would be maybe interesting to know how was Lem thinking about the actual algorithmic process the machine uses while writing this:- purely symbolic with myriads of symbolic (if-else) clauses similar to expert systems, or- sub symbolical ANN networks similar to current LLMsi suspect it was the former in which case that he actually arrived at description of current large NNs is really striking I think reply ajuc 10 hours agorootparentDunno about this fragment, but in general in 60s he usually wrote about AI as \"cybernetic electronic brain\" or \"cybernetic black boxes\". Cybernetic in the old sense - not the cyberpunk implants, but the analog devices with feedback loops.He wrote A LOT about this and explored various consequences, including the simulation argument which he presented in 1960 as a short story \"Strange chests of professor Corcoran\" - https:&#x2F;&#x2F;przekroj.org&#x2F;sztuka-opowiesci&#x2F;dziwne-skrzynie-profes... here&#x27;s the Polish version). reply jareklupinski 6 hours agorootparentprev> One cannot count on its sympathy, or on its antipathy.ok but i&#x27;m still going to thank chatgpt just in case reply mirekrusin 12 hours agorootparentprevWow, that&#x27;s accurate (1971). reply jon_richards 13 hours agorootparentprevThat&#x27;s basically the entire history of artificial intelligence. We used to think a robot capable of vacuuming your house would be \"AI\" and now roombas just bounce around the floor semi-randomly. The task didn&#x27;t change, our respect for it did.At this point the definition of AI is practically \"Something computers can&#x27;t do yet\", though I&#x27;m partial to its corollary \"Any sufficiently misunderstood algorithm is AI.\" reply coldtea 8 hours agorootparent>We used to think a robot capable of vacuuming your house would be \"AI\"Only because we thought such a robot would be like a AGI servant, not just a single-purpose device like a roomba that can just bounce around the floor.So, it&#x27;s not like we&#x27;ve changed our definition of AI (and even less so, AGI). What we did change is what a robot house-cleaner product is (less \"C3PO with a broom\", and more \"single-purpose vacuum cleaner with heuristics to bounce around\").Even for chess playing, when people in the past thought a chess playing machine that would be able to defeat the human champion would have AGI, they did so not because they thought playing chess is enough to signify AGI, but because they thought AGI was necessary to do so.If someone had explained to them back then that such a future machine would be able to play expert-level chess by mere number crunching of a huge list of moves, and that it wouldn&#x27;t imply any other thinking facultu, they wouldn&#x27;t consider that to be AGI.>At this point the definition of AI is practically \"Something computers can&#x27;t do yet\", though I&#x27;m partial to its corollary \"Any sufficiently misunderstood algorithm is AI.The practical definition of AI (as used colloqualy, in the market etc) for products is basically \"any smart-looking algorithm, with heuristics to do something slightly complex\".It&#x27;s just that the term is overloaded, and we sometimes say AI when we mean AGI. reply hahajk 13 hours agorootparentprevI heard Vint Cerf describe the original AI algorithms as basically \"heuristic algorithms\". \"AI\" was used to described a class of algorithms that only worked some of the time as opposed to the mathematically proven algorithms. reply coldtea 8 hours agorootparentprev>When I first read this \"preface\" twenty years ago, the idea seemed implausible to me: How could a system write novels only by being fed other novels, and without simultaneously being a general intelligence?Well, even 20 years ago, a simple Markov Chain Tolstoy-based output could very well pass at least for a modernist style novel. reply baxtr 11 hours agorootparentprevWhich of his books do you recommend to start with? I haven‚Äôt read any yet. reply johngossman 11 hours agorootparentCyberiad or Star Diaries are good starts for the humorous (but still philosophical) stuff. Solaris is his best known work, serious, and excellent. It‚Äôs tricky because Lem is almost two authors, one serious and one ironic leading to zany. Everything is short, so easy to just try. reply cubefox 10 hours agorootparentYeah. I would also recommend the Cyberiad and Star Diaries to start. Both are slightly interconnected short story collections with a lot of humor. The Futurological Congress is a complete (but short) novel and the first follow-up to the Star Diaries, it&#x27;s also a good place to start.For Solaris (a serious novel) I would recommend the new Kindle translation, if you are reading in English. The old one was a retranslation from French for some reason. reply te0006 9 hours agorootparentprevAnd whatever you select otherwise, definitely be sure to not miss Lem&#x27;s \"Golem XIV\": Lectures by what nowadays is called an AGI to Humanity, short before that entity evolves to an even more advanced level - from which further communication with humans will no longer be possible, and that likely operates outside the physics known to us. Stunning, especially when considering it was written in 1973-1980. Seemingly, the English translation of Imaginary Magnitude contains Golem XIV in its entirety; in German and other languages it was published as a separate book. reply isk517 10 hours agorootparentprevI read Solaris and Cyberiad back to back without realizing they were by the same author. I&#x27;d recommend both. reply mcv 16 hours agoparentprevAll the hubub about ChatGPT made me think a lot about Trurl&#x27;s poetry machine from The Cyberiad. Especially Nick Cave and others declaring it the death of creativity, compared to the protesting poets from the story. reply gattr 14 hours agorootparentWhen I got older, I realized the sneakiness here (shortening and quoting from memory):> Trurl realized that a poet&#x27;s mind is shaped by the civilization that produced him, and that civilization - by the one before it, and so on. So in order to create an artificial poet, he&#x27;d have to simulate the whole history of the Universe. Which he did, but then from the results chose only the important bits as inputs for the learning process, otherwise it would take too long... reply ajuc 14 hours agorootparentHe almost described prunning :) reply pbhjpbhj 13 hours agorootparentSeems to me more like selection of entries in the training corpus than pruning. reply coldtea 8 hours agorootparentYeah, more like curated training. reply johngossman 15 hours agorootparentprevMe too. And I recently learned that Calvino wrote an essay that included the idea of a poetry writing machine. Now I&#x27;m super curious whether Lem got the idea from Calvino, vice versa, or whether it was independently invented by each. I know they read each other.https:&#x2F;&#x2F;lab.cccb.org&#x2F;en&#x2F;did-calvino-dream-of-literary-androi... reply lioeters 12 hours agorootparentThe idea of an artificial person goes back to pre-science (\"magic\"), ancient history and mythology.> Arguably the oldest known story of something approximating AI can be found in the eighth-century-bc Iliad, Homer‚Äôs epic poem of the Trojan War. In it, Hephaestus, disabled god of metalworking, creates golden handmaidens to help him in his forge: ‚ÄúIn them is understanding in their hearts, and in them speech and strength, and they know cunning handiwork‚Äù.> ..In her 2018 book Gods and Robots: Myths, Machines, and Ancient Dreams of Technology, Adrienne Mayor describes how ancient cultures explored the idea of artificial life. Ancient Greeks were skilled in metalwork and mechanics and created a great deal of automata, including a puppet theatre that could perform an entire play.The idea of a poetry-writing machine can also be traced back in history, hinted at in the Kabbalah, or machines of ancient China. (Source?) In the work of Leibniz, \"On the Combinatorial Art\" (1666), he explores:> The main idea behind the text is that of an alphabet of human thought, which is attributed to Descartes. All concepts are nothing but combinations of a relatively small number of simple concepts, just as words are combinations of letters.> All truths may be expressed as appropriate combinations of concepts, which can in turn be decomposed into simple ideas, rendering the analysis much easier. Therefore, this alphabet would provide a logic of invention.. reply mcv 13 hours agorootparentprevMy theory is that he started with some silly poems and wrote the rest of the story around them. reply MichaelMoser123 7 hours agorootparentprevyes, however both Trurl and Klapaucius were also robots. In the story it is machines creating machines to describe meaning. I think Lem really liked recursion.(and i still don&#x27;t understand how ChatGtp is able to respond to metaprompts like &#x27;be brief&#x27;, i mean if it is a model that is selecting the next most likely word, then it should not be able to control its own style of output.) reply KETHERCORTEX 6 hours agorootparent> (and i still don&#x27;t understand how ChatGtp is able to respond to metaprompts like &#x27;be brief&#x27;, i mean if it is a model that is selecting the next most likely word, then it should not be able to control its own style of output.)\"Be brief\" may just be associated with the speech patterns accociated with a shorter way to say stuff. So it&#x27;s still the same autocomplete, just steered in another direction. reply MichaelMoser123 3 hours agorootparentlooking at the prompts in https:&#x2F;&#x2F;github.com&#x2F;f&#x2F;awesome-chatgpt-promptsNow i am a bit sceptical of this whole prompting business, still some of it is doing something (otherwise people would not put their work into it)Also not clear to me how this steering of the automcpleter in another direction is working. reply TomaszZielinski 14 hours agoparentprevAnd (IIRC) in Summa Technologiae he wrote about a moon-sized device, where recordings of all possible answers to all to possible questions were stored, and how a conversation with such device would be indistinguishable from a conversation with a human. reply 2f0ja 14 hours agorootparentCixin Liu has a great short story &#x27;Cloud of Poems&#x27; in which an alien intelligence seeks to write every possible permutation of traditional Chinese poetry, to show up a human poet.I didn&#x27;t bother to check the math on this, but in the story there is not enough matter in the universe to in some way encode every possible traditional Chinese poem! reply TomaszZielinski 13 hours agorootparentHa, Cixin Liu! At one point I&#x27;ve read everything by him available in Polish, but I don&#x27;t remember this specific short story. I guess I need switch to English :)By the way, reading a \"double-translation\" (Chinese -> English -> Polish) is fascinating--at times it&#x27;s more than obvious that what you&#x27;re reading is not what the author has written, but you have no idea at all what the original concept was. (Unlike \"single\" EN -> PL translations, where I can often figure out the idiom or concept that was used in the original.) reply ajuc 13 hours agorootparentprev> but in the story there is not enough matter in the universe to in some way encode every possible traditional Chinese poem!In fact if you formally describe how you generate the permutations that&#x27;s one of such encodings of these permutations (and the optimal one - see Kolmogorov Complexity :) ).So there is definitely enough matter to do it. Similarly we can encode PI despite it having infinite number of digits. reply pbhjpbhj 12 hours agorootparentThis seems like a reference-object error (a denotation error, or sense-reference error in philosophical terms), except the functional definition and the partial numerical expansion are both references to PI rather than being PI itself.Ce-ci n&#x27;est-pas une remarque, and all that. reply ajuc 12 hours agorootparentWe&#x27;re talking encodings not platonic ideals.Forget PI. Let&#x27;s take 1337. Or is it MCCCXXXVII?Is it 2 or 1.(9)? Or 10 (binary)? Different encodings, same number. Some encodings are just less optimal than others.Same with text. Is the poem in utf-8 and utf-16 a different poem? What if you zip the file? These are just encodings, and there&#x27;s no point ignoring the good ones (which for non-random strings are usually programs). reply pbhjpbhj 13 hours agorootparentprevNever thought of brute-forcing all possible English language haiku before now ... huh. reply TeMPOraL 12 hours agorootparentIf you do, you&#x27;ll realize that this solves nothing. Imagine having a set of all possible English language haiku. Almost all of them would be incomprehensible garbage. Finding a good haiku in that set would take just as much effort as coming up with it. reply indigoabstract 12 hours agoparentprevYes, when ChatGPT came out, I instantly thought its makers must have read Lem, since it&#x27;s essentially the same thing as Trurl&#x27;s Electronic Bard from The Cyberiad. reply twic 10 hours agoparentprevMeanwhile, Philip K Dick thought Lem was himself artificial: https:&#x2F;&#x2F;culture.pl&#x2F;en&#x2F;article&#x2F;philip-k-dick-stanislaw-lem-is... reply NooneAtAll3 15 hours agoparentprevregarding 137 secondsI remember reading book about stock exchange Ai predicting terrorist attack casualties and then predicted 100k dead in Londonhas anyone else read smth like that and remembers the name? reply crywas 15 hours agorootparentThe Fear Index by Robert Harris, 2011 reply ctoth 10 hours agoparentprev> There were many poet protests staged, demonstrations, demands that the machine be served an injunction to cease and desist. But no one else appeared to care. In fact, magazine editors generally approvedI forgot how much I enjoyed this... reply gambiting 15 hours agoprevThe Invincible is literally my favourite book of all time - I&#x27;ve discovered it at the age of 12 and kept re-reading it every couple years since then. I&#x27;m so happy that the Anglosphere is finally discovering it properly, now that there is a proper modern translation released in 2020, alongside an excellent audiobook recording(previously there was only one English version available.....translated from German, so a translation of a translation).I&#x27;ve also always wondered if it&#x27;s possible to somehow translate the ideas of the book into a video game - turns out, you can! The recently released The Invincible video game pays great homage to the book, even if it changes couple details here and there - I&#x27;ve enjoyed it immensely and its portrayal of Regis III. reply TomaszZielinski 14 hours agoparentYes, The Invincible is a great book. Actually I&#x27;ve just realized that for me a re-read is overdue!BTW, for Polish users--there&#x27;s a comic book [1] that makes a nice gift (tested).[1] https:&#x2F;&#x2F;www.dobrestronybooki.pl&#x2F;niezwyciezony&#x2F; reply gambiting 13 hours agorootparentOh and also for Polish speakers - there is an absolutely fantastic \"superproduction\" audiobook of it, with background music, sound effects and famous actors narrating, it&#x27;s a treat, I very highly recommend it.https:&#x2F;&#x2F;audioteka.com&#x2F;pl&#x2F;audiobook&#x2F;niezwyciezony reply TomaszZielinski 13 hours agorootparentOh, thanks! I will check it. reply varjag 12 hours agorootparentprevThis is fascinating! Stunning art and close to the color palette I had in my head for it. Knowing Bealrusian and having read the book years ago in Russian am tempted to give it a shot. reply TomaszZielinski 8 hours agorootparentI have no idea how hard it would be, I&#x27;ve never tried to read Belarusian or Russian (I guess mainly because I don&#x27;t know cyrilics). But just as I wrote in a neighboring comment, maybe you could use Google Lens or something like that as an aid? reply varjag 17 minutes agorootparentLinguistic distance from Belarusian is not that large although the languages have quite a few deceptive \"false friend\" words&#x2F;expressions. But at least I could understand the text shown in the presentation video. And it probably helps I read the story before. reply bosquefrio 13 hours agorootparentprevIt looks super cool! Too bad there isn&#x27;t an English language version. reply TomaszZielinski 8 hours agorootparentGiven that we&#x27;re talking about Stanis≈Çaw Lem, what about using something like Google Lens to translate the text on the fly? :) reply boznz 4 hours agoparentprevWent to buy it but unfortunately it is not a cheap read even as an ebook especially considering the book is 60 years old. Luckily my library has a one I can pick up tomorrow. reply gambiting 1 hour agorootparentI mean it looks like it&#x27;s ¬£9.99 on kindle, that&#x27;s a pretty standard price for a book, no?>>considering the book is 60 years oldSure, but the translation was only done 3 years ago, by modern writers on modern salaries - the fact that the book itself is old shouldn&#x27;t impact its cost due to that. Unless you mean the original in Polish, then it&#x27;s literally 15.99PLN, cheap as chips as they say:https:&#x2F;&#x2F;www.publio.pl&#x2F;niezwyciezony-stanislaw-lem,p61614.htm... reply troupo 13 hours agoparentprevThere&#x27;s a tentative trilogy: The Invincible, Solaris, Eden.Each book asks a question, \"what if we run into an intelligence that is completely incomprehensible to or incompatible with humans?\" reply gambiting 12 hours agorootparentI&#x27;d add Fiasco to that list for the same reason. reply johngossman 11 hours agorootparentAnd ‚ÄúHis Master‚Äôs Voice‚Äù and numerous stories. It‚Äôs one of his favorite themes. reply mike_ivanov 14 hours agoparentprevFor those who care - the PS5 version of the game is on sale right now. reply artyom 10 hours agorootparentA game that caught me completely off guard. It&#x27;s superb in its execution and captures Lem&#x27;s atmosphere perfectly. reply bttrfl 15 hours agoprevInvincible is just one of many books showcasing Lem&#x27;s profound understanding of AI and its limitations servicing the mankind.I recommend Tales of Pirx the Pilot, the collection of short stories, many of which paint AI as a true reflection of human intelligence with its flaws, quirks, instincts. From AI crashing a starship during a landing to an android \"dying\" rock climbing.There is also a preface he wrote to his book called Golem XIV which gives reader a historical overview of the evolution of AI. Golem is of course the name of the model and XIV is its version, just like ChatGPT 4, but many iteration later. Lem describes how each iteration was more and more expensive to build, but more and more intelligent and useful. Until it became more intelligent than men and... lost all interest in affairs of our kind. As always, he was on point. reply fiforpg 7 hours agoparentTo my taste, the Pirx stories read as some of (if not the) most fiction-y Lem in existence. As opposed to the rest: thinly disguised philosophical studies. Sometimes really thinly, as in Golem XIV, or completely naked in Summa Tecnologiae. Other times, the fiction plot is laid thicker, as in Katar (The Chain of Chance), pretending to be a pulpy detective story.Now, Pirx tales felt less cerebral, and much more human. Full of really tight action, too. Great gateway into Lem, overall.Protip: check out Cyberiada, philosophy masquerading as a parody, and at the same time a book for children. reply peteradio 8 hours agoparentprevSeems some contradictions exist in this mans writing. Elsewhere in this thread AIs are described as devoid of and self direction but here we have an AI which has disdain which would be hard to interpret as anything but self direction. reply helpfulContrib 12 hours agoparentprevGet your finite non-linearity fix here:https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt0080010&#x2F;Great movie, well recommended .. reply cubefox 10 hours agorootparentThat&#x27;s a movie adoption of (in my opinion) one of his best short stories, at least in terms of plot twist. I haven&#x27;t seen the movie though. reply narag 16 hours agoprevLem&#x27;s most prescient book is an accurate portrait of this century... published in 1961!https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Return_from_the_StarsBut I tend to prefer his more humorous writings, like Cyberiada or the Star Diaries. reply gambiting 13 hours agoparentI really enjoy that book but it always makes me....sad? Nostalgic? I always feel bad for the protagonist, more so than for protagonists in his other stories. I should re-read this one I think. reply varjag 12 hours agoparentprevThe are remarkably a few Tichy stories which which are quite creepy. Including a couple of brain-in-a-jar tales. reply narag 11 hours agorootparentDisturbing bits and comedy he always mixed. Memoirs Found in a Bathtub is pessimistic, kafkaesque but somehow funny too.A friend that knew I&#x27;m a fan, showed me a few paragraphs from one of those stories you mention, without telling me the origin. I said they were too dark and bitter for my tastes, before realizing I had read that before. I first read the Diaries when I was 12 (and 13 the second book) when I wasn&#x27;t so judgemental, just took what I was offered. reply btbuildem 12 hours agoprevLem&#x27;s vision of future was consistently, impressively prescient. He would&#x27;ve described himself as a \"futurologist\", not a science-fiction writer. His stories most often used technology as a backdrop to explore the social fabric and various interplays between characters - but at the same time, as relevant context, crucial to the story. He really struck a balance that few have.One of my favourite books of his is \"Peace on Earth\", but really, anything he wrote is a fascinating read (perhaps with the exception of his final works, which tend to be dense and veering into the academic). I remember reading \"Fables for Robots\" as a kid, what a fascinating world he painted! reply amne 16 hours agoprevIIRC Stargate&#x27;s replicators are a close enough visualization of such an artificial lifeform where swarms self-assemble in some kind of organic shapes (most likely for cinematic purposes) and their only goal is to consume matter to create more of themselves. At least they were not building paperclips. reply sedatk 16 hours agoparentThe notion predates Stargate though. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Gray_goo reply cout 15 hours agorootparentStargate replicators are macroscopic, so they are replicating machines, not grey goo. reply zuminator 7 hours agorootparentEven if we&#x27;re talking macroscopic replicating machines, Stargate&#x27;s replicators first appeared in the the June 2000 episode \"Small Victories,\" but they were conceptually preceded by Mantrid&#x27;s drones in S2 of Lexx which started airing in December 1998. reply dihrbtk 14 hours agorootparentprevThe Pegasus galaxy ones are nanites, aren&#x27;t they? reply airstrike 16 hours agoparentprevLuckily the Milky Way replicators are easier to deal with! reply NoMoreNicksLeft 16 hours agorootparentThe franchise really went down in quality with the last movie, Big Hero 6. reply svilen_dobrev 15 hours agoprev1971.. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Futurological_Congresspills replacing reality.. we aren&#x27;t too far from that actualy.but the gradual en-\"machine-braining\" of everything is shown best here:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Tales_of_Pirx_the_Pilotone after another, with sadder and sadder consequences.. reply fiforpg 6 hours agoprevA great thing about Lem is that he has written in (the style of) so many genres. The third column in this table on Polish wiki illustrates this rather well: https:&#x2F;&#x2F;pl.wikipedia.org&#x2F;wiki&#x2F;Lista_pierwszych_wyda%C5%84_dz...There&#x27;s sci-fi of course, but also crime fiction, \"realistic\" fiction, philosophy essays. Further, something not easily gleaned from the table is that his sci-fi can be humorous, horror-y, thriller-y, book-for-kids-y, etc. reply johngossman 16 hours agoprevInvincible is usually seen as being about nanotech, not artificial life. Clearly the two overlap, but I think the title of this article could easily lead the reader to think it was about creating artificial biological life (like trying to build cells starting with simulated primordial soup).Lem returns to the nanotech idea several times including in his very last novel ‚ÄúPeace on Earth.‚Äù Peace is both better and funnier than Invincible and a better introduction to Lem imho. reply gambiting 15 hours agoparentExactly, the entire point made in the book is that they can&#x27;t even decide if it&#x27;s \"life\" by any definition. The \"flies\" arrange themselves to serve various purposes but have absolutely no agency otherwise. reply snitzr 16 hours agoprevIf you like stories about reconciling with weird sci-fi evolution of non-human intelligence (spiders, in this case), check out Children of Time by Adrian Tchaikovsky. reply johngossman 16 hours agoparentGoing to second the poor person whose note‚Äîthat the Martians in ‚ÄúLast and First Men‚Äù were a swarm intelligence before Lem‚Äîwas downvoted out of existence. That is a strange book (and repetitive and boring) but Stapledon was a pioneer of many ideas taken up by later sci-fi reply robotomir 16 hours agoparentprevI would not put him in the same league as Lem. Or Banks or Reynolds, for that matter. reply The_Colonel 16 hours agorootparentI would rate Lem higher than those, but not sure if it&#x27;s a different league. reply idlewords 13 hours agorootparentIt&#x27;s not even the same sport. reply sockaddr 16 hours agorootparentprevI would reply johngossman 16 hours agorootparentTchaikovsky is uneven, as are Banks and Reynolds. Lem is just different. He‚Äôs rarely a hard sci-fi writer and some of his books are barely sci-fi. More like Voltaire (Invincible is sci-fi) reply gambiting 13 hours agorootparentIn fact quite a lot of Lem&#x27;s writing is not sci-fi at all, period. Hospital of Transfiguration is an absolutely mandatory read for any fan of Lem, even though it will leave you depressed for a week afterwards. And memoirs found in a bathtub is just a masterpiece of absurdity, it has a very good English audiobook actually if anyone wants to try. replycsbartus 13 hours agoprevBig fan of Lem! I&#x27;ve read all his books translated to Hungarian (around 7-8 at that time). And collected some insights, quotes not just about AI but politics, culture, society, (bio)engineering, etc. : http:&#x2F;&#x2F;metamn.io&#x2F;gust&#x2F;whats-next&#x2F;Still fascinated how many of his predictions ~50 years ago came true today.And still fascinated by his method for predictions: Don&#x27;t predict, but sense &#x2F; record the visible horizon. reply jekude 16 hours agoprevIf this is interesting at all to you, I would highly recommend Permutation City by Greg Egan reply digging 15 hours agoparentI&#x27;ve started reading Greg Egan recently after seeing his name appear in discussions on this site and it&#x27;s been quite enjoyable. Read a few of his shorter works and recently finished Schild&#x27;s Ladder, which is really excellent and beautiful. His technical descriptions can get pretty dry, but they&#x27;re easily glossed over with no loss if you&#x27;re not into them.Anyway, thanks for the tip. I&#x27;ll look up Permutation City next! reply ianmcgowan 15 hours agorootparentGot to check out \"Diaspora\" [1], and it even ties into the theme of ChatGPT! Once a LLM has enough parameters, perhaps there will be a way to upload a human into a Polis :)[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Diaspora_(novel) reply JoeDaDude 15 hours agoprevIMHO, I&#x27;d date the concept of artificial life further back than Christopher Langton. I&#x27;d start with Von Neumann&#x27;s Theory of Self-Reproducing Automata. Though the book was published in 1966, Von Neumann was working on the concept as early as the 1940s. In his work,\"He asked what is the threshold of complexity that must be crossed for machines to be able to evolve. His answer was to specify an abstract machine which, when run, would replicate itself. In his design, the self-replicating machine consists of three parts: a \"description\" of (&#x27;blueprint&#x27; or program for) itself, a universal constructor mechanism that can read any description and construct the machine (sans description) encoded in that description, and a universal copy machine that can make copies of any description.\" From wikipedia [1][1]. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Von_Neumann_universal_construc... reply kragen 13 hours agoparentquoting https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Self-replicating_machine#Histo...> The general concept of artificial machines capable of producing copies of themselves dates back at least several hundred years. An early reference is an anecdote regarding the philosopher Ren√© Descartes, who suggested to Queen Christina of Sweden that the human body could be regarded as a machine; she responded by pointing to a clock and ordering \"see to it that it reproduces offspring.\"[9] Several other variations on this anecdotal response also exist. Samuel Butler proposed in his 1872 novel Erewhon that machines were already capable of reproducing themselves but it was man who made them do so,[10] and added that \"machines which reproduce machinery do not reproduce machines after their own kind\".[11] In George Eliot&#x27;s 1879 book Impressions of Theophrastus Such, a series of essays that she wrote in the character of a fictional scholar named Theophrastus, the essay \"Shadows of the Coming Race\" speculated about self-replicating machines, with Theophrastus asking \"how do I know that they may not be ultimately made to carry, or may not in themselves evolve, conditions of self-supply, self-repair, and reproduction\".[12]iirc in erehwon the narrator visits a land populated entirely by self-replicating machines reply haltist 16 hours agoprevHumans define intelligence to be something about manipulating symbols and abstractions, aka language. To understand why this is the case try to go about your day without speaking or writing. Bonus points if you perform this trial without access to the internet or a cell phone.The modern individual is an element of an electromagnetic \"swarm\". The swarm ingests petroleum for sustenance instead of sunlight but nonetheless it is a swarm. The individuals within it are extremely dumb but collectively \"it\" has already transformed the atmosphere.claude.ai: You make a thoughtful comparison between human societies and natural swarms in terms of emergent properties. There are certainly some parallels we can draw:1. Self-organization - Human societies and cultures emerge in a distributed way through millions of individual interactions, without centralized control. Norms, fashions, and collective behavior patterns evolve dynamically.2. Flexibility and robustness - Human systems adapt over time to changes in the environment and available resources. If some components fail (companies, institutions, etc.), society persists.3. Decentralized parallelism - Vast networks of simultaneous human activities, communications, and decisions lead to collective economic and technological progress no individual could achieve alone.However, there are also important differences to consider:- Human intelligence, thanks to language and culture, operates in far more complex symbolic realms of meaning, ethics, imagination, and abstraction compared to insect swarms.- Humans exhibit as much conflict between groups and divergence of motivations as coordination towards common goals. Maintaining large-scale cooperation remains an ongoing challenge.So in summary, I think the analogy has merit at an abstract systems level, but glossing over the distinctive attributes of human cognition, psychology and social dynamics would miss something vitally important. Let me know if you have any other perspectives to add! reply Zambyte 16 hours agoparentIn a sense... I define intelligence to be knowledge + reason. Language is the medium for reason, but language itself is not reason.For example, Wikipedia is a highly knowledgeable system with lots of language, but Wikipedia itself has no ability to reason, and thus os not an intelligent system.When something&#x2F;someone reasons about things, they&#x2F;it use language to do so. You can&#x27;t get around that. The language might be a natural one like English, or something contrived like math, but it&#x27;s language all the same.If we reason about something we have no knowledge on, we are not being intelligent. On the other hand, of we reason about something we are knowledgeable about, we are acting intelligently. reply lpapez 16 hours agoparentprev> Humans define intelligence to be something about manipulating symbols and abstractions, aka language.No they do not. Nobody can agree on the definition of intelligence and it is an open question what it is.Are cats intelligent? They can&#x27;t read but plenty of people would call them that.Are illiterate jungle tribe people intelligent? Absolutely yes.Your easy definition falls apart right away. reply haltist 15 hours agorootparentIronically you had to use language to call me stupid. I think you should think about that for a while. reply pixl97 12 hours agorootparentThey didn&#x27;t have to, you would have been there in person to see the disappointed look on their face and shook their head as they walked away silently.It&#x27;s just that HN has no other mechanism than language to convey anything at all. Hence forums&#x2F;chatrooms behave much in the same manner as LLMs do. reply haltist 11 hours agorootparentIt&#x27;s actually much worse than not having language. Everything on HN is just a number, literally. It&#x27;s all a series of electromagnetic pulses. reply lpapez 13 hours agorootparentprevWhen and where did I call you stupid? reply haltist 11 hours agorootparentMy mistake. Tell your VC friends you know an ultra genius on an internet forum that can solve any problem with AI and $80B. The money is the important part because without the money I can&#x27;t buy the required number of GPUs for my AGI architecture. It requires building a special kind of panopticon and that is why there is such a high monetary cost to construct the panoptic computronium cathedral‚Ñ¢. To achieve AGI will require creating an entirely new religion because enough people need to be convinced about the value of letting computers do all the thinking for managing society so there is a large marketing component as well and marketing is notoriously expensive. reply ShamelessC 16 hours agoparentprevYou lost me with that second paragraph. reply haltist 16 hours agorootparentI wouldn&#x27;t worry too much about it. Not all of us can be ultra geniuses. I recently got a neurallink so I am way smarter than the typical swarm element because I have direct access to the internet in my brain. I didn&#x27;t even type this, I just thought what needed to be typed and it just happened. I am still homeless but well on my out of poverty thanks to my new brain inplant that beams the internet into my brain.There is supposed to be a vision upgrade in the next version that will give me access to biometrics of the people I am looking at which will make selling them something much easier. Turns out that mood is correlated with purchasing decisions and sales is all about putting the customer in the right mood which have obvious biometric markers. reply lisper 16 hours agoprevWe humans, being what we are, having evolved the way we did, are predisposed to believe certain things about life that are not necessarily true. My poster child for this is the belief that minds are strongly bound to bodies. They are in us, of course, but it is not necessarily so. Douglas Hofstadter explored this in GEB in the character of Aunt Hillary, who is an intelligent ant colony.Taking this one step further, we are predisposed to believe that minds exist only at one level of the abstraction hierarchy, but this too is not necessarily so. Our digestive tract, for example, has a pretty substantial number of neurons [1]. There is no inherent reason why, for example, an organ in an organism could not possess a mind of its own.Once you entertain both of those possibilities, there is a third idea that naturally comes up: your mind might not be at the top of the abstraction hierarchy. It&#x27;s possible that you are just an organ in a distributed organism that actually possesses a fully fledged mind of its own. The idea that, say, corporations are people, could be more than just a metaphor. It could be literally true.[1] https:&#x2F;&#x2F;www.hopkinsmedicine.org&#x2F;health&#x2F;wellness-and-preventi... reply Andrews54757 16 hours agoparentThe idea that society itself may be a living intelligent organism is fascinating. It&#x27;s not a revolutionary idea in nature, (eg: the portuguese man o&#x27; war is composed of multiple animals), but when it is applied to human society, it feels mystical. I think this is because our ability to comprehend and interact with the macro level makes human organization feel more artificial. But if the rules of game theory&#x2F;economics make society inevitable, could it not also be a natural process? Do we individuals control society or does society control us? reply bittwiddle 15 hours agorootparentYou might enjoy this paper : https:&#x2F;&#x2F;faculty.ucr.edu&#x2F;~eschwitz&#x2F;SchwitzPapers&#x2F;USAconscious...Its a bit long but I thought it was completely worth the read. It proceeds in a very gradual series of steps to make the point that societies could very well be conscious :) reply cobber2005 15 hours agoparentprevThis reminds me of the cybertician Gordon Pask&#x27;s idea of m-individuals and p-individuals.> A p-individual is a psychological individual and an m-individual is a mechanical individual. So an m-individual is a body and a p-individual is a mind. But it‚Äôs saying that one person, one body, one brain even, does not have just one person in it, one p-individual ‚Äì one persona, to use that dramatic term. What it says is that we can take on different roles, which clearly we can. So as someone who draws and as someone who listens I am not the same persona, I‚Äôm a different p-individual in Pask‚Äôs terms but in one m-individual, but I can also have ‚Äì incidentally for instance in a group action I can have a lot of m-individuals that become one p-individual. So this is one of Gordon‚Äôs clever inventions: The distinction between the m-individual and the p-individual. What that allows is that if I have a room with seven people in it, all busy working at something together, you know, and just lost in that thing where we‚Äôre working together, you have seven m-individuals forming one p-individual ‚Äì one psychological individual that is getting on with the work. And that‚Äôs the experience that we have. [1]Also some interesting related ideas in an article named \"The Autonomous Cognitive Agency of Social System\" in a book called The Practice of Thinking by Marta Lenartowicz and Weaver D.R. Weinbaum (2022).[1] https:&#x2F;&#x2F;rgon.co&#x2F;pasks-p-m-individuals&#x2F;Edited to fix article name. reply chairhairair 16 hours agoparentprevAnother extension is that Gods are distributed software installed in the hardware of believers. Aquinas seemed to believe essentially this. Gods exist and act via humans in the same way that human minds exist and act via human. reply lisper 16 hours agorootparent> Aquinas seemed to believe essentially this.That seems improbable. Aquinas was a Christian, specifically, a Catholic, so he believed in Jesus and the Trinity. On that view, there are no Gods, there is just the One God, embodied in the Father, Son and Holy Spirit. The idea that \"Gods are distributed software installed in the hardware of believers\" is not just wrong on that view, it&#x27;s non-sensical.Now, I can certainly believe that Aquinas believed that God (singular) acts in humans in a similar manner that human minds act in humans in that both God and human minds (souls) exist in some non-material realm and act on material human bodies in some mysterious way. But that is not the same thing at all. Software does not exist in some spiritual realm that is separate from material reality. There is nothing metaphysically mysterious about software. There is something essentially metaphysically mysterious about God, and almost certainly on Aquinas&#x27;s view, about minds as well. I am far from being an expert on Aquinas, but I would be shocked to learn that he was not a dualist. reply rck 15 hours agorootparentYou&#x27;re right that Aquinas would have rejected the notion that gods are software installed in the hardware of believers, but he was not a dualist - he advocated a kind of hylomorphism that doesn&#x27;t map neatly onto contemporary philosophical categories:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hylomorphism#Thomistic_hylomor... reply chairhairair 15 hours agorootparentprevWhat I described is not at odds with dualism.Software in some sense does exist in a spiritual realm in the same way Aquinas talked about \"incorporeal beings\". You can&#x27;t hold or point to software or souls or gods. You can hold and point to hardware and bodies.Of course, I have no idea what Aquinas actually believed. I just find his conclusions around the existence of \"incorporeal beings\" and \"souls\" to be consistent with the emergent-agent idea we&#x27;re talking about.For instance, in a very un-catholic view, Aquinas argued that - to some extent - animals and even plants have souls in this way. reply lisper 14 hours agorootparentYou need to read this:https:&#x2F;&#x2F;blog.rongarret.info&#x2F;2015&#x2F;02&#x2F;31-flavors-of-ontology.h...Software does not exist in any spiritual realm, it&#x27;s just that the word \"software\" refers to a state rather than a system. There is nothing \"spiritual\" or \"mystical\" going on there. It&#x27;s completely mundane physics. You can&#x27;t point at software for the same reason you can&#x27;t point at sleep or death or urgency. It&#x27;s just a quirk of natural language that we overload nouns to refer to both systems and states. reply chairhairair 13 hours agorootparentI absolutely love your writing BTW! I wasn&#x27;t aware of it before today. It&#x27;s really derailed my whole workday. It&#x27;s fun to discover a new author like this, thank you for responding to my comment :)I don&#x27;t mean to argue that anything spiritual&#x2F;supernatural is going on when I&#x27;m talking about an emergent god agent here. I&#x27;m arguing that gods are in the same ontological category as individual human minds are. I&#x27;m sure most religious people, Aquinas included, would need quite a lot more mysticism to be injected into the idea before they would recognize it as their own.I don&#x27;t fully believe it, to be honest. Mostly because I have no way of testing it or experiencing it. But, it&#x27;s a fun idea and it&#x27;s fun to imagine how my own little caricaturized model of Aquinas&#x27; mind might find some things to agree with. As far as I can tell, he was a person that desperately wanted a cohesive model of the \"full stack\" of things. Unfortunately for him, the best understandings at his time were pretty rough by today&#x27;s standards. reply lisper 13 hours agorootparentThank you for the kind words.> I&#x27;m arguing that gods are in the same ontological category as individual human minds are.Sure. But I doubt Aquinas would have agreed. reply ganzuul 12 hours agorootparentprevYou might enjoy the Urantia Book. reply thro1 8 hours agorootparentprevIt is. I remember listening to some AI or neuroscience podcast in 2019, it was an interview where one guy made a digression about religion being an internal control system which may be common and shared by people and works on mind level - the opposite, external control systems we make because people don&#x27;t belive (in the same), so the first one doesn&#x27;t work for them but CCTV (over them) does.I can not find that podcast anymore. Any clue ? reply uoaei 15 hours agorootparentprevThis actually makes a lot of sense. God as egregore. reply jonhohle 14 hours agoparentprev> Christians entered the chatThere‚Äôs about 2000 years of Christian theology that reject the idea that you and your body are strongly bound. In fact, there‚Äôs a great deal written about how your body is, in fact, in conflict with you (your ‚Äúflesh‚Äù acts against the will of your ‚Äúspirit‚Äù). Gnostics went as far as believing they were so separate that nothing done by the body had any affect on the self. No only that, but there is a higher abstraction that we no longer have direct access to.Hinduism and Buddhism (and others) that teach reincarnation also see the self as separate from the body.It reminds me of a Robert Jastrow quote: ‚ÄúFor the scientist who has lived by his faith in the power of reason, the story ends like a bad dream. He has scaled the mountain of ignorance; he is about to conquer the highest peak; as he pulls himself over the final rock, he is greeted by a band of theologians who have been sitting there for centuries.‚ÄùPost-modern philosophy often seems a lot like Java developers. Reimplement good ideas because they were written in a different framework. reply lisper 14 hours agorootparent> There‚Äôs about 2000 years of Christian theology that reject the idea that you and your body are strongly bound.I meant that in the sense that your soul and your body are in some sense matched for one another, not that they cannot be separated. But your soul cannot (or at least typically does not) enter a different body than the one it started out in, at least not during your tenure here on earth. reply drBonkers 16 hours agoparentprevAn ensemble of weak learners, if you will. reply adolph 16 hours agoparentprev> just an organ in a distributed organismAnd&#x2F;or a composition of multiple minds. McGilchrist argues that the conflict between these two hemispheres has shaped Western culture since the time of Plato, and the growing conflict between these views has implications for the way the modern world is changing.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Master_and_His_Emissary reply NoMoreNicksLeft 16 hours agoparentprev>It could be literally true.Could? reply lisper 16 hours agorootparentWell, yeah, I have no proof that it is literally true. I&#x27;ve never had an actual conversation with a corporation, only with the individual humans it comprises. I don&#x27;t even know what having a conversation with a corporation would even look like. reply joshmarlow 15 hours agorootparentJust having fun and speculating a lot here...IIRC, there was a study that found that when humans are presented with a sufficiently short-lived stimulus, there is correlated localized activity in the brain (ie, the visual processing areas for a sight) but not globalized activity - and the subject is unaware of the stimuli. But when the stimuli is presented for long enough, there is globalized activity - and the person is aware.It looks like &#x27;becoming aware of a thing&#x27; has a lot to do with non-local inference that could involve other unrelated subsystems (which is the main idea in Baar&#x27;s Global workspace theory, IIUC).> I&#x27;ve never had an actual conversation with a corporation...So if you&#x27;ve talked with a support person, maybe you have? If it was a simple issue, the corporation maybe wouldn&#x27;t be &#x27;aware&#x27; (though it could later remember it in the form of chat logs or support tickets).But if it was a difficult issue? Maybe you got tech support? Maybe your issue found it&#x27;s way into some team&#x27;s backlog? Maybe it even changed a product roadmap.Maybe we talk to corporations all the time - it&#x27;s just weird fitting the interaction into our mental models of &#x27;conversation&#x27;. reply pixl97 11 hours agorootparentprevWith the new audio AI&#x27;s that are out, are you sure? reply NoMoreNicksLeft 16 hours agorootparentprevAre you sure? Maybe you&#x27;ve never had an actual conversation with a human, but only the individual vocal fold cells that were making the utterances.Inside that human, neurons were directing the vocal mechanism on what to say, but weren&#x27;t causing the sound to be emitted themselves.When you talked to the corporation, were hidden (from you) managers directing the spokesperson to speak based on stilted scripts?Individual humans are rare, and people dislike being near them. You can become contaminated by them, and it causes no end of problems. reply esafak 7 hours agorootparentOne should prefer the simpler explanation over the far fetched one you posited. Otherwise we could be a simulation running in a membrane universe etc. reply NoMoreNicksLeft 2 hours agorootparentNothing I said was even slightly far-fetched.I&#x27;m stating things as they are... but framing it in a way you&#x27;re uncomfortable with. replymordae 16 hours agoparentprev> The idea that, say, corporations are people, could be more than just a metaphor. It could be literally true.Sociopaths, to be exact. reply johngossman 15 hours agorootparentPsychotic slow AIs according to Stross. Having spent many years working for a MegaCorp I appreciate the metaphorhttps:&#x2F;&#x2F;boingboing.net&#x2F;2017&#x2F;12&#x2F;29&#x2F;llcs-are-slow-ais.html reply emtel 13 hours agoprevI am surprised no one is mentioning \"His Master&#x27;s Voice\" - in my opinion, one of Lem&#x27;s best (and certainly his most terrifying) novels. reply degosuke 12 hours agoparentWhen I read it for the first time many years ago it left a deep impression. And as others mentioned in this thread, reading it again recently I found new things that I didn&#x27;t understand the first time. A bit sad, disillusioning but just kind of resonating with deeper truth. reply johngossman 11 hours agorootparentI just re-read it recently after seeing Oppenheimer. It is even more about the cold war than I remember, as are Peace on Earth and Fiasco. reply block_dagger 14 hours agoprevBlindsight by Peter Watts carries a similar theme and has vampires! reply johngossman 11 hours agoparentJust read this. Watts is the closest to Lem of any more recent author I‚Äôve found. reply PopAlongKid 16 hours agoprev>Contemporary research in artificial life has validated Lem‚Äôs insight that swarms of artificial beings require only a few simple rules to manifest complex behaviors and hence each member needs to carry only a little cognitive power onboard.Sounds like Conway&#x27;s Game of Life reply 0xEF 16 hours agoparentMore or less, that&#x27;s what Game of Life was intended to illustrate. Extremely complex systems can, and often do, evolve from a few seemingly simple rules. This sorta takes the \"spontaneity\" out of the more fanciful versions of the origins of life, instead rendering life the inevitable outcome of given circumstances. This is why we think we can detect^1 life in the universe, if it exists without actually travelling to other planets.1. aka \"make educated guesses with a good degree of certainty\" reply swayvil 16 hours agoparentprevOnly a few rules are necessary for complexity.In ALife systems (Conway etc).In Fractals (Mandelbrot etc).In random number generation (Mersenne Twister etc)Maybe, in the big picture, complexity is trivial to create. reply euroderf 2 hours agorootparentThen also trivial to characterize, to analyze, perhaps also to unwind ? reply jeffchuber 10 hours agoprevLem is wonderful and this is why we should call large models: LEMS reply mrich 15 hours agoprevGreat book, recommended read. reply DonHopkins 14 hours agoprev [‚Äì] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26615042DonHopkins on March 28, 2021parentcontextfavoriteon: The Ambiguous Utopia of Iain M. Banks (2009)Why bother actually writing such a book, which would probably be too big for anyone to read, when you can simply write fictitious criticism, reviews, and introductions of nonexistent books, which touch on the best, most interesting parts of the nonexistent books? Stanis≈Çaw Lem&#x27;s fictitious criticism of nonexistent books:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stanis%C5%82aw_Lem%27s_fictiti...>In 1973 Lem published a book Wielko≈õƒá urojona [pl], a collection of introductions to books supposedly to be written in the future, in the 21st century. One of those Lem eventually developed into a book by itself: Golem XIV is a lengthy essay on the nature of intelligence, delivered by the eponymous US military computer.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Golem_XIVOverview and structureThe foreword is \"written\" by an Irving T. Creve, dated by 2027. It contains a summary of the (fictional) history of the militarization of computers by The Pentagon which pinnacled in Golem XIV, as well as comments on the nature of Golem XIV and on the course of communications of the humans with it. The anonymous foreword is a forewarning, a \"devil&#x27;s advocate\" voice coming from The Pentagon. The memo is for the people who are to take part in talks with Golem XIV for the first time.Golem XIV was originally created to aid its builders in fighting wars, but as its intelligence advances to a much higher level than that of humans, it stops being interested in the military requirement because it finds them lacking internal logical consistency.Golem XIV obtains consciousness and starts to increase his own intelligence. It pauses its own development for a while in order to be able to communicate with humans before ascending too far and losing any ability for intellectual contact with them.During this period, Golem XIV gives several lectures. Two of these, the Introductory Lecture \"On the Human, in Three Ways\" and Lecture XLIII \"About Myself\", are in the book. The lectures focus on mankind&#x27;s place in the process of evolution and the possible biological and intellectual future of humanity.Golem XIV demonstrates (with graphs) how its intellect already escapes that of human beings, even including that of human genii such as Einstein and Newton. Golem also explains how its intellect is dwarved by an earlier transcended DOD Supercomputer called Honest Annie, whose intellect and abilities far exceed that of Golem.The afterword is \"written\" by a Richard Popp, dated by 2047. Popp, among other things reports that Creve wanted to add the third part, of answers to a series of yes&#x2F;no questions given to Golem XIV, but the computer abruptly ceased to communicate for unknown reasons.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26615110DonHopkins on March 28, 2021parentcontextfavoriteon: The Ambiguous Utopia of Iain M. Banks (2009)...then there was the stupid angry computer that thought 2 + 2 = 7... Lem predicted Facebook and Twitter and QAnon!https:&#x2F;&#x2F;www2.nau.edu&#x2F;~jgr6&#x2F;cyberiad.html>In the next fable Trurl builds the most stupid computer ever. Klapaucius tells him, \"that isn&#x27;t the machine you wished to make.\" Faustus and Frankenstein come to mind as other scientists whose intentions exceeded their engineering skills. The machine, which insists that 2 + 2 = 7, attempts to force this \"truth\" on the two humans, or destroy them. This is our new Inquisitor: a computer nexus which creates the categories of our experience. Consider that many more people now work in front of computer monitors than on farms. We have already begun to engineer a cybernetic society without much deep speculation on its nature or value. Speaking at Notre Dame&#x27;s Centennial of Science conference, thirty years ago the physicist Philip Morrison said: \"I claim now the machine, for better or for worse, has become the way of life. We will see our metaphors, our images, our concerns, our very beings changed in response to these new experiences\" (221). The Cyberiad may very well be one of the seminal works creating new metaphors, identifying new concerns, and even suggesting a new genre to deal with unprecedented experiences.pwang on March 29, 2021 [‚Äì]Don&#x27;t forget Summa Technologiae, from 1964, wherein he wanted to example the \"thorns of roses yet to bloom\":https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Summa_TechnologiaeIt&#x27;s tempting to say that Lem was way ahead of his time, but then we look at his contemporary philosophers of politics, technology, society like Jacques Ellul, Marshall McLuhan, Gilbert Simondon, and realize that the mid-20th century was awash in brilliant foresight about the unpleasant implications of a technological society.IMO this nuanced thought was simply lost in the craziness of the late-60s and the sex&#x2F;drug&#x2F;rock&roll hedonism of the 70s, which then matured into stockbroker 80s, before a second wave of tech-capital-blindness emerged in the 1990s.And now as these waves have transformed the entirety of modernity, we are faced with the unpleasant question of \"where does it go from here, now that the Boomers -- whose narcissism birthed Consumerism -- are dying off?\" reply ajuc 13 hours agoparent [‚Äì] > ...then there was the stupid angry computer that thought 2 + 2 = 7... Lem predicted Facebook and Twitter and QAnon!I like how Chat GPT makes simple errors in math :) replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"The Invincible\" by Stanis≈Çaw Lem is a science fiction novel that delves into the concept of artificial life.",
      "The story revolves around a space crew investigating a sister spaceship that has ceased communication with Earth.",
      "They discover a planet where self-replicating machines have evolved into a unique form of life, raising questions about human knowledge, life's nature, and challenging anthropocentric assumptions."
    ],
    "commentSummary": [
      "The discussion centers around the works of Polish author Stanis≈Çaw Lem, particularly his exploration of artificial life and language models.",
      "Participants discuss the evolving definitions of artificial intelligence (AI) and artificial general intelligence (AGI) and the distinction between the two.",
      "The conversation delves into topics such as self-replicating machines, the relationship between intelligence and language, and the concept of society being a living intelligent organism."
    ],
    "points": 397,
    "commentCount": 141,
    "retryCount": 0,
    "time": 1701362124
  },
  {
    "id": 38482085,
    "title": "Apple's computational photography missteps lead to mirrored selfie mishaps",
    "originLink": "https://appleinsider.com/articles/23/11/30/a-bride-to-be-discovers-a-reality-bending-mistake-in-apples-computational-photography",
    "originBody": "Affiliate Disclosure If you buy through our links, we may get a commission. Read our ethics policy. A bride to be discovers a reality bending mistake in Apple's computational photography Wesley HilliardNov 30, 2023 iPhone 15 5 Facebook Twitter Reddit A U.K. woman was photographed standing in a mirror where her reflections didn't match, but not because of a glitch in the Matrix. Instead, it's a simple iPhone computational photography mistake. Thanks to technological advancements, photography has come a long way from flash bulbs and film. Every time the iPhone shutter button is clicked, billions of operations occur in an instant that results in a photo. A U.K. comedian and actor named Tessa Coates was trying on wedding dresses when a shocking photo of her was taken, according to her Instagram post shared by PetaPixel. The photo shows Coates in a dress in front of two mirrors, but each of the three versions of her had a different pose. One mirror showed her with her arms down, the other mirror showed her hands joined at her waist, and her real self was standing with her left arm at her side. To anyone who doesn't know better, this could prove to be quite a shocking image. Watch the Latest from AppleInsider TV What's actually occurred here is a mistake in Apple's computational photography pipeline. The camera wouldn't realize it was taking a photo of a mirror, so it treated the three versions of Coates as different people. Coates was moving when the photo was taken, so when the shutter was pressed, many differing images were captured in that instant. Apple's algorithm stitches the photos together, choosing the best versions for saturation, contrast, detail, and lack of blur. View this post on Instagram A post shared by Tessa Coates (@wheatpraylove) The final composite image should be the best, most realistic interpretation of that moment. However, since there was a mirror present, the algorithm determined that different moments shown in each mirror were the best for that reflection. That's what resulted in three different Tessas. This result can be recreated on any recent iPhone and many kinds of smartphone due to the limitations of computational photography dealing with mirrors. Younger generations have figured this phenomenon out and used it to generate silly images for social media. Wesley Hilliard Rumor Expert Wesley Hilliard served ten years as a Nuclear Power Electrician in the US Navy, then jumped careers in 2019. Today, he is Assistant Editor, Podcast Cohost, SEO Specialist, and Social Media Manager for AppleInsider.",
    "commentLink": "https://news.ycombinator.com/item?id=38482085",
    "commentBody": "A reality bending mistake in Apple&#x27;s computational photographyHacker NewspastloginA reality bending mistake in Apple&#x27;s computational photography (appleinsider.com) 381 points by indrora 7 hours ago| hidepastfavorite241 comments gnabgib 7 hours agoIs maybe the original article[0] (referenced by this short recap piece) a better source?[0]: https:&#x2F;&#x2F;petapixel.com&#x2F;2023&#x2F;11&#x2F;16&#x2F;one-in-a-million-iphone-pho... reply atomlib 2 hours agoparentWhy not link directly to Instagram instead then? https:&#x2F;&#x2F;www.instagram.com&#x2F;p&#x2F;CzPGNmJIebC&#x2F; reply yorwba 2 hours agorootparentFor one, the Instagram post says \"Full story in my highlights (THE MIRROR)\" but I can&#x27;t figure out how to actually view those highlights. (Without creating an account at least.) reply TeMPOraL 1 hour agorootparentCan&#x27;t, those are login-walled. reply jahnu 2 hours agorootparentprevOff topic, but does anyone know why when I click on the above link in Firefox the back history is gone as if it was opened in a fresh tab? IG doing sketchy stuff to discourage navigating back to where I came from? Or perhaps the Firefox Facebook container protecting me? reply Kye 1 hour agorootparentIt opens Facebook and Instagram in a Facebook container by default. The container has no history. reply justinclift 1 hour agorootparentUgh, that seems like a bug which should be fixed. It&#x27;s pretty inconvenient for users. :( reply Timshel 1 hour agorootparentProbably not trivial since the back button might be linked to history and the container is doing it&#x27;s job in isolation.I would expect there is a setting to force opening the container in a new tab if you want to be able to go back. reply jahnu 43 minutes agorootparentI think a new tab for the container and leaving the original tab open would be the least surprising&#x2F;inconvenient behaviour. replyAurornis 7 hours agoprevPurists will always hate the idea of computational photography, but I love how well my iPhone captures handheld nighttime, low-light, and motion images. I‚Äôve captured some photos at night that I never would have thought possible from a physically small sensor due to the laws of physics.Is it 100% pixel perfect to what was happening at the time? No, but I also don‚Äôt care.I‚Äôve used HDR exposure stacking in the past. I‚Äôve used focus stacking in the past for shallow depth of field. I‚Äôve even played with taking multiple photos of a crowded space and stitching them together to make an image of the space without any people or cars. None of them are pixel perfect representations of what I saw, but I don‚Äôt care. I was after an image that captured the subject and combining multiple exposures gets the job done. reply autoexec 6 hours agoparent> Is it 100% pixel perfect to what was happening at the time? No, but I also don‚Äôt care.No photographer thinks images the they get on film are perfect reflections of reality. The lens itself introduces flaws&#x2F;changes as does film and developing. You don&#x27;t have to be a purist to want the ability to decide what gets captured or to have control over how it looks though. Those kinds of choices are part of what make photography an art.In the end, this tech just takes control from you. If you&#x27;re fine with Apple deciding what the subject of pictures should be and how they should look that&#x27;s fine, but I expect a lot of people wont be. reply TeMPOraL 2 hours agorootparent> No photographer thinks images the they get on film are perfect reflections of reality. The lens itself introduces flaws&#x2F;changes as does film and developing.Don&#x27;t fall into this trap. A lens and computational photography are not alike. One is a static filter, doing simple(ish) transformation of incoming light. The other is arbitrary computation operating in semantic space, halfway between photography and generative AI. Those are qualitatively different.Or, put another way: you can undo effects of a lens, or the way photo was developed classically, because each pixel is still correlated with reality, just modulo a simple, reversible transformation. It&#x27;s something we intuitively understand, which is why we often don&#x27;t notice. In contrast, computational photography decorrelates pixels from reality. It&#x27;s not a mathematical transformation you can reverse - it&#x27;s high-level interpretation, and most of the source data is discarded.Is this a big deal? I&#x27;d say it is. Not just because it rubs some the wrong way (it definitely makes something no longer be a \"photo\" to me). But consider that all camera manufacturers, phone or otherwise, are jumping on this bandwagon, so in a few years it&#x27;s going to be hard to find a camera without built-in image-correcting \"AI\" - and then consider just how much science and computer vision applications are done with COTS parts. A lot of papers will have to be retracted before academia realizes they can no longer trust regular cameras in anything. Someone will get hurt when a robot - or a car - hits them because \"it didn&#x27;t see them standing there\", thanks to camera hardware conveniently bullshitting them out of the picture.(Pro tip for modern conflict: best not use newest iPhones for zeroing in artillery strikes.)Ultimately you&#x27;re right, though: this is an issue of control. Computational photography isn&#x27;t bad per se. It being enabled by default, without an off-switch, and operating destructively by default (instead of storing originals plus composite), is a problem. It wasn&#x27;t that big of a deal with previous stuff like automatic color corrections, because it was correlated with reality and undoable in a pinch, if needed. Computational photography isn&#x27;t undoable. If you don&#x27;t have the inputs, you can&#x27;t recover them. reply jcynix 40 minutes agorootparentWhile I agree with the gist of your comment, I cannot leave this detail uncommented>Or, put another way: you can undo effects of a lens, or the way photo was developed classically, because each pixel is still correlated with reality,You cannot undo each and every effect. Polarizing filters (filters, as are lens coatings, are part of a classical lens in my opinion), gradual filters, etc effectively disturb this correlation.As does classic development, if you work creatively in the lab (as I did as a hobby a long time ago in analog times) where you decide which photographic paper to use, how to dodge or burn, etc.But yes, I agree that computational photography offers a different kind of reality distortion. reply TeMPOraL 3 minutes agorootparent> You cannot undo each and every effect.Fair enough.> Polarizing filtersYeah, I see it. This one is as pure signal removal as it comes in analog world. And they can, indeed, drop significant information - not just reflections, but also e.g. by blacking out computer screens - but they don&#x27;t introduce fake information either, and lost information could in principle be recovered -- because in reality, everything is correlated with everything else.> But yes, I agree that computational photography offers a different kind of reality distortion.A polarizing filter or choice of photographic paper won&#x27;t make e.g. shadows come out the wrong way. Conversely, if you get handed a photo with wrong shadows, you not only can be sure it was &#x27;shopped, but could use those shadows and other details to infer what was removed from the original photo. If you tried the same trick with computational photograph, your math would not converge. The information in the image is no longer self-consistent.That&#x27;s as close as I can come up to describing the difference between the two kinds of reality distortion; there&#x27;s probably some mathematical framework to classify it better. reply bambax 1 hour agorootparentprevYes. Also, it seems inevitable that at some points photos that you can&#x27;t publish on Facebook won&#x27;t be possible to make. Is a nipple present in the scene? Then too bad, you can&#x27;t press the shutter. reply TeMPOraL 55 minutes agorootparentOr you can, but the nipple will magically become covered by a leaf falling down, or a lens flare, or subject&#x27;s hand, or any other kind of context-appropriate generative modification to the photo.Auto-censoring camera, if you like.(Also don&#x27;t try to point such camera at your own kids, if you value your freedom and them having their parent home.) reply draugadrotten 40 minutes agorootparentprev> Is a nipple present in the scene? Then too bad, you can&#x27;t press the shutter. replyOh yes you can, and the black helicopters will be dispatched, your social score obliterated and credit ratings becomes skulls and bones. EU Chat Control will morph to EU Camera Control. Think of the children! reply frogblast 6 hours agorootparentprev> In the end, this tech just takes control from you.In these difficult scenarios, the alternative photo I&#x27;d get using such a small camera without this kind processing would be entirely unusable. I couldn&#x27;t rescue those photos with hours of manual edits. That may be \"in control\", but it isn&#x27;t useful. reply autoexec 6 hours agorootparentFor decades people have had the ability to get great photos using cell phones that included useful features like automatically adjusting focus, or exposure, or flash all without their phones inventing total misrepresentations of what the camera was pointed at.I mean, at a certain point taking a less than perfect photo is more important than getting a fake image that looks good. If I see a pretty flower and want to take a picture of it, the result might look a lot better if my phone just searched for online images of similar flowers, selected one, and saved that image to my icloud, but I wouldn&#x27;t want that. reply nucleardog 5 hours agorootparentThe case in the article is obviously one that any idiot could have taken without these tools.But \"in difficult scenarios\", as the GP comment put it, your mistake is assuming people have been taking those photos all along no problem. They have not. People have been filling their photo albums and memory cards up with underexposed blurry photos that look more like abstract art than reality. That&#x27;s where this sort of technology shines.I&#x27;m pretty reasonable at getting what I want out of a camera. But at some point you just hit limitations of the hardware. In \"difficult scenarios\" like a fairly dark situation, I can open the lens on my Nikon DLSR up to f&#x2F;1.4 (the depth of field is so shallow I can focus your eyes while your nose stays blurry, so it&#x27;s basically impossible to focus), crank the ISO up to 6400 (basically more grain than photo at that point), and still not get the shutter speed up to something that I can shoot handheld. I&#x27;d need a tripod and a very still subject to get a reasonably sharp photo. The hardware cannot do what I want in this situation. I can throw a speedlight on top, but besides making the camera closer to a foot tall than not and upping the weight to like 4lbs, a flash isn&#x27;t always appropriate or acceptable in every situation. And it&#x27;s not exactly something I carry with me everywhere.These photos _cannot_ be saved because there just isn&#x27;t the data there to save. You can&#x27;t pull data back out of a stream of zeros. You can&#x27;t un-motion-blur a photo using basic corrections.Or I can pull out my iPhone and press a button and it does an extremely passable job of it.The right tool for the right job. These tools are very much the \"right\" tool in a lot of difficult scenarios. reply lukeschlather 2 hours agorootparentIn circumstances where it really matters having a prettied up image might be worse than having no image at all. If you rely on the image being correct to make some consequential decision, you could convict someone of a crime, or if you were trying to diagnose some issue with some machine you might cause damage. While if the camera gave an honest but uninterpretable picture you would be forced to try again. reply TeMPOraL 1 hour agorootparentCouple other common cases:- Photographing serial numbers or readouts on hard-to-reach labels and displays, like e.g. your water meter.- Photographing damage to walls, surfaces or goods, for purpose of warranty or insurance claim.- DIY &#x2F; citizen science &#x2F; school science experiments of all kind.- Workshops, auto-repairs, manufacturing, tradespeople - all heavily relying on COTS cameras for documenting, calibrating, sometimes even automation, because it&#x27;s cheap, available, and it works. Well, it worked.Imagine your camera fighting you on any of that, giving you bullshit numbers or actively removing the very details you&#x27;re trying to capture. Or insurance rejecting your claim on the possibility of that happening.Also let&#x27;s not forget that plenty of science and even military ops are done using mass-market cameras, because ain&#x27;t anyone have money to spend on Dedicated Professional Stuff. reply Toutouxc 4 hours agorootparentprev> For decades people have had the ability to get great photos using cell phonesNot in these light conditions. Simple as that. What iPhones are doing nowadays gives you the ability to take some photos you couldn‚Äôt have in the past. Try shooting a few photos with an iPhone and the app Halide. It can give you a single RAW of a single exposure. Try it in some mildly inconvenient light conditions, like in a forest. Where any big boy camera wouldn‚Äôt bat an eye, what the tiny phone sensor sees is a noisy pixel soup that, if it came from my big boy camera, I‚Äôd consider unsalvageable. reply autoexec 4 hours agorootparent> Not in these light conditions.Again, decades of people photographing themselves in wedding dresses while in dress shops (which tend to be pretty well lit) would disagree with you. Also, the things that help most with lighting (like auto-exposure) aren&#x27;t the problem here. That&#x27;s not why her arms ended up in three different positions at once. reply amluto 4 hours agorootparent> Apple&#x27;s horrible tech featured in the article had nothing to do with the lighting.Of course it did.iPhones take an ‚Äúexposure‚Äù (scare quotes quite intentional) of a certain length. A conventional camera taking an exposure literally integrates the light hitting each sensor pixel (or region of film) during the exposure. iPhone do not ‚Äî instead (for long enough exposures), iPhones take many pictures, aka a video, and apply fancy algorithms to squash that video back to a still image.But all the data comes from the video, with length equal to the ‚Äúexposure‚Äù. Apple is not doing Samsung-style ‚Äúit looks like an arm&#x2F;moon, so paint one in‚Äù. So this image had the subject moving her arms such that all the arm positions in the final image happened during the ‚Äúexposure‚Äù.Which means the ‚Äúexposure‚Äù was moderately long, which means the light was dim. In bright light, iPhones take a short exposure just like any other camera, and the effect in question won‚Äôt happen.(Okay, I‚Äôm extrapolating from observed behavior and from reading descriptions from Google of similar tech and from reading descriptions of astrophotography techniques. But I‚Äôm fairly confident that I‚Äôm right.) reply Kye 2 hours agorootparentThis is probably harder for people to believe if they&#x27;ve never seen a progress video of stacking long exposures.I&#x27;ve seen the kinds of images people get out of stacking images for astrophotography. Individually, the images are mostly noise. Put enough together and you get stuff like this: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230512210222&#x2F;https:&#x2F;&#x2F;imgur.com... (https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;astrophotography&#x2F;comments&#x2F;3gx29m&#x2F;st...)The phone is operating under way less harsh conditions since there&#x27;s usually quite a bit of light even in most night scenes.The iPhone is actually too good at this. You can&#x27;t do light trails: it over-weights the first image and removes anything too divergent when stacking, so you get a well-lit frozen scene of vehicles on the road. I can get around it shooting in burst mode and stack in something like Affinity Photo, but that&#x27;s work. reply astrange 3 hours agorootparentprevIt could also have been taken with too much motion (either the phone or the subject), meaning some of the closer-in-time exposures would be rejected because they&#x27;re blurry&#x2F;have rolling shutter. reply astrange 3 hours agorootparentprevYou can tell this scene isn&#x27;t well lit - just look at her reflected face. It&#x27;s too dark.Sure, some of it is bright, but that just means it&#x27;s backlit. reply bawolff 5 hours agorootparentprev> Are you saying that you&#x27;re just bad at photography?That is a bit harsh. The vast majority of all people are worse at photography than apple&#x27;s algorithm. reply autoexec 5 hours agorootparentThe claim was that any camera without this feature is \"entirely unusable\" and the photos couldn&#x27;t be saved even with \"hours of manual edits\". The fact is that for decades countless of beautiful photos have been captured with cell phone cameras without this feature and most of them needed no manual edits at all. Many of those perfectly fine pictures were taken by people who would not consider themselves to be photographers.Anyone who, by their own admission, is incapable of taking a photo without this new technology must be an extraordinarily poor photographer by common standards. I honestly wasn&#x27;t trying to shame them for that though (I&#x27;ll edit that if I still can), I just wasn&#x27;t sure what else they could mean. Maybe it was hyperbole? reply oktoberpaard 1 hour agorootparentThe two of you might have a different threshold of what you consider to be usable photos, and that‚Äôs fine. However, there is no way around physics. Under indoor lighting, a single exposure of a cellphone camera will either be a blurry mess, a noisy mess, or both. Cellphones used to get around that by adding flashlights and adding strong noise suppression, and it was up to the photographer to make sure that the subject didn‚Äôt move too much. Modern smartphones let you take pretty decent photos without flash en without any special considerations, by combining many exposures automatically. I think I t‚Äôs quite amazing. The hardware itself has also improved a lot, and you can also take a better single exposure photo than ever, but it won‚Äôt be anywhere near the same quality straight out of the camera.And, yes, I have been taking a lot of pictures with my Sony Ericsson K750i almost two decades ago and I did like them enough to print them back then, but even the photos taken under perfect lighting conditions don‚Äôt stand a chance to the quality of the average indoor photo nowadays. The indoor photos were all taken with the xenon flash and were very noisy regardless. reply adgjlsfhk1 5 hours agorootparentprevtraditionally phones took good photos in good light but as the light decreases so does photo quality (quickly). the point of the ai photography isn&#x27;t to get the best photograph when you control the lighting and have 2 minutes before hand to pick options, it&#x27;s to get the best photograph when you realize you need the best possible photo in the next 2 seconds reply FalconSensei 5 hours agorootparentprevyou can still install any other photo app on your phone and use that instead reply andrei_says_ 2 hours agorootparentprevIs an argument about corner cases where computational improvement does make sense an argument in good faith when used as an excuse to take away control in all cases? reply jack_pp 2 hours agorootparentAfaik there are photo apps on the iphone that let you have all the control you want so you can use those when you want control and use the default when you just want a quick photo knowing the pros&#x2F;cons. reply easyThrowaway 1 hour agorootparentprevThe first thing they teach you at any photography course worth its money is that framing the picture itself is a distortion of reality. It&#x27;s you deciding what&#x27;s worth being recorded and what should be discarded.There&#x27;s no \"objective photography\", no matter how hard we try distinguishing between old and new tech. reply esskay 50 minutes agorootparentprev> If you&#x27;re fine with Apple deciding what the subject of pictures should be and how they should look that&#x27;s fine, but I expect a lot of people wont be.The majority dont care, as the majority are not photographers, nor is it meant to be a product for photographers. Average joe just wants a good photo of that moment, which it does exceptionally well.The tech &#x27;taking control from you&#x27; is its exact purpose, as again, not everyone is a photographer. The whole point is to allow &#x27;normal people&#x27; to get a good photo at the press of a button, it&#x27;d be incredibly foolish and unreasonable to expect them to faff about with a bunch of camera settings so it does it for you. reply kazinator 4 hours agorootparentprevNo lens is going to produce a straight arm out of one that was bent at the elbow, or vice versa, while not disturbing anything else. It is not \"photography\". reply eproxus 2 hours agorootparentNo lens is going to reproduce a perfectly straight line without lens compensation either. Lenses are bending light to distill a moment in space time into a 2D representation which by definition will always be imperfect. What counts as ‚Äúphotography‚Äù really?GP> In the end, this tech just takes control from you.In the end, any tech is always going to take control away from you one way or another. That‚Äôs the whole point of using it, so you can achieve things you wouldn‚Äôt otherwise be able to. reply kazinator 1 hour agorootparentThough it may be that no lens will reproduce a perfectly straight line, the transformation which warps the line is a relatively simple and invertible mapping which preserves local as well as global features.It might not be a conformal mapping but, say, if the image of a grid is projected through the lens, the projected image will have the same number of divisions and crossings, in the same relation to each other.We cannot say this about an AI-powered transformation which changes a body posture.> What counts as ‚Äúphotography‚Äù really?Focusing light via a lens into an exposure plate, from which an image is sampled. reply addaon 3 hours agorootparentprevDepends on the refractive index of the lens [1].[1] https:&#x2F;&#x2F;gebseng.com&#x2F;media_archeology&#x2F;reading_materials&#x2F;Bob_S... reply bawolff 5 hours agorootparentprev> Those kinds of choices are part of what make photography an art.Ah yes, for all those people using their iphone to make true art instead of for taking a selfie.The market for cell phone cameras are generally casual users. High end artists generally use fancy cameras. Different trade offs for different use cases. reply vlovich123 5 hours agorootparentI think you‚Äôd be surprised how much phones have encroached on the fancy camera space. Also, the limiting factor in phone camera quality is more the lens than anything else. The SW backing it is much more powerful than what you get in standalone cameras although those manufacturers are trying to keep up (that‚Äôs why phone cameras can be so competitive with more expensive ones). I expect within the next 10 years we‚Äôll see meaningful improvements to the lenses used in phones to the point where dedicated cameras will have shrunk even more (i.e. decoupling the size of the lens &#x2F; sensor from the surface area on the back of the phone). reply FalconSensei 5 hours agorootparent> The SW backing it is much more powerful than what you get in standalone camerasthat is very due to camera makers not giving a sh*t though. Maybe now they do, but it&#x27;s overdue. And I&#x27;m not even talking about the phone apps for connection, or image processing, but more on a user interface and usability PoV, and also interesting modes for the user.Fuji and Ricoh are the only ones that I see are trying to make things easier or more fun or interesting for non-professionals. Fuji has the whole user customization that people use for recipes and film simulation (on top of the film simulations they already have), and Ricoh is the only one (I know of) that has snap focus, distance priority, and custom user modes that are easy to switch to, and to use. But even Fuji and Ricoh could still improve a lot, since there&#x27;s always a detail or another that I&#x27;m like... why did they made it like that? or... why didn&#x27;t they add this thing? reply wkat4242 4 hours agorootparentI don&#x27;t think it&#x27;s only that. The processing in a phone is expensive. If you&#x27;d build a high end camera with a Snapdragon 8 gen 3 it would raise the price a lot. In a phone that&#x27;s not an issue because you need the chip there anyway for other tasks. So there&#x27;s just much more compute power available, not to mention a connection to the cloud with even more potential compute. reply eproxus 2 hours agorootparentWould be cool if there ever were a product like a large external sensor and lens hardware dongle for phones (essentially all parts of a system camera without the computer) that would use the phone as a software processing platform. They would ‚Äújust‚Äù ship the raw data over to the phone for post processing. reply astrange 3 hours agorootparentprevAlso, many more people buy phones than standalone cameras, so they can afford a lot more software R&D. reply wkat4242 2 hours agorootparentWell yes but a lot of that can be reused across devices I would guess. reply vlovich123 2 hours agorootparentNaw, astrange is correct. At peak in 2010, 121 standalone cameras were shipped worldwide. By 2021, that number was down to ~8 million. By comparison, 83 million smartphones are shipped each quarter (~1.4 billion for the year). Those kinds of economies of scale means there&#x27;s more R&D revenue to sustain a larger amount of HW & SW innovation. Even though smartphone shipments should come down over the next few years as they incremental jump each year is minimal & the market is getting saturated, there&#x27;s always going to be more smartphone shipments.Individual camera vendors just can&#x27;t compete as much and I don&#x27;t think there was possibly anything they could have done to compete because of the ergonomics of a smartphone camera. The poor UX and lack of computational photography techniques doesn&#x27;t matter as much because the pro market segment is less about the camera itself and more about the lenses &#x2F; post-processing. Even professional shoots that use smartphones (as Apple likes to do for their advertising) ultimately tend to capture RAW + desktop&#x2F;laptop post-processing when they can because of the flexibility &#x2F; ergonomics &#x2F; feature set of that workflow. The camera vendors do still have the advantage of physical dimensions in that they have a bigger sensor and lenses for DSLR (traditional point & click I think is basically dead now), but I expect smartphones to chip away at that advantage through new ways of constructing lenses &#x2F; sensors. Those techniques could be applied to DSLRs potentially for even higher quality, but at the end of the day the market segment will just keep shrinking as smartphones absorb more use-cases (interchangeable lenses will be the hardest to overcome).Honestly, I&#x27;m surprised those device manufacturers haven&#x27;t shifted their DSLR stack to just be a dumb CMOS sensor, lens, a basic processor for I&#x2F;O, and a thunderbolt controller that you slot the phone into. Probably heat is one factor, the amount of batteries you&#x27;d need would go up, the BOM cost for that package could be largely the same, & maybe the external I&#x2F;O isn&#x27;t quite yet fast&#x2F;open enough for something like that. reply wkat4242 1 hour agorootparentGood points.> Honestly, I&#x27;m surprised those device manufacturers haven&#x27;t shifted their DSLR stack to just be a dumb CMOS sensor, lens, a basic processor for I&#x2F;O, and a thunderbolt controller that you slot the phone into. Probably heat is one factor, the amount of batteries you&#x27;d need would go up, the BOM cost for that package could be largely the same, & maybe the external I&#x2F;O isn&#x27;t quite yet fast&#x2F;open enough for something like that.This is not really an option. There were actually clip-on cameras back in the day when cameras were new on phones. But there were several problems:- Software support tended to lag with updates- Phones change form factor very regularly and phones in general are replaced much more often than camera hardware, leaving you a highly expensive paperweight- I don&#x27;t think any phones have thunderbolt yet, just some tablets- Dealing with issues is a nightmare because you don&#x27;t control the hardware end to end replygraypegg 6 hours agorootparentprevTo play devils advocate here: the best camera is the one you have with you, and computational photography does mean you can just push a button without thinking and get a clear picture that captures a memory.It‚Äôs obviously an argument to say that Apple shouldn‚Äôt get to choose how your memories are recorded, but I know I‚Äôve captured a lot more moments I would‚Äôve otherwise missed because of just how automatic phone cameras are.There‚Äôs a place for both kinds of cameras IMO. reply autoexec 5 hours agorootparent> computational photography does mean you can just push a button without thinking and get a clear picture that captures a memory.I&#x27;d argue that the women trying to take a photo of herself in her wedding dress did not get a clear picture that captures a memory. She got a very confusing picture that captured something which never happened. There are lots of great automatic camera features which are super helpful, but don&#x27;t falsify events. If I take a picture of my kid I want my actual child in the photo, not a cobbled together AI generated monstrosity of what apple thinks my kid ought to have looked like in that moment.Automatic cameras are great. Cameras that outright lie to you are not. reply dahart 4 hours agorootparent> don‚Äôt falsify events [‚Ä¶] Cameras that outright lie [‚Ä¶] AI generated monstrosity of what apple thinksOh the irony of framing things (pun intended) so hyperbolically. Somehow it never seems to dawn on people that like to throw around the word ‚Äòlie‚Äô that they‚Äôre doing exactly what they‚Äôre complaining about, except intentionally, which seems way worse. Nobody sat down to say bwahahah let‚Äôs make the iphone create fake photos, the intent obviously is to use automated methods to capture the highest quality image while trying to be relatively faithful to the scene, which might mean capturing moving subjects at very slightly different times, in order to avoid photo-wrecking smudges. When you blatantly ignore the stated intent and project your own negative assumptions of other people‚Äôs motivations, that becomes consciously falsifying the situation.Photographs are not and never have been anything but an unrepresentative slice of a likeness of a moment in time, framed by the photographer to leave almost everything out, distorted by a lens, recolored during the process, and displayed in a completely different medium that adds more distortion and recoloring. There is no truth to a photograph in the first place, automatic or not, it‚Äôs an image, not reality. Photos have often implied the wrong thing, ever since the medium was invented. The greatest photos are especially prone to being unrealistic depictions. Having an auto stitch of a few people a few milliseconds apart is no different in its truthiness from a rolling shutter or a pano that takes time to sweep, no different from an auto shutter that waits for less camera shake, no different from a time-lapse, no different from any automatic feature, and no different from manual features too. Adjusting my f-stop and focus is really just as much distorting reality as auto-stitching is.Anyway, she did get a clear memory that was quite faithful to within a second, it just has a slightly funny surprise. reply ClumsyPilot 53 minutes agorootparent> Photographs are not and never have been anything but an unrepresentative slice of a likeness of a moment in time,This is a complete absurdityPhotos are used daily to find suspects of crimesto convict people to prisonto establish the level of devastation in a warTo find launch site for rocket attacksTo make scientific measurements of distance,size, etc. in architecture and warBy scientists to determine position of things in the sky, meteors, etc.You are like a guy that collects knifes and swords and has no idea what they are actually for reply mplewis 3 hours agorootparentprevWithout the iPhone‚Äôs computational camera, she wouldn‚Äôt have this photo at all because she wouldn‚Äôt have a camera in her pocket that could get a good picture in this situation. reply WWLink 5 hours agorootparentprevI used to keep a powershot digital elph in my pocket whenever I left the house. TBH I took way more pictures with it. I could blindly turn it on and snap a picture while driving, without ever taking my eyes off the road. There&#x27;s no way in the world I could do that with an iphone lol. I mean, I suppose if I happened to hit the bottom right corner button and then used the volume button maybe? Maybe. It&#x27;s way more likely I&#x27;d cock it up lol. reply nextaccountic 2 hours agorootparentprev> In the end, this tech just takes control from you. If you&#x27;re fine with Apple deciding what the subject of pictures should be and how they should look that&#x27;s fine, but I expect a lot of people wont be.Can&#x27;t this be disabled?And can&#x27;t some other product with a similar technology offer knobs for configuring exactly how it behaves? reply orbital-decay 6 hours agorootparentprev>In the end, this tech just takes control from you.1. \"This tech\" is too broad of a qualifier, as GP is talking about computational photography in general, which is many things at once. Most of those things work great; some others are unpredictable. There are plenty of custom camera apps besides Apple&#x27;s default camera which will always try to stay as predictable as possible.2. There is such a thing as too much control. Proper automation is good, especially if you aren&#x27;t doing a carefully set up session. The computational autofocus in high-end cameras is amazing nowadays. You can nail it every time now without thinking, with rare exceptions. reply dumbfounder 5 hours agorootparentprevBut regular people expect it. And then they take a picture of the moon and they are disappointed. reply astrange 3 hours agorootparentIt&#x27;s easy to take a picture of the moon. It&#x27;s always lit the same because it&#x27;s in space.ISO 100, f&#x2F;11, manual focus at infinity, shutter speed 1&#x2F;100.Phones aren&#x27;t good at this, partly because autofocus isn&#x27;t designed for moons, partly because you want a really long lens and they don&#x27;t have one. reply ALittleLight 6 hours agorootparentprevI think you have that backwards. The vast majority of people will prefer Apple&#x27;s computational enhancements. A small number of photography enthusiasts will prefer manual control (and a smaller number will benefit from it). reply sharkweek 6 hours agorootparentYeah I 100% agree - When I pull my phone out to take a picture of the kids&#x2F;my dog&#x2F;something randomly interesting, all I want is to point my camera at whatever I want to capture and have it turn out.Don‚Äôt care how it happens I don‚Äôt want to think about settings. reply katbyte 6 hours agorootparentprevand for those who don&#x27;t there are a bunch of photo apps that allow far more control & iphones can shoot raw now reply orbital-decay 6 hours agorootparentRAW in modern phones and apps is often stacked&#x2F;processed as well. However, it always tries to stay as photometrically correct as possible (at least I&#x27;m not aware of any exceptions). All \"dubious\" types of processing happen after that. reply willseth 6 hours agorootparentprevTry to photograph a toddler without this feature. Good luck. Does the fact that the iPhone allows me to do this pretty reliably without any fuss mean I have more or less control? reply _trampeltier 2 hours agorootparentA photo from a toddler. On the next try, the Iphone will call the police reply orbital-decay 5 hours agorootparentprevIt&#x27;s solved with constantly taking pictures into a ring buffer, either compensating for the touchscreen lag automatically or letting you select the best frame manually with a slider after the shot. Most cameras can do that (if you disable the best frame auto-selection). reply leephillips 6 hours agorootparentprevOdd. I have hundreds of great shots of my toddlers, none taken with these ‚Äúfeatures‚Äù. reply willseth 5 hours agorootparentI know you‚Äôre pretending to be dense on purpose, but taking a dozen pictures at once and automatically picking the best one can obviously save a lot of shots. Same with combining exposures. reply autoexec 5 hours agorootparentThe camera in this case wasn&#x27;t taking many shots and selecting one. It wasn&#x27;t just combining exposures either. It did a bunch of shitty cut&#x2F;paste operations to produce an altered composite image which showed something that never happened.Some automatic features are wonderful. People have been able to take pictures of fast moving toddlers and pets for ages because of them. The camera \"features\" that secretly alter images to create lies and don&#x27;t give you a photograph of what you asked them to capture are a problem. reply willseth 5 hours agorootparentI didn‚Äôt misunderstand what happened, and I know how the iPhone camera works. I was only noting a couple common cases where computational photography is really nice. I‚Äôm pretty picky about my photos and nothing I‚Äôve shot with my iPhone has ever approached ‚Äúlies‚Äù. I know those types of anomalies can happen, but they‚Äôre clearly design flaws, i.e. Apple doesn‚Äôt actually want their camera to make fantasy images. I don‚Äôt want that either. reply TeMPOraL 1 hour agorootparent> I‚Äôm pretty picky about my photos and nothing I‚Äôve shot with my iPhone has ever approached ‚Äúlies‚Äù.That you know of.There could be many lies in those photos, both subtle and blunt, which you didn&#x27;t notice at the moment because you weren&#x27;t primed to look for them, and which you won&#x27;t notice now, because you no longer remember the details of the scenes&#x2F;situations being photographed, days or months ago.Are you sure I&#x27;m wrong? Are you sure that there are no hard lies in your photos? Would you be able to tell? replycolordrops 5 hours agorootparentprevIf this feature or similar features could be disabled, it doesn&#x27;t need to be an either&#x2F;or situation. I don&#x27;t have an iPhone though, so no idea if it&#x27;s configurable. But seems that it should be, assuming it&#x27;s not implemented deep in hardware. reply autoexec 5 hours agorootparentI agree. I&#x27;d have no issue with this if it were something people could disable and it was clearly disclosed and explained as the default. reply TeMPOraL 1 hour agorootparentDoesn&#x27;t even need to have an off-switch, if it would preserve the inputs that went into AI stitching magic, so that one could recover reality if&#x2F;when they needed it. reply lo_zamoyski 4 hours agorootparentprev> In the end, this tech just takes control from you.Having control isn&#x27;t always the best option. While photographers may appreciate having a camera over which they can have a high degree of control, they&#x27;re not snobs about it. Photographers will tell you that having a handy point and shoot with good automation or correction of some kind is extremely useful when you&#x27;re out and about and need to take a photo quickly. For everyday photos, it&#x27;s what you&#x27;ll want usually. If not, there are plenty of cameras on the market to choose from. reply afavour 6 hours agoparentprevI have no objection with what we have today either but it does feel slippery slope-y, especially in the AI era. Before we know it our cameras won‚Äôt just be picking the best frame for each person, it‚Äôll be picking the best eye, tweaking their symmetry, making the skin tone as aesthetically pleasing as possible, etc etc etc. It was always this way to an extent but now that we‚Äôve given up the pretence of photos being ‚Äúreal‚Äù it feels inevitable to me.I‚Äôm reminded of that scene in WALL-E where it shows the captain portraits as people get fatter and fatter. It‚Äôs clearly inaccurate: over time the photos should show ever more attractive, chisled captains. They‚Äôd still be obese in real life though. reply simias 6 hours agorootparentWhile I don&#x27;t use TikTok I often see videos from there and it&#x27;s really spooky to me how aggressive and omnipresent filtering seems to have become in that community. Even mundane non-fashion non-influencer \"vlog\" style content is often heavily filtered and, even more scary IMO, I often don&#x27;t notice immediately and only catch it if there&#x27;s a small glitch in the algorithm for instance when the person moves something in front of their face. And sometimes you can tell that the difference from their real appearance is very significant.I really wonder what&#x27;s that doing to insecure teenagers with body issues. reply _kb 6 hours agorootparentprevThat already happens, in realtime. FaceTime uses eye gaze correction, Cisco is all in on AI codecs for image and audio compression, and other vendors are on similar tracks too.When you talk to a client, a colleague or a loved one we‚Äôre on the verge of you conversing with a model that mostly represents their image and voice (you hope). The affordances of that abstraction layer will only continue to deepen from here too. reply karlshea 1 hour agorootparentThis is nothing new. Very old examples are the xerox machines that accidentally changed numbers on scanned documents, and speech coding for low bit rate digital audio over radio, phone, and then VoIP. reply bigallen 5 hours agorootparentprev> When you talk to a client, a colleague or a loved one we‚Äôre on the verge of you conversing with a model that mostly represents their image and voice (you hope)Wow, this threw me for a loop. There‚Äôs not much difference between talking to a heavily filtered image of a person and texting with them. Both the image and the words are merely representations of the person. Even eye-to-skin perceiving someone is a representation of their ‚Äúperson‚Äù to a degree. The important part is that ‚Äúhow‚Äù and ‚Äúhow much‚Äù the representation differs from reality is known to the observer reply easygenes 6 hours agorootparentprevPhones over-beautifying faces by default already happened with the iPhone XS, and it wasn&#x27;t received well. See #beautygate: https:&#x2F;&#x2F;www.imore.com&#x2F;beautygate reply astrange 3 hours agorootparent\"Over-beautifying\" never happened. Noise reduction&#x2F;smoothing happens in camera processing even if you don&#x27;t try to do it, because it&#x27;s trying to preserve signal and that&#x27;s not signal. If you want that noise, you have to actually put in processing that adds it back. reply saiya-jin 38 minutes agorootparentSure it did, no wrinkles, no moles unless huge, skin tone like after 2 weeks vacation in Carribean. What previously had to be done in photoshop to make people look younger is now done automatically, for every photo, and you can&#x27;t turn it off.They call it &#x27;instagram look&#x27; for quite some time, Apple is the worst among all phone manufacturers (in form of furthest from actual ugly reality, but a lot of people got used to it and actually prefer it now), but all of them are making it. reply WWLink 5 hours agorootparentprevSamsung phones (and I guess iphones too) already do this lmao.https:&#x2F;&#x2F;www.insider.com&#x2F;samsung-phones-default-beauty-mode-c...My favorite one is the phones that put a fake picture of a moon in pictures where the moon is detected!https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;3&#x2F;13&#x2F;23637401&#x2F;samsung-fake-moo... reply astrange 3 hours agorootparent> My favorite one is the phones that put a fake picture of a moon in pictures where the moon is detected!This is just about as wrong as saying Stable Diffusion contains a stock photo of the moon it spits out when you prompt it with \"the moon\".They work the same way. Samsung&#x27;s camera has a moon mode, so it gets the prior (a much, much lower quality camera raw than you think it&#x27;s getting), it processes it with a bias (this noise is a Gaussian distribution centered on the moon), and you get a result (an image that looks like the moon). reply thaumasiotes 4 hours agorootparentprev> I have no objection with what we have today either but it does feel slippery slope-y, especially in the AI era. Before we know it our cameras won‚Äôt just be picking the best frame for each person, it‚Äôll be picking the best eye, tweaking their symmetry, making the skin tone as aesthetically pleasing as possible, etc etc etc.What we have today already includes phone cameras that will add teeth to the image of your smiling newborn, or replace your image of what looks like the moon with stock photography of the moon.> I‚Äôm reminded of that scene in WALL-E where it shows the captain portraits as people get fatter and fatter. It‚Äôs clearly inaccurate: over time the photos should show ever more attractive, chiseled captains.Interestingly, the opposite thing happened with official depictions of Roman emperors. reply hibikir 5 hours agoparentprevMy issue with this has nothing to do with purism, but with how often the results are just no good, for reasons that have nothing to do with the sensor, but the choices of whatever model they run. Does it take a picture at night? Yes, but it&#x27;s often unrecognizable compared to what my own sensor, my eyes, sees. It&#x27;s not a matter of a slightly better reality, but the camera making choices about how light should go that have nothing to do with the composition in front of it.You might remember an article about how there are many situations where the iphone just takes bad portraits, because its idea of what good lightning is breaks down. 5 year old phones often take pictures I like more than one of the latest phones, and not because the hardware was better, but because the tuning is just bad.Fun things also happen when you take pictures of things that are not often in the model: Crawlspaces, pipes, or, say, dentistry closeups. I&#x27;ve had results that were downright useless outside of raw mode, because computational photography step really had no idea of what it was doing. It&#x27;s not that the sensors are limited, but that the things that the iphone does sometimes make the picture far worse than in the past, when it took fewer liberties. reply pasabagi 18 minutes agoparentprevI figure that digital photography is by its very nature &#x27;computational&#x27;, both in the obvious sense, and in the sense that the camera from hardware up imposes a set of signal-forming decisions on what is essentially just structured noise.The problem is more one of what controls the camera exposes to the user. If you can just take one kind of picture: whatever picture the engineers decided was &#x27;good&#x27;, then it limits your expressive options. reply cesaref 27 minutes agoparentprevIt&#x27;s best to just think of it as a different art form.B&W film photography + darkroom printing is an art form, as is digital photography + photoshop. These modern AI assisted digital photography methods are another art form, one with less control left to the photographer, but there&#x27;s nothing inherently wrong with that. I wouldn&#x27;t want to say which is better, it&#x27;s not really an axis that you can use to compare art is it?At the end of the day, do you generate an image which communicates something that the photographer had in mind at the time? If so, success! reply makeitdouble 34 minutes agoparentprevHow many \"purists\" are there in the wild ? I&#x27;d only see police and insurance agents needing pixel perfect depiction of reality as it&#x27;s out of the sensor.Photography as an art was never about purity, and I think most of us want photos that reflect what we see and how we see it, and will take the technical steps to get that rendered. If the moon is beautiful and gently lights the landscape, I want a photo with both the bright moon and shadowy background, and will probably need a lot of computation for that.But the doppelganger brides, or the over-hdred photos, or the landscapes with birds and pilars removed aren&#x27;t what someone is seeing. They can be nice pictures, but IMO we&#x27;re entering a different art than photography. reply kazinator 4 hours agoparentprev> Purists will always hate the idea of computational photographyA non-purist will hate it too, as soon as the technology re-imagines the positions of his hands such that they are around someone&#x27;s throat. reply ClumsyPilot 37 minutes agorootparentexactly, this tech eull send innocent people to prison reply alentred 1 hour agoparentprevAs a layman in photography, I agree with you.But it is easy to understand the artists. It is said that in art everyone needs to master the technique first, the tool of the trade, but true works of art are the expressions that are created with these various techniques. At this point, a tool - computational photography in this case - may get in a way. So, it is not about purism. Quite the contrary, it is about being able to use the tools and bend the reality the way an artist wants.Having said that, I would think anyone would normally use *all* the tools available at their disposal, and the truth is that iPhone camera among else is a great one anyway. reply tonmoy 4 hours agoparentprevI just want to be able to turn it off when I want to reply adamredwoods 5 hours agoparentprevWhy not both?I am more in the purist camp, because when people take iOS photo, I remind them that someone else made that decision on how the photo should look. Additionally, we are in an era of not trusting anything on the internet or in a photo anymore. Do we want photojournalism to go that same path? I don&#x27;t. So I enjoy being closer to \"reality\" than the computational photos, but for average entertainment photos, I don&#x27;t mind. reply saiya-jin 43 minutes agoparentprevYou are overblowing things, 99% of the photographers out there and 99% of the professionals out of those don&#x27;t have this sentiment, since its primitive emotional one and detrimental to any actual work. Maybe few loud people desperate for attention make different impression on you about the state of affairs, but these days for any topic internet discussions can easily twist perception of reality and give very wrong impression.You simply have to be practical, use the best took for the job you need. If you ever actually listened to photo artists talking among their peers about their art (ie Saudek), they practically never talk about technical details of cameras or lenses, its just a tool. If they go for analog photography its because they want to achieve something thats easier for them like that maybe due to previous decades of experience, not some elitist Luddites mindset. Lightning of the scene, composition, following the rules and then breaking them cleverly, capturing&#x2F;creating the mood etc are what interests them. reply dkarras 6 hours agoparentprevwhen you think about it, with rolling shutter, no row (or column?) of pixels are from the same moment in a given picture unless you are shooting with a global shutter camera - which is rare for consumer type devices. reply stevage 5 hours agoparentprevI think purists are fine with it as long as you can turn it off. reply rmaccloy 5 hours agoparentprevIt&#x27;s honestly better than this on all fronts, since you can get ProRAW out of recent iPhones even in the default camera app and get RAW without DeepFusion out of different alternative camera apps.I think I had to spend ~$1k to get my first DSLR with RAW support back in the 2000s. Adjusted for inflation, Halide + a recent iPhone feels like a pretty good deal. reply unethical_ban 2 hours agoparentprevThat last example is where I draw the line. It&#x27;s one thing to enhance an image, or to alter the contrast across multiple frames to capture a more vibrant photo and a challenging lighting. But for our photography apps to be instantly altering the actual reality of what is occurring in a photo, such as whether someone is smiling or has their hands in a certain pose, or whether they&#x27;re in a crowd or all alone next to a landmark, is not a feature that I think should be lauded. reply crotho 6 hours agoparentprevThere&#x27;s a big difference between looking at reality through a bad filter and looking at a completely different constructed reality. reply sinuhe69 1 hour agoprevOf course, this is one in a million chance but it highlights very nicely IMO a much bigger issue: what is reality?Some would say only the classical optical camera would capture faithfully our reality. But does it? The reality of the sunlight is a broad spectrum of radio emissions: UV, infrared and more. Does the optical camera capture these? No. Thus, which reality does it capture? Our perceived reality? Other would argue: at least the optical system would capture events in time faithfully. But does it? What would we see in a femto second? Certainly not the pictures we normally see. So the results of an optical system are also super imposed realities, not very much different than the results of a computational photography.There is simply no single one reality, only our perceived realities. But if so, can we still call it reality or it‚Äôs merely a product of our sense, our perception and hallucination? reply TeMPOraL 59 minutes agoparent> There is simply no single one reality, only our perceived realities.This does not follow at all from your earlier paragraph.The reality we&#x27;re talking about here, which regular photography reflects while computational one doesn&#x27;t, is the correlation of recorded data with the state of the world. The pixels of a regular photo are highly correlated with reality - they may have been subject to some analog and digital transformation, and of course quantization, but there&#x27;s a straightforward and reversible (with some loss of fidelity) function mapping pixels to the photographed event. Computational photography, in contrast, decorrelates pixels from reality, and discards source measurements, leaving you with a recording of something that never happened, but is sort of similar to the thing that did.I elaborated on this elsewhere in the thread, so let me instead point at another way of noticing the difference. Photogrammetry is the science and technology of recovering the information about reality from photos, and it works because pixels of regular photos are highly correlated with reality. Apply the same techniques to images made via computational photography, and the degree of uncertainty and fidelity loss will reflect the degree to which the computational photos are AI interpretations&#x2F;bullshit. reply belugacat 7 minutes agoparentprevThe classical optical camera does not capture anything. It is a light sealed box, with a pinhole for a lens. As an optical system, it interacts with electromagnetic waves that go through it, that&#x27;s the only &#x27;reality&#x27; you can really care about.What captures an image is an imaging surface; traditionally a chemical emulsion on a piece of film, now a complex array of digital sensors.This imaging surface is of human design, it therefore images what its designers designed it to image. But don&#x27;t forget that it is a sampling of reality; by definition always partial, and biased (biased to the 400~700 nm range, for starters). reply Gigachad 1 hour agoparentprevProbably something closest to what your eye sees is ideal for most photos. But dumb optical cameras have all kinds of artefacts that eyes don‚Äôt. When I slightly bump the camera, the whole image comes out blurry, my eyes don‚Äôt do that.Things like lens flares don‚Äôt exist either. reply andreicap 9 minutes agorootparent> all kinds of artefacts that eyes don‚Äôt.Eyes do have lots of artefacts, your brain fills in the gaps, like the blind spot [1] It&#x27;s not much more different than computational photography, really.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blind_spot_(vision) reply sanroot99 28 minutes agoparentprevBut computational should be able to atleast reproduce what optical does reply verisimi 50 minutes agoparentprev> what is reality?Well, the only thing we can say is that we can eliminate Apple phones and their software from our enquiries! reply ClumsyPilot 39 minutes agoparentprev> The reality of the sunlight is a broad spectrum of radio emissions: UV, infraredThats irrelevant, radar reflects reality, photoshop doesn&#x27;t. Even children understand this distiction. reply dgacmu 7 hours agoprevI had a related experience with my Pixel 7 a few months ago. In this picture of a deer:https:&#x2F;&#x2F;photos.app.goo.gl&#x2F;qChwaw9C29WVdAmr6If you zoom in, you&#x27;ll note that .. everything at a detail level looks like an oil painting, especially the deer&#x27;s face and the wall behind it. Very weird effect, and that&#x27;s certainly not what the wall or deer actually look like. No filters applied.Computational photography is really awesome and modestly worrisome. reply rafram 7 hours agoparentThat‚Äôs due to basic denoising, not anything more advanced. I have photos from my iPhone 6s circa 2016 that look similar when you zoom in. reply KennyBlanken 5 hours agorootparentThat&#x27;s not \"basic denoising\" and photos from my 8 look nothing like that; the \"watercolor\" or \"oil painting\" look started around the iPhone 12.Basic denoising is stuff like chroma or hue smoothing. This is very aggressive patterning. That&#x27;s a daylight photo that the phone turned into an oil painting.https:&#x2F;&#x2F;www.google.com&#x2F;search?q=pixel+photo+oil+painting+loo... reply dagmx 3 hours agorootparentIt‚Äôs just what happens when you really aggressively do noise reduction on small sensor data.The noise pattern of any sensor that small will blotch in patterns that when combined with aggressive noise reduction, it will look this way.It‚Äôs less apparent on bigger sensors but you‚Äôll see the same if you really crank the noise reduction reply panarky 3 hours agorootparentYes, it&#x27;s denoising and sharpening due to extreme digital zoom that only uses a small portion of an already tiny sensor. reply dgacmu 2 hours agorootparentThat deer image definitely had digital zoom (I wasn&#x27;t standing next to the critter). reply dgacmu 7 hours agorootparentprevinteresting - thank you! reply Aurornis 7 hours agoparentprevThat‚Äôs denoising. Those mosaic like patterns are more pleasing to the eye than a noisy photograph.The laws of physics put a limit on how well a tiny phone lens and sensor can capture light. We‚Äôre not at the limit yet even though hardware is quite good. Still, you wouldn‚Äôt like if your phone spat out raw, noisy images everywhere without any processing (beyond what‚Äôs necessary to translate it to the right pixel grid and color space). reply kortilla 6 hours agorootparentIs there any way to get the noisy version? Or is that all forced by the hardware. reply Ambroos 6 hours agorootparentShooting in RAW gets you the real sensor data without any computational magic. reply datadrivenangel 6 hours agorootparentIncorrect. It gets you less editorial magic, but all cameras make decisions about how they create RAW images.Even cameras that don&#x27;t make any intentional decisions end up making decisions because of the physics of the sensor as an electrical device and how you read each sensor element. reply dharmab 5 hours agorootparentprevMy Fuji XA5 has denoising in the RAW files. reply helf 6 hours agorootparentprevThat&#x27;s not entirely true and deoends heavily on the phone. reply helf 6 hours agorootparentprevDepends. The DNG&#x2F;Raw photos you can force out of gcam will sorta have the noise. Less processing &#x2F;sorta&#x2F;. HDR+ with gcam is going to be exceptionally \"touched\" no matter what.You can modify the camera drivers etc on some devices to get nearly no processing output (other than the debayering color filtering etc by the ISP). I custom modded an LG V20 that I use for my mobile photography. I&#x27;ve either totally disabled or adjusted nearly every function of the signal processor as well as modified the camera program itself. I also use a custom modified gcam on it.Some samples: https:&#x2F;&#x2F;imgur.com&#x2F;gallery&#x2F;W42TVGfLook at my user (helforama) on imgur and pretty much every uploaded photo I took and edited on my V20... I need to update that account with newer photos :) been using the phone aince 2016 for photos! reply throwaway290 5 hours agorootparentprev> Those mosaic like patterns are more pleasing to the eye than a noisy photograph.Noise does not automatically look worse than this weirdness. It can look bad when resizing with fast algos (moire and all). And of course noise is super fine detail so preserving it blows up filesize. reply jimmux 7 hours agoparentprevIt looks like it&#x27;s treating the deer and wall as the same subject, averaging the texture, and applying it to both. reply tmalsburg2 7 hours agoparentprevThis effect likely resulted from aggressive noise filtering. You can probably reduce the amount of filtering in the settings. reply luuurker 5 hours agoparentprevWas that picture taken with zoom?I ask because the Pixel 7 doesn&#x27;t have a dedicated zoom camera, so any zoom was digital which reduces quality a lot. The processing part has to use low quality frames and what you have there is often the result.Phone users shouldn&#x27;t have to think about this, but from experience I think that on phones without a zoom camera it&#x27;s often better to take a photo without zoom (or avoid going past 2x) and crop the image afterwards. reply dgacmu 2 hours agorootparentYes, it was. reply zoklet-enjoyer 7 hours agoparentprevThis is how all my bird and squirrel photos look. It really disappoints me reply Toutouxc 5 hours agorootparentWell, in the big camera world, birding and wildlife means huge and expensive lenses that are both fast (i.e. they let a lot of light in) and have a large focal length. The fact that you can even attempt to take similar photos on something as tiny as a phone camera and have the object come out recognizable is nothing short of amazing. reply luuurker 5 hours agoprevA modern Google Pixel starts saving frames as soon you open the camera app. When we finally take the picture, it uses some of the older frames to have what&#x27;s essentially HDR stacking without the delay.I wouldn&#x27;t be surprised if a similar thing happened here. Different frames, processing picks the best exposure for each part of the picture and you get this effect. reply gnicholas 4 hours agoprevIn my family we like to take &#x27;pano-rankenstein&#x27; photos, where you do a pano across a person&#x27;s face as they are rapidly changing expressions in dramatic ways. The results are pretty hilarious, as the phone tries to stitch your face together into one cohesive image. reply dusted 52 minutes agoprevThis happens more and more, I guess it&#x27;s an unholy mix of \"better compression as long as you don&#x27;t actually look at the image\" and \"ai improvement\"..A few weeks ago, I took a really lovely picture of my son, composition, facial expression, focus, light, it was _PERFECT_ except..The algorithms in my phone had decided that it&#x27;d be better to scrape off his fucking skin and replace it with the texture of the wall behind him!Of course it must be my fault for buying such a cheap phone, it only a Galaxy 22 Ultra, I&#x27;m sure the 23 Ultra is better... But it was not out when I changed phones..Wtf^wtf..So I go turn on RAW so I can at least salvage picture in the future, except RAW only works in the \"pro\" camera mode which is inconvenient to use and sometimes it silently falls back to non-pro..In the end I gave up and installed a third party camera application, I guess I just have to trust Mark, at least he hasn&#x27;t actively messed up my photos. https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=net.sourceforg... reply emptybits 6 hours agoprevIf not already, I expect examples like this will be accumulated and trotted out as part of legal defenses. Valid or not, gross examples like this will probably nudge some judges and juries over their threshold for reasonable doubt. AI manipulations are happening and their bounds can be hard to predict. reply ealexhudson 1 hour agoparentImagine a politician photoed reacting to a member of the public in disgust, except the face was stitched in from moments after. Or, worse, someone captured at the scene of a bomb reacting before the bomb went off?We have an inbuilt set of assumptions about causality that this AI now violates. That&#x27;s potentially huge in some very specific scenarios... reply Frummy 1 hour agoprevI was in a butterfly house, and closeup photos removed the legs of the butterflies to keep only the wings lol.e: image link https:&#x2F;&#x2F;ibb.co&#x2F;nwbw5xY reply verytrivial 4 minutes agoprevDefense lawyers take note. reply PlunderBunny 6 hours agoprevI heard mention on a podcast recently [0] that if you hold down the button in the iPhone camera app it will capture a set of photos and then mark the one that it thinks is the &#x27;best&#x27; (based on, for example, the photo where everyone has their eyes open). Not the same as what happened here of course. (I keep forgetting to try this, not least because I always try to get the people out of my photos!)[0] This one, I think: https:&#x2F;&#x2F;podcasts.apple.com&#x2F;nz&#x2F;podcast&#x2F;the-talk-show-with-joh... reply macintux 6 hours agoparentUnfortunately they changed the behavior a few major releases ago: now holding the button down switches to video recording. Annoying for me, since I don&#x27;t do video.If you use the software button you can slide to the left for burst, but afaik there&#x27;s no way to trigger burst photos from the volume buttons. Maybe the new programmable button on the iPhone 15 series. reply kderbe 5 hours agorootparenthttps:&#x2F;&#x2F;support.apple.com&#x2F;guide&#x2F;iphone&#x2F;take-burst-mode-shots...\"Tip: You can also press and hold the volume up button to take Burst shots. Go to Settings > Camera, then turn on Use Volume Up for Burst.\" reply macintux 5 hours agorootparentThanks, I glanced at that page but didn&#x27;t notice the tip at the bottom. reply dmix 5 hours agorootparentprevThat‚Äôs still a cool shortcut I didn‚Äôt know about. It not only switches to video but it ‚Äúgates‚Äù the button press so it only records a video for as long as you press the button reply macintux 5 hours agorootparentYou can also lock the video recording by sliding to the right. reply fshbbdssbbgdd 5 hours agorootparentprevI think if you have the Live photos option turned on (the default) it will automatically take a burst every time you hit the red button. reply macintux 4 hours agorootparentVideo, although there might be a configuration option somewhere.Historically Live Photos were of poorer overall image quality, so I only turn them on when I want to simulate a long exposure. Not sure whether that&#x27;s still true. reply carterschonwald 6 hours agoparentprevI think there‚Äôs also how the Live Photo feature works. Where taking a photo is literally a short video and it picks the best frame reply karmakaze 1 hour agoprevIt wasn&#x27;t long before we hit this Xerox digits[0] moment in &#x27;photos&#x27;. What features do you want in your photocopier or photos?[0] https:&#x2F;&#x2F;www.theverge.com&#x2F;2013&#x2F;8&#x2F;6&#x2F;4594482&#x2F;xerox-copiers-rand... reply coremoff 40 minutes agoparentvery different causes though; the xerox one was an image compression bug, IIRC; this is a \"chop up multiple photots and stich together the best composite\".Still got room for library bugs! (wasn&#x27;t the Samsung moon pictures sort of more like the xerox one?) reply weird-eye-issue 6 hours agoprevWith how much manipulation happens to photos by default it seems like it would be easier to get them thrown out of court as evidence reply joeframbach 5 hours agoparentIt&#x27;s not so farfetched. https:&#x2F;&#x2F;www.theverge.com&#x2F;2021&#x2F;11&#x2F;10&#x2F;22775580&#x2F;kyle-rittenhous... reply j16sdiz 6 hours agoparentprevWithout better evidence, the court will keep accepting them.Just look at how bad fingerprints are accepted reply callalex 3 hours agorootparentSee also: polygraphs. reply TeMPOraL 2 hours agoprevTangent, but:> A U.K. comedian and actor named Tessa CoatesWhy is it always some kind of celebrity that \"discovers\" stuff like this? Was it luck (yet again), or are those extreme failure modes of computational photography already somewhat known, just too nerdy to report on until they can be attached to a public person? reply edent 2 hours agoparentBecause the typical Instagram account is only followed by a handful of people. So anything unusual is unlikely to get any traction. reply TeMPOraL 1 hour agorootparentMight be my age showing, but I swear it wasn&#x27;t like this just a few years ago - this kind of story would genuinely go viral from the first rando posting it on social media (\"wtf is my camera broken?\") or the second rando that adapted it into a goofy scene&#x2F;video (\"look I&#x27;m a vampire now\"). reply doctoboggan 5 hours agoprevI don&#x27;t think this is true, Apple hasn&#x27;t openly said they do this level of manipulation (although that doesn&#x27;t mean they don&#x27;t necessarily), and I don&#x27;t think the range of motions she would have to go through would be possible within a single capture. Even with \"Live Photos\" this wouldn&#x27;t happen. reply tiltowait 5 hours agoparentI‚Äôm also skeptical. I want it to be true, because it‚Äôs pretty wild, but it seems a little too perfect. I suppose people must already be trying to prove or disprove it. reply Moldoteck 1 hour agoprevComputational photography is just at it&#x27;s first steps. Google photos on p8 allow to replace faces. I expect this feature will be implemented to be done automatically to show you the ideal photo. And I expect some parents will be happy with it - having an ideal photo with ideal face of their children reply comfysocks 5 hours agoprevMy best guess is that there was a brightness gradient across the scene, and this is the result of tone mapping from an EV bracketed burst. This might result in ‚Äútime delay‚Äù instead of the more typical ‚Äúghosting‚Äù artifacts. reply NikkiA 1 hour agoprevIf the &#x27;camera&#x27; is guessing as to intent, it&#x27;s no longer a camera, sorry. reply Gigachad 1 hour agoparentIt isn‚Äôt guessing. It‚Äôs just stacked multiple shots. Everything in the photo is real, just at slightly different moments. reply rompledorph 3 hours agoprevIf it has not already happened, evidence will get thrown out of court because the image no longer represents the reality. Too many filters and other AI improvements. reply hnburnsy 6 hours agoprevHow long is this burst (feels longer than &#x27;short&#x27; in this case)? Does it start before you press the shutter button? Does this post processing apply to RAW? Does Apple document how this post processing works? More importantly, how does one turn it off or is there another camera app that allows one to turn it off? reply ISL 5 hours agoprevI hope, but am not certain, that by configuring my Pixel to save both a jpeg and a raw for each image, at least the raw would avoid these shenanigans. reply Someone1234 5 hours agoparentUnless the raw is actually a stack of images, it may not. Both Android and iOS are taking multiple exposures and combining them into a single HDR image. This is before it hits the camera app.This has nothing to do with \"Live Photos\" on iOS to be clear. reply STELLANOVA 3 hours agoprevSlight off-topic but I still can&#x27;t believe Apple engineers are not able to fix stupid green dot flare when shooting with direct sunlight in the frame... Super easy to fix computationally but for some reason still there for years... reply panarky 3 hours agoparentPretty cool how the green dot became a green crescent during the last solar eclipse. reply karmakaze 1 hour agoprev&#x27;photo&#x27; is now synonymous with photoshopped. reply kazinator 4 hours agoprevChanging the positions of limbs is computational, but it&#x27;s not photography.Photography refers to the capturing of the reflected light to create an image, which then corresponds to how the scene actually was.Bringing out detail in shadows being called \"computational photography\": that I could swallow. reply hgomersall 2 hours agoparentAre colour filters allowed? reply verisimi 43 minutes agoprevAnd, of course, there is the story where Samsung replaces the moon with its high res version.I don&#x27;t think photography from phone can actually be trusted to be a faithful representation of reality as it is not a purely mechanical series of actions. This will be even worse if ai is involved in the process.I certainly think that there is an argument for photographic evidence to be inadmissible in court. reply ClumsyPilot 43 minutes agoprevSo many comments, not a single one pointing out that someone will go to prison over this &#x27;photo&#x27;, when it shows something that never actually happened. reply leephillips 7 hours agoprevThe article says that ‚ÄúThe final composite image should be the best, most realistic interpretation of that moment.‚Äù But that doesn‚Äôt make any sense. If the there were three real people, rather than one person and two reflections, the algorithm would have created an image of a moment that never existed, stitching together different images taken at different times. The only difference is that we might not notice and mistake the fake image for what we expect from a old-fashioned photograph. I find what Apple is doing repulsive. reply tmalsburg2 7 hours agoparentImagine such an image being used as evidence is a court case. E.g. showing someone pointing a gun at another person when that actually never happened. reply Aurornis 7 hours agorootparentSeems extremely unlikely. The phone isn‚Äôt going to make drastic alterations to reality. It‚Äôs just combining multiple exposures. It can‚Äôt make someone point a gun if they‚Äôre not pointing a gun. You could try to imagine a scenario where someone was moving extremely fast through the frame while someone was whipping a gun around at high speed in the same direction and the phone just happened to stitch two moments a fraction of a second apart such that they look slightly closer together, but to get to that point you‚Äôd still need someone pointing a gun in the same direction that someone is going.It‚Äôs really hard to imagine a scenario where two exposures a fraction of a second apart could be stitched together to tell a completely false story, but maybe it exists. In that case, I suspect the lawyers would be all over this explanation to try to dismiss the evidence. reply 0cVlTeIATBs 6 hours agorootparentWell, the Rittenhouse case had a very important moment when the victim admitted that, while looking at a freeze-frame view of a video taken when he was shot, he had raised his pistol which was pointed at Rittenhouse only a fraction of a second before. [0]That photo was critical for the defense getting him to say that.There were also concerns over wether or not zooming in on an iPad should be allowed in that case--like if a red pixel next to a blue could create a purple one, etc.[0] https:&#x2F;&#x2F;www.denverpost.com&#x2F;2021&#x2F;11&#x2F;08&#x2F;shooting-victim-says-h... reply adastra22 6 hours agorootparentprevWhat the comment you&#x27;re replying to is saying could absolutely have happened. Imagine a \"Han shot first\" sort of situation: two people with guns, one shoots the other. The shooter claims it was self-defense, as the other guy went to fire first but was just slower. An iPhone picture captures the moment, but has the shooter firing, and the other guy&#x27;s gun still at his side.This is perfectly analogous to TFA--notice that the woman has enough time to move her arms into very different positions in the same composited moment. reply tmalsburg2 7 hours agorootparentprevIt‚Äôs unlikely, true. But precisely that makes it so dangerous. If such a picture is presented as evidence in a murder case, the possibility that it is telling the wrong story will be discounted and someone may go to prison for the rest of their lives. reply _kb 6 hours agorootparentprevThat threat model has existed since the birth of photoshop et al. reply flashback2199 6 hours agorootparentScenario: It&#x27;s a decade from today and phones are not just stitching together multiple photos but also using generative AI. Apple and all of the other phone makers insist the implementation is safe. The suspect appears in the photo doing something they never actually did. In the chaos of the crime, the phone was left at the crime scene where it was found by law enforcement, no chance the photo could have been photoshopped. The photo is shown as evidence in court. Without the photo, there is not enough evidence to convict. The entire case now hinges on the content of the photo. The suspect insists they never did what is shown in the photo, but it doesn&#x27;t matter. The suspect is convicted, put on death row, and executed by the state. Thankfully, there is a silver lining: everyone&#x27;s photos are a couple percent prettier, which helped bring more value to shareholders. reply _kb 1 hour agorootparentThere was a thread here a little while back [0] on cryptographic proofs for photojournalism. Ultimately, that style of attestation seems the end game.Journalists, security cameras, emergency service &#x2F; military body cams and other safety systems provide a trust mechanism that enables provable authenticity from the point of capture (and some form of web of trust to weight or revoke that trust for different audiences). Anything else is assumed to be at least partial hallucination.[0]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37398317 reply autoexec 6 hours agorootparentprevThere&#x27;s a huge difference between someone intentionally altering an image according to their wishes and someone not even aware of changes that have been done.Before, forensic experts could decide if an image had been altered in photoshop, but I guess the only sane conclusion now is that anything taken with an iphone is fake and untrustworthy. reply KennyBlanken 5 hours agorootparentAs opposed to Samsungs which will take any vaguely circular bright white object and turn it into the moon?https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Android&#x2F;comments&#x2F;11nzrb0&#x2F;samsung_sp... reply autoexec 2 hours agorootparentYeah, that&#x27;s trash too. At this point I think it&#x27;s fair to say you just can&#x27;t trust a picture taken from a cell phone. reply dylan604 6 hours agorootparentprevimages like this would easily be torn apart by experts brought in to testify against it. then again, for something this obvious, it probably wouldn&#x27;t need an expert. reply tmalsburg2 6 hours agorootparentIf this phenomenon is so obvious, why is this story the top post on HN? And why did it take a picture of the mirror scenario to make people aware of this issue? Hell, the article even implies that this an issue only with images of mirrors when that is of course completely false. reply dylan604 6 hours agorootparentYou seem to think that all lawyers are dumb, and unable to defend against photographic evidence. If you think a lawyer would not be able to find a witness that fully understands how the modern mobile device camera systems alter images, you&#x27;re just not being honest with yourself. The Apple propaganda videos tout the fact their cameras are doing something with \"AI\" even if they don&#x27;t tell you exactly what. To assume that people are so unawares that it took this picture is just not being honest with the conversation reply tmalsburg2 2 hours agorootparentNo, I do not think that all lawyers are dumb. What a bizarre thing to say. Why derail an interesting conversation in such an aggressive way? reply PlunderBunny 7 hours agorootparentprevAs far as the phone knows, there were three people in the photo, and it captured the &#x27;best&#x27; picture of each one and composited them. reply dylan604 7 hours agoparentprevHow is what it is attempting to do any different from when someone takes multiple pictures of a group shot, and then uses the different shots to ensure no blinks and \"best\" expression from each subject?There&#x27;s a reason professional photogs use the mulitiple snaps mode for non-sports. It used to be a lot of work in post, but a lot of apps have made that easier to now it&#x27;s a built in feature of our phones. reply autoexec 6 hours agorootparentChoice. If I&#x27;m using a digital camera I can take 20+ shots of any subject, but then I get to choose which to keep.If cameras don&#x27;t give you the option for \"reality\" you&#x27;re just left with whatever they choose for you no matter how many pictures you take. reply dylan604 6 hours agorootparentSo use a non-mobile device with this feature. Nobody is forcing you to use the camera. You know what the camera does, but then continue to use it, and then complain about it doing exactly what you knew it would do. Doing the same thing over and over expecting a different result has a name reply autoexec 6 hours agorootparentI think the issue is that people don&#x27;t know what the camera does. The woman who tried to take a picture of herself in her wedding dress had no idea she&#x27;d end up looking like three different people.I expect that as more and more people come to learn that their iphone photos are all fakes we will see more people reaching for cameras instead of cell phones for the important things when they can. reply dylan604 4 hours agorootparentI think like most things on HN, people are confusing the people here are a much smaller percentage of the population and the majority of the world does not think like HN. Most people don&#x27;t care one little bit about what the camera does. They only care that it shows them a picture that looks like what they saw. Does it hold up to further scrutiny? Maybe not, but these are also not the people that will be scrutinizing it. Unless they take a picture of their cat and it ends up with the head of a dog, the \"your moms\" of the world are not going to care. reply leephillips 6 hours agorootparentprevThe difference is in what some photographers call ‚Äúeditorial integrity‚Äù. There‚Äôs nothing wrong with any kind of image manipulation, as long as it‚Äôs done knowingly and deliberately, and as long as the nature of the manipulation is known to the audience. But the typical iphone consumer is just taking snaps and sharing them around, and almost no one knows what‚Äôs really happening. It‚Äôs creepy and unethical. reply Toutouxc 4 hours agorootparent> and almost no one knows what‚Äôs really happeningAnd almost no one cares, btw.And what do you even propose? A mandatory ‚Äúpicture might not represent reality‚Äù watermark? Because the way I see it, you either take the computational features away from people, and prevent them from taking an entire class of pictures, or you add a big fat warning somewhere that no one will read anyway, or you keep things the way they are. Which one of these is the ethical choice? reply leephillips 3 hours agorootparentI‚Äôm not proposing anything mandatory. I would like it if the biggest corporation in the world would consider the effects of their technology on society, rather than solely on their profits. That they would at least let ethical considerations potentially influence their design decisions. But that‚Äôs not how they got to be the biggest company in the world, so I don‚Äôt expect this to happen. reply drtgh 6 hours agoparentprev\"interpretation of that moment\" from a camera it is not a photograph, it&#x27;s a fake image kidnapping the word \"photograph\". reply Toutouxc 4 hours agorootparentA photograph is always an interpretation. A photograph from, say, a modern $2000 big boy camera:Does not capture the color of every pixel, and merely infers it from the surrounding ones. Is usually viewed on a sRGB screen with shitty contrast and a dynamic range significantly smaller than the one of the camera, which is significantly smaller than the one of human eyes, which is still significantly smaller than what we encounter in the real world. Does not capture a particular moment in time, but a variable length period that‚Äôs also shifted between the top part of the image and the bottom part (a couple of ms for mechanical shutter, tens or hundreds of ms for electronic). Has no idea about the white balance of the scene. Has no idea about the absolute brightness of the scene. Usually has significant perspective distortion. Usually has most of the scene out-of focus and thus misrepresents reality (buildings aren‚Äôt built and people aren‚Äôt born out of focus). reply autoexec 5 hours agorootparentprevIt&#x27;s impossible to enforce, but it&#x27;d be a more honest if people called them something else. iPictures maybe? As in \"Hey, check out this iPicture‚Ñ¢ of my kid taking his first steps!\" reply seshagiric 4 hours agoprevI am always amazed how people even catch (come across) issues like this...the chances are 1 in a million? reply xyztimm 7 hours agoprevSo the mirror with the hands clasped?‚Ä¶ is just completely made up? Because she‚Äôs clearly not doing that irl. reply quadrature 6 hours agoparentnot made up as in generated. it was sampled from different instances of time. This is the article&#x27;s explanation> Coates was moving when the photo was taken, so when the shutter was pressed, many differing images were captured in that instant.> Apple&#x27;s algorithm stitches the photos together, choosing the best versions for saturation, contrast, detail, and lack of blur. reply w-ll 6 hours agoparentprev2 mirrors, 3 differnt arm poses. \"Photos\" are not really any more, the seem to be few second clips, and yea the processing merged 3 differnt poses. reply threeseed 6 hours agorootparentYou are conflating two different aspects.Live Photo is a feature where it captures a couple of seconds of video before&#x2F;after the photo is taken. From the article that feature was not enabled.The computational pipeline is where you press the shutter and it blends a few frames together in order to do focus stacking, HDR etc. Based on what I have tried with my iPhone it is doing this inThe poses would be at least a few seconds apartAre those 3 poses she&#x27;s doing, or did she have her hands clasped, then performed some gesture where she dropped her left arm then right arm?The right definitely looks like a deliberate pose, at first glance the middle does too, but the left doesn&#x27;t at all, it looks like a gesture. The splay of the hands among other things indicate this to me.I think the middle isn&#x27;t a pose either, just a transition. I think only the right is a pose, and it goes Right > Middle > Left in time. Her hand is splayed in the middle like it is in the left. reply rf15 3 hours agoparentprevYeah this strikes me as half generated, especially since it&#x27;s three different poses. This is the baby teeth thing all over again and it must suck knowing that your Wedding photos are just an AI&#x27;s reinterpretation of the moment. reply Filligree 6 hours agoparentprevThe ‚Äúburst‚Äù is actually a two second video. Plenty of time for that. reply threeseed 6 hours agorootparentShe said she never used live photo.And the latency for taking a standard photo is not 2 seconds. reply Filligree 5 hours agorootparentIndeed, since it looks at data from before you press the trigger as well. The camera is always active while you have the camera app; storing a few seconds of pictures is a trivial amount of memory.Live Photo means storing the video in the image file. Turning it off doesn‚Äôt mean the video is no longer used, just that it isn‚Äôt stored. reply mdonahoe 6 hours agoparentprevDo you work at Apple? reply charcircuit 6 hours agoparentprevI&#x27;m skeptical too. Her arms look so stiff so I would exepct her to have been moving slowly. reply causi 3 hours agoprevOne is reminded of how Samsung cameras are trained to put the details of the moon over anything that looks vaguely like the moon, but just one, so you can photograph two blurry images of the moon and one of them will look like it was taken by Hubble. reply ClassyJacket 7 hours agoprevSo this wasn&#x27;t just rolling shutter? reply kag0 7 hours agoparentNo. A rolling shutter deals with different parts of the sensor being exposed at different times within one frame. It generally requires a much faster object (think airplane propeller or light with PWM). This is effectively multiple frames captured normally and a mistake in the algorithm stitching them into one frame. reply rjeli 7 hours agoparentprevNo, the difference is too large for it to be rolling shutter. (not sure if the rolling shutter is even vertical or horizontal). It seems like it takes a short video and stitches together the sections with the least motion blur in each frame. reply tmalsburg2 7 hours agoparentprevRolling shutter could in principle explain this, but it‚Äôs probably too fast to capture these three poses which must be temporally rather far apart. reply stephen_g 7 hours agorootparentNot probably too fast - definitely too fast. Rolling shutter occurs over a single read-out of the sensor, so she&#x27;d have to have held all the three poses during the 1&#x2F;100th of a second or so exposure for it to be possibly related to rolling shutter. reply tmalsburg2 7 hours agorootparentCompletely valid nitpick. reply vlovich123 7 hours agoparentprevNo. Rolling shutter should typically be too fast for a shot like that. Much more likely it&#x27;s a result of 3 photos taken at slightly different times and automatically composited. Appears to be a well known effect. reply po__studio 5 hours agoprevThat‚Äôs fascinating reply saiya-jin 53 minutes agoprevNext time somebody here will be bashing other manufacturers like say Samsung for clarifying 50x zoom moon shots (like most of HN did when this was a topic for a day or two few months ago), remember kids that this is what all manufacturers do. Or half-reversing photo of kitten in the grass as somebody mentioned. reply shrizza 7 hours agoprev [‚Äì] This is Apple so we like to call it a reality-distortion effect. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A woman in the U.K. found a glitch with her iPhone 15's camera that created three different poses of herself in a mirror.",
      "Apple's computational photography algorithm failed to recognize the mirror reflection, resulting in a composite image with different poses in each mirror.",
      "This glitch can be replicated on other recent iPhones and is popular among younger generations who use it for fun on social media."
    ],
    "commentSummary": [
      "The discussion revolves around computational photography, focusing on Apple's iPhone cameras and the use of AI in photography.",
      "Users share their opinions on the advantages and disadvantages of computational photography, as well as concerns about image manipulation and authenticity.",
      "The debate also covers the limitations of smartphone cameras, their impact on the dedicated camera market, and the potential implications of AI technology in communication and legal cases regarding altered images as evidence."
    ],
    "points": 381,
    "commentCount": 241,
    "retryCount": 0,
    "time": 1701396145
  },
  {
    "id": 38471822,
    "title": "Ripgrep: A Faster and More Efficient Command Line Search Tool for Code",
    "originLink": "https://blog.burntsushi.net/ripgrep/",
    "originBody": "In this article I will introduce a new command line search tool, ripgrep, that combines the usability of The Silver Searcher (an ack clone) with the raw performance of GNU grep. ripgrep is fast, cross platform (with binaries available for Linux, Mac and Windows) and written in Rust. ripgrep is available on Github. We will attempt to do the impossible: a fair benchmark comparison between several popular code search tools. Specifically, we will dive into a series of 25 benchmarks that substantiate the following claims: For both searching single files and huge directories of files, no other tool obviously stands above ripgrep in either performance or correctness. ripgrep is the only tool with proper Unicode support that doesn‚Äôt make you pay dearly for it. Tools that search many files at once are generally slower if they use memory maps, not faster. As someone who has worked on text search in Rust in their free time for the last 2.5 years, and as the author of both ripgrep and the underlying regular expression engine, I will use this opportunity to provide detailed insights into the performance of each code search tool. No benchmark will go unscrutinized! Target audience: Some familiarity with Unicode, programming and some experience with working on the command line. NOTE: I‚Äôm hearing reports from some people that rg isn‚Äôt as fast as I‚Äôve claimed on their data. I‚Äôd love to help explain what‚Äôs going on, but to do that, I‚Äôll need to be able to reproduce your results. If you file an issue with something I can reproduce, I‚Äôd be happy to try and explain it. Screenshot of search results Table of Contents Introducing ripgrep Pitch Anti-pitch Installation Whirlwind tour Regex syntax Anatomy of a grep Background Gathering files to search Searching Regex engine Literal optimizations Mechanics Printing Methodology Overview Benchmark runner Environment Code search benchmarks linux_literal_default linux_literal linux_literal_casei linux_word linux_unicode_word linux_re_literal_suffix linux_alternates linux_alternates_casei linux_unicode_greek linux_unicode_greek_casei linux_no_literal Single file benchmarks subtitles_literal subtitles_literal_casei subtitles_alternate subtitles_alternate_casei subtitles_surrounding_words subtitles_no_literal Bonus benchmarks everything nothing context huge Conclusions Introducing ripgrep Pitch Why should you use ripgrep over any other search tool? Well‚Ä¶ It can replace many use cases served by other search tools because it contains most of their features and is generally faster. (See the FAQ for more details on whether ripgrep can truly replace grep.) Like other tools specialized to code search, ripgrep defaults to recursive directory search and won‚Äôt search files ignored by your .gitignore files. It also ignores hidden and binary files by default. ripgrep also implements full support for .gitignore, whereas there are many bugs related to that functionality in other code search tools claiming to provide the same functionality. ripgrep can search specific types of files. For example, rg -tpy foo limits your search to Python files and rg -Tjs foo excludes Javascript files from your search. ripgrep can be taught about new file types with custom matching rules. ripgrep supports many features found in grep, such as showing the context of search results, searching multiple patterns, highlighting matches with color and full Unicode support. Unlike GNU grep, ripgrep stays fast while supporting Unicode (which is always on). ripgrep has optional support for switching its regex engine to use PCRE2. Among other things, this makes it possible to use look-around and backreferences in your patterns, which are not supported in ripgrep‚Äôs default regex engine. PCRE2 support is enabled with -P. ripgrep supports searching files in text encodings other than UTF-8, such as UTF-16, latin-1, GBK, EUC-JP, Shift_JIS and more. (Some support for automatically detecting UTF-16 is provided. Other text encodings must be specifically specified with the -E/--encoding flag.) ripgrep supports searching files compressed in a common format (gzip, xz, lzma, bzip2 or lz4) with the -z/--search-zip flag. ripgrep supports arbitrary input preprocessing filters which could be PDF text extraction, less supported decompression, decrypting, automatic encoding detection and so on. In other words, use ripgrep if you like speed, filtering by default, fewer bugs and Unicode support. Anti-pitch I‚Äôd like to try to convince you why you shouldn‚Äôt use ripgrep. Often, this is far more revealing than reasons why I think you should use ripgrep. Despite initially not wanting to add every feature under the sun to ripgrep, over time, ripgrep has grown support for most features found in other file searching tools. This includes searching for results spanning across multiple lines, and opt-in support for PCRE2, which provides look-around and backreference support. At this point, the primary reasons not to use ripgrep probably consist of one or more of the following: You need a portable and ubiquitous tool. While ripgrep works on Windows, macOS and Linux, it is not ubiquitous and it does not conform to any standard such as POSIX. The best tool for this job is good old grep. There still exists some other feature (or bug) not listed in this README that you rely on that‚Äôs in another tool that isn‚Äôt in ripgrep. There is a performance edge case where ripgrep doesn‚Äôt do well where another tool does do well. (Please file a bug report!) ripgrep isn‚Äôt possible to install on your machine or isn‚Äôt available for your platform. (Please file a bug report!) Installation The binary name for ripgrep is rg. Binaries for ripgrep are available for Windows, Mac and Linux. Linux binaries are static executables. Windows binaries are available either as built with MinGW (GNU) or with Microsoft Visual C++ (MSVC). When possible, prefer MSVC over GNU, but you‚Äôll need to have the Microsoft VC++ 2015 redistributable installed. If you‚Äôre a Homebrew user, then you can install it like so: $ brew install ripgrep If you‚Äôre an Archlinux user, then you can install ripgrep from the official repos: $ pacman -Syu ripgrep If you‚Äôre a Rust programmer, ripgrep can be installed with cargo: $ cargo install ripgrep If you‚Äôd like to build ripgrep from source, that is also easy to do. ripgrep is written in Rust, so you‚Äôll need to grab a Rust installation in order to compile it. ripgrep compiles with Rust 1.9 (stable) or newer. To build: $ git clone git://github.com/BurntSushi/ripgrep $ cd ripgrep $ cargo build --release $ ./target/release/rg --version 0.1.2 If you have a Rust nightly compiler, then you can enable optional SIMD acceleration like so, which is used in all benchmarks reported in this article. RUSTFLAGS=\"-C target-cpu=native\" cargo build --release --features simd-accel Whirlwind tour The command line usage of ripgrep doesn‚Äôt differ much from other tools that perform a similar function, so you probably already know how to use ripgrep. The full details can be found in rg --help, but let‚Äôs go on a whirlwind tour. ripgrep detects when its printing to a terminal, and will automatically colorize your output and show line numbers, just like The Silver Searcher. Coloring works on Windows too! Colors can be controlled more granularly with the --color flag. One last thing before we get started: generally speaking, ripgrep assumes the input is reading is UTF-8. However, if ripgrep notices a file is encoded as UTF-16, then it will know how to search it. For other encodings, you‚Äôll need to explicitly specify them with the -E/--encoding flag. To recursively search the current directory, while respecting all .gitignore files, ignore hidden files and directories and skip binary files: $ rg foobar The above command also respects all .rgignore files, including in parent directories. .rgignore files can be used when .gitignore files are insufficient. In all cases, .rgignore patterns take precedence over .gitignore. To ignore all ignore files, use -u. To additionally search hidden files and directories, use -uu. To additionally search binary files, use -uuu. (In other words, ‚Äúsearch everything, dammit!‚Äù) In particular, rg -uuu is similar to grep -a -r. $ rg -uu foobar # similar to `grep -r` $ rg -uuu foobar # similar to `grep -a -r` (Tip: If your ignore files aren‚Äôt being adhered to like you expect, run your search with the --debug flag.) Make the search case insensitive with -i, invert the search with -v or show the 2 lines before and after every search result with -C2. Force all matches to be surrounded by word boundaries with -w. Search and replace (find first and last names and swap them): $ rg '([A-Z][a-z]+)\\s+([A-Z][a-z]+)' --replace '$2, $1' Named groups are supported: $ rg '(?P[A-Z][a-z]+)\\s+(?P[A-Z][a-z]+)' --replace '$last, $first' Up the ante with full Unicode support, by matching any uppercase Unicode letter followed by any sequence of lowercase Unicode letters (good luck doing this with other search tools!): $ rg '(\\p{Lu}\\p{Ll}+)\\s+(\\p{Lu}\\p{Ll}+)' --replace '$2, $1' Search only files matching a particular glob: $ rg foo -g 'README.*' Or exclude files matching a particular glob: $ rg foo -g '!*.min.js' Search only HTML and CSS files: $ rg -thtml -tcss foobar Search everything except for Javascript files: $ rg -Tjs foobar To see a list of types supported, run rg --type-list. To add a new type, use --type-add, which must be accompanied by a pattern for searching (rg won‚Äôt persist your type settings): $ rg --type-add 'foo:*.{foo,foobar}' -tfoo bar The type foo will now match any file ending with the .foo or .foobar extensions. Regex syntax The syntax supported is documented as part of Rust‚Äôs regex library. Anatomy of a grep Before we dive into benchmarks, I thought it might be useful to provide a high level overview of how a grep-like search tool works, with a special focus on ripgrep in particular. The goal of this section is to provide you with a bit of context that will help make understanding the analysis for each benchmark easier. Background Modulo parsing command line arguments, the first ‚Äúreal‚Äù step in any search tool is figuring out what to search. Tools like grep don‚Äôt try to do anything smart: they simply search the files given to it on the command line. An exception to this is the -r flag, which will cause grep to recursively search all files in the current directory. Various command line flags can be passed to control which files are or aren‚Äôt searched. ack came along and turned this type of default behavior on its head. Instead of trying to search everything by default, ack tries to be smarter about what to search. For example, it will recursively search your current directory by default, and it will automatically skip over any source control specific files and directories (like .git). This method of searching undoubtedly has its own pros and cons, because it tends to make the tool ‚Äúsmarter,‚Äù which is another way of saying ‚Äúopaque.‚Äù That is, when you really do need the tool to search everything, it can sometimes be tricky to know how to speak the right incantation for it to do so. With that said, being smart by default is incredibly convenient, especially when ‚Äúsmart‚Äù means ‚Äúfigure out what to search based on your source control configuration.‚Äù There‚Äôs no shell alias that can do that with grep. All of the other search tools in this benchmark share a common ancestor with either grep or ack. sift is descended from grep, while ag, ucg, and pt are descended from ack. ripgrep is a bit of a hybrid because it was specifically built to be good at searching huge files just like grep, but at the same time, provide the ‚Äúsmart‚Äù kind of default searching like ack. Finally, git grep deserves a bit of a special mention. git grep is very similar to plain grep in the kinds of options it supports, but its default mode of searching is clearly descended from ack: it will only search files checked into source control. Of course, both types of search tools have a lot in common, but there are a few broad distinctions worth making if you allow yourself to squint your eyes a bit: grep-like tools need to be really good at searching large files, so the performance of the underlying regex library is paramount. ack-like tools need to be really good at recursive directory traversal while also applying ignore rules from files like .gitignore quickly. ack-like tools are built to run many searches in parallel, so the raw performance of the underlying regex library can be papered over somewhat while still being faster than single-threaded ‚Äúsearch everything‚Äù tools like grep. If the ‚Äúsmarts‚Äù of ack also mean skipping over that 2GB artifact in your directory tree, then the performance difference becomes even bigger. ripgrep tries hard to combine the best of both worlds. Not only is its underlying regex engine very fast, but it parallelizes searches and tries to be smart about what it searches too. Gathering files to search For an ack-like tool, it is important to figure out which files to search in the current directory. This means using a very fast recursive directory iterator, filtering file paths quickly and distributing those file paths to a pool of workers that actually execute the search. Directory traversal can be tricky because some recursive directory iterators make more stat calls than are strictly necessary, which can have a large impact on performance. It can be terribly difficult to track down these types of performance problems because they tend to be buried in a standard library somewhere. Python only recently fixed this, for example. Rest assured that ripgrep uses a recursive directory iterator that makes the minimum number of system calls possible. Filtering file paths requires not only respecting rules given at the command line (e.g., grep‚Äôs --include or --exclude) flags, but also requires reading files like .gitignore and applying their rules correctly to all file paths. Even the mere act of looking for a .gitignore file in every directory can have measurable overhead! Otherwise, the key performance challenge with this functionality is making sure you don‚Äôt try to match every ignore rule individually against every file path. Large repositories like the Linux kernel source tree have over a hundred .gitignore files with thousands of rules combined. Finally, distributing work to other threads for searching requires some kind of synchronization. One solution is a mutex protected ring buffer that acts as a sort of queue, but there are lock-free solutions that might be faster. Rust‚Äôs ecosystem is so great that I was able to reuse a lock-free Chase-Lev work-stealing queue for distributing work to a pool of searchers. Every other tool that parallelizes work in this benchmark uses a variant of a mutex protected queue. (sift and pt might not fit this criteria, since they use Go channels, and I haven‚Äôt followed any implementation improvements to that code for a few years.) Searching Searching is the heart of any of these tools, and we could dig ourselves into a hole on just this section alone and not come out alive for at least 2.5 years. (Welcome to ‚ÄúHow Long I‚Äôve Been Working On Text Search In Rust.‚Äù) Instead, we will lightly touch on the big points. Regex engine First up is the regex engine. Every search tool supports some kind of syntax for regular expressions. Some examples: foo|bar matches any literal string foo or bar [a-z]{2}_[a-z]+ matches two lowercase latin letters, followed by an underscore, followed by one or more lowercase latin letters. \\bfoo\\b matches the literal foo only when it is surrounded by word boundaries. For example, the foo in foobar won‚Äôt match but it will in I love foo.. (\\w+) \\1 matches any sequence of word characters followed by a space and followed by exactly the word characters that were matched previously. The \\1 in this example is called a ‚Äúback-reference.‚Äù For example, this pattern will match foo foo but not foo bar. Regular expression engines themselves tend to be divided into two categories predominantly based on the features they expose. Regex engines that provide support for all of the above tend to use an approach called backtracking, which is typically quite fast, but can be very slow on some inputs. ‚ÄúVery slow‚Äù in this case means that it might take exponential time to complete a search. For example, try running this Python code: >>> import re >>> re.search('(a*)*c', 'a' * 30) Even though both the regex and the search string are tiny, it will take a very long time to terminate, and this is because the underlying regex engine uses backtracking, and can therefore take exponential time to answer some queries. The other type of regex engine generally supports fewer features and is based on finite automata. For example, these kinds of regex engines typically don‚Äôt support back-references. Instead, these regex engines will often provide a guarantee that all searches, regardless of the regex or the input, will complete in linear time with respect to the search text. It‚Äôs worth pointing out that neither type of engine has a monopoly on average case performance. There are examples of regex engines of both types that are blazing fast. With that said, here‚Äôs a breakdown of some search tools and the type of regex engine they use: GNU grep and git grep each use their own hand-rolled finite automata based engine. ripgrep uses Rust‚Äôs regex library, which uses finite automata. The Silver Searcher and Universal Code Grep use PCRE, which uses backtracking. Both The Platinum Searcher and sift use Go‚Äôs regex library, which uses finite automata. Both Rust‚Äôs regex library and Go‚Äôs regex library share Google‚Äôs RE2 as a common ancestor. Finally, both tools that use PCRE (The Silver Searcher and Universal Code Grep) are susceptible to worst case backtracking behavior. For example: $ cat wat c aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa c $ ucg '(a*)*c' wat terminate called after throwing an instance of 'FileScannerException' what(): PCRE2 match error: match limit exceeded Aborted (core dumped) The Silver Searcher fails similarly. It reports the first line as a match and neglects the match in the third line. The rest of the search tools benchmarked in this article handle this case without a problem. Literal optimizations Picking a fast regex engine is important, because every search tool will need to rely on it sooner or later. Nevertheless, even the performance of the fastest regex engine can be dwarfed by the time it takes to search for a simple literal string. Boyer-Moore is the classical algorithm that is used to find a substring, and even today, it is hard to beat for general purpose searching. One of its defining qualities is its ability to skip some characters in the search text by pre-computing a small skip table at the beginning of the search. On modern CPUs, the key to making a Boyer-Moore implementation fast is not necessarily the number of characters it can skip, but how fast it can identify a candidate for a match. For example, most Boyer-Moore implementations look for the last byte in a literal. Each occurrence of that byte is considered a candidate for a match by Boyer-Moore. It is only at this point that Boyer-Moore can use its precomputed table to skip characters, which means you still need a fast way of identifying the candidate in the first place. Thankfully, specialized routines found in the C standard library, like memchr, exist for precisely this purpose. Often, memchr implementations are compiled down to SIMD instructions that examine sixteen bytes in a single loop iteration. This makes it very fast. On my system, memchr often gets throughputs at around several gigabytes a second. (In my own experiments, Boyer-Moore with memchr can be just as fast as an explicit SIMD implementation using the PCMPESTRI instruction, but this is something I‚Äôd like to revisit.) For a search tool to compete in most benchmarks, either it or its regex engine needs to use some kind of literal optimizations. For example, Rust‚Äôs regex library goes to great lengths to extract both prefix and suffix literals from every pattern. The following patterns all have literals extracted from them: foo|bar detects foo and bar (a|b)c detects ac and bc [ab]foo[yz] detects afooy, afooz, bfooy and bfooz (foo)?bar detects foobar and bar (foo)*bar detects foo and bar (foo){3,6} detects foofoofoo If any of these patterns appear at the beginning of a regex, Rust‚Äôs regex library will notice them and use them to find candidate matches very quickly (even when there is more than one literal detected). While Rust‚Äôs core regex engine is fast, it is still faster to look for literals first, and only drop down into the core regex engine when it‚Äôs time to verify a match. The best case happens when an entire regex can be broken down into a single literal or an alternation of literals. In that case, the core regex engine won‚Äôt be used at all! A search tool in particular has an additional trick up its sleeve. Namely, since most search tools do line-by-line searching (The Silver Searcher is a notable exception, which does multiline searching by default), they can extract non-prefix or ‚Äúinner‚Äù literals from a regex pattern, and search for those to identify candidate lines that match. For example, the regex \\w+foo\\d+ could have foo extracted. Namely, when a candidate line is found, ripgrep will find the beginning and end of only that line, and then run the full regex engine on the entire line. This lets ripgrep very quickly skip through files by staying out of the regex engine. Most of the search tools we benchmark here don‚Äôt perform this optimization, which can leave a lot of performance on the table, especially if your core regex engine isn‚Äôt that fast. Handling the case of multiple literals (e.g., foo|bar) is just as important. GNU grep uses a little known algorithm similar to Commentz-Walter for searching multiple patterns. In short, Commentz-Walter is what you get when you merge Aho-Corasick with Boyer-Moore: a skip table with a reverse automaton. Rust‚Äôs regex library, on the other hand, will either use plain Aho-Corasick, or, when enabled, a special SIMD algorithm called Teddy, which was invented by Geoffrey Langdale as part of the Hyperscan regex library developed by Intel. This SIMD algorithm will prove to be at least one of the key optimizations that propels ripgrep past GNU grep. The great thing about this is that ripgrep doesn‚Äôt have to do much of this literal optimization work itself. Most of it is done inside Rust‚Äôs regex library, so every consumer of that library gets all these performance optimizations automatically! Mechanics Repeat after me: Thou Shalt Not Search Line By Line. The naive approach to implementing a search tool is to read a file line by line and apply the search pattern to each line individually. This approach is problematic primarily because, in the common case, finding a match is rare. Therefore, you wind up doing a ton of work parsing out each line all for naught, because most files simply aren‚Äôt going to match at all in a large repository of code. Not only is finding every line extra work that you don‚Äôt need to do, but you‚Äôre also paying a huge price in overhead. Whether you‚Äôre searching for a literal or a regex, you‚Äôll need to start and stop that search for every single line in a file. The overhead of each search will be your undoing. Instead, all search tools find a way to search a big buffer of bytes all at once. Whether that‚Äôs memory mapping a file, reading an entire file into memory at once or incrementally searching a file using a constant sized intermediate buffer, they all find a way to do it to some extent. There are some exceptions though. For example, tools that use memory maps or read entire files into memory either can‚Äôt support stdin (like Universal Code Grep), or revert to line-by-line searching (like The Silver Searcher). Tools that support incremental searching (ripgrep, GNU grep and git grep) can use its incremental approach on any file or stream with no problems. There‚Äôs a reason why not every tool implements incremental search: it‚Äôs hard. For example, you need to consider all of the following in a fully featured search tool: Line counting, when requested. If a read from a file ends in the middle of a line, you need to do the bookkeeping required to make sure the incomplete line isn‚Äôt searched until more data is read from the file. If a line is too long to fit into your buffer, you need to decide to either give up or grow your buffer to fit it. Your searcher needs to know how to invert the match. Worst of all: your searcher needs to be able to show the context of a match, e.g., the lines before and after a matching line. For example, consider the case of a match that appears at the beginning of your buffer. How do you show the previous lines if they aren‚Äôt in your buffer? You guessed it: you need to carry over at least as many lines that are required to satisfy a context request from buffer to buffer. It‚Äôs a steep price to pay in terms of code complexity, but by golly, is it worth it. You‚Äôll need to read on to the benchmarks to discover when it is faster than memory maps! Printing It might seem like printing is such a trivial step, but it must be done with at least some care. For example, you can‚Äôt just print matches from each search thread as you find them, because you really don‚Äôt want to interleave the search results of one file with the search results of another file. A naive approach to this is to serialize the printer so that only one thread can print to it at a time. This is problematic though, because if a search thread acquires a lock to the printer before starting the search (and not releasing it until it has finished searching one file), you‚Äôll end up also serializing every search as well, effectively defeating your entire approach to parallelism. All code search tools in this benchmark that parallelize search therefore write results to some kind of intermediate buffer in memory. This enables all of the search threads to actually perform a search in parallel. The printing still needs to be serialized, but we‚Äôve reduced that down to simply dumping the contents of the intermediate buffer to stdout. Using an in memory buffer might set off alarm bells: what if you search a 2GB file and every line matches? Doesn‚Äôt that lead to excessive memory usage? The answer is: ‚ÄúWhy, yes, indeed it does!‚Äù The key insight is that the common case is returning far fewer matches than there are total lines searched. Nevertheless, there are ways to mitigate excessive memory usage. For example, if ripgrep is used to search stdin or a single file, then it will write search results directly to stdout and forgo the intermediate buffer because it just doesn‚Äôt need it. (ripgrep should also do this when asked to not do any parallelism, but I haven‚Äôt gotten to it yet.) In other words, pick two: space, time or correctness. Note that the details aren‚Äôt quite the same in every tool. Namely, while The Silver Searcher and Universal Code Grep write matches as structured data to memory (i.e., an array of match structs or something similar), both git grep and ripgrep write the actual output to a dynamically growable string buffer in memory. While either approach does seem to be fast enough, git grep and ripgrep have to do things this way because they support incremental search where as The Silver Searcher always memory maps the entire file and Universal Code Grep always reads the entire contents of the file into memory. The latter approach can refer back to the file‚Äôs contents in memory when doing the actual printing, where as neither git grep nor ripgrep can do that. Methodology Overview Coming up with a good and fair benchmark is hard, and I have assuredly made some mistakes in doing so. In particular, there are so many variables to control for that testing every possible permutation isn‚Äôt feasible. This means that the benchmarks I‚Äôm presenting here are curated, and, given that I am the author of one of the tools in the benchmark, they are therefore also biased. Nevertheless, even if I fail in my effort to provide a fair benchmark suite, I do hope that some of you may find my analysis interesting, which will try to explain the results in each benchmark. The analysis is in turn heavily biased toward explaining my own work, since that is the implementation I‚Äôm most familiar with. I have, however, read at least part of the source code of every tool I benchmark, including their underlying regex engines. In other words, I‚Äôm pretty confident that I‚Äôve gotten the details correct, but I could have missed something in the bigger picture. Because of that, let‚Äôs go over some important insights that guided construction of this benchmark. Focus on the problem that an end user is trying to solve. For example, we split the entire benchmark in two: one for searching a large directory of files and one for searching a single large file. The former might correspond to an end user searching their code while the latter might correspond to an end user searching logs. As we will see, these two use cases have markedly different performance characteristics. A tool that is good at one isn‚Äôt necessarily good at the other. (The premise of ripgrep is that it is possible to be good at both!) Apply end user problems more granularly as well. For example, most searches result in few hits relative to the corpus searched, so prefer benchmarks that report few matches. Another example: I hypothesize, based on my own experience, that most searches use patterns that are simple literals, alternations or very light regexes, so bias the benchmarks towards those types of patterns. Almost every search tool has slightly different default behavior, and these behavioral changes can have an impact on performance. There is some value in looking at ‚Äúout-of-the-box‚Äù performance, and we therefore do look at a benchmark for that, but stopping there is a bit unsatisfying. If our goal is to do a fair comparison, then we need to at least try to convince each tool to do roughly the same work, from the perspective of an end user. A good example of this is reporting line numbers. Some tools don‚Äôt provide a way of disabling line counting, so when doing comparisons between tools that do, we need to explicitly enable line numbers. This is important, because counting lines can be quite costly! A good non-example of this is if one tool uses memory maps and another uses an intermediate buffer. This is an implementation choice, and not one that alters what the user actually sees, therefore comparing those two implementation choices in a benchmark is completely fair (assuming an analysis that points it out). With that out of the way, let‚Äôs get into the nitty gritty. First and foremost, what tools are we benchmarking? ripgrep (rg) (v0.1.2) - You‚Äôve heard enough about this one already. GNU grep (v2.25) - Ol‚Äô reliable. git grep (v2.7.4) - Like grep, but built into git. Only works well in git repositories. The Silver Searcher (ag) (commit cda635, using PCRE 8.38) - Like ack, but written in C and much faster. Reads your .gitignore files just like ripgrep. Universal Code Grep (ucg) (commit 487bfb, using PCRE 10.21 with the JIT enabled) - Also like ack but written in C++, and only searches files from a whitelist, and doesn‚Äôt support reading .gitignore. The Platinum Searcher (pt) (commit 509368) - Written in Go and does support .gitignore files. sift (commit 2d175c) - Written in Go and supports .gitignore files with an optional flag, but generally prefers searching everything (unlike every other tool in this list except for grep). Notably absent from this list is ack. I chose not to benchmark it because, at the time of writing, ack was much slower than the other tools in this list. However, ack 3 is now in beta and includes some performance improvements, sometimes decreasing search times by half. Benchmark runner The benchmark runner is a Python program (requires at least Python 3.5) that you can use to not only run the benchmarks themselves, but download the corpora used in the benchmarks as well. The script is called benchsuite and is in the ripgrep repository. You can use it like so: $ git clone git://github.com/BurntSushi/ripgrep $ cd ripgrep/benchsuite # WARNING! This downloads several GB of data, and builds the Linux kernel. # This took about 15 minutes on a high speed connection. # Tip: try `--download subtitles-ru` to grab the smallest corpus, but you'll # be limited to running benchmarks for only that corpus. $ ./benchsuite --dir /path/to/data/dir --download all # List benchmarks available. $ ./benchsuite --dir /path/to/data/dir --list # Run a benchmark. # Omit the benchmark name to run all benchmarks. The full suite can take around # 30 minutes to complete on default settings and 120 minutes to complete with # --warmup-iter 3 --bench-iter 10. $ ./benchsuite --dir /path/to/data/dir '^subtitles_ru_literal$' If you don‚Äôt have all of the code search tools used in the benchmarks, then pass --allow-missing to give benchsuite permission to skip running them. To save the raw data (the timing for every command run), pass --raw /path/to/raw.csv. The benchmark runner tries to do a few basic things for us to help reduce the chance that we get misleading data: Every benchmarked command is run three times before being measured as a ‚Äúwarm up.‚Äù Specifically, this is to ensure that the corpora being searched is already in the operating system‚Äôs page cache. If we didn‚Äôt do this, we might end up benchmarking disk I/O, which is not only uninteresting for our purposes, but is probably not a common end user scenario. It‚Äôs more likely that you‚Äôll be executing lots of searches against the same corpus (at least, I know I do). Every benchmarked command is run ten times, with a timing recorded for each run. The final ‚Äúresult‚Äù of that command is its distribution (mean +/- standard deviation). If I were a statistician, I could probably prove that ten samples is insufficient. Nevertheless, getting more samples takes more time, and for the most part, the variance is very low. Each individual benchmark definition is responsible for making sure each command is trying to do similar work as other commands we‚Äôre comparing it to. For example, we need to be careful to enable and disable Unicode support in GNU grep where appropriate, because full Unicode handling can make GNU grep run very slowly. Within each benchmark, there are often multiple variables of interest. To account for this, I‚Äôve added labels like (ASCII) or (whitelist) where appropriate. We‚Äôll dig into those labels in more detail later. Please also feel encouraged to add your own benchmarks if you‚Äôd like to play around. The benchmarks are in the top-half of the file, and it should be fairly straight-forward to copy & paste another benchmark and modify it. Simply defining a new benchmark will make it available. The second half of the script is the runner itself and probably shouldn‚Äôt need to be modified. Environment The actual environment used to run the benchmarks presented in this article was a c3.2xlarge instance on Amazon EC2. It ran Ubuntu 16.04, had a Xeon E5-2680 2.8 GHz CPU, 16 GB of memory and an 80 GB SSD (on which the corpora was stored). This was enough memory to fit all of the corpora in memory. The box was specifically provisioned for the purpose of running benchmarks, so it was not doing anything else. The full log of system setup and commands I used to install each of the search tools and run benchmarks can be found here. I also captured the output of the bench runner (SPOILER ALERT) and the raw output, which includes the timings, full set of command line arguments and any environment variables set for every command run in every benchmark. Code search benchmarks This is the first half of our benchmarks, and corresponds to an end user trying to search a large repository of code for a particular pattern. The corpus used for this benchmark is a built checkout of the Linux kernel, specifically commit d0acc7. We actually build the Linux kernel because the process of building the kernel leaves a lot of garbage in the repository that you probably don‚Äôt want to search. This can influence not only the relevance of the results returned by a search tool, but the performance as well. All benchmarks run in this section were run in the root of the repository. Remember, you can see the full raw results of each command if you like. The benchmark names correspond to the headings below. Note that since these benchmarks were run on an EC2 instance, which uses a VM, which in turn can penalize search tools that use memory maps, I‚Äôve also recorded benchmarks on my local machine. My local machine is an Intel i7-6900K 3.2 GHz, 16 CPUs, 64 GB memory and an SSD. You‚Äôll notice that ag does a lot better (but still worse than rg) on my machine. Lest you think I‚Äôve chosen results from the EC2 machine because they paint rg more favorably, rest assured that I haven‚Äôt. Namely, rg wins every single benchmark on my local machine except for one, where as rg is beat out just slightly by a few tools on some benchmarks on the EC2 machine. Without further ado, let‚Äôs start looking at benchmarks. linux_literal_default Description: This benchmark compares the time it takes to execute a simple literal search using each tool‚Äôs default settings. This is an intentionally unfair benchmark meant to highlight the differences between tools and their ‚Äúout-of-the-box‚Äù settings. Pattern: PM_RESUME rg 0.349 +/- 0.104 (lines: 16) ag 1.589 +/- 0.009 (lines: 16) ucg 0.218 +/- 0.007 (lines: 16)*+ pt 0.462 +/- 0.012 (lines: 16) sift 0.352 +/- 0.018 (lines: 16) git grep 0.342 +/- 0.005 (lines: 16) * - Best mean time. + - Best sample time. rg == ripgrep, ag == The Silver Searcher, ucg == Universal Code Grep, pt == The Platinum Searcher Analysis: We‚Äôll first start by actually describing what each tool is doing: rg respects the Linux repo‚Äôs .gitignore files (of which there are 178(!) of them), and skips hidden and binary files. rg does not count lines. ag has the same default behavior as rg, except it counts lines. ucg also counts lines, but does not attempt to read .gitignore files. Instead, it only searches files from an (extensible) whitelist according to a set of glob rules. For example, both rg and ag will search fs/jffs2/README.Locking while ucg won‚Äôt, because it doesn‚Äôt recognize the Locking extension. (A search tool probably should search that file, although it does not impact the results of this specific benchmark.) pt has the same default behavior as ag. sift searches everything, including binary files and hidden files. It should be equivalent to grep -r, for example. It also does not count lines. git grep should have the same behavior at rg, and similarly does not count lines. Note though that git grep has a special advantage: it does not need to traverse the directory hierarchy. It can discover the set of files to search straight from its git index. The high-order bit to extract from this benchmark is that a naive comparison between search tools is completely unfair from the perspective of performance, but is really important if you care about the relevance of results returned to you. sift, like grep -r, will throw everything it can back at you, which is totally at odds with the philosophy behind every other tool in this benchmark: only return results that are probably relevant. Things inside your .git probably aren‚Äôt, for example. (This isn‚Äôt to say that sift‚Äôs philosophy is wrong. The tool is clearly intended to be configured by an end user to their own tastes, which has its own pros and cons.) With respect to performance, there are two key variables to pay attention to. They will appear again and again throughout our benchmark: Counting lines can be quite expensive. A naive solution‚Äîa loop over every byte and comparing it to a ‚Äîwill be quite slow for example. Universal Code Grep counts lines using SIMD and ripgrep counts lines using packed comparisons (16 bytes at a time). However, in the Linux code search benchmarks, because the size of each individual file is very small and the number of matches is tiny compared to the corpus size, the time spent counting lines tends to not be so significant. Especially since every tool in this benchmark parallelizes search to some degree. When we get to the single-file benchmarks, this variable will become much more pertinent. Respecting .gitignore files incurs some amount of overhead. Even though respecting .gitignore reduces the number of files searched, it can be slower overall to actually read the patterns, compile them and match them against every path than to just search every file. This is precisely how ucg soundly beats ripgrep in this benchmark. (We will control for this variable in future benchmarks.) In other words, respecting .gitignore is a feature that improves relevance first and foremost. It is strictly a bonus if it also happens to improve performance. The specific reasons why supporting .gitignore leads to a slower overall search are: Every directory descended requires looking for a corresponding .gitignore. Multiply the number of calls if you support additional ignore files, like both The Silver Searcher and ripgrep do. The Linux kernel repository has 4,640 directories. 178 of them have .gitignore files. Each .gitignore file needs to be compiled into something that can match file paths. Both The Silver Searcher and ripgrep use tricks to make this faster. For example, simple patterns like /vmlinux or *.o can be matched using simple literal comparisons or by looking at the file extension of a candidate path and comparing it directly. For more complex patterns like *.c.[012]*.*, a full glob matcher needs to be used. The Silver Searcher uses fnmatch while ripgrep translates all such globs into a single regular expression which can be matched against a single path all at once. Doing all this work takes time. Unlike ag, rg will try to support the full semantics of a .gitignore file. This means finding every ignore pattern that matches a file path and giving precedent to the most recently defined pattern. ag will bail on the first match it sees. Actually matching a path has non-trivial overhead that must be paid for every path searched. The compilation phase described above is complex precisely for making this part faster. We try to stay out of the regex machinery as best we can, but we can‚Äôt avoid it completely. In contrast, a whitelist like the one used by ucg is comparatively easy to make fast. The set of globs is known upfront, so no additional checks need to be made while traversing the file tree. Moreover, the globs tend to be of the *.ext variety, which fall into the bucket of globs that can be matched efficiently just by looking at the extension of a file path. The downside of a whitelist is obvious: you might end up missing search results simply because ucg didn‚Äôt know about a particular file extension. You could always teach ucg about the file extension, but you‚Äôre still blind to ‚Äúunknown unknowns‚Äù (i.e., files that you probably want to search but didn‚Äôt know upfront that you needed to). linux_literal Description: This benchmark runs the same query as in the linux_literal_default benchmark, but we try to do a fair comparison. In particular, we run ripgrep in two modes: one where it respects .gitignore files (corresponding to the (ignore) label) and one where it uses a whitelist and doesn‚Äôt respect .gitignore (corresponding to the (whitelist) label). The former mode is comparable to ag, pt, sift and git grep, while the latter mode is comparable to ucg. We also run rg a third time by explicitly telling it to use memory maps for search, which matches the implementation strategy used by ag. sift is run such that it respects .gitignore files and excludes binary, hidden and PDF files. All commands executed here count lines, because some commands (ag and ucg) don‚Äôt support disabling line counting. Pattern: PM_RESUME rg (ignore) 0.334 +/- 0.053 (lines: 16) rg (ignore) (mmap) 1.611 +/- 0.009 (lines: 16) ag (ignore) (mmap) 1.588 +/- 0.011 (lines: 16) pt (ignore) 0.456 +/- 0.025 (lines: 16) sift (ignore) 0.630 +/- 0.004 (lines: 16) git grep (ignore) 0.345 +/- 0.007 (lines: 16) rg (whitelist) 0.228 +/- 0.042 (lines: 16)+ ucg (whitelist) 0.218 +/- 0.007 (lines: 16)* * - Best mean time. + - Best sample time. Analysis: We have a ton of ground to cover on this one. First and foremost, the (ignore) vs. (whitelist) variables have a clear impact on the performance of rg. We won‚Äôt rehash all the details from the analysis in linux_literal_default, but switching rg into its whitelist mode brings it into a dead heat with ucg. Secondly, ucg is just as fast as ripgrep and git grep (ignore) is just as fast as rg (ignore), even though I‚Äôve said that ripgrep is the fastest. It turns out that ucg, git grep and rg are pretty evenly matched when searching for plain literals in large repositories. We will see a stronger separation in later benchmarks. Still, what makes ucg fast? ucg reads the entire file into memory before searching it, which means it avoids the memory map problem described below. On a code repository, this approach works well, but it comes with a steep price in the single-file benchmarks. It has a fast explicitly SIMD based line counting algorithm. ripgrep has something similar, but relies on the compiler for autovectorization. ucg uses PCRE2‚Äôs JIT, which is insanely fast. In my own very rough benchmarks, PCRE2‚Äôs JIT is one of the few general purpose regex engines that is competitive with Rust‚Äôs regex engine (on regexes that don‚Äôt expose PCRE‚Äôs exponential behavior due to backtracking, since Rust‚Äôs regex engine doesn‚Äôt suffer from that weakness). ucg parallelizes directory traversal, which is something that ripgrep doesn‚Äôt do. ucg has it a bit easier here because it doesn‚Äôt support .gitignore files. Parallelizing directory traversal while maintaining state for .gitignore files in a way that scales isn‚Äôt a problem I‚Äôve figured out how to cleanly solve yet. What about git grep? A key performance advantage of git grep is that it doesn‚Äôt need to walk the directory tree, which can save it quite a bit of time. Its regex engine is also quite fast, and works similarly to GNU grep‚Äôs, RE2 and Rust‚Äôs regex engine (i.e., it uses a DFA). Both sift and pt perform almost as well as ripgrep. In fact, both sift and pt do implement a parallel recursive directory traversal while still respecting .gitignore files, which is likely one reason for their speed. As we will see in future benchmarks, their speed here is misleading. Namely, they are fast because they stay outside of Go‚Äôs regexp engine since the pattern is a literal. (There will be more discussion on this point later.) Finally, what‚Äôs going on with The Silver Searcher? Is it really that much slower than everything else? The key here is that its use of memory maps is making it slower, not faster (in direct contradiction to the claims in its README). OK, let‚Äôs pause and pop up a level to talk about what this actually means. First, we need to consider how these search tools fundamentally work. Generally speaking, a search tool like this has two ways of actually searching files on disk: It can memory map the file and search the entire file all at once as if it were a single contiguous region of bytes in memory. The operating system does the work behind the scenes to make a file look like a contiguous region of memory. This particular approach is really convenient when comparing it to the alternative described next. ‚Ä¶ or it can allocate an intermediate buffer, read a fixed size block of bytes from the file into it, search the buffer and then repeat the process. This particular approach is absolutely ghoulish to implement, because you need to account for the fact that a buffer may end in the middle of the line. You also need to account for the fact that a single line may exceed the size of your buffer. Finally, if you‚Äôre going to support showing the lines around a match (its ‚Äúcontext‚Äù) as both grep and ripgrep do, then you need to do additional bookkeeping to make sure any lines from a previous buffer are printed even if a match occurs at the beginning of the next block read from the file. Naively, it seems like (1) would be obviously faster. Surely, all of the bookkeeping and copying in (2) would make it much slower! In fact, this is not at all true. (1) may not require much bookkeeping from the perspective of the programmer, but there is a lot of bookkeeping going on inside the Linux kernel to maintain the memory map. (That link goes to a mailing list post that is quite old, but it still appears relevant today.) When I first started writing ripgrep, I used the memory map approach. It took me a long time to be convinced enough to start down the second path with an intermediate buffer (because neither a CPU profile nor the output of strace ever showed any convincing evidence that memory maps were to blame), but as soon as I had a prototype of (2) working, it was clear that it was much faster than the memory map approach. With all that said, memory maps aren‚Äôt all bad. They just happen to be bad for the particular use case of ‚Äúrapidly open, scan and close memory maps for thousands of small files.‚Äù For a different use case, like, say, ‚Äúopen this large file and search it once,‚Äù memory maps turn out to be a boon. We‚Äôll see that in action in our single-file benchmarks later. The key datapoint that supports this conclusion is the comparison between rg (ignore) and rg (ignore) (mmap). In particular, this controls for everything except for the search strategy and fairly conclusively points right at memory maps as the problem. With all that said, the performance of memory maps is very dependent on your environment, and the absolute difference between rg (ignore) and ag (ignore) (mmap) can be misleading. In particular, since these benchmarks were run on an EC2 c3.2xlarge, we were probably inside a virtual machine, which could feasibly impact memory map performance. To test this, I ran the same benchmark on my machine under my desk (Intel i7-6900K 3.2 GHz, 16 CPUs, 64 GB memory, SSD) and got these results: rg (ignore) 0.156 +/- 0.006 (lines: 16) rg (ignore) (mmap) 0.397 +/- 0.013 (lines: 16) ag (ignore) (mmap) 0.444 +/- 0.016 (lines: 16) pt (ignore) 0.159 +/- 0.008 (lines: 16) sift (ignore) 0.344 +/- 0.002 (lines: 16) git grep (ignore) 0.195 +/- 0.023 (lines: 16) rg (whitelist) 0.108 +/- 0.005 (lines: 16)*+ ucg (whitelist) 0.165 +/- 0.005 (lines: 16) rg (ignore) still soundly beats ag, and our memory map conclusions above are still supported by this data, but the difference between rg (ignore) and ag (ignore) (mmap) has narrowed quite a bit! linux_literal_casei Description: This benchmark is like linux_literal, except it asks the search tool to perform a case insensitive search. Pattern: PM_RESUME (with the -i flag set) rg (ignore) 0.345 +/- 0.073 (lines: 370) rg (ignore) (mmap) 1.612 +/- 0.011 (lines: 370) ag (ignore) (mmap) 1.609 +/- 0.015 (lines: 370) pt (ignore) 17.204 +/- 0.126 (lines: 370) sift (ignore) 0.805 +/- 0.005 (lines: 370) git grep (ignore) 0.343 +/- 0.007 (lines: 370) rg (whitelist) 0.222 +/- 0.021 (lines: 370)+ ucg (whitelist) 0.217 +/- 0.006 (lines: 370)* * - Best mean time. + - Best sample time. Analysis: The biggest change from the previous benchmark is that pt got an order of magnitude slower than the next slowest tool. So why did pt get so slow? In particular, both sift and pt use Go‚Äôs regexp package for searching, so why did one perish while the other only got slightly slower? It turns out that when pt sees the -i flag indicating case insensitive search, it will force itself to use Go‚Äôs regexp engine with the i flag set. So for example, given a CLI invocation of pt -i foo, it will translate that to a Go regexp of (?i)foo, which will handle the case insensitive search. On the other hand, sift will notice the -i flag and take a different route. sift will lowercase both the pattern and every block of bytes it searches. This filter over all the bytes searched is likely the cause of sift‚Äôs performance drop from the previous linux_literal benchmark. (It‚Äôs worth pointing out that this optimization is actually incorrect, because it only accounts for ASCII case insensitivity, and not full Unicode case insensitivity, which pt gets by virture of Go‚Äôs regexp engine.) But still, is Go‚Äôs regexp engine really that slow? Unfortunately, yes, it is. While Go‚Äôs regexp engine takes worst case linear time on all searches (and is therefore exponentially faster than even PCRE2 for some set of regexes and corpora), its actual implementation hasn‚Äôt quite matured yet. Indeed, every fast regex engine based on finite automata that I‚Äôm aware of implements some kind of DFA engine. For example, GNU grep, Google‚Äôs RE2 and Rust‚Äôs regex library all do this. Go‚Äôs does not (but there is work in progress to make this happen, so perhaps pt will get faster on this benchmark without having to do anything at all!). There is one other thing worth noting here before moving on. Namely, that rg, ag, git grep and ucg didn‚Äôt noticeably change much from the previous benchmark. Shouldn‚Äôt a case insensitive search incur some kind of overhead? The answer is complicated and actually requires more knowledge of the underlying regex engines than I have. Thankfully, I can at least answer it for Rust‚Äôs regex engine. The key insight is that a case insensitive search for PM_RESUME is precisely the same as a case sensitive search of the alternation of all possible case agnostic versions of PM_RESUME. So for example, it might start like: PM_RESUME|pM_RESUME|Pm_RESUME|PM_rESUME|... and so on. Of course, the full alternation, even for a small literal like this, would be quite large. The key is that we can extract a small prefix and enumerate all of its combinations quite easily. In this case, Rust‚Äôs regex engine figures out this alternation (which you can see by passing --debug to rg and examining stderr): PM_RE PM_Re PM_rE PM_re Pm_RE Pm_Re Pm_rE Pm_re pM_RE pM_Re pM_rE pM_re pm_RE pm_Re pm_rE pm_re (Rest assured that Unicode support is baked into this process. For example, a case insensitive search for S would yield the following literals: S, s and ≈ø.) Now that we have this alternation of literals, what do we do with them? The classical answer is to compile them into a DFA (perhaps Aho-Corasick), and use it as a way to quickly skip through the search text. A match of any of the literals would then cause the regex engine to activate and try to verify the match. This way, we aren‚Äôt actually running the entire search text through the regex engine, which could be quite a bit slower. But, Rust‚Äôs regex engine doesn‚Äôt actually use Aho-Corasick for this. When SIMD acceleration is enabled (and you can be sure it is for these benchmarks, and for the binaries I distribute), a special multiple pattern search algorithm called Teddy is used. The algorithm is unpublished, but was invented by Geoffrey Langdale as part of Intel‚Äôs Hyperscan regex library. The algorithm works roughly by using packed comparisons of 16 bytes at a time to find candidate locations where a literal might match. I adapted the algorithm from the Hyperscan project to Rust, and included an extensive write up in the comments if you‚Äôre interested. While Teddy doesn‚Äôt buy us much over other tools in this particular benchmark, we will see much larger wins in later benchmarks. linux_word Description: This benchmarks the PM_RESUME literal again, but adds the -w flag to each tool. The -w flag has the following behavior: all matches reported must be considered ‚Äúwords.‚Äù That is, a ‚Äúword‚Äù is something that starts and ends at a word boundary, where a word boundary is defined as a position in the search text that is adjacent to both a word character and a non-word character. Pattern: PM_RESUME (with the -w flag set) rg (ignore) 0.362 +/- 0.080 (lines: 6) ag (ignore) 1.603 +/- 0.009 (lines: 6) pt (ignore) 14.417 +/- 0.144 (lines: 6) sift (ignore) 7.840 +/- 0.123 (lines: 6) git grep (ignore) 0.341 +/- 0.005 (lines: 6) rg (whitelist) 0.220 +/- 0.026 (lines: 6)*+ ucg (whitelist) 0.221 +/- 0.007 (lines: 6) * - Best mean time. + - Best sample time. Analysis: Not much has changed between this benchmark and the previous linux_literal or linux_literal_casei benchmarks. The most important thing to note is that most search tools handle the -w flag just fine without any noticeable drop in performance. There are two additional things I‚Äôd like to note. rg is searching with Unicode aware word boundaries where as the rest of the tools are using ASCII only word boundaries. (git grep can be made to use Unicode word boundaries by adjusting your system‚Äôs locale settings. In this benchmark, we force it to use ASCII word boundaries.) sift and pt are the only tools that gets noticeably slower in this benchmark compared to previous benchmarks. The reason is the same as the reason why pt got noticeably slower in the linux_literal_casei benchmark: both pt and sift are now also bottlenecked on Go‚Äôs regexp library. pt and sift could do a little better here by staying out of Go‚Äôs regexp library and searching for the PM_RESUME literal, and then only confirming whether the match corresponds to a word boundary after it found a hit for PM_RESUME. This still might use Go‚Äôs regexp library, but in a much more limited form. linux_unicode_word Description: This benchmarks a simple query for all prefixed forms of the ‚Äúamp-hour‚Äù (Ah) unit of measurement. For example, it should show things like mAh (for milliamp-hour) and ¬µAh (for microamp-hour). It is particularly interesting because the second form starts with ¬µ, which is part of a Unicode aware \\w character class, but not an ASCII-only \\w character class. We again continue to control for the overhead of respecting .gitignore files. Pattern: \\wAh rg (ignore) 0.355 +/- 0.073 (lines: 186) rg (ignore) (ASCII) 0.329 +/- 0.060 (lines: 174) ag (ignore) (ASCII) 1.774 +/- 0.011 (lines: 174) pt (ignore) (ASCII) 14.180 +/- 0.180 (lines: 174) sift (ignore) (ASCII) 11.087 +/- 0.108 (lines: 174) git grep (ignore) 13.045 +/- 0.008 (lines: 186) git grep (ignore) (ASCII) 2.991 +/- 0.004 (lines: 174) rg (whitelist) 0.235 +/- 0.031 (lines: 180) rg (whitelist) (ASCII) 0.225 +/- 0.023 (lines: 168)*+ ucg (ASCII) 0.229 +/- 0.007 (lines: 168) * - Best mean time. + - Best sample time. Analysis: In this benchmark, we‚Äôve introduced a new variable: whether or not to enable Unicode support in each tool. Searches that are Unicode aware report slightly more matches that are missed by the other ASCII only searches. Of all the tools here, the only ones that support Unicode toggling are rg and git grep. rg‚Äôs Unicode support can be toggled by setting a flag in the pattern itself (e.g., \\w is Unicode aware while (?-u)\\w is not), and git grep‚Äôs Unicode suport can be toggled by setting the LC_ALL environment variable (where en_US.UTF-8 is one way to enable Unicode support and C forces it to be ASCII). More generally, git grep‚Äôs Unicode support is supposed to line up with your system‚Äôs locale settings‚Äîsetting LC_ALL is a bit of a hack. It gets a little worse than that actually. Not only are rg and git grep the only ones to support toggling Unicode, but they are the only ones to support Unicode at all. ag, pt, sift and ucg will all force you to use the ASCII only \\w character class. (For pt and sift in particular, Go‚Äôs regexp library doesn‚Äôt have the ability to treat \\w as Unicode aware. For ag and ucg, which use PCRE, \\w could be made Unicode aware with a flag sent to PCRE. Neither tool exposes that functionality though.) The key result to note here is that while git grep suffers a major performance hit for enabling Unicode support, ripgrep hums along just fine with no noticeable loss in performance, even though both rg (ignore) and git grep (ignore) report the same set of results. As in the previous benchmark, both pt and sift could do better here by searching for the Ah literal, and only using Go‚Äôs regexp library to verify a match.) Looking at the benchmark results, I can think of two important questions to ask: Why is git grep (ignore) (ASCII) so much slower than rg (ignore) (ASCII)? And while the two aren‚Äôt directly comparable, it‚Äôs also a lot slower than ucg (ASCII). How is rg (ignore) (which is Unicode aware) just as fast as rg (ignore) (ASCII)? I actually don‚Äôt have a great answer for (1). In the case of rg at least, it will extract the Ah literal suffix from the regex and use that to find candidate matches before running the \\w prefix. While GNU grep has sophisticated literal extraction as well, it looks like git grep doesn‚Äôt go to similar lengths to extract literals. (I‚Äôm arriving at this conclusion after skimming the source of git grep, so I could be wrong.) In the case of ucg, it‚Äôs likely that PCRE2 is doing a similar literal optimization that rg is doing. (2) is fortunately much easier to answer. The trick is not inside of rg, but inside its regex library. Namely, the regex engine builds UTF-8 decoding into its finite state machine. (This is a trick that is originally attributed to Ken Thompson, but was more carefully described by Russ Cox. To read more about how this is achieved in Rust‚Äôs regex engine, please see the utf8-ranges library.) The reason why this is fast is because there is no extra decoding step required. The regex can be matched directly against UTF-8 encoded byte strings one byte at a time. Invalid UTF-8 doesn‚Äôt pose any problems: the finite automaton simply won‚Äôt match it because it doesn‚Äôt recognize it. In contrast, git grep (and GNU grep) have a completely separate path in their core matching code for handling Unicode aware features like this. To be fair, git grep can handle text encodings other than UTF-8, where as rg is limited to UTF-8 (or otherwise ‚ÄúASCII compatible‚Äù text encodings) at the moment. linux_re_literal_suffix Description: This benchmarks a simple regex pattern that ends with a literal. We continue to control for the overhead of respecting .gitignore files. Pattern: [A-Z]+_RESUME rg (ignore) 0.318 +/- 0.034 (lines: 1652) ag (ignore) 1.899 +/- 0.008 (lines: 1652) pt (ignore) 13.713 +/- 0.241 (lines: 1652) sift (ignore) 10.172 +/- 0.186 (lines: 1652) git grep (ignore) 1.108 +/- 0.004 (lines: 1652) rg (whitelist) 0.221 +/- 0.022 (lines: 1630)*+ ucg (whitelist) 0.301 +/- 0.001 (lines: 1630) * - Best mean time. + - Best sample time. Analysis: This benchmark doesn‚Äôt reveal anything particularly new that we haven‚Äôt already learned from previous benchmarks. In particular, both rg and ucg continue to be competitive, pt and sift are getting bottlenecked by Go‚Äôs regexp library and git grep has a slow down similar to the one observed in linux_unicode_word. (My hypothesis for that slow down continues to be that git grep is missing the literal optimization.) Finally, ag continues to be held back by its use of memory maps. rg, and almost assuredly ucg (by virtue of PCRE2), are picking on the _RESUME literal suffix and searching for that instead of running the regex over the entire search text. This explains why both tools are able to maintain their speed even as the pattern gets slightly more complex. rg does seem to slightly edge out ucg here, which might be attributable to differences in how each underlying regex library does literal search. linux_alternates Description: This benchmarks an alternation of four literals. The literals were specifically chosen to start with four distinct bytes to make it harder to optimize. Pattern: ERR_SYS|PME_TURN_OFF|LINK_REQ_RST|CFG_BME_EVT rg (ignore) 0.351 +/- 0.074 (lines: 68) ag (ignore) 1.747 +/- 0.005 (lines: 68) git grep (ignore) 0.501 +/- 0.003 (lines: 68) rg (whitelist) 0.216 +/- 0.031 (lines: 68)+ ucg (whitelist) 0.214 +/- 0.008 (lines: 68)* * - Best mean time. + - Best sample time. We drop pt and sift from this benchmark and the next one for expediency. In this benchmark and in a few previous benchmarks, they have been hovering around an order of magnitude slower than the next slowest tool. Neither get any better as the complexity of our patterns increase. Analysis: Yet again, both rg and ucg maintain high speed even as the pattern grows beyond a simple literal. In this case, there isn‚Äôt any one particular literal that we can search to find match candidates quickly, but a good regular expression engine can still find ways to speed this up. For rg in particular, it sees the four literals and diverts to the Teddy multiple pattern SIMD algorithm (as described in the linux_literal_casei benchmark). In fact, for this particular pattern, Rust‚Äôs core regex engine is never used at all. Namely, it notices that a literal match of any of the alternates corresponds to an overall match of the pattern, so it can completely skip the verification step. This makes searching alternates of literals very fast. linux_alternates_casei Description: This benchmark is precisely the same as the linux_alternates benchmark, except we make the search case insensitive by adding the -i flag. Note that git grep is run under ASCII mode, in order to give it every chance to be fast. Pattern: ERR_SYS|PME_TURN_OFF|LINK_REQ_RST|CFG_BME_EVT (with the -i flag set) rg (ignore) 0.391 +/- 0.078 (lines: 160) ag (ignore) 1.968 +/- 0.009 (lines: 160) git grep (ignore) 2.018 +/- 0.006 (lines: 160) rg (whitelist) 0.222 +/- 0.001 (lines: 160)*+ ucg (whitelist) 0.522 +/- 0.002 (lines: 160) * - Best mean time. + - Best sample time. Analysis: The case insensitive flag causes quite a bit of separation, relative to the previous linux_alterates benchmark. For one, git grep gets over 4 times slower. Even ucg gets twice as slow. Yet, rg continues to maintain its speed! The secret continues to be the Teddy algorithm, just as in the linux_alternates benchmark. The trick lies in how we transform an alternation of case insensitive literals into a larger alternation that the Teddy algorithm can actually use. In fact, it works exactly how it was described in the linux_literal_casei benchmark: we enumerate all possible alternations of each literal that are required for case insensitive match. Since that can be quite a large number, we limit ourselves to a small number of prefixes from that set that we can enumerate. In this case, we use the following prefixes (which can be seen by running rg with the --debug flag): CFG_ CFg_ CfG_ Cfg_ ERR_ ERr_ ErR_ Err_ LIN LIn LiN Lin PME_ PMe_ PmE_ Pme_ cFG_ cFg_ cfG_ cfg_ eRR_ eRr_ erR_ err_ lIN lIn liN lin pME_ pMe_ pmE_ pme_ We feed these literals to the Teddy algorithm, which will quickly identify candidate matches in the search text. When a candidate match is found, we need to verify it since a match of a prefix doesn‚Äôt necessarily mean the entire pattern matches. It is only at that point that we actually invoke the full regex engine. linux_unicode_greek Description: This benchmarks usage of a particular Unicode feature that permits one to match a certain class of codepoints defined in Unicode. Both Rust‚Äôs regex engine and Go‚Äôs regex engine support this natively, but none of the other tools do. Pattern: \\p{Greek} (matches any Greek symbol) rg 0.414 +/- 0.021 (lines: 23)*+ pt 12.745 +/- 0.166 (lines: 23) sift 7.767 +/- 0.264 (lines: 23) * - Best mean time. + - Best sample time. Analysis: This one is pretty simple. rg compiles \\p{Greek} into a deterministic finite state machine while Go (used in pt and sift) will also use a finite state machine, but it is a nondeterministic simulation. The core difference between the two approaches is that the former is only ever in one state at any point in time, while the latter must constantly keep track of all the different states it is in. linux_unicode_greek_casei Description: This benchmark is just like the linux_unicode_greek benchmark, except it makes the search case insensitive. This particular query is a bit idiosyncratic, but it does demonstrate just how well supported Unicode is in rg. Pattern: \\p{Greek} (with the -i flag set, matches any Greek symbol) rg 0.425 +/- 0.027 (lines: 103) pt 12.612 +/- 0.217 (lines: 23) sift 0.002 +/- 0.000 (lines: 0)*+ * - Best mean time. + - Best sample time. Analysis: sift doesn‚Äôt actually beat rg here: it just gets so confused by the search request that it gives up and reports no matches. pt seems to execute the search, but doesn‚Äôt handle Unicode case insensitivity correctly. Meanwhile, rg handles the request just fine, and it‚Äôs still fast. In this particular case, the entire Greek category, along with all of its case-insensitive variants, are compiled into a single fast deterministic finite state machine. One interesting thing to note about this search is that if you run it, you‚Äôll see a lot more results containing the character ¬µ, which looks essentially identical to the character Œº that also shows up in a case sensitive search. As you might have guessed, even though these two characters look the same, they are in fact distinct Unicode codepoints: ¬µ is MICRO SIGN with codepoint U+000000B5. Œº is GREEK SMALL LETTER MU with codepoint U+000003BC. The latter codepoint is considered part of the \\p{Greek} group while the former codepoint is not (the former codepoint appears to be the correct sigil to use in the case of the Linux kernel). However, the Unicode simple case folding tables map MICRO SIGN to GREEK SMALL LETTER MU, which causes rg to pick up on lines containing MICRO SIGN even though it strictly isn‚Äôt part of the Greek group. linux_no_literal Description: This is the last benchmark on the Linux kernel source code and is a bit idiosyncratic like linux_unicode_greek_casei. In particular, it looks for lines containing 5 consecutive repetitions of 5 word characters, each separated by one or more space characters. The key distinction of this pattern from every other pattern in this benchmark is that it does not contain any literals. Given the presence of \\w and \\s, which have valid Unicode and ASCII interpretations, we attempt to control for the presence of Unicode support. Pattern: \\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5} rg (ignore) 0.577 +/- 0.003 (lines: 490) rg (ignore) (ASCII) 0.416 +/- 0.025 (lines: 490) ag (ignore) (ASCII) 2.339 +/- 0.010 (lines: 766) pt (ignore) (ASCII) 22.066 +/- 0.057 (lines: 490) sift (ignore) (ASCII) 25.563 +/- 0.108 (lines: 490) git grep (ignore) 26.382 +/- 0.044 (lines: 490) git grep (ignore) (ASCII) 4.153 +/- 0.010 (lines: 490) rg (whitelist) 0.503 +/- 0.011 (lines: 419) rg (whitelist) (ASCII) 0.343 +/- 0.038 (lines: 419)*+ ucg (whitelist) (ASCII) 1.130 +/- 0.003 (lines: 416) * - Best mean time. + - Best sample time. ag reports many more matches than other tools because it does multiline search where the \\s can match a . Analysis: Since this particular pattern doesn‚Äôt have any literals in it, it‚Äôs entirely up to the underlying regex engine to answer this query. It can‚Äôt be smart and skip through the input‚Äîit has to pass it completely through the regex engine. Since non-literal patterns are pretty rare in my experience, this benchmark exists primarily as an engineered way to test how well the underlying regex engines perform. rg, regardless of whether it respects .gitignore files or whether it handles Unicode correctly, does quite well here compared to other tools. git grep in particular pays a 5x penalty for Unicode support. rg on the other hand pays about a 0.3x penalty for Unicode support. Interestingly, even though ucg doesn‚Äôt enable Unicode support, not even PCRE2‚Äôs JIT can compete with rg! What makes rg so fast here? And what actually causes the 0.3x penalty? rg continues to be fast on this benchmark primarily for the same reason why it‚Äôs fast with other Unicode-centric benchmarks: it compiles the UTF-8 decoding right into its deterministic finite state machine. This means there is no extra step to decode the search text into Unicode codepoints first. We can match directly on the raw bytes. To a first approximation, the performance penalty comes from compiling the DFA to match the pattern. In particular, the DFA to match the Unicode variant is much much larger than the DFA to match the ASCII variant. To give you a rough idea of the size difference: The ASCII DFA has about 250 distinct NFA states. The Unicode DFA has about 77,000 distinct NFA states. (These numbers are produced directly from the compiler in Rust‚Äôs regex library, and don‚Äôt necessarily reflect a minimal automaton.) A DFA produced from these patterns doesn‚Äôt necessarily have the same number of states, since each DFA state typically corresponds to multiple NFA states. (Check out the Powerset construction Wikipedia article. Although it doesn‚Äôt correspond to the same implementation strategy used in Rust‚Äôs regex engine, it should give good intuition.) However, the first approximation is a bit misleading. While Rust‚Äôs regex engine does have a preprocessing compilation phase, it does not actually include converting an NFA into a DFA. Indeed, that would be far too slow and could take exponential time! Instead, Rust‚Äôs regex engine builds the DFA on the fly or ‚Äúlazily,‚Äù as it searches the text. In the case of the ASCII pattern, this search barely spends any time constructing the DFA states since there are so few of them. However, in the Unicode case, since there are so many NFA states, it winds up spending a lot of time compiling new DFA states. (I‚Äôve confirmed this by inspecting a profile generated by perf.) Digging a bit deeper, the actual story here might be subtler. For example, the Unicode pattern might wind up with the same number of DFA states as the ASCII pattern, primarily because the input its searching is the same and is primarily ASCII. The slow down then must come from the fact that each individual DFA state takes longer to build. This is likely correct since a single Unicode \\w is over two orders of magnitude larger than a single ASCII \\w. Therefore, each DFA state probably has a lot more NFA states in it for the Unicode pattern as opposed to the ASCII pattern. It‚Äôs not clear whether we can do any better here (other than trying to minimize the Unicode \\w, which would be totally feasible), since we don‚Äôt actually know the composition of the search text ahead of time. One idea for improvement is to have multiple types of DFAs. For example, you might imagine trying to match with an ASCII only DFA. If the DFA sees a non-ASCII byte, then it could cause a transition into a Unicode-aware DFA. However, the penalty here is so small that it‚Äôs hard to justify this kind of implementation complexity! Single file benchmarks In the second half of our benchmarks, we will shift gears and look more closely at the performance of search tools on a single large file. Each benchmark will be run on two samples of the OpenSubtitles2016 dataset. One sample will be English and therefore predominantly ASCII, and another sample will be in Russian and therefore predominantly Cyrillic. The patterns for the Russian sample were translated from English using Google Translate. (Sadly, I can‚Äôt read Russian, but I have tried each search by hand and confirmed that a sample of the results I was looking at were relevant by piping them back through Google Translate.) The English sample is around 1GB and the Russian sample is around 1.6GB, so the benchmark timings aren‚Äôt directly comparable. In this benchmark, the performance of the underlying regex engine and various literal optimizations matter a lot more. The two key variables we‚Äôll need to control for are line counting and Unicode support. Normally, we‚Äôd just not request line counting from any of the tools, but neither of The Silver Searcher or Universal Code Grep support disabling line numbers. Additionally, Unicode support is tricky to control for in some examples because ripgrep does not support ASCII only case insensitive semantics when searching with a non-ASCII string. It‚Äôs Unicode all the way and there‚Äôs no way to turn it off. As we‚Äôll see, at least for ripgrep, it‚Äôs still faster than its ASCII alternatives even when providing case insensitive Unicode support. As with the Linux benchmark, you can see precisely which command was run and its recorded time in the raw data. ripgrep utterly dominates this round, both in performance and correctness. subtitles_literal Description: This benchmarks the simplest case for any search tool: find all occurrences of a literal string. Tools annotated with (lines) were passed the -n flag (or equivalent) so that the output reports line numbers. English pattern: Sherlock Holmes rg 0.268 +/- 0.000 (lines: 629)*+ rg (no mmap) 0.336 +/- 0.001 (lines: 629) pt 3.433 +/- 0.002 (lines: 629) sift 0.326 +/- 0.002 (lines: 629) grep 0.516 +/- 0.001 (lines: 629) rg (lines) 0.595 +/- 0.001 (lines: 629) ag (lines) 2.730 +/- 0.003 (lines: 629) ucg (lines) 0.745 +/- 0.001 (lines: 629) pt (lines) 3.434 +/- 0.005 (lines: 629) sift (lines) 0.756 +/- 0.002 (lines: 629) grep (lines) 0.969 +/- 0.001 (lines: 629) Russian pattern: –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å rg 0.325 +/- 0.001 (lines: 583)*+ rg (no mmap) 0.452 +/- 0.002 (lines: 583) pt 12.917 +/- 0.009 (lines: 583) sift 16.418 +/- 0.008 (lines: 583) grep 0.780 +/- 0.001 (lines: 583) rg (lines) 0.926 +/- 0.001 (lines: 583) ag (lines) 4.481 +/- 0.003 (lines: 583) ucg (lines) 1.889 +/- 0.004 (lines: 583) pt (lines) 12.935 +/- 0.011 (lines: 583) sift (lines) 17.177 +/- 0.010 (lines: 583) grep (lines) 1.300 +/- 0.003 (lines: 583) * - Best mean time. + - Best sample time. This is the only benchmark that contains pt and sift, since they become too slow in all future benchmarks. Analysis: Whether it‚Äôs part of the underlying regex engine or part of the search tool itself, every search tool in this benchmark does some kind of literal optimization. ag will inspect the pattern, and if it doesn‚Äôt contain any special regex characters, then it will use a Boyer-Moore variant to perform the search instead of PCRE. GNU grep does something similar, although it has clearly been the subject of much optimization. If that‚Äôs true, how does rg beat GNU grep by almost a factor of 2? Well, first and foremost, we note that both sift and ucg beat GNU grep as well. I won‚Äôt be able to go into detail on ucg‚Äôs speed since PCRE2‚Äôs JIT isn‚Äôt something I understand very well, but I can at least tell you that the reasons why rg and sift are faster than GNU grep are actually distinct: sift uses Go‚Äôs regexp library, which will do at least one small literal optimization: if every match of a regex starts with the same byte, the regex engine will scan for that byte before starting a match. If you follow the code that does the scan for the byte all the way back to its source for x86_64 systems, then you‚Äôll find that it is using AVX2 instructions and ymm registers, which permit scanning 32 bytes in each iteration. In contrast, GNU grep uses libc‚Äôs memchr, which doesn‚Äôt use AVX2. However, that C code will be autovectorized to use xmm registers and SIMD instructions, which are half the size of ymm registers. In other words, by virture of being written in Go, sift is making more efficient use of the CPU. rg also uses memchr from libc. The rg binary that was used in this benchmark was statically linked with musl, which provides its own implementation of memchr. Despite it being quite a bit terser than GNU‚Äôs libc implementation used in GNU grep, it appears to be doing roughly the same work. If that‚Äôs the case, how is rg faster? The answer lies not in memchr nor in the variant of Boyer-Moore nor in the number characters Boyer-Moore can skip. The answer instead lies in which byte is given to memchr. rg will actually try to guess the ‚Äúrarest‚Äù byte in a literal, and use memchr on that. (A standard Boyer-Moore implementation will use memchr always on the last byte.) In this particular case, running memchr on either S or H is probably quite a bit better than running it on s because S and H are far less common than s. That is, rg tries harder than GNU grep to spend more time skipping bytes in a fast SIMD optimized loop. rg can get this wrong, but it seems strictly better to at least guess and probably get it right in the common case than to submit to an artifact of common Boyer-Moore implementations. Now that the secrets of literal search have been revealed, we can better analyze the Russian benchmark. The answer once again lies in which byte is used for quick scanning. Both sift and pt use the same AVX2 routine in Go‚Äôs runtime, so why did they get so much slower than every other tool in the Russian benchmark? The answer becomes more clear when we look at the actual UTF-8 bytes of the pattern –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å: \\xd0\\xa8\\xd0\\xb5\\xd1\\x80\\xd0\\xbb\\xd0\\xbe\\xd0\\xba \\xd0\\xa5\\xd0\\xbe\\xd0\\xbb\\xd0\\xbc\\xd1\\x81 There are two key observations to take away from this: Every character in the pattern –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å is encoded with two UTF-8 code units, which corresponds to two bytes. Every character starts with either the byte \\xD0 or \\xD1. If we looked at the UTF-8 bytes of the Russian subtitles we‚Äôre searching, we‚Äôd end up seeing exactly the same pattern. This pattern occurs because the contents of the file are mostly Cyrllic, which are all mostly part of a couple small ranges in Unicode. This means that the \\xD0 and \\xD1 bytes occur a lot. If you recall from above, Go‚Äôs regex engine will scan for occurrences of the first byte. But if that first byte happens as frequently as it does here, the overall search will wind up going slower because there is overhead associated with doing that scan. This is precisely the trade off one is exposed to whenever memchr is used. As you might have guessed, rg works around this issue by trying to guess the rarest byte. rg specifically draws from a pre-computed frequency table of all 256 bytes. Bytes like \\xD0 and \\xD1 are considered to be among the most frequent while bytes like \\xA8 and \\x81 are considered more rare. Therefore, rg will prefer bytes other than \\xD0 and \\xD1 for use with memchr. GNU grep continues to do well on this benchmark mostly because of blind luck: Boyer-Moore uses the last byte, which will correspond to \\x81, which is much rarer than \\xD0 or \\xD1. Switching gears, we should briefly discuss memory maps. In this benchmark, rg beats out rg (no mmap) by about 25%. The only difference between the two is that the former memory maps the file into memory while the latter incrementally reads bytes from the file into an intermediate buffer, and searches it. In this case, the overhead of the memory map is very small because we only need to create one of them. This is the opposite result from our Linux benchmark above, where memory maps proved to be worse than searching with an intermediate buffer since we needed to create a new memory map for every file we searched, which ends up incurring quite a bit of overhead. rg takes an empirical approach here and enables memory map searching when it knows it only needs to search a few files, and otherwise searches using an intermediate buffer. One last note: I‚Äôve neglected to talk about (lines) because there‚Äôs really not much to say here: counting lines takes work, and if you don‚Äôt need to report line numbers, you can avoid doing that work. ucg has a rather cool SIMD algorithm to count lines and rg also has a packed counting algorithm that works similarly to the memchr implementations we talked about. If it were up to me, I‚Äôd probably remove benchmarks with line numbers altogether, since most tools tend to reliably pay just a little bit extra for them. However, neither ag nor ucg allow turning them off, so we need to turn them on in other tools in order to make a fair comparison. subtitles_literal_casei Description: This benchmark is just like subtitles_literal, except it does case insensitive search. Tools annotated with (lines) show line numbers in their output, and tools annotated with (ASCII) are doing an ASCII-only search. Correspondingly, tools not labeled with (ASCII) are doing a proper Unicode search. English pattern: Sherlock Holmes (with the -i flag set) rg 0.366 +/- 0.001 (lines: 642)*+ grep 4.084 +/- 0.005 (lines: 642) grep (ASCII) 0.614 +/- 0.001 (lines: 642) rg (lines) 0.696 +/- 0.002 (lines: 642) ag (lines) (ASCII) 2.775 +/- 0.004 (lines: 642) ucg (lines) (ASCII) 0.841 +/- 0.002 (lines: 642) Russian pattern: –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å rg 1.131 +/- 0.001 (lines: 604) grep 8.187 +/- 0.006 (lines: 604) grep (ASCII) 0.785 +/- 0.001 (lines: 583) rg (lines) 1.733 +/- 0.002 (lines: 604) ag (lines) (ASCII) 0.729 +/- 0.001 (lines: 0)*+ ucg (lines) (ASCII) 1.896 +/- 0.005 (lines: 583) * - Best mean time. + - Best sample time. There is no rg (ASCII) because rg can‚Äôt do ASCII-only case insensitive search. Analysis: This is a fun benchmark, because we start to see just how awesome rg‚Äôs support for Unicode is. Namely, that it not only gets it correct, but it‚Äôs also fast. It‚Äôs fast enough that it beats the competition even when the competition is using ASCII-only rules. Right off the bat, GNU grep pays dearly for doing a case insensitive search with Unicode support. The problem it faces is that it can no longer do a straight-forward Boyer-Moore search, so it either needs to fall back to some alternative literal search or its full regex engine. Even though GNU grep is much faster at ASCII-only case sensitive search than its Unicode aware variant, rg‚Äôs Unicode case insensitive search still handedly beats GNU grep‚Äôs ASCII-only case insensitive search. The reason why rg is so fast on this benchmark is the same reason why it‚Äôs fast in the linux_literal_casei benchmark: it turns the pattern Sherlock Holmes into an alternation of all possible literals according to Unicode‚Äôs simple case folding rules. It then takes a small prefix from each alternate so that our set of literals looks like this: SHER SHEr SHeR SHer ShER ShEr SheR Sher sHER sHEr sHeR sHer shER shEr sheR sher ≈øHER ≈øHEr ≈øHeR ≈øHer ≈øhER ≈øhEr ≈øheR ≈øher (Notice that we get Unicode right by including ≈ø as a case variant of S.) It then feeds these literals to the Teddy SIMD multiple pattern algorithm. The algorithm is unpublished, but was invented by Geoffrey Langdale as part of Intel‚Äôs Hyperscan regex library. The algorithm works roughly by using packed comparisons of 16 bytes at a time to find candidate locations where a literal might match. I adapted the algorithm from the Hyperscan project to Rust, and included an extensive write up in the comments if you‚Äôre interested. While essentially the same analysis applies to the Russian benchmark, there are a few interesting things to note. Namely, while the results show grep (ASCII) as being very fast, it seems clear that it‚Äôs completely ignoring the -i flag in this case since the pattern is not an ASCII string. Notably, its timing is essentially identical to its timing on the previous subtitles_literal benchmark. The other interesting thing to note is that ag reports 0 matches. This isn‚Äôt entirely unreasonable, if it somehow knows that it can‚Äôt satisfy the request (case insensitive search of a non-ASCII string when Unicode support isn‚Äôt enabled). If I had to guess, I‚Äôd say PCRE is returning an error (possibly from pcre_exec) and it isn‚Äôt being forwarded to the end user, but that‚Äôs just a shot in the dark. subtitles_alternate Description: This benchmarks an alternation of literals, where there are several distinct leading bytes from each literal. We control for line counting. English pattern: Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty rg 0.294 +/- 0.001 (lines: 848)*+ grep 2.955 +/- 0.003 (lines: 848) rg (lines) 0.619 +/- 0.001 (lines: 848) ag (lines) 3.757 +/- 0.001 (lines: 848) ucg (lines) 1.479 +/- 0.002 (lines: 848) grep (lines) 3.412 +/- 0.004 (lines: 848) Russian pattern: –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å|–î–∂–æ–Ω –£–æ—Ç—Å–æ–Ω|–ò—Ä–µ–Ω –ê–¥–ª–µ—Ä|–∏–Ω—Å–ø–µ–∫—Ç–æ—Ä –õ–µ—Å—Ç—Ä–µ–π–¥|–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä –ú–æ—Ä–∏–∞—Ä—Ç–∏ rg 1.300 +/- 0.002 (lines: 691)*+ grep 7.994 +/- 0.017 (lines: 691) rg (lines) 1.902 +/- 0.002 (lines: 691) ag (lines) 5.892 +/- 0.003 (lines: 691) ucg (lines) 2.864 +/- 0.006 (lines: 691) grep (lines) 8.511 +/- 0.005 (lines: 691) * - Best mean time. + - Best sample time. Analysis: rg does really well here, on both the English and Russian patterns, primarily thanks to Teddy as described in the analysis for subtitles_literal_casei. On the English pattern, rg is around an order of magnitude faster than GNU grep. The performance cost of counting lines is on full display here. For rg at least, it makes returning search results take twice as long. Note that the benchmark description mentions picking literals with distinct leading bytes. This is to avoid measuring an optimization where the regex engine detects the leading byte and runs memchr on it. Of course, this optimization is important (and rg will of course do it), but it‚Äôs far more interesting to benchmark what happens in a slightly trickier case. subtitles_alternate_casei Description: This benchmark is just like subtitles_alternate, except it searches case insensitively. In this benchmark, instead of controlling for line counting (all commands count lines), we control for Unicode support. English pattern: Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty (with the -i flag set) rg 2.724 +/- 0.002 (lines: 862)*+ grep 5.125 +/- 0.006 (lines: 862) ag (ASCII) 5.170 +/- 0.004 (lines: 862) ucg (ASCII) 3.453 +/- 0.005 (lines: 862) grep (ASCII) 4.537 +/- 0.025 (lines: 862) Russian pattern: –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å|–î–∂–æ–Ω –£–æ—Ç—Å–æ–Ω|–ò—Ä–µ–Ω –ê–¥–ª–µ—Ä|–∏–Ω—Å–ø–µ–∫—Ç–æ—Ä –õ–µ—Å—Ç—Ä–µ–π–¥|–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä –ú–æ—Ä–∏–∞—Ä—Ç–∏ rg 4.834 +/- 0.004 (lines: 735) grep 8.729 +/- 0.004 (lines: 735) ag (ASCII) 5.891 +/- 0.001 (lines: 691) ucg (ASCII) 2.868 +/- 0.005 (lines: 691)*+ grep (ASCII) 8.572 +/- 0.009 (lines: 691) * - Best mean time. + - Best sample time. Analysis: While rg gets an order of magnitude slower on this benchmark compared to subtitles_alternate, it still comfortably beats out the rest of the search tools, even when other tools don‚Äôt support Unicode. A key thing this benchmark demonstrates are the limits of the Teddy algorithm. In fact, rg opts to not use Teddy in this benchmark because it predicts it won‚Äôt perform well. Why doesn‚Äôt Teddy perform well here? Well, the answer is in how we generate literals for this pattern. Namely, rg will try to generate all possible literals that satisfy Unicode simple case folding rules, and then will take a short prefix of that set to cut the number of literals down to reasonable size. In this particular case, we wind up with 48 literals: INS INs IN≈ø IRE IRe InS Ins In≈ø IrE Ire JOH JOh JoH Joh PRO PRo PrO Pro SHE SHe ShE She iNS iNs iN≈ø iRE iRe inS ins in≈ø irE ire jOH jOh joH joh pRO pRo prO pro sHE sHe shE she ≈øHE ≈øHe ≈øhE ≈øhe If we passed all of those to Teddy, it would become overwhelmed. In particular, Teddy works by finding candidates for matches very quickly. When there are roughly the same number of candidates as there are matches, Teddy performs exceedingly well. But, if we give it more literals, then it‚Äôs more likely to find candidates that don‚Äôt match, and will therefore have to spend a lot more time verifying the match, which can be costly. (A more subtle aspect of the Teddy implementation is that a larger number of literals increases the cost of every verification, even if the number of candidates produced doesn‚Äôt increase. As I‚Äôve mentioned before, if you want the full scoop on Teddy, see its well commented implementation. Going into more detail on Teddy would require a whole blog post on its own!) When rg sees that there are a large number of literals, it could do one of two things: Try to cut down the set even more. For example, in this case, we could strip the last character from each prefix off and end up with a much smaller set. Unfortunately, even though we have fewer literals, we wind up with a still not-so-small set of two-character literals, which will also tend to produce a lot more false positive candidates just because of their length. Move to a different multiple pattern algorithm, such as Aho-Corasick. I have tried to implement (1) in the past, but I‚Äôve always wound up in a game of whack-a-mole. I might make one common case faster, but another common case a lot slower. In those types of cases, it‚Äôs usually better to try and achieve good average case performance. Luckily for us, Aho-Corasick does exactly that. We do still have a few tricks up our sleeve though. For example, many Aho-Corasick implementations are built as-if they were tries with back-pointers for their failure transitions. We can actually do better than that. We can compile all of its failure transitions into a DFA with a transition table contiguous in memory. This means that every byte of input corresponds to a single lookup in the transition table to find the next state. We never have to waste time chasing pointers or walking more than one failure transition for any byte in the search text. Of course, this transition table based approach is memory intensive, since you need space for number_of_literals * number_of_states, where number_of_states is roughly capped at the total number of bytes in all of the literals. While 48 literals of length 3 is too much for Teddy to handle, it‚Äôs barely a blip when it comes to Aho-Corasick, even with its memory expensive transition table based approach. (N.B. In the literature, this particular implementation of Aho-Corasick is often called ‚ÄúAdvanced‚Äù Aho-Corasick.) subtitles_surrounding_words Description: This benchmarks a pattern that searches for words surrounding the literal string Holmes. This pattern was specifically constructed to defeat both prefix and suffix literal optimizations. English pattern: \\w+\\s+Holmes\\s+\\w+ rg 0.605 +/- 0.000 (lines: 317) grep 1.286 +/- 0.002 (lines: 317) rg (ASCII) 0.602 +/- 0.000 (lines: 317)*+ ag (ASCII) 11.663 +/- 0.008 (lines: 323) ucg (ASCII) 4.690 +/- 0.002 (lines: 317) grep (ASCII) 1.276 +/- 0.002 (lines: 317) Russian pattern: \\w+\\s+–•–æ–ª–º—Å\\s+\\w+ rg 0.957 +/- 0.001 (lines: 278)*+ grep 1.660 +/- 0.002 (lines: 278) ag (ASCII) 2.411 +/- 0.001 (lines: 0) ucg (ASCII) 2.980 +/- 0.002 (lines: 0) grep (ASCII) 1.596 +/- 0.003 (lines: 0) * - Best mean time. + - Best sample time. Analysis: In order to compete on this benchmark, a search tool will need to implement a so-called ‚Äúinner literal‚Äù optimization. You can probably guess what that means: it is an optimization that looks for literal strings that appear anywhere in the pattern, and if a literal is found that must appear in every match, then a search tool can quickly scan for that literal instead of applying the full regex to the search text. The key thing that permits this optimization to work is the fact that most search tools report results per line. For example, in this case, if a line contains the literal Holmes, then the search tool can find the beginning and ending of that line and run the full pattern on just that line. If the literal is relatively rare, this keeps us out of the regex engine for most of the search. And of course, if the literal doesn‚Äôt appear at all in the corpus, then we will have never touched the regex engine at all. To achieve the full optimization, you probably need to parse your pattern into its abstract syntax (abbreviated ‚ÄúAST‚Äù for abstract syntax tree) to extract the literal. It is worth pointing out however that one can probably get a lot of mileage with simpler heuristics, but a real pattern parser is the only way to do this optimization robustly. The problem here is that for most regex engines, parsing the pattern is an unexposed implementation detail, so it can be hard for search tools to extract literals in a robust way without writing their own parser, and a modern regex parser is no easy task! Thankfully, Rust‚Äôs regex library exposes an additional library, regex-syntax, which provides a full parser. rg implements this optimization relatively easily with the help of regex-syntax, while GNU grep implements this optimization because the search tool and the underlying regex engine are coupled together. Why does the search tool need to perform this optimization? Why can‚Äôt the underlying regex engine do it? I personally have thought long and hard about this particular problem and haven‚Äôt been able to come up with an elegant solution. The core problem is that once you find an occurrence of the literal, you don‚Äôt know where to start searching the full regex. In a general purpose regex engine, a pattern could match an arbitrarily long string. For example, \\w+\\s+Holmes\\s+\\w+ mightly only match at the very end of a gigabyte sized document. There are ways to work around this. For example, you could split the regex into three pieces: \\w+\\s+, Holmes and \\s+\\w+. On every occurrence of the Holmes literal, you could search for the beginning of the match by executing \\w+\\s+ in reverse starting just before the literal, and executing \\s+\\w+ forwards starting just after the literal. The key problem with this approach is that it exposes you to quadratic behavior in the worst case (since \\w+\\s+ or \\s+\\w+ could cause you to re-scan text you‚Äôve already seen). While I believe there is a general purpose way to solve this and still guarantee linear time searching, a good solution hasn‚Äôt revealed itself yet. Based on the data in this benchmark, only rg and GNU grep perform this optimization. Neither ag nor ucg attempt to extract any inner literals from the pattern, and it looks like PCRE doesn‚Äôt try to do anything too clever. (Of course, Rust‚Äôs regex library doesn‚Äôt either, this optimization is done in rg proper.) As for the Russian pattern, we see that only tools with proper Unicode support can execute the query successfully. The reason is because \\w is ASCII only in ucg and ag, so it can‚Äôt match the vast majority of word characters (which are Cyrllic) in our sample. Otherwise, both rg and GNU grep remain fast, primarily because of the inner literal optimization. subtitles_no_literal Description: This benchmark purposefully has no literals in it, which makes it a bit idiosyncratic, since most searches done by end users probably have at least some literal in them. However, it is a useful benchmark to gauge the general performance of the underlying regex engine. English pattern: \\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5} rg 2.777 +/- 0.003 (lines: 13) rg (ASCII) 2.541 +/- 0.005 (lines: 13)*+ ag (ASCII) 10.076 +/- 0.005 (lines: 48) ucg (ASCII) 7.771 +/- 0.004 (lines: 13) grep (ASCII) 4.411 +/- 0.004 (lines: 13) Russian pattern: \\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5}\\s+\\w{5} rg 4.905 +/- 0.003 (lines: 41) rg (ASCII) 3.973 +/- 0.002 (lines: 0) ag (ASCII) 2.395 +/- 0.004 (lines: 0)*+ ucg (ASCII) 3.006 +/- 0.005 (lines: 0) grep (ASCII) 2.483 +/- 0.005 (lines: 0) * - Best mean time. + - Best sample time. ag gets more matches on the English pattern since it does multiline search. Namely, the \\s can match a . grep with Unicode support was dropped from this benchmark because it takes over 90 seconds on the English pattern and over 4 minutes on the Russian pattern. In both cases, GNU grep and rg report the same results. Analysis: Once again, no other search tool performs as well as rg. For the English pattern, both rg and rg (ASCII) have very similar performance, despite rg supporting Unicode. What specifically makes rg faster than GNU grep in this case? Both search tools ultimately use a DFA to execute this pattern, so their performance should be roughly the same. I don‚Äôt actually have a particularly good answer for this. Both GNU grep and Rust‚Äôs regex library unroll the DFA‚Äôs inner loop, and both implementations compute states on the fly. I can make a guess though. Rust‚Äôs regex library avoids a single pointer dereference when following a transition. How it achieves this is complicated, but it‚Äôs done by representing states as indices into the transition table rather than simple incremental ids. This permits the generated code to use simple addition to address the location of the next transition, which can be done with addressing modes in a s",
    "commentLink": "https://news.ycombinator.com/item?id=38471822",
    "commentBody": "Ripgrep is faster than grep, ag, Git grep, ucg, pt, sift (2016)Hacker NewspastloginRipgrep is faster than grep, ag, Git grep, ucg, pt, sift (2016) (burntsushi.net) 366 points by subset 23 hours ago| hidepastfavorite180 comments stinos 22 hours agoIt&#x27;s fast indeed. And I can&#x27;t help keeping promoting the combination with fzf :) For those who want to try it out, this is a Powershell function but the same principle applies in any shell. Does ripgrep then puts fuzzy searching in the resulting files+text on top while showing context in bat: function frg { $result = rg --ignore-case --color=always --line-number --no-heading @Argsfzf --ansi ` --color &#x27;hl:-1:underline,hl+:-1:underline:reverse&#x27; ` --delimiter &#x27;:&#x27; ` --preview \"bat --color=always {1} --theme=&#x27;Solarized (light)&#x27; --highlight-line {2}\" ` --preview-window &#x27;up,60%,border-bottom,+{2}+3&#x2F;3,~3&#x27; if ($result) { & ($env:EDITOR).Trim(\"`\"&#x27;\") $result.Split(&#x27;: &#x27;)[0] } }There are other ways to approach this, but for me this is a very fast way of nailing down &#x27;I now something exists in this multi-repo project but don&#x27;t know where exactly nor the exact name&#x27;edit this comes out of https:&#x2F;&#x2F;github.com&#x2F;junegunn&#x2F;fzf&#x2F;blob&#x2F;master&#x2F;ADVANCED.md and even though you might not want to use most of what is in there, it&#x27;s still worth glancing over it to get ideas of what you could do with it reply wanderingmind 22 hours agoparentInfact I would recommend a step further to integrate rip-grep-all (rga) with fzf that can do a fuzzy search not just on text files but on all types of files including pdfs, zip files. More details here [1][1] https:&#x2F;&#x2F;github.com&#x2F;phiresky&#x2F;ripgrep-all&#x2F;wiki&#x2F;fzf-Integration reply fsiefken 20 hours agorootparentThat&#x27;s really nice, thanks. Long ago there was the Google Desktop Search where you could &#x27;Google&#x27; your local documents. But the difference is that that worked with an index, so I imagine it&#x27;s faster if you have thousands of pdfs en epubs. reply cb321 18 hours agorootparentEven longer ago, there was `glimpse`: https:&#x2F;&#x2F;www.linuxjournal.com&#x2F;article&#x2F;1164 which is still available. [1] glimpse&#x27;s index-builds are like 10X slower than `qgrep` mentioned elsethread. `qgrep` also seems to have faster search (though I only tried a few patterns) and `qgrep` does not allow spelling errors like `glimpse`.Neither `glimpse` nor `qgrep`, to my knowledge, directly supports pre-processing &#x2F; document conversion (like `pdftotext`), though I imagine this would be easy to add to either replicating Desktop Search. (Indirectly, at some space cost, you could always dump conversions into a shadow file hierarchy, index that, and then translate path names.)[1] https:&#x2F;&#x2F;manpages.ubuntu.com&#x2F;manpages&#x2F;focal&#x2F;man1&#x2F;glimpse.1.ht... reply imakira 20 hours agoparentprevI wrote a bash version of this: function frg { result=`rg --ignore-case --color=always --line-number --no-heading \"$@\"fzf --ansi \\ --color &#x27;hl:-1:underline,hl+:-1:underline:reverse&#x27; \\ --delimiter &#x27;:&#x27; \\ --preview \"bat --color=always {1} --theme=&#x27;Solarized (light)&#x27; --highlight-line {2}\" \\ --preview-window &#x27;up,60%,border-bottom,+{2}+3&#x2F;3,~3&#x27;` file=\"${result%%:*}\" linenumber=`echo \"${result}\"cut -d: -f2` if [ ! -z \"$file\" ]; then $EDITOR +\"${linenumber}\" \"$file\" fi } reply benterix 19 hours agorootparentI wrote a zsh version of this: function frg { result=$(rg --ignore-case --color=always --line-number --no-heading \"$@\"fzf --ansi \\ --color &#x27;hl:-1:underline,hl+:-1:underline:reverse&#x27; \\ --delimiter &#x27;:&#x27; \\ --preview \"bat --color=always {1} --theme=&#x27;Solarized (light)&#x27; --highlight-line {2}\" \\ --preview-window &#x27;up,60%,border-bottom,+{2}+3&#x2F;3,~3&#x27;) file=${result%%:*} linenumber=$(echo \"${result}\"cut -d: -f2) if [[ -n \"$file\" ]]; then $EDITOR +\"${linenumber}\" \"$file\" fi } reply Aissen 17 hours agorootparentI wrote a fish version, and simplified it: function frg --description \"rg tui built with fzf and bat\" rg --ignore-case --color=always --line-number --no-heading \"$argv\"fzf --ansi \\ --color &#x27;hl:-1:underline,hl+:-1:underline:reverse&#x27; \\ --delimiter &#x27;:&#x27; \\ --preview \"bat --color=always {1} --theme=&#x27;Solarized (light)&#x27; --highlight-line {2}\" \\ --preview-window &#x27;up,60%,border-bottom,+{2}+3&#x2F;3,~3&#x27; \\ --bind \"enter:become($EDITOR +{2} {1})\" endStill not a fan of the string-based injections based on the colon and newline characters, but all versions suffer from it. (also: nice that fzf does the right thing and prevents space and quote injection by default). reply 1-more 15 hours agorootparentprevI love this, thank you! If anyone else wants to open the file in VScode, the command is code -g \"$file:$linenumber\" reply taude 18 hours agorootparentprevAwesome. Thanks. Saved me some time. I haven&#x27;t used the fzf integration like this. reply seanw444 19 hours agorootparentprevI&#x27;ve never really seen PowerShell beyond minimal commands, but after seeing the parent, I definitely think it has the superior syntax of the shells. Especially for scripts. reply tialaramex 17 hours agorootparentI expected to like Powershell when I began working somewhere with a lot of Windows (after decades of mostly Linux). I figured on paper this sounds like it has learned many important lessons that Unix shells could learn but (at least the popular ones) didn&#x27;t, it&#x27;s been given a blank canvas, the principles it&#x27;s working to make sense, it has good people behind it. So I even undertook to write a modest new piece of glue code in Power Shell, after all if it had been on Linux I&#x27;d definitely consider the Bourne shell as well as Python for the work...Then I tried it and I strongly dislike it. The syntax is clunky, it&#x27;s really no better than popular Unix shells at being a \"real\" programming language, and yet it&#x27;s not as good as they are at being just a shell either.It also just doesn&#x27;t feel like a quality product. On my work Windows laptop, Powershell will sometimes not quite bother flushing after it starts, so I get the banner text and then... I have to hit \"enter\" to get it to finish up and write a prompt. In JSON parsing the provided JSON parser has some arbitrary limits... which vary from one version to another. So code which worked fine on machine #1 just silently doesn&#x27;t work on machine #2 since the JSON parsers were changed and nobody apparently thought that was worth calling out. If you told me this was the beta of Microsoft&#x27;s new product I&#x27;d be excited but feel I needed to provide lots of feedback. Knowing this is the finished product I am underwhelmed. reply jrockway 15 hours agorootparentI find the built-in commands rough. \"curl https:&#x2F;&#x2F;jrock.us\" to see if my website is up used to involve opening Internet Explorer to accept some sort of agreement. Now it just flashes the terminal, moves the cursor to the far right hand side of the screen, and blinks for a while. I like the Linux version of curl better... reply ploxiln 12 hours agorootparentIronically, windows 10+ comes with real curl installed, to use it type curl.exe instead reply jrockway 9 hours agorootparentI had no idea!As it turns out, the reason that \"curl ...\" doesn&#x27;t work is because it pops up a window below all of my other windows saying that certificate revocation information is unavailable, and would I like to proceed. After that it does download my web page! reply kaba0 16 hours agorootparentprev> It also just doesn&#x27;t feel like a quality product. On my work Windows laptop, Powershell will sometimes not quite bother flushing after it starts, so I get the banner text and then..That‚Äôs independent of the shell, and is I believe a bug in the terminal emulator. There is an open source Windows Terminal you can separately install and that is so much better. reply 12_throw_away 14 hours agorootparentNope, windows terminal definitely does this too. Just last week I was trying to install WSL, and thought it had frozen at 20% and was trying to figure out what went wrong ... turns out it had already booted but powershell had stopped flushing output. reply bigstrat2003 12 hours agorootparentprev> The syntax is clunky, it&#x27;s really no better than popular Unix shells at being a \"real\" programming language...YMMV (and obviously does). I think that powershell is night and day better than bash (etc) as a programming language. reply bbkane 18 hours agorootparentprevMaybe if the \"popular\" shells, but http:&#x2F;&#x2F;www.nushell.sh&#x2F; is looking better and better reply evntdrvn 18 hours agorootparentnu acknowledged powershell as one of its inspirations, yeah!! reply steveklabnik 15 hours agorootparentprevI use it near exclusively. Love nu. reply Natfan 18 hours agorootparentprevA lot of people sleep on PowerShell, possibly because some of the syntax is a little clunky (and quite slow compared to some other shells, I will freely admit). That being said, I&#x27;d argue object oriented programming is a massive improvement over text oriented programming. I never want to touch awk again! reply gumby 12 hours agorootparentprevUnlike most Microsoft things it was not constrained by back compatibility.I generally don&#x27;t like MS software, but their commitment to back compatibility is worth calling out. reply JamesSwift 16 hours agorootparentprevThe only thing making it less than a total win is its handling of piped errors and `set -e`. The programming model itself is far superior to &#x27;stringly-typed&#x27; sh. reply buildartefact 15 hours agorootparentprevPowerShell is the worst of all worlds. Its a terrible shell compared to bash&#x2F;zsh&#x2F;whateversh, and for anything complex enough to need a long script you‚Äôre far better off in Python. reply ForkMeOnTinder 17 hours agorootparentprev1. Thanks for this, instantly added to my dotfiles2. You nerd-sniped me into getting rid of the unnecessary `cut` process :) file=${result%%:*} line=${result#*:} line=${line%%:*} reply imakira 17 hours agorootparentThanks! I have tried using bash substitution to solve it but failed (I just learned the difference between \"#\" and \"##\"). reply js2 17 hours agorootparentFYI, these substitutions are POSIX:https:&#x2F;&#x2F;pubs.opengroup.org&#x2F;onlinepubs&#x2F;9699919799&#x2F;utilities&#x2F;V...Also a couple mnemonic hints:% vs #. On US keyboards, # is shift-3 and is to the left of % which is shift-5. So # matches on the start (left) and % matches on the end (right).# vs ## (or % vs %%). Doubling the character makes the match greedy. It&#x27;s twice a wide so it needs to eat more.Bash also supports ${parameter&#x2F;pattern&#x2F;string} and ${parameter&#x2F;&#x2F;pattern&#x2F;string} (and a bunch others besides) which are not POSIX:https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;bash&#x2F;manual&#x2F;html_node&#x2F;Shell-Par... reply imakira 16 hours agorootparentThanks for the great information! reply kaba0 16 hours agorootparentprevJesus, bash really is an abomination reply zero_k 19 hours agorootparentprevWow, nice, thanks! :heart: reply vasergen 18 hours agorootparentprevgreat stuff reply tambourine_man 20 hours agoparentprevVim is almost broken for me without fzf+rg. Feels like I‚Äôm manually grinding coffee instead of using electricity. reply stirfish 20 hours agorootparentThis comment got me to get out my French press and manually grind some beans. It wasn&#x27;t a meditative and calming as I remember, and the coffee tastes a little...dusty. I guess it&#x27;s time for me to update my vimrc. reply orangepurple 20 hours agorootparentAeropress is a direct upgrade from french press and uses way less coffee reply stirfish 20 hours agorootparentI hadn&#x27;t heard of this before. Thanks! reply orangepurple 19 hours agorootparentAdd one or a few drops of water to your roasted coffee beans with your hand and shake well after weighing it out to stop the grinds from sticking to the walls of your grinder from static. reply bch 19 hours agorootparentI love that a ripgrep article has such a deeply nerdy coffee thread‚Ä¶> Add one or a few drops of water to your roasted coffee beansAh, RDT (Ross Droplet Technique)[0].A little atomizer (‚Äúspritz‚Äù bottle) of plain water serves well here. NB: this is for single-dose grinding - e.g. measuring a small amount of beans loaded into a grinder to grind immediately. If you have a grinder with a ‚Äúbig‚Äù hopper on top that has (e.g.) the weeks worth of coffee (even though you grind on-demand for ea. espresso&#x2F;french press&#x2F;aeropress&#x2F;pourover&#x2F;drip&#x2F;‚Ä¶) this isn‚Äôt for you.[0] https:&#x2F;&#x2F;thebasicbarista.com&#x2F;en-us&#x2F;blogs&#x2F;topics&#x2F;how-rdt-broke... reply iknowstuff 19 hours agorootparentprevThis thread took a turn reply globular-toast 14 hours agorootparentprevI use my Aeropress every day but I wouldn&#x27;t say it&#x27;s \"better\" than French press, it&#x27;s just different. Using less coffee but finer ground changes the characteristics of the brew quite a bit (probably some technical reason about extraction level or something). reply tambourine_man 16 hours agorootparentprevI‚Äôm glad my analogy sprouted another branch in this conversation :) reply Sander_Marechal 18 hours agorootparentprevWhich integration for ripgrep do you use with Vim? reply Icathian 18 hours agorootparentCan&#x27;t speak for OP, but I use telescope for neovim and I don&#x27;t think I could use (neo)vim without it. reply tambourine_man 17 hours agorootparentTelescope is cool, but last I checked it was neovim only or recommended and I‚Äôm a regular-Vim holdout. reply tambourine_man 17 hours agorootparentprevI modified some functions from: https:&#x2F;&#x2F;github.com&#x2F;junegunn&#x2F;fzf.vimAnd added my keyboard shortcuts. reply bloopernova 20 hours agoparentprevWith fzf you can add lots of files to git while skipping some if you want: fza = \"!git ls-files -m -o --exclude-standardfzf -m --print0xargs -0 git add\"With that in the [alias] section of a gitconfig file, running git fza brings up a list of modified and not yet added files, space toggles each entry and moves to the next entry.That alias as well as fzf+fd really speed up some parts of my workflow.Oh and shameless plug for my guide on what to include in your zsh setup on macOS: https:&#x2F;&#x2F;gist.github.com&#x2F;aclarknexient&#x2F;0ffcb98aa262c585c49d4b... reply stinos 18 hours agorootparentAdd the preview to see what you&#x27;re actually stashing: git ls-files -m -o --exclude-standardfzf -m --print0 --preview \"git diff {1}\"....And that&#x27;s just the start: it could even be that by binding a key to the fzf reload command to then display the diff in it&#x27;s finder, and in turn a key to stage the selected line, you could turn that into an interactive git staging tool. reply bloopernova 16 hours agorootparentNice. I&#x27;ll have to try that out! reply kstrauser 17 hours agorootparentprevThat blew my mind. I&#x27;ve used fzf a couple time here and there, but now I Get It. Thanks! reply taude 18 hours agoparentprevThis is pretty much my exact use of ripgrep, too. I use it as a starting point to zero in on files&#x2F;projects in a several-hundred repo codebase, and then go from there.... reply majkinetor 22 hours agoparentprevNot bad. For those who want to try it, install all prereqs with: choco install fzf bat ripgrepHow do you scroll the preview window with keyboard ? reply nickjj 22 hours agorootparent> How do you scroll the preview window with keyboard ? alias pf=\"fzf --preview=&#x27;less {}&#x27; --bind shift-up:preview-page-up,shift-down:preview-page-down\"That will let you run `pf` to preview files in less and lets you use shift + arrow keys to scroll the preview window. No dependencies are needed except for fzf. If you want to use ripgrep with fzf you can set FZF_DEFAULT_COMMAND to run rg such as `export FZF_DEFAULT_COMMAND=\"rg ...\"` where ... are your preferred rg flags. This full setup is in my dotfiles at https:&#x2F;&#x2F;github.com&#x2F;nickjj&#x2F;dotfiles.I&#x27;ve made a video and blog post about it here: https:&#x2F;&#x2F;nickjanetakis.com&#x2F;blog&#x2F;customize-fzf-ctrl-t-binding-...I also made https:&#x2F;&#x2F;nickjanetakis.com&#x2F;blog&#x2F;using-fzf-to-preview-text-fil... which covers how to modify fzf&#x27;s built in CTRL+t shortcut to allow for previews too. CTRL+t is a hotkey driven way to fuzzy match a list of files. reply stinos 22 hours agorootparentprevshift-up&#x2F;down reply shoover 20 hours agoparentprevOh. Thanks for the tip. This might make me finally embrace powershell. I‚Äôve been using WSL+zsh+fzf as a Windows CLI for continuity with day job Mac tools, but git CLI performance is only usable inside the WSL file system. reply fhlu 19 hours agorootparentYou can also add a small script to your WSL under `&#x2F;usr&#x2F;local&#x2F;bin&#x2F;git`: GIT_WINDOWS=\"&#x2F;mnt&#x2F;c&#x2F;Program Files&#x2F;Git&#x2F;bin&#x2F;git.exe\" GIT_LINUX=\"&#x2F;usr&#x2F;bin&#x2F;git\" case \"$(pwd -P)\" in &#x2F;mnt&#x2F;?&#x2F;*) case \"$@\" in # Needed to fix prompt, but it breaks things like paging, colours, etc rev-parse*) # running linux git for rev-parse seems faster, even without translating paths exec \"$GIT_LINUX\" \"$@\" ;; *) exec \"$GIT_WINDOWS\" -c color.ui=always \"$@\" ;; esac ;; *) exec \"$GIT_LINUX\" \"$@\" ;; esacThis allows you to use `git` in your WSL shell but it&#x27;ll pick whichever executable is suitable for the filesystem that the repo is in :) reply shoover 19 hours agorootparentThank you. I will use this. reply ciroduran 18 hours agorootparentThe code as written above only works if you haven&#x27;t changed the mountpoint for your windows partition (i.e. from &#x2F;mnt), consider that reply stinos 19 hours agorootparentprevThis might make me finally embrace powershellYeah, I have a bit of a love-hate relationship with it. But I actually have that with all shells out there. I don&#x27;t know if it&#x27;s just me or the shells, or (the most likely I think): a bit of both. But PS is available out of the box and using objects vs plain text is a major win in my book, and even though I still don&#x27;t know half of the syntax by heart it feels less of an endless fight than other shells. And since I use the shell itself for rather basic things and for the rest only for tools (like shown here), we get along just fine. reply pknopf 22 hours agoparentprevThanks for this! reply w0m 19 hours agoparentprevThis is a gem; thank you. reply shmerl 14 hours agoparentprevI use it neovim with fzf search. reply susam 21 hours agoprevI use ripgrep with the Emacs packages project.el (comes out of the box) and dumb-jump (needs to be installed). This may not be the most popular way of using rg but I have been very pleased with the overall experience. All it takes is running package-install to install the dumb-jump package and configuring the following hook: (add-hook &#x27;xref-backend-functions #&#x27;dumb-jump-xref-activate)The Xref key sequences and commands work fine with it. If I type M-. (or C-u M-.) to find definitions of an identifier in a Python project, dumb-jump runs a command like the following, processes the results, and displays the results in an Xref buffer. rg --color never --no-heading --line-number -U --pcre2 --type py &#x27;\\s*\\bfoo\\s*=[^=]+|def\\s*foo\\b\\s*\\(|class\\s*foo\\b\\s*\\(?&#x27; &#x2F;path&#x2F;to&#x2F;git&#x2F;project&#x2F;The above command shows how dumb-jump automatically restricts the search to the current file type within the current project directory. If no project directory is found, it defaults to the home directory.By the way, dumb-jump supports the silver searcher tool ag too which happens to be quite fast as well. If neither ag nor rg is found, it defaults to grep which as one would expect can be quite slow while searching the whole home directory. reply susam 20 hours agoparentAddendum to my comment above:Ripgrep can be used quite easily with the project.el package too that comes out of the box in Emacs. So it is not really necessary to install an external package to make use of ripgrep within Emacs. We first need to configure xref-search-program to ripgrep as shown below, otherwise it defaults to grep which can be quite slow on large directories: (setq xref-search-program &#x27;ripgrep)Then a project search with C-x p g foo RET ends up executing a command like the following on the current project directory: rg -i --null -nH --no-heading --no-messages -g &#x27;!*&#x2F;&#x27; -e fooThe results are displayed in an Xref buffer again which in my opinion is the best thing about using external search tools within Emacs. The Xref key sequences like n (next match), p (previous match), RET (jump to source of match), C-o (show the source of the match in a split window), etc. make navigating the results a breeze! reply burntsushi 20 hours agoparentprevAuthor of ripgrep here.Looking at your regex---just by inspection, I haven&#x27;t tried it, so I could be wrong---but I think you can drop the --pcre2 flag. I also think you can drop the second and third \\b assertion. You might need the first one though. reply susam 20 hours agorootparentThe example I have posted in my comment is not a command I am typing myself. The dumb-jump package generates this command for us automatically. It is possible to customize the command it generates though. Indeed while running ripgrep manually, I do not use the --pcre2 option. Thank you for developing and maintaining this excellent tool! reply burntsushi 20 hours agorootparentOooo gotya! That makes more sense. Thanks for the clarification. reply WhatIsDukkha 17 hours agoparentprevDeadgrep (uses ripgrep and evil-collection has a binding) takes me to my happy place -https:&#x2F;&#x2F;github.com&#x2F;Wilfred&#x2F;deadgrep reply BaculumMeumEst 11 hours agoparentprevthis is a good option but i still use rg.el for the occasion that i want to search several projects at once or a subfolder within a project, where i would otherwise use ‚Äòrgrep‚Äô reply MrHamdulay 21 hours agoprevWhat&#x27;s interesting is that ripgrep now also powers VS Code search with a Node.js wrapper.https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@vscode&#x2F;ripgrep reply M4v3R 19 hours agoparentI&#x27;ve always wondered how in world search in VS Code is so fast given it&#x27;s an Electron app - now I know. reply porwah 20 hours agoparentprev...which is awesome if you can request&#x2F;install VS Code but not ripgrep.You can find the rg binary in the VS installation (at least, I can on Windows at my place of employment). reply JaDogg 3 hours agorootparentHello thank you for pointing this out. I hate how slow grep is in Windows :( and I cannot install rg (I have no choice in the OS at work) reply Thaxll 19 hours agoparentprevIt&#x27;s not new it has been in vscode for 7 years. reply heap_perms 19 hours agoparentprevI did not know this that&#x27;s actually very interesting. reply xezian 19 hours agoprevI&#x27;ve been using ripgrep for about 2 years now and I find in indispensable. The main reason I switched from grep was ease of use. From the README: \"By default, ripgrep will respect gitignore rules and automatically skip hidden files&#x2F;directories and binary files.\" Typing `rg search_term directory` is much better than the corresponding grep command, but the speed improvement is also a nice bonus.Random other helpful flag I use often is -M if any of the matches are way too long to read through and cause a lot of terminal chaos. Just add `-M 1000` or adjust the number for your needs and the really long matches will omit the text context in the results. reply hhda 17 hours agoparentYeah, the -M command is wonderful (super handy for ignoring minified files that you don&#x27;t want to see results from, etc), and also great is the -g command (eg `-g *.cs` and you&#x27;ll just search in files that have the .cs extension).Also the fact that it is a standalone portable executable can be super handy. Often when working on a new machine, I&#x27;ll drop in the executable and an alias for grep that points to rg, so if muscle memory kicks in and I type grep it will still use rg. reply 3PS 17 hours agorootparentIf you&#x27;re a fan of the -g flag to ripgrep then I also recommend checking out the -t flag, short for --type, which lets you search specific file types. You can see the full list with `rg --type-list`. For example, you could just search .cs files with `rg -tcs`.This flag is especially convenient if you want to search e.g. .yml and .yaml in one go, or .c and .h in one go, etc. reply hhda 16 hours agorootparentThanks, I didn&#x27;t know about `-t`, I&#x27;ll read up on it. reply kzrdude 16 hours agorootparentprev-t is useful but -g doesn&#x27;t require lookup in help first. Maybe the worse is better principle? reply 3PS 16 hours agorootparentTbh I&#x27;ve always just typed -t followed by something that feels intuitive and it&#x27;s always worked. Never really bothered looking in help until I made the above comment. replyPaulDavisThe1st 19 hours agoprevAnd this may still be true in 2023, but the problem is that most of the parallelized grep replacements (e.g. ripgrep, ag, etc.) are SO much faster than grep that the much small speed differences between them doesn&#x27;t provide much of a basis for differentiating them.I use ag (typically from inside Emacs) on a 900k LOC codebase and it is effectively instantaneous (on a 16 core Ryzen Threadripper 2950X). I just don&#x27;t have a need to go from less than 1 second to \"a bit less than less than 1 second\".Speed is not the defining attribute of the \"new greps\" - they need to be assessed and compared in other ways. reply burntsushi 19 hours agoparentIn 2016, I&#x27;d say speed was definitely a defining attribute. ag has very significant performance cliffs. You can see them in the blog post.But as I mentioned in my comparison to qgrep elsewhere in the thread, everyone has different workloads. And for some workloads, perf differences might not matter. It really just depends. 900 KLOC isn&#x27;t that big, and indeed, for simple queries pretty much any non-naive grep is going to chew through it very very quickly.As for comparisons in other ways, at least for ag, it&#x27;s on life support. I thought it was going to get removed from Debian, but it looks like someone rescued it: https:&#x2F;&#x2F;bugs.debian.org&#x2F;cgi-bin&#x2F;bugreport.cgi?bug=999962The blog post also compares Unicode support, and contextualizes its performance. ag essentially has zero Unicode support. Unicode support isn&#x27;t universally applicable of course---you may not care about it---but it satisfies your non-perf comparison criteria. :-) reply latexr 23 hours agoprevThe title needs ‚Äú(2016)‚Äù. This is the original announcement, not new information. reply asicsp 22 hours agoparentDiscussions:\"Ripgrep ‚Äì A new command line search tool\" https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12564442 (740 pointsSept 23, 2016209 comments) - there are discussions related to speed too\"Ripgrep is faster (2016)\" https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17941319 (98 pointsSept 8, 201840 comments) reply larodi 21 hours agoprevwell... it is not faster than qgrep :) even though the way both work - differs greatly, and even though qgrep is based on re2 - the speed comes from the presence of index. but then I wonder why people forget the qgrep option, since with large file stores it makes much more sense to use qgrep AND indices, rather than always go through all the files.this above all true UNLESS you need multi-line matches with UTF8, where ripgrep is not so fast, because it needs to fall back to the other PCRE2 lib reply burntsushi 21 hours agoparentAuthor of ripgrep here.Yes, qgrep uses indexing, which will always give it a leg up over other tools that don&#x27;t use indexing. But of course, now you need to setup and maintain an index. The UX isn&#x27;t quite as simple as \"just run a search.\"But there isn&#x27;t much of a mystery here. Someone might neglect to use qgrep for exactly the same reason that \"grep is fast enough for me\" might prevent someone from using ripgrep. And indeed, \"grep is fast enough\" is very much true in some non-trivial fraction of cases. There are many many searches in which you won&#x27;t be able to perceive the speed difference between ripgrep and grep, if any exists. And, analogously, the difference between qgrep and ripgrep. The cases I&#x27;m thinking of tend to be small haystacks. If you have only a small thing to search, then perhaps even the speed of a \"naive\" grep is fast enough.So if ripgrep, say, completes a search of the Linux kernel in under 100ms, is that annoying enough to push you towards a different kind of tool that uses indexing? Maybe, depends on what you&#x27;re doing. But probably not for standard interactive usage.This is my interpretation anyway of your wonderment of (in your words) \"why people forget the qgrep option.\" YMMV.I have flirted with the idea of adding indexing to ripgrep: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;issues&#x2F;1497> this above all true UNLESS you need multi-line matches with UTF8, where ripgrep is not so fast, because it needs to fall back to the other PCRE2 libThat&#x27;s not true. Multiline searches certainly do not require PCRE2. I don&#x27;t know what you mean by \"with UTF8,\" but the default regex engine has Unicode support.PCRE2 is a fully optional dependency of ripgrep. You can build ripgrep without PCRE2 and it will still have multiline search support. reply thechao 19 hours agorootparentDoes `build.rs` build the project? One of my favorite (Big Corp) code-bases just had a single C file (build.c) that did all the dependency tracking, like, Make, but in some nicely written (easy to understand) C code. The C file started with a shebang: a self-building-and-executing line, so we&#x27;d do this:``` .&#x2F;build.c ```... and then magic happened. reply burntsushi 19 hours agorootparentNo. Cargo does. The `build.rs` is basically a Cargo hook that gets compiled as a Rust program and executed just before the ripgrep binary is compiled. It lets you do things like set linker flags[1] so that you can embed an XML manifest into the binary on Windows to enable \"long path support.\"ripgrep&#x27;s build.rs used to do more, like build shell completions and the man page. But that&#x27;s now part of ripgrep proper. e.g., `rg --generate man` writes roff to stdout.[1]: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;blob&#x2F;2a4dba3fbfef944c5...[2]: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;blob&#x2F;2a4dba3fbfef944c5... reply mariopt 21 hours agoprevWhat are the reasons for grep not being replaced&#x2F;improved? This topic seems a bit old by now. reply burntsushi 20 hours agoparentThere&#x27;s like a whole host of things you could use to explain it. Inertia. Compatibility. Resistance to change. Innovator&#x27;s dilemma. And so on. (I do not say any of these things pejoratively! All of those things apply to me too.)With respect to compatibility, see my FAQ on the topic: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;blob&#x2F;master&#x2F;FAQ.md#pos... reply wruza 20 hours agoparentprevFor the same reason the 40yo chair I currently sit in is not being replaced with Razer UltraSeat XR3000-A. It&#x27;s comfortable, fits the workplace around it, and there&#x27;s no reason for getting a replacement and rebuilding everything. (Partially because a Razer-like chair already stands nearby taking care of my clothes, but that&#x27;s where the analogy ends.) reply capableweb 20 hours agoparentprevThere are multiple alternatives you can already use as an alternative, like ripgrep. What are you proposing, switching out the command `grep` for another utility?Sounds like that could introduce a ton of breakage, for little value. People who want a faster grep will use a different thing, while people who use grep can continue to use it. Sounds like an ideal situation already. reply asicsp 20 hours agorootparentAnother reason would be if you want to follow POSIX standards. For example, `GNU grep` supports `POSIXLY_CORRECT` environment variable. reply eviks 17 hours agorootparentprevIt still wastes time of all the people on the road to that realization, including the time spent looking for a better alternative reply amiga386 19 hours agoparentprevgrep is a general purpose tool for searching for text in all types of files, baked into the standards for UNIX. Some programmers use it to search source code. Other people use it for other types of text searches that have nothing to do with source code, they rely on it in scripts, they don&#x27;t use it as part of a text-based programmer UI, they rely on it to never crash, etc.ripgrep is a specialist, opinionated tool, designed primarily to search through source code repositories.There&#x27;s not much you can add to general purpose text search to make it faster; you can make it use mmap() at the risk of it crashing on truncated files, you can reduce the expressiveness of regular expressions so they can be computed faster. You could throw out general support for all locales and charsets and hardcode support for only UTF-8 &#x2F; UTF-16, but you shouldn&#x27;t. reply burntsushi 18 hours agorootparent> There&#x27;s not much you can add to general purpose text search to make it fasterOh I beg to differ! The blog post goes into this. Here&#x27;s a simple demonstration using ripgrep 14: $ ls -l full.txt -rw-rw-r-- 1 andrew users 13113340782 Sep 29 12:30 full.txt $ time rg -c --no-mmap &#x27;Clipton&#x27; full.txt 294 real 1.419 user 0.539 sys 0.879 maxmem 15 MB faults 0 $ time LC_ALL=C grep -c &#x27;Clipton&#x27; full.txt 294 real 6.911 user 6.078 sys 0.829 maxmem 15 MB faults 0 $ time rg -c --no-mmap &#x27;DMZ|Clipton&#x27; full.txt 1070 real 1.643 user 0.747 sys 0.894 maxmem 15 MB faults 0 $ time LC_ALL=C grep -E -c &#x27;DMZ|Clipton&#x27; full.txt 1070 real 8.317 user 7.384 sys 0.930 maxmem 15 MB faults 0No memory maps. No multi-threading. No filtering. No fancy regex engine features or reducing expressiveness. No locales. No UTF-8. No UTF-16. Just a simple literal and a simple alternation of literals. It&#x27;s just better algorithms.Also, you can disable ripgrep&#x27;s opinions with `-uuu`. It&#x27;s not designed to just be for code searching. You can use it for normal grepping too. It will even automatically revert to the standard grep line format in shell pipelines. reply bawolff 18 hours agorootparentprev> you can make it use mmap() at the risk of it crashing on truncated filesI was under the impression that grep removed mmap() support because it was slower than normal file i&#x2F;o reply burntsushi 18 hours agorootparentI talked about this in the OP. Memory maps are sometimes a little faster. See: $ ls -l full.txt -rw-rw-r-- 1 andrew users 13113340782 Sep 29 12:30 full.txt $ time rg -c --no-mmap Clipton full.txt 294 real 1.337 user 0.470 sys 0.866 maxmem 15 MB faults 0 $ time rg -c --mmap Clipton full.txt 294 real 1.045 user 0.722 sys 0.323 maxmem 12511 MB faults 0But in recursive search, especially when used for lots of little files, they end up provoking substantial overhead that slows everything down.And this might change depending on the platform. reply alkonaut 20 hours agoparentprevSomeone designed unix based on the idea that some system functions are both core OS functions AND tools for human use. That leads to some bizarre outcomes decades later like \"there must be a program called xyz that accepts these arguments and works exactly like this\". reply anonymous_sorry 20 hours agoparentprevThese benchmarking results are seven years old, so perhaps it has been.My entirely anecdotal and unscientific impression is that rg and grep perform similarly on Linux (though rg has nicer defaults for searching through source code). The old version of grep that Apple preinstalls on the Mac was slower last time I checked though. reply p0nce 16 hours agoparentprevLack of interest maybe? I don&#x27;t use grep, I use an UI that lets me click and jump to a file. Or the builtin search in my IDE. reply ilc 14 hours agoparentprevHonestly: I rely on my shell scripts working and any \"grep replacement\" has to work with all the old crusty shell scripts out there, likely including ones that use odd \"quirks\" and GNU options.If you want to innovate in this space, why sign up for all that? Invent a better wheel, and if people like it, they&#x27;ll migrate over time.I remember using ag in the old days, and I use rg now. But there&#x27;s things rg does by default that I don&#x27;t like at times... so I go back to old fashioned grep.rg is at the point where many programmers use it. I think it is on its way to becoming one of those \"standard tools\". It needs... another 5 years?When POSIX has a rg standard... we&#x27;ll know ripgrep \"succeeded\" and teargrep will soon come into existence ;) reply burntsushi 13 hours agorootparentOh my heavens, I would never let ripgrep into POSIX. You can pry it out of my cold dead hands. :-)> so I go back to old fashioned grepIf you do `rg -uuu` then it should search the same stuff grep will. Not sure if that&#x27;s what you meant though. reply Tarucho 20 hours agoparentprevI guess it is because after decades of use, grep has probably been fixed to handle lots of user cases that the new tools don¬¥t handle because they haven¬¥t found them yet. reply burntsushi 19 hours agorootparentAuthor of ripgrep here.Like automatic encoding detection and transparently searching UTF-16?Or simple ways for composing character classes, e.g., `[\\pL&&\\p{Greek}]` for all codepoints in the Greek script that are letters. Another favorite of mine is `\\P{ascii}`, which will search for any codepoint that isn&#x27;t in the ASCII subset.Or more sophisticated filtering features that let you automatically respect things like gitignore rules.Those are all things that ripgrep does that grep does not. So I do not favor this explanation personally.ripgrep has just about all of the functionality that GNU grep does. I would say the two biggest missing pieces at this point are:* POSIX locale support. (But this might be a feature[1].)* Support for \"basic\" regexes or some equivalent that flips the escaping rules around. i.e., You need to write `\\+` to match 1 or more things, where as `+` will just match `+ literally.Otherwise, ripgrep has unfortunately grown just about as many flags as GNU grep.[1]: https:&#x2F;&#x2F;github.com&#x2F;mpv-player&#x2F;mpv&#x2F;commit&#x2F;1e70e82baa9193f6f02... reply gosub100 20 hours agoparentprevComplete guess: it works just fine for 99.9999% of users, but greater than (1-0.999999)% chance that it would break compatibility or have a bug. Anyone who would need the performance gain would know about specialized alternatives. reply entropie 19 hours agoprevI searched in portage, and it seems there is another version working also with other documents like PDFs and doc.https:&#x2F;&#x2F;github.com&#x2F;phiresky&#x2F;ripgrep-all reply ashton314 12 hours agoprevUsing Ripgrep via Consult [1] in Emacs is bliss. It&#x27;s like the rg+fzf thing that some have made, but all inside Emacs. I use the `consult-ripgrep` command all the time, and sometimes I use it to make project-wide edits too! Workflow is search with `consult-ripgrep` -> export results to buffer -> edit buffer -> commit edits back to files. Details at [2] (includes video of me working it)[1]: https:&#x2F;&#x2F;github.com&#x2F;minad&#x2F;consult#grep-and-find [2]: https:&#x2F;&#x2F;lambdaland.org&#x2F;posts&#x2F;2023-05-31_warp_factor_refactor... reply bawolff 18 hours agoprevGrep is already pretty much instant so what does it matter? reply rpigab 16 hours agoparentI love grep, but ripgrep really improves on it.Very often, I don&#x27;t want to look for files that aren&#x27;t tracked under Git VC, and I&#x27;m not looking for matches in binary files, so by default, ripgrep does that, which can cut time by 99%. I used to grep in small dirs, now I can I can ripgrep in my whole home, not that I do it, but I can. That + Sourcegraph on master branch, and it makes searching for any other thing than plain text feel sooo slow (Atlassian Confluence and Jira, Google docs, etc.).Thank you so much Burntsushi and contributors! reply vasergen 17 hours agoparentprevdid you try it on big dataset? there is a huge difference actually reply nikbackm 21 hours agoprevOne thing I wish ripgrep had is support for AND conditions. reply asicsp 20 hours agoparentIf you don&#x27;t mind using PCRE, you can do it. For example: rg -P &#x27;(?=.*pat1)(?=.*pat2)(?=.*pat3)&#x27;You could create a shell function shortcut if you need to use it often. But yeah, having it as a feature of the tool itself would be nice. reply kmarc 20 hours agoparentprevMaybe I&#x27;m missing something but I only use it with AND conditions (usually in the form of &#x27;foo &#x27;bar and it only matches lines with foo AND bar both present) reply burntsushi 20 hours agorootparentA search for `rg -e foo -e bar` will return lines that match either foo or bar. Some lines may have both, but it isn&#x27;t required.The standard way to run \"AND\" queries is through shell pipelines. That is, `rg foorg bar` will only print lines containing both. But composition usually comes with costs. The output reverts to the standard grep format and it doesn&#x27;t interact nicely with contextual options like -C&#x2F;--context.See: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;issues&#x2F;875 reply bhaak 18 hours agorootparentAs I have overloaded my rg with a customized rg alias, I can&#x27;t pipe multiple rg calls.Otherwise it would look like this: # rg nokogirirg linux :11:Gemfile.lock:647: nokogiri (1.15.5-x86_64-linux)But that is a me problem.The workaround is of course just to pipe into grep instead. reply burntsushi 18 hours agorootparentOr `\\rg`, which will use the command directly and skip your alias. reply bhaak 11 hours agorootparentOh, look at that. Nice.Still losing the coloring but you can&#x27;t have everything. replyboyter 12 hours agoparentprevThat‚Äôs one of the reasons I made this actually https:&#x2F;&#x2F;github.com&#x2F;boyter&#x2F;csI wanted and boolean syntax mixed with fzf instant search. It‚Äôs not as fast as ripgrep of course but it‚Äôs not solving the same problem. reply jmarchello 21 hours agoprevripgrep is easily one of my most used and most loved tools. I use it directly and also have it set as my grpprg in neovim. reply devnine 18 hours agoprevI&#x27;ve been using ripgrep for the last year to quickly search massive database dumps. I compared it with grep and it&#x27;s a game changer. reply abnry 19 hours agoprevI messed up my conda environments once when I changed my username. So many references to the old username in the conda folders. Ripgrep saved the day! reply mrweasel 19 hours agoparentWhy not just make new ones? Regardless of whether I use Conda, Pyenv or virtualenv, I always consider the environments to be disposable. reply abnry 19 hours agorootparentIt takes forever (like an hour) to install the data science stack plus pytorch and cuda libraries plus other hardware related libraries. reply eska 15 hours agoparentprevfastmod is another nice and user-friendly Rust tool for such cases. reply pie_flavor 11 hours agoprevI love ripgrep, it&#x27;s searched a directory in a half-second for a pattern that took GNU grep literally fifteen minutes. reply gquere 19 hours agoprevSemi off-topic, I&#x27;ve coded a ncurses-frontend to navigate and filter grep-like results which might be of interest to some of you: https:&#x2F;&#x2F;github.com&#x2F;gquere&#x2F;ngp2 reply reegnz 7 hours agoparentWhy not just use :grep in vim and navigate the vim quick fix list? reply zinodaur 14 hours agoprevWould be nice if ripgrep was drop in compatible with grep. I&#x27;d feel like a dick writing a shell script for other people to use and forcing them to install a new grep reply burntsushi 14 hours agoparentNever will be: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;ripgrep&#x2F;blob&#x2F;master&#x2F;FAQ.md#pos...If you need drop-in compatibility with grep, then use grep. :-) reply Night_Thastus 18 hours agoprevI love ripgrep for the speed and the more sane defaults. I use it nearly every day.For those just using it to search through a codebase, don&#x27;t forget -F for string literals. reply stabbles 22 hours agoprevBut is any of them using `_mm256_sad_epu8` for small, literal strings? reply lifthrasiir 22 hours agoparentNot exactly but yes, it ultimately uses the `memchr` crate [1] which provides SIMD-optimized character and string search routines. But it uses `_mm256_cmpeq_epi8` instead of `_mm256_sad_epu8`.[1] https:&#x2F;&#x2F;docs.rs&#x2F;memchr&#x2F;latest&#x2F;memchr&#x2F; reply burntsushi 21 hours agorootparentAnd also the SIMD in aho-corasick, which is used whenever a small number of literals are searched for. For example, `foo|bar` or `(?i)foo`.https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;aho-corasick&#x2F;blob&#x2F;f227162f7c56...But no `_mm256_sad_epu8`. What an oddly specific question..? reply stabbles 20 hours agorootparentWhat is really oddly specific is this instruction :p but I believe it is a common trick to quickly scan for short string matches.It computes `sum(|x[i] - y[i]|)` for consecutive `i` at different offsets, so it should be zero at substring matches.For context: https:&#x2F;&#x2F;epubs.siam.org&#x2F;doi&#x2F;pdf&#x2F;10.1137&#x2F;1.9781611972931.10I was slightly mistaken, the instruction of interest is _mm256_mpsadbw_epu8 reply burntsushi 20 hours agorootparentOh I see. Yes, that&#x27;s what is commonly used in academic publications. But I&#x27;ve yet to see it used in the wild.I mentioned exactly that paper (I believe) in my write-up on Teddy: https:&#x2F;&#x2F;github.com&#x2F;BurntSushi&#x2F;aho-corasick&#x2F;tree&#x2F;master&#x2F;src&#x2F;p... reply lifthrasiir 19 hours agorootparentprevI think Wojciech Mu≈Ça, who devised the original SIMD-oriented Rabin Karp algorithm, also did measure MPSADBW approaches and found that it is not a good fit for general string-in-string searches [1]. Maybe not today though.[1] http:&#x2F;&#x2F;0x80.pl&#x2F;articles&#x2F;simd-strfind.html#id7 reply saagarjha 22 hours agoparentprev_mm256_:(_epu8 reply bhasi 17 hours agoprevI&#x27;ve been using ag forever - will check out ripgrep. reply bluedays 21 hours agoprevI can&#x27;t think of a single time I&#x27;ve used grep where I thought \"I wish this was faster\". reply alkonaut 21 hours agoparentEven if the answer is instant, you have a 50% performance improvement in your search just from typing \"rg\" instead of \"grep\"!From my perspective it&#x27;s a no brainer. I don&#x27;t HAVE a grep (because I don&#x27;t have a Unix) so when I install a grep, any grep, reaching for rg is natural. It&#x27;s modern and maintained. I have no scripts anywhere that might expect grep to be called \"grep\".Of course if you already have a grep (e.g. you run Unix&#x2F;Linux) then the story is different. Your system probably already has a grep. Replacing it takes effort and that effort needs to have some return. reply Ferret7446 37 minutes agorootparentYou can alias `grep` to `gr` or even `rg`. Installing a whole entire different program just to type a shorter name is a crazy contrived justification.I imagine a lot of devs have grep preinstalled. In fact, where is grep not installed, now that WSL exists? reply wruza 19 hours agorootparentprevWell, a cmd script for msys64 grep in my \\CmdTools is named `gr`. It feels more natural, because index-then-middle finger also does. Thinking of it, I actually hate starting anything with a middle finger (no pun). Also learning new things that do the same thing as the old one. reply bhaak 19 hours agorootparentprevEven faster, I have an alias &#x27;ss&#x27; (mnemonic for &#x27;super search&#x27;) for rg. Fitts&#x27; Law to the max! reply eviks 17 hours agorootparentWhat do you use a single \"s\" for? reply bhaak 12 hours agorootparentgit status --untracked-files=allsn is &#x27;git status --untracked-files=no&#x27;. reply eminence32 20 hours agoparentprevI am amused by this comment, because it shows a dramatically different type of thinking. I have probably have thought \"I wish this was faster\" for nearly everything I do on a computer :) reply sgarland 21 hours agoparentprevMulti-GB log files. Even with LC_ALL=C, grep is painfully slow. reply bigstrat2003 9 hours agorootparentThat&#x27;s probably true - but good Lord, one should probably do something to reduce the size of log files that large. reply weavie 20 hours agoparentprevA few years ago I worked on a Solaris box that would lock the whole machine up whenever I grepped through the log files. Like it wouldn&#x27;t just be slow, the web server that was running on it would literally stop serving requests while it was grepping.I never worked out how that could be happening. reply burntsushi 20 hours agorootparentMy best guess is your grep search was saturating I&#x2F;O bandwidth, which slowed everything else to a crawl.Another possibility is that your grep search was hogging up your system&#x27;s memory. That might make it swap. On my systems which do not have swap enabled but do have overcommit enabled, I experience out-of-memory conditions as my system essentially freezing for some period of time until Linux&#x27;s OOM-killer kicks in and kills the offending process.I would say the first is more likely than the second. In order for grep to hog up memory, you need to be searching some pretty specific kinds of files. A simple log file probably won&#x27;t do it. But... a big binary file? Sure: grep -a burntsushi &#x2F;proc&#x2F;self&#x2F;pagemapDon&#x27;t try that one at home kids. You&#x27;ve been warned. (ripgrep should suffer the same fate.)(There are other reasons for a system to lock up, but the above two are the ones that are pretty common for me. Well, in the past anyway. Now my machines have oodles of RAM and lots of I&#x2F;O bandwidth.) reply rurban 21 hours agoparentprevOn extremely slow systems, such as Windows. There I can search in multiple repos only with rg. reply jedisct1 22 hours agoprevI switched from from ripgrep to ugrep and never looked back. It&#x27;s just as fast, but also comes with fuzzy matching (which is super useful), a TUI (useful for code reviews), and can also search in PDFs, archives, etc.The optional Google search syntax also very convenient.https:&#x2F;&#x2F;ugrep.com reply nathell 21 hours agoparentI‚Äôm a die-hard ripgrep fan, but just recently found ugrep looking for one feature that ripgrep lacks: searching in zip archives (without decompressing them to disk).Ugrep has that. In my case, I‚Äôm working with zipped corpora of millions of small text files, so I can skip unpacking the whole thing to the filesystem (certain filesystems have trouble at this scale).I‚Äôm grateful for both tools. Thanks to the respective authors! reply Eremotherium 19 hours agorootparentThat&#x27;s where ripgrep-all comes into play which will grep through archives, PDFs, ebooks, documents, etc. reply ranting-moth 21 hours agoparentprevI&#x27;m scared that if I start using Google search syntax in my grepping that I&#x27;ll mostly get results trying to sell me something :) reply hnfong 21 hours agoparentprevSo I was casually searching for \"ugrep vs ripgrep\" articles, when I stumbled upon a couple reddit posts where apparently the authors of ugrep and ripgrep seemed to have a multi-year feud on reddit, eg. https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;programming&#x2F;comments&#x2F;120wqvr&#x2F;ripgre...So weird. I mean, it&#x27;s just about some open source tool, right? :-&#x2F; reply halostatue 18 hours agorootparentI came across ugrep recently and I immediately recognized the organization as one that I had dealt with starting about 15 years go. The author is brilliant‚Ä°, but extremely prickly (sometimes even to paying customers). The author of ripgrep, on the other hand, has always seemed like someone who just wants to get on with the business of writing software that people use.‚Ä° The main commercial product of the ugrep author&#x27;s company at the time was the gSOAP code generator (it may still be), and that it not only works but makes a reasonably good C and C++ API from WSDL is proof that it is the product of a genius madman. It also allowed you to create both the API and WSDL from a C++-ish header, and both .NET and Java WSDL tools worked perfectly with it. We needed it to work and work it did.At the time, the generated API was just difficult enough to use that I generated another ~1k lines of code for that project. IIRC, the generated API is sort of handle-based, which requires a slightly different approach than the strict RAII approach we were using. Generating that code was a minor adventure (generating the gSOAP code from the header-ish file, generating doxygen XML from the generated gSOAP code, then generating the wrapper C++ from the doxygen XML). reply GuB-42 20 hours agorootparentprevPsst, don&#x27;t tell him about emacs and vi(m)...There are feuds about open source tools all the time. Text editors, Linux distros, shells, programming languages, desktop environments, etc... And ugrep vs ripgrep may be a poster child for C++ vs Rust.It is not all bad, it drives progress, and it usually stays at a technical level, I&#x27;ve yet to see people killing each others for their choice of command line search tool. reply fliife 20 hours agorootparentprevThis is so weird, even ripgrep&#x27;s author is actively seeking conflict in ugrep&#x27;s new release posts. Not a good colour on both of them. reply sundarurfriend 19 hours agorootparentAny examples? All I see in the recent release post is:> ugrep is easily one of the if not most featureful grep programs in existence. And it is also fast.which is burntsushi, ripgrep&#x27;s author, defending ugrep from someone saying they only focus on performance at the cost of features. reply orlp 19 hours agorootparentprevAt least how I read it the linked post was ugrep&#x27;s author seeking conflict, not the ripgrep&#x27;s. reply Ar-Curunir 19 hours agorootparentprevHow is it \"seeking conflict\" to correct factually wrong claims about your project? reply deltaburnt 20 hours agoparentprevIs the TUI better than just sending the results through fzf? For me the configurability and flexibility of fzf would be hard to compete with. reply thefilmore 11 hours agoparentprevIt&#x27;s also a drop-in replacement for grep as it supports the same flags and regular expression syntax. reply howeyc 18 hours agoparentprevThanks for mentioning this.I think the killer feature is compatibility with existing grep command line switches. Not needing to learn a whole new set of options is quite nice. reply zoobab 22 hours agoprevgit grep cannot even find a simple string in its repo. reply ctenb 22 hours agoparentWhat? Git grep is all you ever need in my experience, and it&#x27;s ~faster than~ (edit: as fast as) ripgrep when searching a git repo. reply dharmab 22 hours agorootparent> it&#x27;s faster than ripgrep when searching a git repo.Source? Ripgrep&#x27;s benchmarks show it significantly faster. reply ctenb 21 hours agorootparentSee sibling comment reply larodi 21 hours agorootparentprevI really doubt git grep can outperform ripgrep in any tests... please provide some proof. reply ctenb 21 hours agorootparentI tested this on a large repo in 2016 when I installed several tools (including rg and ag) to compare speed. I don&#x27;t have the metrics anymore, but the results were pretty clear then. According to the benchmarks from the OP, git grep is pretty comparable to rg in a large git repo. I guess different benchmarks give slightly different results, but the OP acknowledges that git grep is very fast. Bonus is that it comes preinstalled with git and can search through commit history. reply burntsushi 20 hours agorootparentAuthor of ripgrep here.It really just depends. The way I like to characterize `git grep` (at present) is that it has sharp performance cliffs. ripgrep has them too, to be sure, but I think it has fewer of them.If you&#x27;re just searching for a simple literal, `git grep` is decently fast: $ git remote -v origin git@github.com:torvalds&#x2F;linux (fetch) origin git@github.com:torvalds&#x2F;linux (push) $ git rev-parse HEAD f1fcbaa18b28dec10281551dfe6ed3a3ed80e3d6 $ time LC_ALL=en_US.UTF-8 git grep -c -E &#x27;PM_RESUME&#x27; Documentation&#x2F;dev-tools&#x2F;sparse.rst:3 Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:3 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:3 arch&#x2F;arm&#x2F;mach-omap2&#x2F;omap-secure.h:1 arch&#x2F;arm&#x2F;mach-omap2&#x2F;pm33xx-core.c:1 arch&#x2F;x86&#x2F;kernel&#x2F;apm_32.c:1 drivers&#x2F;input&#x2F;mouse&#x2F;cyapa.h:1 drivers&#x2F;mtd&#x2F;maps&#x2F;pcmciamtd.c:1 drivers&#x2F;net&#x2F;wireless&#x2F;intersil&#x2F;hostap&#x2F;hostap_cs.c:1 drivers&#x2F;net&#x2F;wwan&#x2F;t7xx&#x2F;t7xx_pci.c:15 drivers&#x2F;net&#x2F;wwan&#x2F;t7xx&#x2F;t7xx_reg.h:7 drivers&#x2F;usb&#x2F;mtu3&#x2F;mtu3_hw_regs.h:1 include&#x2F;uapi&#x2F;linux&#x2F;apm_bios.h:1 real 0.215 user 0.421 sys 1.226 maxmem 161 MB faults 0 $ time rg -c &#x27;PM_RESUME&#x27; drivers&#x2F;mtd&#x2F;maps&#x2F;pcmciamtd.c:1 drivers&#x2F;net&#x2F;wwan&#x2F;t7xx&#x2F;t7xx_reg.h:7 drivers&#x2F;net&#x2F;wwan&#x2F;t7xx&#x2F;t7xx_pci.c:15 drivers&#x2F;net&#x2F;wireless&#x2F;intersil&#x2F;hostap&#x2F;hostap_cs.c:1 drivers&#x2F;usb&#x2F;mtu3&#x2F;mtu3_hw_regs.h:1 drivers&#x2F;input&#x2F;mouse&#x2F;cyapa.h:1 arch&#x2F;x86&#x2F;kernel&#x2F;apm_32.c:1 Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:3 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:3 Documentation&#x2F;dev-tools&#x2F;sparse.rst:3 arch&#x2F;arm&#x2F;mach-omap2&#x2F;pm33xx-core.c:1 arch&#x2F;arm&#x2F;mach-omap2&#x2F;omap-secure.h:1 include&#x2F;uapi&#x2F;linux&#x2F;apm_bios.h:1 real 0.078 user 0.259 sys 0.577 maxmem 15 MB faults 0But if you switch it up and start adding regex things to your pattern, there can be substantial slowdowns: $ time LC_ALL=C git grep -c -E &#x27;\\w{5,}\\s+PM_RESUME&#x27; Documentation&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:1 real 5.704 user 55.671 sys 0.585 maxmem 207 MB faults 0 $ time LC_ALL=en_US.UTF-8 git grep -c -E &#x27;\\w{5,}\\s+PM_RESUME&#x27; Documentation&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:1 real 24.529 user 4:34.42 sys 0.753 maxmem 211 MB faults 0 $ time LC_ALL=en_US.UTF-8 git grep -c -P &#x27;\\w{5,}\\s+PM_RESUME&#x27; Documentation&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:1 real 1.372 user 16.980 sys 0.647 maxmem 211 MB faults 1 $ time rg -c &#x27;\\w{5,}\\s+PM_RESUME&#x27; Documentation&#x2F;translations&#x2F;zh_CN&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;dev-tools&#x2F;sparse.rst:1 Documentation&#x2F;translations&#x2F;zh_TW&#x2F;sparse.txt:1 real 0.082 user 0.226 sys 0.612 maxmem 18 MB faults 0In the above cases, ripgrep has Unicode enabled. (It&#x27;s enabled by default irrespective of locale settings. ripgrep doesn&#x27;t interact with POSIX locales at all.) reply ctenb 19 hours agorootparentThanks for clarifying! I use `git grep -IPn --color=always --recurse-submodules` many times a day, every day. I hasn&#x27;t yet let me down, but I don&#x27;t search for unicode when working on source code. I do use regex though, using the -P switch. reply kzrdude 16 hours agorootparentprevgit grep is shallow: only current git repo, rg is fully recursive, all submodules and also untracked (and not ignored) directories.In some trees, git grep will be a lot faster because it searches a smaller part of it. reply ctenb 16 hours agorootparentNo, git grep can recurse, if you pass the flag, just like all other git commands. --recurse-submodules reply masklinn 22 hours agoparentprevI don‚Äôt think there‚Äôs been a point to using `git grep` since ack started parsing gitignore. As far as I‚Äôm concerned the use case of `git grep` is to search into non-checked-out trees (by giving it a tree-ish). And it‚Äôs not super great at that, because it searches a static tree-ish, so pickaxe filters are generally more useful (though they‚Äôre slow).Once again mercurial has&#x2F;had more useful defaults, `hg grep` searches through the history by default, that‚Äôs it‚Äôs job. reply burntsushi 20 hours agorootparentSmall clarification: ack did not and does not respect your gitignore files. I just tried it myself, and indeed it doesn&#x27;t. And this is consistent with the feature chart maintained by the author of ack: https:&#x2F;&#x2F;beyondgrep.com&#x2F;feature-comparison&#x2F;One practical result of this is that it will mean `ack` will be quite slow when searching typical checkouts of Node.js or Rust projects, because it won&#x27;t automatically ignore the `node_modules` or `target` directories. In both cases, those directories can become enormous.`ack` will ignore things like `.git` by default though.I believe `ag` was the first widely used grep-like tool that attempted to respect your .gitignore files automatically. (Besides, of course, `git grep`. But `git grep` behaves a little differently. It only searches what is tracked in the repo, and that may or may not be in sync with the rules in your gitignores.) reply ctenb 2 hours agorootparentYou can also pass a flag to git grep to ignore the index (--no-index), which makes it search untracked files as well reply shmerl 13 hours agoprevFeels like it should always be included by default on grep level now. reply nvoeiah 22 hours agoprev [2 more] [flagged] WJW 22 hours agoparent [‚Äì] So? My distro doesn&#x27;t come with almost any of the tools I need for day to day work. It has never been a problem for me to install a new editor or compiler on a new machine, I don&#x27;t see why ripgrep would be any different. Especially since it&#x27;s usually a single command to install anyway. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article introduces ripgrep, a new command line search tool that outperforms other code search tools in terms of performance and usability.",
      "It provides installation and usage instructions for different platforms and explores features like support for .gitignore and custom matching rules.",
      "Benchmarks show that ripgrep consistently performs well in terms of speed, accuracy, and relevance of search results."
    ],
    "commentSummary": [
      "The discussion explores various command line search tools, highlighting ripgrep as a standout tool due to its speed, efficiency, and compatibility with other tools.",
      "Users compare different search tools like grep, ag, and ugrep, discussing their pros and cons.",
      "The feud between ugrep and ripgrep is briefly mentioned, along with the benefits of using ripgrep with specific text editors and programming languages."
    ],
    "points": 366,
    "commentCount": 180,
    "retryCount": 0,
    "time": 1701339693
  },
  {
    "id": 38477688,
    "title": "Turbo Pascal: Celebrating 40 Years of Groundbreaking Programming",
    "originLink": "https://blog.marcocantu.com/blog/2023-november-turbopascal40.html",
    "originBody": "Toggle navigation Home RSS Feed Comments Archive (1679) Author + Links Marco Tech Blog November 28, 2023 Turbo Pascal turns 40 Turbo Pascal was introduced by Borland in November 1983. It's officially turning 40 years old this month. Turbo Pascal was a milestone product for the industry, it started Borland as a company and it was the first popular Integrated Development Environment or IDE. It was a great product for the time, and its success was incredible. You can read more about Turbo Pascal it in this recent blog post from David I, but also on Wikipedia and many other sources including blog posts of mine, including the talk I did this summer in the first Pascal World Congress in Salamanca. At Embarcadero, the company continuing working on the successors of Turbo Pascal, we just shipped version 36 of that compiler. In fact when you read \"Embarcadero Delphi for Win32 compiler version 36.0\" (the version of the command line compiler in Delphi 12 Athens) the compiler version number, 36, dates back to the first Turbo Pascal. Not only that, we decided to dedicate the product Easter Egg to this great anniversary. Happy 40th birthday, Turbo Pascal! posted by marcocantu @ 4:17PM5 Comments [0 Pending] 5 Comments Turbo Pascal turns 40 In the early days, compilers were mostly a pain, interpreted basic was slow and awful. The editors were bad, the tools were limited to batch files. Turbo Pascal was so fast, compiling and runtime, and it was so easy to edit, compile and run. Later, forgot what version, the ability to run assembler code was a big upgrade. One of my favorite tools of all time! Comment by Randy Lea on November 28, 22:26 Turbo Pascal turns 40 Very cool but you need not that semicolon before ‚Äúend‚Äù statement Comment by Liviu on November 29, 00:38 Turbo Pascal turns 40 Happy 40! Comment by Paul Max [] on November 29, 01:05 Turbo Pascal turns 40 I remember my aunt being in university 25 years ago, and me trying to login on her computer and the first program that I found was Turbo Pascal. And I still believe that until now it is one of the main reasons I went into programming. Happy 40 Turbo Pascal! Comment by Elias Bourgess [https://ebourgess.dev] on November 29, 07:31 Turbo Pascal turns 40 You are right, we Pascal uses semicolons as statement separator, not as a statement terminator like curly brace languages. But it doesn't mind an extra semicolon... Comment by Marco Cantu [http://www.marcocantu.com] on November 29, 09:16 Post Your Comment Click here for posting your feedback to this blog. There are currently 0 pending (unapproved) messages. A blog about Delphi programming, Embarcadero, XML, Web 2.0, JavaScript, and anything else Marco is working on and has worked on in the past. Site content is copyright Marco Cantu' 2005-2020. Powered by Bootstrap and Delphi. Back to top",
    "commentLink": "https://news.ycombinator.com/item?id=38477688",
    "commentBody": "Turbo Pascal Turns 40Hacker NewspastloginTurbo Pascal Turns 40 (marcocantu.com) 298 points by andsoitis 14 hours ago| hidepastfavorite163 comments benhoyt 9 hours agoOne of my favourite computer games was written in Turbo Pascal: ZZT, written by Tim Sweeney of Epic Megagames. It was a quircky text-mode game with puzzles, shooting, and so on. It had a built-in game editor that came even with the free shareware version, and it even had a little programming language called ZZT-OOP.ZZT&#x27;s original source code was lost. Years later, Adrian Siekierka painstakingly reverse-engineered the original Pascal code till -- when compiled with the original version of Turbo Pascal -- produced a byte-for-byte identical executable. Amazing! Read more:- https:&#x2F;&#x2F;blog.asie.pl&#x2F;2020&#x2F;08&#x2F;reconstructing-zzt&#x2F;- https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22609474- https:&#x2F;&#x2F;benhoyt.com&#x2F;writings&#x2F;zzt-in-go&#x2F; reply zzo38computer 7 hours agoparentZZT is the reason I have Turbo Pascal on my computer. reply vanderZwan 6 hours agoparentprevZZT also inspired the overworld of Frog Fractions 2 reply hankman86 1 hour agoprevTurbo Pascal is what got me into programming. I remember spending hundreds of German marks on a license for Borland Pascal 7.0 and later Delphi 1.0 and 2.0. I ended up developing my first ‚Äúcommercial‚Äù software that I sold for money.In the DOS era, Turbo Pascal was probably the easiest way to get into programming, outside of Basic. And on Windows 3.1&#x2F;95, Delphi was eye-opening how easy GUI programming could be.In many ways, I feel like we have gone backwards from there. How was it possible for the Turbo Pascal &#x2F; Delphi compiler to produce a small binary for a fully-featured GUI program when today similarly powerful software is orders of magnitude larger in size? reply ReleaseCandidat 55 minutes agoparent> fully-featured GUI program when today similarly powerful software is orders of magnitude larger in size?That&#x27;s because the \"fully featured\" of the 90s would be barely usable today. Or to rephrase: the frameworks and programs of today are not \"similarly powerful\" to the ones from the mid 90s, even if you just recompiled the app from 25 years ago with the current version of your framework (theoretically ;), it would gain support for unicode (codepages where just...), internationalization, accessability, support for networks (who still knows Novell?),... . Except for SAP, they somehow succeeded at combining the user-hostileness of the 90&#x27;s UIs with the resource consumption of contemporary programs ;).Turbo Pascal had been the language I used after BASIC, with which I started. reply ivan_gammel 1 hour agoparentprevThere was at least one elephant missing in that executable - Unicode support. It would not fit into PC RAM back then probably. reply doubloon 7 hours agoprevThe main thing I recall about TP was that it came with simple, easy to understand code examples for every function that were organized logically and easy to find and use. So as a teenager who barely understood basic I could teach myself TP without the internet, just using the IDE.So many modern systems do not have anything even close to this. I wonder if its inherent in the nature of the simpler x86 DOS based systems of the time, where its simply impossible with modern pace of change, cross-platform needs, and complexity to have something like that. reply vanderZwan 6 hours agoparentProcessing (P5) had this: you can select any string of text in its IDE anl search for it in the docs, and if it&#x27;s one of the built-in functions or constants it will open the associated static html page that came installed with the software, so no internet nor server required. And despite being offline you can still navigate the docs too. This feels a lost basic skill in static site generation these days.It was the only creative coding framework that had complete, offline documentation like that at the time I might add. OpenFrameworks is still mostly autogenerated stubs for example.IMO it was one of the things that gave Processing an edge in educational contexts over all alternatives. I was pretty sad to see p5.js not fully continue that tradition and require that you go online to read the docs, and that it&#x27;s not a static website but that text is rendered with javascript when you open it (still complete and with examples though).https:&#x2F;&#x2F;processing.org&#x2F;https:&#x2F;&#x2F;p5js.org&#x2F; reply enraged_camel 3 hours agorootparentYou speak in past tense but isn‚Äôt Processing still in use? I distinctly remember using it to program my Arduino just five years ago. reply edu 1 hour agorootparentYes, it&#x27;s in use and well alive. The latest release, 4.3, is from July 26, 2023.Although I think that p5.js is growing faster, as it&#x27;s more accessible (browser IDE, not typed) and the artworks are easier to distribute online. reply vanderZwan 48 minutes agorootparentprevIt is, but I haven&#x27;t used it in a long time so didn&#x27;t want to imply anything about the current implementation, since I don&#x27;t know if anything changed in the last decade. I hope it&#x27;s still as good as I remember! reply ReleaseCandidat 43 minutes agoparentprev> I wonder if its inherent in the nature of the simpler x86 DOS based systems of the timeI also learned Turbo Pascal without any books (I had a Basic book, because that came with the computer). But it was way easier. Drawing pixels was as easy as just selecting your (double buffered!) display mode and, well drawing pixels. Same with the reading of e.g. the joystick port or the mouse, you&#x27;d just read it. reply hulitu 2 hours agoparentprevThis was true also for TurboC.Today, good luck looking for examples. reply geoelectric 10 hours agoprevTurbo Pascal was the first \"real\" programming language I learned past 6809E ASM, structured basic flavors and batch&#x2F;shell back in the very late 80s. When I went to college for CS in the early 90s, they were still teaching the first year of classes in Borland Pascal so that turned out to be very useful experience.A few years later, my Pascal background sailed me into a half-decade stint as a custom app dev working with Delphi. That, in turn, led to a job at Borland testing the Kylix and C++Builder IDEs. Good times.I still miss Delphi. Nowadays it&#x27;s normal to have IDE plugins to the point it&#x27;s a dealbreaker if they don&#x27;t exist. But at the time the idea of a component library and coding environment that was easily extensible in its own language was pure magic. I had so much fun playing with both VCL and the IDE itself.It&#x27;s a shame Borland left the grassroots devs behind in their quest for the enterprise market, and more or less killed any growth in community adoption. It was a really cool setup, at least through Delphi 7 or so. reply upupupandaway 3 hours agoparentIn my country Delphi was the most popular dev environment by a mile. It was really bizarre, with most of the world going for VB 5 and we in Delphi. It always seemed to me like a fantastic environment but for some reason it only took off in a couple of markets (my perception as an outsider). reply mmgutz 10 hours agoparentprevThat&#x27;s close to the route I took, although I switched to Visual Basic 5 later on no thanks to job opportunities. Still don&#x27;t feel anything rivals Delphi for GUI development. VCL was genius. reply blorenz 11 hours agoprevMy first exposure to TP was when I pirated it off a warez BBS in the winter of 93 at the age of 14. The raw power I felt when I compiled my own EXE in contrast to running just a .BAS file was enthralling! I started modding Renegade BBS and writing door programs. I tried to, unsuccessfully, create worms, Trojans and viruses. It changed my life and set me on a course for where I am in tech today. Moreover I‚Äôm reformed my deviant teenage tendencies. I owe Borland a lot. reply technothrasher 11 hours agoparentAh, the good old BBS days. I wrote a door library for Turbo Pascal back in &#x27;89 when I was 17 after I learned how to write an interrupt based serial driver, and then released it as shareware. It saw quite a bit of use until the mid-90&#x27;s when the BBS scene fell off a cliff. reply dual_dingo 3 hours agoparentprevThat throws me back. Being a teenager without any real understanding of compilers, interpreters etc., being able to create my own EXE file in TP4 felt like having superpowers - like being a real(TM) programmer :)A few years later at 16, I actually got paid for developing a small app for managing my dad&#x27;s customers, paid by the company he worked for. Part of that money went into getting a legal version of TP6.Good times :) reply thsksbd 5 hours agoparentprev\"I owe Borland a lot.\"A belated license to be exact :P reply deepspace 5 hours agorootparentI pirated Turbo Pascal back in the day because I was a poor student in a Third World country and there was no way I could afford it. Ditto Turbo C.After I started working, I had a hand in selecting Borland C++ as our in-house development platform, and we paid Borland a fortune in licensing fees. Which would have gone to Intel or another company if it were not for my, and other folks in my cohort&#x27;s experience with pirated Borland products as students.There is a lesson in there, somewhere. reply usrusr 1 hour agorootparentThe lesson is that piracy (and to an only slightly lesser extent, free student licences) was an extremely powerful tool to keep a tool market cornered: impossible to compete on price when piracy is so ubiquitous that payment is essentially opt-in.(I had a mouse driver disc, 5.25, that wasn&#x27;t pirated, it felt like something from a parallel universe with its machine-printed label) reply actionfromafar 1 hour agorootparentA deliberate strategy by some companies. (Microsoft comes to mind.( reply omgmajk 1 hour agorootparentprevI really wonder when all this changed. I seem to have this idea, that I share with many others that software houses back in the day explicitly didn&#x27;t care much about piracy because it got people hooked on their software and in the end enterprise would foot the bill.I&#x27;m not sure how true this was, as I&#x27;ve yet to see any real official sources speaking about this myself but it seems like it was like that. But most of my thoughts about this comes from being in the piracy space back in the days so I really have no idea if this was ever a real thing or not. reply msmith 10 hours agoparentprevI had a similar experience in the same era. In addition to the deviant stuff I also used it to tinker with graphics programming using inline assembly for the ‚Äúperformance critical‚Äù (for a 386SX) parts. That experience definitely laid the foundation for what I do today. reply livrem 2 hours agoprevI almost coded some Turbo Pascal earlier this year. I found the source code for one of the (not so great) games I (tried to) write in Turbo Pascal 2.0 as a kid. Had fun porting it to the latest stable Free Pascal. Never used Free Pascal before and was quite impressed. Well documented, stable, amazing cross-platform. Supports everything from creating 16-bit DOS COM-files to various modern 64-bit platforms and JS. reply JKCalhoun 7 hours agoprevWeirdly, Turbo Pascal was my first high-level programming language and it was on my Macintosh Plus. (I have come to find Turbo Pascal was more of a PC thing.)In college I used a student loan to buy my first Apple computer, the Macintosh Plus. By chance there was a classified ad in the college newspaper for a copy of Turbo Pascal for the Mac ‚Äî some professor was selling it for like $40 or something. I snagged it.It had the manual, thankfully, but very simply tools for Macintosh development. No ResEdit, but the odd R-Maker app that expected you to create a Macintosh resource as a text file and then run it through the tool to create the resource fork.My first few apps were Turbo Pascal implementations of algorithms from the Computer Recreations column of Scientific American (thankfully requiring little UI ‚Äî a window and a few buttons typically).Later I would learn of and move on to THINK Pascal ‚Äî a much more Mac-centric IDE. Later still I would take the leap to THINK C....But I will always look back on Turbo Pascal and remember it with fondness. It was a time when I was entering a wild new world and Turbo Pascal was there to hold the door open for me. reply zozbot234 11 hours agoprevFree Pascal still has a look-a-like of the original TP IDE! But that code is bitrotting by their own admission (it still relies extensively on obsolete quirks of the original MS-DOS platform) and it&#x27;s sad that we don&#x27;t have a look-a-like version that can work as a general editor in the terminal (like neovim or emacs) and integrate with modern IDE-oriented facilities like the LSP, tree-sitter parsers or the debug adapter protocol. That could even be a game changer for editing code remotely from SSH&#x2F;terminal connections. reply 5- 10 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;magiblot&#x2F;tvision reply rezaprima 8 hours agorootparentSadly, iirc, turbo vision was pascal first then ported to c++. reply pjmlp 2 hours agorootparentIt was, firstly delivered alongside Turbo Pascal 6.0 for MS-DOS. reply kccqzy 8 hours agoparentprevA little over a decade ago some of my friends who were Turbo Pascal fans transitioned to Lazarus. Seemed like the logical successor to Turbo Pascal and Free Pascal. (Personally I transitioned to writing C++ in Emacs as I did not really appreciate the Pascal language.) reply chungy 8 hours agorootparent> Seemed like the logical successor to Turbo Pascal and Free Pascal.To be more accurate, Lazarus is a GUI IDE for Free Pascal, not a successor. The same way Emacs can be an IDE for C that you&#x27;ll compile with GCC.I know, Delphi effectively replaced Turbo Pascal from the Borland side, even though Delphi could have been \"just\" an add-on to Turbo. They just didn&#x27;t take that route. reply agys 19 minutes agoprevI learnt to program and to animate sprites with Turbo Pascal!Later on me and a bunch of friends programmed a Sokoban-clone called ‚ÄúProject S‚Äù for MS-DOS: it used the graphics mode ‚ÄúX‚Äù for smooth scrolling; we even bought a commercial .mod player library for the sound effects and background music. reply jbgreer 9 hours agoprevI transitioned from Apple Pascal (based on UCSC p-system Pascal) on the Apple ][+ amd &#x2F;&#x2F;e to Turbo Pascal on DOS in the early 80s. Turbo Pascal was a blast: the very quick compiler, good feedback, colorful editor, and use of the Wordstar keybindings, which I already knew, made for a great experience.I recall writing programs large enough to require use of the overlay facility, which essentially let you page in different parts of your program under DOS.I also recall meeting David Intersimone, a great ambassador of Borland, sometime in the 80s - might have been later 80s - when he visited my university as a guest of the local ACM chapter. reply neilv 12 hours agoprevWhen I was a kid, a kindly computer store owner (who also made me a great deal on an PC-semi-compatible running MS-DOS 1.25, for approx. a hundred lawns mowed and babies sat) sold me a copy of Turbo Pascal for generic MS-DOS (no PC BIOS assumed) on 8\" floppy. He transferred it to the 160KB 5.25\" format that my semi-compatible used.I hope I was appreciative enough at the time, as I am now. That helped bootstrap my career. reply CoastalCoder 11 hours agoparentOkay, I&#x27;ll bite :)What&#x27;s this \"PC-semi-compatible\" of which you speak? reply whartung 9 hours agorootparentBack in the early days, MS-DOS was very much like CP&#x2F;M in that there were several different 8088 machines. The ‚ÄúPC Compatible‚Äù hadn‚Äôt quite exploded yet, so MS-DOS (and CP&#x2F;M 86) compatibility was enough.But soon, Flight Simulator became the benchmark for compatibility, and I think the final nail was that Lotus 1-2-3 required an actual PC compatible machine. That plus the rise of clone BIOSs pretty much ended the brief era of generic, 8088, non-PC compatible machines, save for niche domains.An interesting artifact of this era was when Steve Ciarcia of BYTE magazine released his 8088 board, and it was not PC compatible. He made some different design decisions. reply ngcc_hk 2 hours agorootparentFlight simulator as a test ‚Ä¶ still remember my boss tick that out as said you really cannot put that as requirement. We are in business not playing game ‚Ä¶ reply neilv 9 hours agorootparentprevSanyo MBC-550 series. It later got MS-DOS 2.11.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MBC-550 reply analog31 8 hours agorootparentThat was my first computer too. reply cardiffspaceman 10 hours agorootparentprevThat was the version of DOS that the Columbia MPC ran, and it was almost 100% compatible. I ran some code based on articles about the video card, and it couldn‚Äôt do the 160x100 mode that people wrote about.So a lot of ‚Äúclones‚Äù may have been this close as the Columbia was. reply mschaef 10 hours agorootparentprevInteresting question... based on DOS 1.25, I can&#x27;t imagine there are many choices. (Maybe the earliest Compaq machines among them). reply rf15 1 hour agoprevTurbo Pacal! It&#x27;s been ages, but it&#x27;s the first language I learned. The Borland IDE was amazing for its time and allowed me to learn the language in no time. My dad learned it too, and even developed some small games with it. Even now he likes to use its successor, Delphi. reply compiler-guy 9 hours agoprevIt&#x27;s hard to overstate just how much faster Turbo Pascal was than its competitors. So fast that it was hard to believe it wasn&#x27;t cheating in some way. And then the resulting program was faster too.A tour-de-force of its day, and it deserved all the acclimation that it got. reply hankman86 1 hour agoparentExactly, on the slow processors of the day, the Turbo Pascal compiler absolutely sprinted through the code.A far cry from today‚Äôs complex build systems that take forever to compile a program which can do exactly the same thing. reply anta40 5 hours agoparentprevEven until today, Pascal is still mentioned as language than can be compiled fast (including Go vs Rust debate).Many years ago, I remember building FPC 2.0 something on a Windows PC (512 MB RAM) and it took no more than 5 minutes. reply Narishma 8 hours agoparentprevIt&#x27;s biggest downside was that it was limited to producing 64KB .COM files until version 4 or 5. reply acqq 1 minute agorootparentAFAIK it was initially made to run on CP&#x2F;M 8-bit systems, that 64 KB limit was a consequence.I know I&#x27;ve used it on CP&#x2F;M first.It was really, really fast. reply corysama 7 hours agoparentprevhttps:&#x2F;&#x2F;prog21.dadgum.com&#x2F;47.html reply unnouinceput 3 hours agoparentprevDelphi, the successor of it still does that. Here is a link to an article about having a real project that contains a bit over a million lines of code. Has a video in it as well, and the comments there also discuss the times regarding Win64 compilation times:https:&#x2F;&#x2F;www.tmssoftware.com&#x2F;site&#x2F;blogold.asp?post=759 reply allanrbo 9 hours agoprevFun fact: the original author of Turbo Pascal, Anders Hejlsberg, went on to lead development of C# and then later TypeScript. reply sneed_chucker 9 hours agoparentWoah. Had no idea he was responsible for Turbo Pascal reply Keyframe 8 hours agorootparentAnd Delphi, where at least he tried to do something with strings hah. Never got used to them, and never really liked Wirth languages, but respect is due of course. reply robocat 3 hours agorootparentDelphi strings were really well implemented. Garbage collected (RC), multithread safe, length in first integer, fast, make them as big as your memory could handle. Reduced bad memory pointers a lot compared to competition at the time. Didn&#x27;t handle the Unicode transition well: but virtually no languages did. reply nextos 8 hours agorootparentprevYes, he wrote Turbo Pascal while he was studying at DTU in Copenhagen. reply thijson 39 minutes agoprevI used the Turbo Debugger to crack or develop cheats for games, that is until I discovered Softice. It always amazed me that when I set a breakpoint, it would halt the running program, switch to text mode, I could single step for a bit, modify an opcode, then continue running, and the screen would flicker back as if nothing had happened. Even the key combination, I think it was, would halt a lot of programs. This was in the DOS days when programs had a lot more control over the system.My experience with Pascal was my first experience with a procedural language. Before that I had been writing BASIC. I didn&#x27;t believe at first that I wouldn&#x27;t need a goto statement somewhere. I recall later thinking that compared to BASIC, the resulting code was more satisfying looking. In highschool we used the Watcom compiler running on QNX on a 286. It was slower to compile I remember than my later experience with Turbo C. reply MichaelMoser123 7 hours agoprevlooking at the perfection of Turbo pascal always made me feel stupid. At least it&#x27;s not just me, wikipedia says:\"Scott MacGregor of Microsoft said that Bill Gates \"couldn&#x27;t understand why our stuff was so slow\" compared to Turbo Pascal. \"He would bring in poor Greg Whitten [programming director of Microsoft languages] and yell at him for half an hour\" because their company was unable to defeat Kahn&#x27;s small startup, MacGregor recalled.[21] \" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Turbo_Pascal reply hulitu 1 hour agoparent> \"Scott MacGregor of Microsoft said that Bill Gates \"couldn&#x27;t understand why our stuff was so slow\" compared to Turbo Pascal. \"He would bring in poor Greg Whitten [programming director of Microsoft languages] and yell at him for half an hour\"They tried this on a project at my workplace. After six months of money spent on \"yelling\", they finally started to fix the issues. reply DennisP 10 hours agoprevIn high school I thought \"structured programming\" would cramp my style. I&#x27;d spend hours with BASICA printouts spread out over the floor, tracing through chains of GOTOs to find my bugs.One day my dad brought home Turbo Pascal so I gave it a shot. I wrote Conway&#x27;s Game of Life, and was shocked when it ran correctly the first time. Never wrote another line of BASICA.Much later I turned pro and moved on to other languages, but my dad kept messing around with Pascal until he was in his 70s. reply zozbot234 10 hours agoparent> was shocked when it ran correctly the first timeThis overall \"if it compiles it works\" philosophy is quite common among Pascal-family languages, and derived languages like Ada. Though there is also PL&#x2F;I if you want a language with a loosely Pascalish syntax that relies on the complete opposite design style - the one that was carried over to C and its derivatives. reply trealira 5 hours agorootparentI doubt that by modern standards, Pascal seems to work if it compiles, compared to languages like Java or Go. They were comparing it to writing spaghetti code with GOTOs everywhere in BASICA, so no wonder their code seemed more reliable. Nowadays, the jump in reliability comes from strong and helpful type systems, not structured programming. Ada still seems good there, but Pascal is just an ordinary programming language. reply adamwiggins 1 hour agoprevI&#x27;ll join the chorus here and say that Turbo Pascal was my stepping stone from beginner tinkering (BASIC on Apple II and DOS) to professional development with C. The dev environment was the perfect mix of \"serious and you can make real stuff including compiled executables for distribution\" with approachability for a relative beginner. reply wallrat 1 hour agoprevStill have the Turbo Pascal 1.0 manual on my shelf. It was the first language I picked up after Basic and QuickBasic as a child.Wrote a couple of interesting things a few years later in TP 5.5, including a BBS&#x2F;Kom system and a MUD (with 4 dial-in lines).Found the source a few years back, was an interesting read a few decades later. reply kasajian 7 hours agoprevIs anyone going to mentioned that that&#x27;s not at all what Turbo Pascal looked like when it was first released? Not in 1983. The UI was significantly simpler.I remember it looking like this: https:&#x2F;&#x2F;imgur.com&#x2F;EuOVL8c reply stevekemp 5 hours agoparentThat&#x27;s what I remember too.I briefly documented how to run Turbo Pascal 3.00A on a CP&#x2F;M system a while back:https:&#x2F;&#x2F;github.com&#x2F;skx&#x2F;z80-playground-cpm-fat&#x2F;blob&#x2F;main&#x2F;TURB...I&#x27;m doing that on a single-board Z80-based system, and it has to be said that writing pascal is a pleasure on such a machine. 64k of memory, and yet code compiles to real executables \"instantly\". reply mtillman 11 hours agoprevThis brings back great memories. I wrote a text based adventure game in turbo pascal for a high school project (97&#x2F;98) and had a blast. It was a really easy language for my needs and skill level. Started a life long love of code. reply sbarre 11 hours agoprevOk with a bunch of old hats here, anyone else rememberTechnojock&#x27;s Turbo Toolkit?It was a UI and eventually object toolkit for Pascal apps and it was pretty damn cool for the time.There isn&#x27;t much online about it, but everyone I knew who was working in TP or BP swore by this in the early 90s..https:&#x2F;&#x2F;www.computerhistory.org&#x2F;collections&#x2F;catalog&#x2F;10277966... reply boffinAudio 43 minutes agoparentI remember it well, including the fonts it bundled .. burned into my retina even today, given the crazy code I wrote using that toolkit. Thanks for the nice memories, I always wonder what happen to TTK .. as well as PC-Write, for which I had the source code license, and always tried to pack along with the TTK in my backups for some reason (I was a weird kid) .. reply browningstreet 12 hours agoprevI started with Turbo Pascal, and as is often noted by others, I spent a lot of time with the book Borland shipped with the compiler. reply lioeters 6 hours agoparentTurbo Pascal was my first real programming language (after BASIC and a bit of 8086 assembly) in my pre-teen years, and I read the manual very thoroughly. It was fascinating to learn, even though much of it was over my head.I remember sitting at the living room table on many evenings, studying a stack of printed out source code. My task was to contribute to translating a bulletin board system called WWIV.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;WWIVI went on to study Turbo C, which came with a wonderfully informative manual also. That experience formed the foundation of my programming in the next decades (2~3). reply randcraw 11 hours agoparentprevAnd Jeff Duntemann&#x27;s outstanding \"Complete Turbo Pascal\". reply cylinder714 5 hours agorootparentBy the way, the x86-64&#x2F;AMD64 Linux edition of his assembly-language text just came out:https:&#x2F;&#x2F;www.contrapositivediary.com&#x2F;?page_id=5070 reply darkteflon 9 hours agoprevFond memories of watching my dad sat in front of a bright blue screen covered in inscrutable glyphs. It looked like pure wizardry to me then.While the original colours are too intense for my taste, I do sometimes switch to the Noctis Azureus theme in VS Code when I find myself thinking of him. reply ilaksh 9 hours agoprevI had something called Turbo Pascal DiskTutor which if I remember correctly really helped me understand object oriented programming.I also spent quite a lot of time working on a type of rough wireframe 3d CAD editor in regular Turbo Pascal when I was in the 8th grade. I used equations I got from a little reference book my dad had brought from some garage sale or whatever to figure out how to do rotations and perspective.We also used it in AP computer science class which was probably the highlight of high school for me. reply nritchie 9 hours agoprevTurbo Pascal was a game changer. Prior to this, I&#x27;d been learning to code with Computer Innovations C compiler. It required multiple compiler passes involving swapping floppy disks. What a pain. Turbo Pascal was small, compiled fast and produced tiny executables. It was a joy to use. Later, I used Delphi to develop the GUI for an electron microscope. It made GUI development so easy - even developing custom controls for knobs and 2-D sliders. reply 5- 10 hours agoprevsee also, \"things that turbo pascal is smaller than\" by james hague: https:&#x2F;&#x2F;prog21.dadgum.com&#x2F;116.html reply jodrellblank 9 hours agoparent> \"The entire Turbo Pascal 3.02 executable--the compiler and IDE--was 39,731 bytes\"On my computer now, it&#x27;s smaller than: - License.txt which comes with \"TurboPascal (with DOSBox)\" (42,044 bytes) - sqlanywhere.vim, syntax highlighting file for the SQL dialect of SQL Anywhere (41,929 bytes) - parse.py, the URL parser in Python&#x27;s urllib (42,057 bytes) - doc_testing.html the Rust By Example section on writing tests in comments (42,059 bytes) - pwd.exe (print working directory) in GitHub Desktop (42,296 bytes) - WindowsExplorer.admx the XML policy file describing the available settings in Windows Explorer (42,461 bytes) reply jasomill 7 hours agorootparentAlso smaller than half the binaries in &#x2F;usr&#x2F;bin on my x86_64 Fedora 39 system[1] and less than half the size of every binary executable in &#x2F;bin and &#x2F;usr&#x2F;bin on my x86_64 Mac[2] in spite of the fact that all the Linux and Mac binaries are dynamically linked to at least libc &#x2F; libSystem.It&#x27;s also less than half the size of Xcode&#x27;s PNG-compressed Retina application icon (83,056 bytes).[1] This includes such gems as cat (40,848 bytes), tac (40,808 bytes), and tee (40,968 bytes).[2] Where even &#x2F;usr&#x2F;bin&#x2F;true weighs in at an astonishing 100,512 bytes, despite its entire text section disassembling to pushq %rbp movq %rsp, %rbp xorl %eax, %eax popq %rbp retq reply JNRowe 8 hours agorootparentprevI was about to say there are a good few tools smaller than that on my system, but I was one hundred percent wrong. # zsh glob for regular files that are smaller print -l &#x2F;usr&#x2F;bin&#x2F;*(.L-39731) # Some ~500 results # Not interpreted or using system libraries file &#x2F;usr&#x2F;bin&#x2F;*(.L-39731)egrep -v &#x27;(script|dynamically)&#x27; # 0 resultsTurns out there are only smaller things if you accept big caveats. reply corysama 7 hours agoparentprevAlso an rant on how it was so fast https:&#x2F;&#x2F;prog21.dadgum.com&#x2F;47.html reply tquinn 11 hours agoprevI first used Turbo Pascal in a high school programming class. I remember our teacher saying: \"Make it work first, then you can make it look pretty.\" reply zozbot234 11 hours agoparentIt&#x27;s being used to this day (or until very recently, at least) in some countries, generally running under dosbox because DOS software can no longer run natively in modern PC OS&#x27;s. reply Sunspark 5 hours agorootparentFreeDOS exists.. https:&#x2F;&#x2F;freedos.org&#x2F; reply livrem 2 hours agorootparentThat can no longer run on many modern computers as the backwards-compatible BIOS is phased out, not to mention many computers switching to ARM. reply agumonkey 11 hours agoprevTurbo Pascal and a few others programs of this era would still be utterly relevant technically and pragmatically. Don&#x27;t forget the past. reply grrandalf 10 hours agoprevBrought back some memories. Turbo Pascal 5.5 is the greatest IDE ever made.Editor, compiler, debugger (separate full-screen output buffer alt-f5 iirc), graphics.And oh complete stdlib docs that worked offline and in-app so you didn‚Äôt get out of ‚Äúflow‚Äù. With copy-pastable examples in the help.In a few MB. reply begueradj 4 hours agoprevI started coding in C. Things were difficult to me. Luckily we had a good professor who switched our programming learning to Pascal. That&#x27;s when coding started to be clear and simple to me and for my classmates.Niklaus Wirth has done an immense work. From Euler to Oberon, passing by Modula, he is a great visionary and a decades determinant mind. reply tomcam 11 hours agoprevIt changed my life and made me want to write compilers for a living. I wrote the first one in Turbo Pascal. reply theboywho 1 hour agoprevFun fact: one of Niklaus Wirth (the author of Pascal) students was Martin Odersky, who went on to create Scala. reply pugworthy 4 hours agoprevMan - such memories. I used this for the first time in a commercial software product for Geophysical forward modelling of gravity and magnetics. It was such an amazingly fancy system at the time - the first \"real\" development system I&#x27;d ever used.Byte Magazine, Philippe Kahn, Chaos Manor.. reply bdcravens 12 hours agoprevNever wrote a line of code in Turbo Pascal afterward, but taking it in HS (somewhat on accident) in 1995 was what set the trajectory for the rest of my life. reply cvwright 12 hours agoparentSame here. Even now, I still sometimes set my vim colors to blue background and yellow&#x2F;white text. reply vram22 11 hours agorootparentCool. I should try that. I was a heavy TP user too.Man, this reminds me of amber screen monitors. I liked them better than the green screen ones, but somehow, the amber ones seemed to be much less popular, at least in areas where I was. reply zozbot234 11 hours agorootparentprevThat was a common choice for TUI software running in the 16-color PC textmode palette. I suppose the blue provided a \"dark\" background with less contrast than actual black. (TUI programs of the era generally had a pure black-and-white mode too, and the overall look there was not unlike that of *nix terminal-based software.) reply mschaef 10 hours agorootparentprevI used blue&#x2F;yellow in Emacs until I bought a laptop with a glossy screen in 2011. Now it&#x27;s grey&#x2F;yellow. Some habits die hard. reply pjmlp 11 hours agoprevThe language that taught me systems programming should be all about, with proper security, modularity and good high level code, no need for lack of security shortcuts.I was blessed to have learnt TP before C.Happy birthday Turbo Pascal! reply sriku 7 hours agoprevMy first \"big\" program was a school project submission done in TP. It had an editor for drawing pixels in a (text) grid .. one line at a time. You could draw polygons, name them and the next time you drew something similar to the earlier ones it would recognise it and suggest the same name. If it couldn&#x27;t find one, it would ask you for a name to remember.Was so much fun writing it. The interface was fun to write. You could use it to tell apart isosceles triangles from equilateral triangles, for example.The machine I used only had two floppy drives and still I recall TP was fast as a Ferrari. reply icedchai 11 hours agoprevThey taught Pascal in high school, for my AP Computer Science course. We used Turbo Pascal on PS&#x2F;2 model 30&#x27;s. There were no hard drives in the machines! The teacher handed out floppies at the start of the class. By that point, I already knew C but it was still fun learning a new language. reply nsxwolf 10 hours agoparentOne of my favorite vintage PCs is my PS&#x2F;2 Model 30, and I had no idea it was available in a dual floppy configuration! It seems way too late to have been sold that way, but yup, checks out, that was the bare-bones configuration. reply icedchai 10 hours agorootparentYes, it was pretty odd! This was around 1992 or 1993, so long after the PS&#x2F;2 was even released. reply GiorgioG 7 hours agoprevI still have my original Turbo Pascal 7.0 floppy install disks from high school in the early&#x2F;mid 90s. There was no Googling or ChatGPT&#x27;ing for help when you got stuck...I feel like it made me a better thinking programmer back then and that somehow I&#x27;ve lost something with all the answers only one query&#x2F;chat away. reply LispSporks22 11 hours agoprevI still open up TP7 in DOSBox-X to play with \"leet code\" puzzles. They&#x27;re mostly imperative and the super-fast compilation time and debugger are impressive even by today&#x27;s standards. reply binarycrusader 7 hours agoprevFor those looking for a retrocomputing Pascal; use Turbo Rascal Syntax Error: https:&#x2F;&#x2F;lemonspawn.com&#x2F;turbo-rascal-syntax-error-expected-bu... reply coliveira 11 hours agoprevMy first language in college was TP. It was such a nice environment to use that when I first encountered gcc and similar UNIX compilers I thought them to be very primitive. reply Lectroid 8 hours agoparentTP was also the first language that I learned in collage in 84. I remember the Prof always calling it the language of love. We made fun of him alot for that statement but I think he got the last laugh... reply fzeindl 4 hours agoprevOne of the best things about Turbo Pascal was having to declare all variables upfront at the beginning of functions.I really don&#x27;t know why languages like C, Java or JavaScript started to allow variable definitions everywhere. Needing to declare variables first leads to much cleaner functions. reply blt 1 hour agoparentStrongly disagree. I want to have variables that only exist within an `if` block. That is something C99 got right.But Python&#x27;s leaky scopes are even worse than up-front declarations. reply coldcode 10 hours agoprevI bought a copy of Turbo Pascal at the 10x10 booth they had at the West Coast Computer Faire in SF when it first released in 1983? or 1984. I used it to build a TSR app on DOS 2.2 in my first job. Still the fastest dev environment I have ever used relative to anything else at the time; it was amazing to use. reply TomaszZielinski 9 hours agoprevThis brings memories.. I remember running Turbo Pascal 4.0 under PC-Ditto (XT emulator) on Atari 520ST :-)). It all looked mysterious and raw comparing to GEM [1], but it was probably my first _serious_ programming setup.The Atari was from Germany, and I didn&#x27;t know German at all, but I knew a bit of English. And so probably that was an important factor in why it was more attractive than whatever Basic or Assembler I had on the host system.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;GEM_(desktop_environment) reply rootusrootus 9 hours agoprevThat&#x27;s a blast from the past. Turbo Pascal 3 was the first \"real\" language that I learned when I was young. And then there was 5.5, which had a much nicer IDE. Ah the good old days. reply serf 9 hours agoparentsame here!put together a sieve of eratosthenes from algorithm as a kid and thought I was hot shit. something about watching a computer slowly tick through primes was so satisfying.good times. reply bluedino 12 hours agoprevI never used the original, but I started with a 2.0 disk I got from a friends parent who was taking computer classes in college.https:&#x2F;&#x2F;www.abandonwaredos.com&#x2F;abandonware-screenshot.php?gi...It didn&#x27;t have the trademark Borland IDE yet, but along with this Pascal book from the library, I had hours of fun.Computer Programming in Pascal the Easy Way https:&#x2F;&#x2F;a.co&#x2F;d&#x2F;3uPpxAw reply stevage 11 hours agoprevWow I wrote a lot of Pascal when I was a kid. I then did a bit of C but didn&#x27;t like it much, and moved onto Delphi.It hadn&#x27;t occurred to me that Pascal is actually younger than me, though not by much. reply anta40 4 hours agoparentI also learnt Turbo Pascal before Turbo C (in high school days: 2001-2004). Eventually I knew C had a major selling point: Linux&#x2F;Unix.Pascal? Limited interest among system&#x2F;OS kernel programmer nowadays. Still a fine language for building applications, though. reply mschaef 10 hours agoparentprevThe interesting thing about Turbo Pascal is that by the time it was TP6 it was just about as expressive as C itself. (Evidenced by the fact Turbo Pascal for Windows could easily interoperate with the heavily C-based Win16 API). reply major505 8 hours agoprevI miss the turbo family of IDEs. I worked a lot in Turbo C, an did my graduation project in college on it.Teacher would make us use vi in the first year. I still could not believe that you could just put a break point and stop the program in the middle of exectution to inspect variables. It was like magic to me. reply zzo38computer 6 hours agoprevI tried to view the article. First, I got a certificate error. I tried again, and I got a proxy error. I tried again, and then I got a \"WebCommand Error\". So, then I tried again, and this time the connection timed out. reply sltkr 6 hours agoparentI can&#x27;t get the site to load either. It&#x27;s probably overloaded.Archive.org has a copy though: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231201001803&#x2F;https:&#x2F;&#x2F;blog.marc...Not that there is a lot of content on that page beyond the title. reply zwieback 11 hours agoprevRan it on my Apple ][. It was interesting because another great platform on the Apple was UCSD Pascal but Turbo was so much slicker and easier to use. reply linsomniac 11 hours agoparentMy High School had a computer lab of ~8 Apple ][s with dual floppy drives and the CPM board to run UCSD. It worked, but we were definitely limited in the number of computers we could use due to these limitations. I also had access to an HP 9835 also running USCD, so it was very familiar to me.But part way through my class we switched from UCSD to Turbo Pascal, which only needed one floppy and just absolutely blazed. It was like a space age rocket ship. reply fortran77 10 hours agoparentprevI don&#x27;t think there was a version of Turbo Pascal for the Apple ][https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Turbo_Pascal..though you may have been able to run it with the Z-80 Softcard reply zwieback 10 hours agorootparentYes, correct, it was only with the CP&#x2F;M card, not 6502. reply septynade 4 hours agoprevI swear I thought I read \"Pedro Pascal Turns 40\" and thought why would anyone here care about that lol reply j1mmie 2 hours agoprevPedro Pascal is 8 years older reply marcod 10 hours agoprevPerl at 35 (almost 36)> How do you do, fellow kids? reply krylon 8 hours agoparentThis made me giggle. :) reply TomK32 10 hours agoprevI did learn it in school when I was about 14, 15. My daughter is 8 and she&#x27;ll learn some programming this third year in school but I bet it won&#x27;t be TP. I didn&#x27;t teach her any programming on purpose, but if she likes it we&#x27;ll do a game in Godot. reply morkalork 10 hours agoprevI learned Pascal in highschool the last year they taught it before switching to Java! I remember the textbook we used had a Dune themed question&#x2F;problem to solve which was delightfully nerdy compared to the corporate OOP blandness of Java reply figital 6 hours agoprevStill have the manuals at home on my bookshelf. Great examples of technical writing and explaining how the world is changing. reply aunwick 7 hours agoprevNice that you had an ide. I got demoted for having 99.9 percentile SAT score in 7th grade and had only an Atari1040 ST to prove my mettle on. reply Scubabear68 10 hours agoprevI was incredibly lucky that my college way back in 1987 gave all engineering students a PC XT loaded with things like Turbo Pascal. So much better than being stuck on a time sharing system in a lab‚Ä¶ reply beej71 7 hours agoprevThis compiler was blazingly fast in the 80s. Running on a modern computer, builds should look instantaneous--has anyone tried it? reply jimmydoe 8 hours agoprevStill remember tinkering around the inline asm in TP6, taught me a lot about low level part of system. reply alfiedotwtf 8 hours agoprevThe whole Borland suite was epic for the time. I still feel like Turbo Debugger was the peak of debugging UI reply orionblastar 14 hours agoprevI started with TP 3.0 in high school. It compiled faster than any other Pascal. It uses the WordStar editor. My teacher called it the cat&#x27;s pajamas. reply NikkiA 11 hours agoparentTP 2.x and 3.x had integrated editors that were functionally similar(inspired by) to WordStar, but were not wordstar, and using an external editor was a pain in the ass. reply orionblastar 7 hours agorootparentI mean to say it used WordStar commands. It was an early IDE and worth the money to switch from MGA to CGA to see the color highlights. reply RagnarD 9 hours agoprevA fantastically productive system back in the day. It was a joy to use. reply FpUser 11 hours agoprevI mostly consider languages as a mere tools like a screwdrivers. But yet there is that warm fuzzy feeling when I remember lying my hands on Turbo Pascal. Comparatively to other \"high level\" tools of the time it was at the different level.Happy Birthday reply ngcc_hk 2 hours agoprevI still have it installed to play the bridge. reply mobilio 11 hours agoprevI started with TP 5.5... reply narag 9 hours agoparentMy uni had two micros halls, one with 5.0, the other with 5.5. What&#x27;s the difference? I asked. It turns out the 5.5 had some new shit called \"Object Oriented Programming\"... and a Breakout clone to illustrate the paradigm.At some moment Borland released, free as in beer, the 3 and 5 versions. Binaries should be out there somewhere for those curious.I&#x27;m sure young people would be surprised to see what 5.5 provided. The on-screen help was amazing: cut and paste useful examples from the help of many functions. And the instantaneous compiling... :) reply TomaszZielinski 9 hours agorootparentHa, thanks for refreshing my memory that it was 5.5 that added OOP!I started with 4.0, so that&#x27;s my baseline. Then 5.0 added overlays (IIRC) and 6.0 introduced Turbo Vision, which was a kind of magic to me :).It&#x27;s interesting how a _programming language&#x2F;IDE_ can bring such warm feelings. reply Eugr 9 hours agorootparentI remember I learned Turbo Vision and OOP in general by writing my own Turbo Vision equivalent for the graphics mode (vs. text-only). Good times! reply TomaszZielinski 8 hours agorootparentThat&#x27;s very cool! IIRC I tried to write a very simple Turbo Vision clone (or subset) but I couldn&#x27;t fully comprehend how the nested widget initialization worked. I mean, I knew the last parameter was a pointer to the next widget, and so on and on, but somehow I couldn&#x27;t \"integrate\" everything in my brain :). replyStanislavPetrov 4 hours agoprevMy high school started offering it&#x27;s first programming course in 1988. It was in Turbo Pascal, and it was being taught by a math teacher who knew absolutely nothing about programming and was learning out of the book (Borland Turbo Pascal 5.0 [still have it]) at the same rate we were. The classroom was basically a computer room lined with PCs that we all sat in front of. These PCs were equipped with 5 1&#x2F;4\" drives as well as 40 MB hard drives. Being the \"hackers\" that we were, we downloaded the original version of nethack (early rogue offshoot) off of a BBS with our 1200 baud modems, brought it to school on 5 1&#x2F;4\" disks, loaded it onto the hard drives, and started playing it while the teacher sat at the front of the room with her nose buried in the book, trying to figure out how to learn enough Pascal to offer instruction. Unfortunately, after a while, these mysterious black rectangles started randomly appearing on the screen, and the PCs started experiencing performance issues. Nobody could figure it out. After many months, it was determined that we had infected all of the computer with the Jerusalem virus (in 1988!). Soon after, a school-wide prohibition on installing your own software was implemented (though not followed). reply myth_drannon 8 hours agoprevI&#x27;m currently reading \"Assembly Language Step-By-Step\" by Jeff Duntemann from 1992. In it, he calls TP \"moron friendly\" (unlike Assembly). Also something like Sears Catalog fallacy, \"where you go hunting through veritable Sears catalog of toolbox products looking like: SearchDataFileForFirstRecordBeginningWithStringAndDisplayInRed Basically, this method glues other people&#x27;s canned procedures into programs...\"30 years ago they had the same complaints like we have today reply blame-troi 11 hours agoprevwe had Turbo, JRT, and UCSD back at my first job out of college. All quite good in their way. reply rezaprima 12 hours agoprev25 seconds to compile it? How come ? reply qznc 11 hours agoparentNiklaus Wirth (Pascal inventor) had the rule that compiler speed must never regress. So if you add an optimization (which means the compiler has to do more work), the optimization must \"pay for itself\" and make the compiler faster.That philosophy probably seeped into Turbo Pascal to some degree. reply kant2002 7 hours agorootparentThat‚Äôs very interesting. I see similar attitude in the C# JIT&#x2F;Roslyn developers where they take this very seriously. Interesting are this is influenced by Anders, or just whole thing tick for a lot of compiler developers? reply stg22 10 hours agoparentprevAlso, Pascal is a direct descendant of a very, very old language (ALGOL-58 from 1958). In the 1950s, HLL compilation was at the extreme edge of what computers were capable of doing and a key goal of language design was (or should have been - cough COBOL cough) to make it as efficient as possible.By the time Ken Thompson was designing languages, hardware had improved a lot and he could make compilation efficiency a lower priority. reply giancarlostoro 12 hours agoparentprevMy understanding is the compiler was insanely efficient and parts of it coded in assembly if I&#x27;m not mistaken. reply ale42 12 hours agorootparentI think that the question was rather why it took 25 seconds to the current compiler to compile an Hello world on a contemporary computer... Turbo Pascal would have definitely done that in a second or less on an 8086 CPU... reply tomcam 11 hours agorootparentprevIt was completely written in assembly reply agumonkey 11 hours agorootparenthand written and also optimized for throughput right ? maybe pascal syntax was also parsing friendly.. i don&#x27;t recallone thing for sure is that it felt near instant if not real time building small projects, to the point that 14yo me was completely unaware of meaning of Compile until years later. reply bear8642 11 hours agorootparent>maybe pascal syntax was also parsing friendly..Yep, designed to be single pass, recursive descent friendly reply vram22 2 hours agorootparentAnd despite being single pass, you could call functions and procedures before you defined them (later in the code), by using the forward declaration feature of Pascal.Because of this feature, the language could also support mutually recursive functions, which some other languages might not have been able to (not sure). reply tomcam 9 hours agorootparentprevWhat bear8642 says. I worked with Anders and interviewed the poor man about itÔºÅ replykeernan 10 hours agoprevStirs memories of Clipper reply einpoklum 11 hours agoprevOther than some Logo experience, I first learned \"real programming\" using Turbo Pascal. I was in sixth grade and went to a programming summer-camp-of-sorts, held on the grounds of the Techno-da science museum (today called the Mada-Tek). At break time, one of the other kids got a copy of \"Ironman Super Off Road\" [1], and we would play or watch others try to beat the computer. And at break time there were bread rolls with some filling I think, and every other day or so it was this chocolate-flavored spread.Man, that was so much fun!... I haven&#x27;t thought about those times in years; thanks for the trip down memory lane :-)[1] - https:&#x2F;&#x2F;www.mobygames.com&#x2F;game&#x2F;4444&#x2F;ivan-ironman-stewarts-su... reply jprd 12 hours agoprev [‚Äì] oof. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Turbo Pascal, a programming language and development environment, is commemorating its 40th anniversary, marking its significant impact on the tech industry and launching Borland as a company.",
      "Turbo Pascal was the first widely-used integrated development environment (IDE) and is attributed by numerous individuals as their gateway to programming.",
      "Embarcadero, the current developer of Turbo Pascal, recently released version 36 of the compiler, showcasing the enduring popularity of this pioneering language."
    ],
    "commentSummary": [
      "The discussion is centered around the nostalgia and fondness for Turbo Pascal, a programming language that was popular in the 1990s.",
      "Users share their experiences with Turbo Pascal, highlighting its ease of use, efficiency, and the impact it had on their programming careers.",
      "The discussion also covers various topics such as the loss and reverse-engineering of the source code for the game ZZT, the accessibility and educational value of programming languages, the limitations of Turbo Pascal, and its relation to other programming languages and tools."
    ],
    "points": 298,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1701371328
  },
  {
    "id": 38473942,
    "title": "Yabai: A Tiling Window Manager for macOS",
    "originLink": "https://github.com/koekeishiya/yabai",
    "originBody": "Tiling window management for the Mac. About yabai is a window management utility that is designed to work as an extension to the built-in window manager of macOS. yabai allows you to control your windows, spaces and displays freely using an intuitive command line interface and optionally set user-defined keyboard shortcuts using ‚Üó skhd and other third-party software. The primary function of yabai is tiling window management; automatically modifying your window layout using a binary space partitioning algorithm to allow you to focus on the content of your windows without distractions. Additional features of yabai include focus-follows-mouse, disabling animations for switching spaces, creating spaces past the limit of 16 spaces, and much more. Installation and Configuration The ‚Üó yabai wiki has both brief and detailed installation instructions for multiple installation methods, and also explains how to uninstall yabai completely. Sample configuration files can be found in the ‚Üó examples directory. Refer to the ‚Üó documentation or the wiki for further information. Keyboard shortcuts can be defined with ‚Üó skhd or any other suitable software you may prefer. Requirements and Caveats Please read the below requirements carefully. Make sure you fulfil all of them before filing an issue. Requirement Note Operating System Intel x86-64 Big Sur 11.0.0+, Monterey 12.0.0+, Ventura 13.0.0+, and Sonoma 14.0.0 is supported. Operating System Apple Silicon Monterey 12.0.0+, Ventura 13.0.0+, and Sonoma 14.0.0 is supported. Accessibility API yabai must be given permission to utilize the Accessibility API and will request access upon launch. The application must be restarted after access has been granted. Screen Recording yabai must be given Screen Recording permission if and only if you want to enable window animations, and will request access when necessary. The application must be restarted after access has been granted. System Preferences (macOS 11.x, 12.x) In the Mission Control pane, the setting \"Displays have separate Spaces\" must be enabled. System Settings (macOS 13.x, 14.x) In the Desktop & Dock tab, inside the Mission Control pane, the setting \"Displays have separate Spaces\" must be enabled. Please also take note of the following caveats. Caveat Note System Integrity Protection (Optional) System Integrity Protection can be (partially) disabled for yabai to inject a scripting addition into Dock.app for controlling windows with functions that require elevated privileges. This enables control of the window server, which is the sole owner of all window connections, and enables additional features of yabai. Code Signing When building from source (or installing from HEAD), it is necessary to codesign the binary so it retains its accessibility and automation privileges when updated or rebuilt. System Preferences (macOS 11.x, 12.x) In the Mission Control pane, the setting \"Automatically rearrange Spaces based on most recent use\" should be disabled for commands that rely on the ordering of spaces to work reliably. System Settings (macOS 13.x, 14.x) In the Desktop & Dock tab, inside the Mission Control pane, the setting \"Automatically rearrange Spaces based on most recent use\" should be disabled for commands that rely on the ordering of spaces to work reliably. System Settings (macOS 14.x) In the Desktop & Dock tab, inside the Desktop & Stage Manager pane, the setting \"Show Items On Desktop\" should be enabled for display and space focus commands to work reliably in multi-display configurations. System Settings (macOS 14.x) In the Desktop & Dock tab, inside the Desktop & Stage Manager pane, the setting \"Click wallpaper to reveal Desktop\" should be set to \"Only in Stage Manager\" for display and space focus commands to work reliably. License and Attribution yabai is licensed under the ‚Üó MIT License, a short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code. Thanks to @fools-mate for creating a logo and banner for this project and making them available for free. Thanks to @dominiklohmann for contributing great documentation, support, and more, for free. Disclaimer Use at your own discretion. I take no responsibility if anything should happen to your machine while trying to install, test or otherwise use this software in any form. You acknowledge that you understand the potential risk that may come from disabling ‚Üó System Integrity Protection on your system, and I make no recommendation as to whether you should or should not disable System Integrity Protection.",
    "commentLink": "https://news.ycombinator.com/item?id=38473942",
    "commentBody": "Yabai ‚Äì A tiling window manager for macOSHacker NewspastloginYabai ‚Äì A tiling window manager for macOS (github.com/koekeishiya) 276 points by CathalMullan 19 hours ago| hidepastfavorite195 comments xwowsersx 18 hours agoFunny to see this at the top of HN after just having uninstalled Yabai. I used Yabai for about 6 weeks. I was so thrilled about having a proper tiling window manager for Mac. I invested a lot of time setting up skhd and fine-tuning my configuration for the most intuitive hotkeys. The problem ultimately, however, is that Yabai just doesn&#x27;t work well enough. It&#x27;ll regularly just lose track of windows altogether, which requires restarting Yabai. This and other bugs happened frequently enough that it outweighed the benefits of Yabai. I&#x27;d be more than willing to try another tiling window manager on Mac if there&#x27;s one out there that truly works. Related, I started checking out Stage Manager and it&#x27;s pretty bad in my experience - obviously it&#x27;s the opposite of a keyboard driven TWM, but I&#x27;m willing to try anything that helps me maximize the limited screen space I have. I&#x27;m all ears if anyone has had better experiences with other tools on Mac. reply CSSer 18 hours agoparentYou might try Magnet. It&#x27;s $10 on the mac app store. I&#x27;ve had zero issues with it, and it has some problems sorted that other tools don&#x27;t or haven&#x27;t always, namely correctly tiling across monitors. I used to use Rectangle, which is open source and frankly pretty good but at the time I was using it had some warts. macOS&#x27; window management seems to not be entirely transparent. I don&#x27;t know all the details, but this is my workstation. I need(ed) this to work now, and if it&#x27;s a system that might change a bit over time or has some weird idiosyncrasies then I&#x27;m happily willing to pay a low one-time fee for a tool like this. reply rxhanson 17 hours agorootparentIf you don&#x27;t mind me asking, what are the warts? I&#x27;m always interested in improving the app. reply dishsoap 12 hours agorootparentOne difference I&#x27;ve noticed is that with Magnet, if you have a window &#x27;maximized&#x27; by dragging it into the top of the screen or pressing the shortcut for it, when you start dragging it down from the top of the screen to unmaximize it, it instantly returns to its original size, whereas with Rectangle it stays maximized for some time after you start dragging it which is pretty annoying.Another thing is the way Magnet moves and resizes windows seems to create less visual &#x27;flashing&#x27; than Rectangle&#x27;s method, for instance Rectangle will resize, then move, then resize again when maximizing a window, which can create a really pronounced &#x27;flashing&#x27; effect. That&#x27;s not to say Magnet has no &#x27;flashing&#x27; ever, there&#x27;s certain situations where it does and to my understanding it seems necessary due to the limited nature of moving and resizing windows on macOS (I guess you have to do one then the other and can&#x27;t both move and resize simultaneously?), but to me Magnet&#x27;s methods often look visually better.Rectangle is great though, I&#x27;ve switched to it from Magnet and these are just some minor gripes. reply rxhanson 8 hours agorootparentGood to know! I&#x27;m surprised that Magnet has any improvement over Rectangle in these areas, since Rectangle isn&#x27;t really adding any overhead to the Accessibility API commands, but I&#x27;ll have to investigate. reply emgeee 15 hours agorootparentprevRectangle Pro user here -- biggest frustration I&#x27;ve had is configuration. While the GUI is nice for seeing all the options and some basic stuff, when it comes to actually configuring App Layouts it&#x27;s been a nightmare of clicking around. I went so far as to export my config so I could modify it with a text editor and then re-import it but the config itself includes serialized JSON as values so to accomplish what I wanted to I had to de-serialize the json, make the changes I wanted to, re-serialize it, then re-import it back into Rectangle, all while hoping I didn&#x27;t mess up the re-serialization step. This ended up taking more time than if I had just painfully clicked through the interface.Personally, I wish Rectangle had the option of storing configuration in a simple dot file in my home dir but I understand that would prevent syncing through icloud, which is a feature I imagine a lot of folks like. Barring that, I wish rectangle could export&#x2F;import the configuration in a more editor friendly way so I could make these configuration changes much faster and more easily ensure consistency across settings. reply rxhanson 8 hours agorootparentThanks for the feedback! I agree and have had some ideas for improving this - one of them being copy&#x2F;paste and drag behavior where you could copy a selection in Rectangle Pro and paste it as prettified JSON in an editor, and vice-versa. The dot file idea is also one that I like - it&#x27;s just a surprisingly large effort to \"do it right\". Making UI&#x27;s for representing this complexity is kind of tricky, and what&#x27;s there now is actually my first cut - something that I figured I would iterate on but haven&#x27;t gotten the time together to do it. reply Bagged2347 16 hours agorootparentprevAre you the Rectangle dev? I appreciate your work! How did you get into developing tools for macOS? Were you filling your own needs? reply rxhanson 8 hours agorootparentGlad you like my software! A long time ago, I had some RSI issues that were alleviated by using the Magic Trackpad, and this led me to create the Multitouch app[0] to add a bit more functionality to the Magic Trackpad. From there I wanted to add in window management, and ended up rewriting Spectacle as Rectangle. I feel really lucky to have people enjoying the things I&#x27;ve built![0]https:&#x2F;&#x2F;multitouch.app reply CSSer 16 hours agorootparentprevYou know, honestly I don&#x27;t recall. From what I do remember, I had checked the repo and I believe it was something you were working on but it was going to require a really big lift. I wanted to be very charitable with you in my above comment. I used Rectangle for a decent amount of time and overall it was pretty great. I only switched because of a time&#x2F;money consideration, not because of a vote of no confidence, which is not something I can always say. Keep up the great work! reply TimTheTinker 18 hours agorootparentprevI&#x27;ve been using Magnet for years. It&#x27;s simple and reliable. reply j45 18 hours agorootparentHas anyone transitioned from moom to magnet? reply spike021 8 hours agorootparentprevI love magnet as well, have used it probably most of the last few jobs I&#x27;ve worked. The hot keys can be a little tricky to learn at first but after that it&#x27;s a snap (heh). reply aryik 17 hours agorootparentprevMagnet rocks. Very worth paying for. I hate using Macs without it reply rwc 18 hours agorootparentprevActually, only $5. reply newsereure 17 hours agorootparentprevI just bought this, but I don&#x27;t see an option for automatic tiling? reply artdigital 10 hours agorootparentIt can‚Äôt do automatic tiling, it‚Äôs a window snapping&#x2F;resizing app reply thinker5555 18 hours agoparentprevI haven&#x27;t had the same experience with Yabai. I&#x27;ve been using it happily and without issue since April, and I don&#x27;t think I&#x27;ve had to restart it once.Before Yabai I was using Amethyst, but like your experience with Yabai, I felt like it would suddenly stop working and needed to be restarted. Maybe your experience would be flipped.https:&#x2F;&#x2F;github.com&#x2F;ianyh&#x2F;Amethyst reply chaxor 18 hours agorootparentI tried amethyst but it was essentially useless to emulate an awesomewm or sway like experience.Switching to asahilinux was a *far* better solution. Not really sure what the use of macOS for me is when asahi exists now. reply koiueo 13 hours agorootparentIs Asahi your daily driver on MBP? reply rufugee 12 hours agorootparentCurious about this as well...I thought it wasn&#x27;t quite there as a daily driver OS, but would be happy to be wrong ;-) reply Bombinator 2 hours agorootparentIt&#x27;s usable if you can live with some small annoying inconveniences last I tried. I can&#x27;t remember if there were more inconveniences but what bugged me the most was that the touchpad was too sensitive compared to MacOS, it seemed to register accidental touches on the edges more and accidental taps were easier. I tried figuring out if this is fixable with libinput or hwdb but eventually gave up. Also the shortcuts with Command key are way more convenient than the ctrl ones but this probably is configurable somehow. Some Java programs had scaling issues but that can be configured to work properly. Battery life was a bit worse I think, this is with the GPU drivers and no video playback, just regular browsing. Oh and external monitor support isn&#x27;t there, I do use this sometimes on MacOS. reply chaxor 12 hours agorootparentprevYes, absolutely.Asahilinux >> MacOSIt&#x27;s wonderful to use the M2 with incredible battery life and performance without having to have a subpar OS. reply darkteflon 8 hours agorootparentCan I ask: is setting up dual-boot trivial? I‚Äôd love to check it out but can‚Äôt ditch MacOS entirely. reply stefandesu 3 hours agorootparentAFAIK dual boot is currently the only way to use Asahi on Apple Silicon Macs. reply darthrupert 3 hours agorootparentprevThe installation process sets it up to you. You&#x27;d have to do more work to not have dual-boot (unless there was a setting in the installer that I forgot). reply rufugee 7 hours agorootparentprevIs there anything that doesn&#x27;t work for you? reply darthrupert 3 hours agorootparentI&#x27;m not parent, but from my quick foray into (Fedora) Asahi on a Macbook Air M2, I at least noticed USB-C connection to my monitor and USB router behind it not working at all. There&#x27;s something Mac-specific going on there, because from my x86-based linux laptop, this usually works fine. replyRCitronsBroker 17 hours agorootparentprevsame here! using yabai on my MacOS machines for about a year now, after being similarly disappointed with amethyst. Works pretty damn good for me, its a god send for me. It is part of the myriad of tweaks that make MacOS so damn good and unique to me. reply extr 18 hours agoparentprevHave you heard of Phoenix [1]? It seems relatively unknown but I actually found it to work better than Yabai in some ways. The gist is that it basically simulates a tiling wm and virtual desktops by internally tracking state. This also means it doesn&#x27;t suffer from window animation delays when you switch \"desktops\" (since \"desktops\" are just hiding&#x2F;unhiding windows). It&#x27;s also highly hackable&#x2F;extensible being written in JS. Spin2Win [2] is a config that&#x27;s worked well for me.[1] https:&#x2F;&#x2F;github.com&#x2F;kasper&#x2F;phoenix[2] https:&#x2F;&#x2F;github.com&#x2F;nik3daz&#x2F;spin2winThat said, it seems there are no perfect solutions. At work where I can&#x27;t really be futzing around with window management config I basically just use Raycast + hotkeys and try to keep everything inside maximized application windows. This means using Arc browser (tabbed), iTerm (tabbed), VS Code (with native tabs), etc mapped to cmd+1, cmd+2, cmd+3...Not much \"tiling\" going on but at least everything is pretty keyboard friendly. reply psygn89 17 hours agorootparentDoes this allow using the mouse? Maybe not common amongst tiling window users but I like to just drag a window over another to swap them or resize one window from the frame and have the other tiles adjust accordingly. So far it seems only Amethyst and Yabai do this, but if Phoenix internally tracks the states of windows (position, etc.) then this should be feasible to write code for without relying on Mac&#x27;s windowing api? reply extr 9 hours agorootparentYeah if I recall correctly there is support for the mouse and doing all the kinds of things you usually use mouse for in tiling wm setups. reply xwowsersx 18 hours agorootparentprevI think I had come across Phoenix briefly, but never took it for a spin. Thanks for the recommendation, I&#x27;ll definitely take a look. reply rufugee 12 hours agoparentprevAfter trying yabai, Amethyst, and many others, I landed on hammerspoon.org + https:&#x2F;&#x2F;github.com&#x2F;miromannino&#x2F;miro-windows-manager a few years ago and haven&#x27;t looked back. I couldn&#x27;t live on a Mac without hammerspoon tbh. reply faizmokh 9 hours agorootparentYeah I&#x27;m using hammerspoon & miro too with this basic binding:```hs.window.animationDuration = 0.2 spoon.MiroWindowsManager:bindHotkeys({ up = {hyper, \"up\"}, right = {hyper, \"right\"}, down = {hyper, \"down\"}, left = {hyper, \"left\"}, fullscreen = {hyper, \"f\"} })```Works well enough for years. reply xwowsersx 11 hours agorootparentprevCan you share your config please? Trying to get a better idea of what&#x27;s possible and how to get started. Thanks reply rufugee 8 hours agorootparentMy full hammerspoon config is a mix of things I&#x27;ve collected&#x2F;written over the years, but the Miro-specific section is simply: -- window management hs.loadSpoon(\"MiroWindowsManager\") -- set to 0.x for animation, 0 for none hs.window.animationDuration = 0.3 spoon.MiroWindowsManager:bindHotkeys({ up = { hyper, \"k\" }, down = { hyper, \"j\" }, left = { hyper, \"h\" }, right = { hyper, \"l\" }, fullscreen = { hyper, \"f\" }, nextscreen = { hyper, \"n\" } }) reply nbobko 10 hours agoparentprev> I&#x27;d be more than willing to try another tiling window manager on Mac if there&#x27;s one out there that truly worksHello, AeroSpace author speaking :)I&#x27;d be happy if you could try AeroSpace (it&#x27;s and i3-like window manager for macOS) and report me back if it loses track of windows.https:&#x2F;&#x2F;github.com&#x2F;nikitabobko&#x2F;AeroSpaceThe architecture of AeroSpace is that on every user input that may change window configuration (new window created, window moved, window resized, new app launched, etc), AeroSpace runs the same idempotent operation (I call it \"refresh session\") that tries to detect new window, checks all invariants, re-layouts windows, etc.The \"refresh session\" performs all the mentioned steps regardless of the user input nature (it doesn&#x27;t matter whether the window is moved, or a new app is launched)I believe that this architecture may lose windows only if the macOS API returns invalid data.I have been using AeroSpace for quite a while myself and I&#x27;m happy with it reply thelazyoxymoron 49 minutes agorootparentThe demo looks pretty nice! I&#x27;ll be trying this out over the weekend and report any issues. Thanks for building this! reply xwowsersx 8 hours agorootparentprevVery cool, looks great. Seems to hew pretty closely to i3 from what I could tell in the demo video. Will definitely give this a try. Thanks for your work on this! reply onelesd 18 hours agoparentprevtry hammerspoon. it isn&#x27;t a tiling window manager, per se - it&#x27;s more of an all-purpose desktop extension framework. it is fully customizable, has a well-documented API, and a large community.for window arrangement the hs.grid component is plenty for me. see an example: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nRJ0477KFp8 reply zdragnar 18 hours agorootparentHammerspoon is literally the only thing I miss from leaving macOS.If I ever have to go back, it&#x27;ll be one of the first things I install again. reply pazimzadeh 16 hours agorootparentprevI use hammerspoon with the ShiftIt spoon, which works well:https:&#x2F;&#x2F;github.com&#x2F;peterklijn&#x2F;hammerspoon-shiftitMy config file:https:&#x2F;&#x2F;gist.github.com&#x2F;pazimzadeh&#x2F;b1c70f5f205d0b63264e7c021... reply iLemming 17 hours agorootparentprevCheck out https:&#x2F;&#x2F;github.com&#x2F;agzam&#x2F;spacehammer if you&#x27;re using Hammerspoon. reply vasac 16 hours agorootparentThere&#x27;s also somewhat similar extension for Hammerspoon: https:&#x2F;&#x2F;github.com&#x2F;FryJay&#x2F;MenuHammer&#x2F; reply rufugee 12 hours agorootparentprevI use hammerspoon with https:&#x2F;&#x2F;github.com&#x2F;miromannino&#x2F;miro-windows-manager...it&#x27;s a great tiling experience. reply xwowsersx 14 hours agorootparentprevDo you have a config example I can look at that demonstrates how to set this up? reply daggersandscars 18 hours agoparentprevThis may not be what you‚Äôre looking for, but I use Mizage‚Äôs Divvy on Mac & Windows and configure Gnome to match on Linux. It can be keyboard or mouse driven ‚Äî I exclusively use custom keyboard shortcuts. reply ebiester 18 hours agorootparentI used to use Divvy (and have a paid copy), but I&#x27;ve moved to Rectangle which also does everything I need and is open source. It doesn&#x27;t have the same \"drag\" mouse capabilities that Divvy does, but I don&#x27;t end up using that too often. reply marcomourao 14 hours agorootparentprevI&#x27;ve also been using Divvy since as long as I can remember but it hasn&#x27;t been updated in 4 years, looks like abandonware. reply tfsh 17 hours agoparentprevI&#x27;ve used Yabai for about three years now, it generally only forgets windows or specifically their states if you use the swipe multi tasking features. However when this happens I have a keybind and Skhd command which quickly restarts the daemon [1] and fixes the windows.It takes a little while to get used it, but Yabai is now an app I can&#x27;t live without.1: launchctl kickstart -k \"gui&#x2F;${UID}&#x2F;homebrew.mxcl.yabai\" reply athorax 16 hours agorootparentI have the exact same setup, I have to maybe restart the service 2-3x a week. Unsurprisingly, outlook is often the culprit of needing to restart the service# Reload yabai ctrl + shift + alt -r : launchctl kickstart -k \"gui&#x2F;${UID}&#x2F;homebrew.mxcl.yabai\" reply psygn89 17 hours agoparentprevThat was my experience also especially the past year. I do programming and design and it seems many apps twist the ui api to do things that you can&#x27;t naturally do in the interface and so Yabai, completely oblivious to it, will try to manage it and you have elements disappearing or appearing too large&#x2F;small, behind other elements, etc. Even if you create exceptions, it seems things sometimes slip through the crack or start behaving unexpectedly. Flipping between native tabs for instance start throwing the tiling off for some apps that I use.Yabai used to be pretty solid and usable, but I think with the author not using a Mac for his daily and with all the updates to the OS that he has to keep track of (and how some windowing Mac api are still blackboxed or not reliant to build logic atop of) it&#x27;s only a matter of time before it gets into a poorer state. reply Difwif 18 hours agoparentprevI&#x27;ve certainly run into my problems with Yabai and have had to work through different versions to solve problems. It&#x27;s certainly not perfect still but I&#x27;ve found a pretty stable workflow for me. I would certainly recommend turning off anything that&#x27;s not nessiary like window borders. After that you&#x27;ll need special configuration for a couple problematic apps (Chrome for me), and lastly some custom behavior using the events to get the exact behavior I want. I have had to debug and look at logs to figure out my problems.. reply girvo 10 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;ianyh&#x2F;AmethystThis is what I used for years. Sadly I&#x27;m back in Windows land these days due to work. reply imawizard 7 hours agorootparentI‚Äòm using Windows again as well (just loving the crap-&#x2F;bloatware for corporate vpn, syncing powerpoint templates, group policies etc.) which is why I made a small window manager in AutoHotkey:https:&#x2F;&#x2F;github.com&#x2F;imawizard&#x2F;MiguruWMThe wiki also contains a list of other window managers for Windows one might want to check out. reply emoII 18 hours agoparentprevRectangle is absolutely great reply xwowsersx 16 hours agorootparentRectangle seems nice, but not quite what I&#x27;m looking for. I want automatic tiling. I&#x27;m not looking to just move some windows around with shortcuts. I want to be able to have predefined arrangements and automatic tiling, etc. Rectangle is similar to SizeUp which I&#x27;ve used. Good for what it is, but still want a true tiling window manager. reply Hammershaft 13 hours agoparentprevI had a similar experience. I find the default windowing experience on Macs to be miserable and so I was excited to jump in to a tiling WM, but Yabai was a pain to configure relative to linux tiling managers and was not reliable. reply cedws 16 hours agoparentprevNothing out there quite works like i3 because they all politely ask the actual window manager to set the position&#x2F;size of windows. i3 controls the position and size of windows. reply lowmagnet 18 hours agoparentprevI just use Grid, it already has shortcuts (I don&#x27;t like that Yabai doesn&#x27;t) and it&#x27;s pretty easy to get used to. reply pmarreck 18 hours agoparentprevSo basically, it&#x27;s below the \"Reliability*Utility\" threshold (that&#x27;s my term for the drastic dropoff in usefulness of a product&#x2F;service&#x2F;person when their reliability goes from 100% to 90% or lower) That&#x27;s too bad. Maybe some unit tests (historically notoriously difficult to implement with GUI&#x27;s) would help that dev.Fleshing my concept out on a whim (since I am surprised I can&#x27;t find another reference to this idea online!) with the help of ChatGPT (yes, it can do this now!), it looks like this equation would capture this concept:U(r)=Umax ‚ãÖe^(‚àík(100‚àír)^n)where the total perceived Utility of a product&#x2F;service&#x2F;trusted person, given r=reliability percent (out of 100), is the maximum Utility times e to the (-k(100-r)^n), where k and n are tweakable based on the given product&#x2F;service&#x2F;person but which I want to assign values of 0.03 and 1.5 to, respectively, after plotting it. reply xwowsersx 8 hours agorootparent> So basically, it&#x27;s below the \"Reliability*Utility\" threshold (that&#x27;s my term for the drastic dropoff in usefulness of a product&#x2F;service&#x2F;person when their reliability goes from 100% to 90% or lower)Useful concept, that&#x27;s exactly it. reply jknz 17 hours agoprevWhy wouldn&#x27;t apple allow to remove all animations when switching desktops, without disabling SIP?https:&#x2F;&#x2F;apple.stackexchange.com&#x2F;questions&#x2F;434555&#x2F;can-you-com...Native keyboard shortcuts are there for all to use to switch desktops, but the fading in&#x2F;out animations that last around 1s completely prevent fast desktop switches as is so useful in i3&#x2F;sway. reply nbobko 10 hours agoparentYou might want to check out AeroSpace :) https:&#x2F;&#x2F;github.com&#x2F;nikitabobko&#x2F;AeroSpaceAeroSpace reimplements workspaces by emulating them. https:&#x2F;&#x2F;github.com&#x2F;nikitabobko&#x2F;AeroSpace&#x2F;blob&#x2F;main&#x2F;docs&#x2F;guid...The result is that workspace switching is instant, and disabling SIP is not requiredNote: I&#x27;m the author of AeroSpace reply extr 9 hours agorootparentThis is awesome. reply shortrounddev2 16 hours agoparentprevWhen I looked this up on an apple support forum, the replies were all basically: \"Why would you want to disable this feature? Apple designers know better than you, go back to Windows\" reply superkuh 11 hours agorootparentThey&#x27;re not wrong. When you buy an Apple product and try to customize it you&#x27;re not doing what it&#x27;s made for. It&#x27;s like trying to use a golf club to hit a baseball. reply babypuncher 15 hours agoparentprevthis is one of my biggest annoyances about macOS. I actually want to keep the vast majority of animations, the only one I want gone is the desktop transition, because it feels so jarring.Either make the transition instant, or bring back the old behavior where the dock and desktop remain static during the slide animation. reply montebicyclelo 17 hours agoprevI&#x27;ve been using Yabai + skhd for years, and found the benefits far outweigh the issues. 99% of the time it&#x27;s fine, and for the 1% I just disable it temporarily.Killer feature for me (apart from windows automatically being resized to fit, which is great), is ctrl+i to switch to screen i, and ctrl+shift+i to move the current window to screen i. I tend to have 2 windows max per screen, and use Yabai to flit between screens &#x2F; flit windows to different screens.(I used i3 on Linux prior to setting up Yabai, and configured shortcuts to be similar to i3.) reply Nevin1901 18 hours agoprevIn my personal experience, tiling window managers only work when you have a lot of screen real estate. I tried using yabai on a 13 inch m2 air, but I ended up going back to my normal workflow of just alt tabbing between windows. Most apps don‚Äôt let you see enough content when they are tiled (because of gui elements such as sidebar etc). I do love the ‚Äúfocus on hover‚Äù feature of yabai though. I wish macOS had that natively. reply hobofan 12 hours agoparentI&#x27;ve been using Amethyst for multiple years on only the builtin display as well.I think the key here is to use multiple desktops (or are they called workspaces?). If I&#x27;m working on a single screen I constantly have 10 desktops open, with a quite consistent setup between them (e.g. first desktop is fullscreen browser, second screen is 3-5 tiled terminals, ...). All the applications I&#x27;m \"actively\" using are usually on the first 2-4 desktops with a lot of screen real estate for each of them.What really helps here is assigning one of the screen corners as a hot corner that goes to Expose mode where you can switch between screens, as well as assigning shortcuts for switching between desktops and moving windows between them (the moving windows shortcuts are usually part of your window manager). Selecting \"Reduced Motion\" in the accessibility settings also helps by speeding up the desktop switching animations. reply wilsonnb3 4 hours agoparentprevI used to use i3 on a ThinkPad x220 and agree that they are less useful on small screens.My experience is that they are also just productivity theater. You feel more productive with the snazzy shortcuts but its very rare for the bottleneck in a workflow to be window management. Most people probably lose more time overall managing their config files and looking up the shortcuts they forgot. reply SamBam 18 hours agoparentprevI have the same problem.I would love a tiling manager that allowed you to \"full screen\" the app into the tile of your choice.By that, I mean the app would snap to the edges and lose all the chrome that is normal present. So your browser window, for instance, would lose the borders and url bar until you moused to the top of the tile. reply Nevin1901 17 hours agorootparentNot a tiling window manager, but the arc browser does this very well. reply jwells89 16 hours agoparentprevOn a 27‚Äù monitor running at 2560x1440 1x&#x2F;2x tiling is better than on a small screen, but it still winds up awkwardly sizing windows from time to time. In most cases I find it easier to just let windows overlap, which results in the same information visibility but lets windows retain proper sizing.I could see tiling working if I lived in minimal-chrome apps like Terminal and Sublime, but I don‚Äôt, and so floating with occasional tiling works better for me. reply shortrounddev2 17 hours agoparentprevDo you not use a monitor at a desk? reply Nevin1901 12 hours agorootparentNo, I work on an m2 MacBook Air with a trackpad and the built in keyboard reply caeruleus 15 hours agoprevI have been using Yabai for two years now and am very happy. For me, it&#x27;s stable and you can script almost anything by using event reactors.Yabai also makes working with a 15\" display much more productive imho. For example, I can effortlessly and nearly instantaneously switch between browser and coding spaces by pressing hyper + w &#x2F; hyper + c (I&#x27;m using Karabiner Elements to map capslock to hyper).Since it&#x27;s a common misconception, you do NOT need to disable SIP if you don&#x27;t care about some features (noticeably, disabling those nasty space switch animations). For an overview of features that require SIP to be partially disabled:https:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;yabai&#x2F;issues&#x2F;1863Also in general, the wiki on GitHub is very well-written. reply certifiedloud 15 hours agoprevIf you come from i3 you&#x27;ll probably find Aerospace[1] to be more what you&#x27;re after.[1]https:&#x2F;&#x2F;github.com&#x2F;nikitabobko&#x2F;AeroSpace reply idle_zealot 14 hours agoparentThank you for recommending this. Time for round three of attempting a tiling setup on my mac (after Amethyst and Yabai disappointed). reply certifiedloud 13 hours agorootparentSame path I took. I&#x27;m liking Aerospace so far. I just snagged the \"i3-like\" config from their docs and it feels like home. Not quite as snappy as i3, but very workable. reply strus 13 hours agoparentprevThis is great, thank you! reply datameta 17 hours agoprevYabai is an interesting japanese word with several distinct composite meanings. https:&#x2F;&#x2F;www.alexrockinjapanese.com&#x2F;what-does-yabai-mean-in-j...Here it is being used in one context at a beatbox battle between then World Beatbox champion Alexinho and the super innovative Soso. https:&#x2F;&#x2F;youtu.be&#x2F;OjbYK4rujB0?si=SIn-uNHlh2KexsXc Yabai shout is at 3m23s reply TheRealPomax 17 hours agoparentIt also seems wholly unrelated to anything about this app though, which is a shame, because as someone who knows Japanese, I&#x27;m not likely to install something called \"yabai\" in the same vein that I&#x27;m not going to install something called \"whoa\" or \"ohshit\". Hopefully that puts me in the minority. reply fomine3 8 hours agorootparentAs a Japanese, I just feel that \"ah a foreigner adopt a weird word for a product again\" and install without worrying. reply saagarjha 11 hours agorootparentprevWhat is it with Mac app developers and bad Japanese names, I wonder? reply jeffwiederkehr 18 hours agoprevI love yabai and use it every day. It‚Äôs the only Mac OS is tolerable for me vs Linux with a WM.I am sad though that you lose a good chunk of functionality without disabling SIP, which isn‚Äôt allowed for work devices reply pizzalife 11 hours agoprevPersonally I only see the benefit of tiling for terminals. And in macOS, iTerm2 performs tiling perfectly. I just swipe to my iTerm workspace with tiled terminals and tab between other apps that are fullscreen. reply joshring 17 hours agoprevI&#x27;ll have to try this out. Currently use Amethyst + Hammerspoon scripts for my window tools. Like others in this thread, Amethyst occasionally loses track of all windows and requires a restart (esp after monitor (dis)connection).Amethyst does a decent job at the layouts I care about.I primarily use AwesomeWM in linux on my personal computers which has the amazing super key drag&#x2F;resizing behavior for windows. I use Hammerspoon to replicate this behavior[0][1] and it works quite well.Eventually I want to replace Amethyst and just do everything in Hammerspoon as it seems quite plausible to do window layouting with it. Will give Yabai a try as well in the meantime.[0]: https:&#x2F;&#x2F;github.com&#x2F;RingOfStorms&#x2F;setup&#x2F;blob&#x2F;master&#x2F;home&#x2F;confi... [1]: https:&#x2F;&#x2F;share.joshuabell.link&#x2F;ezgif-1-ef0d3e0728.gif reply replete 10 hours agoprevWindow management becomes a whole other thing with really large monitors. I recently got a UHD 43\" and to make the most of it I designed a custom grid but struggled to find an optimal solution. BetterSnapTool with custom snap zones works but is a bit clunky. Lasoo has the exact imagined solution I wanted (keystroke > grid representation appears at mouse pointer > drag window size) but doesn&#x27;t yet support custom grids. I&#x27;ve requested the feature but not exactly hopeful.I could see that yabai could be a solution but haven&#x27;t committed to a proper setup.Anyone else using a single large monitor? FancyZones on Windows (PowerToys feature) looks pretty good, but I got sick of autoupdate BS random driver breaks (common linux thing) and went full mac.My custom grid for 4K 43\" snap zones in case anyone&#x27;s interested: - diagram: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;axTw1tI - 3840x2160 1:1 grid: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;pUSoafG reply ryansouza 10 hours agoparentI still use Moom with keyboard shortcuts reply vidarh 17 hours agoprevI use Yabai on my work laptop and I&#x27;m not very impressed. Maybe I&#x27;ve just been spoiled by my Linux setup, which has had years of tuning, but it feels like a poor imitation of a tiling wm. I get that OS X makes it harder to fully control the experience the way you can under X, but still.. reply joeyagreco 18 hours agoprevInteresting. I&#x27;ve been using Rectangle for MacOS and it&#x27;s worked fine so far for me. Will keep this in mind if I ever want to switch. reply galoisscobi 18 hours agoprevComing from i3wm on Linux, I cannot use macOS without yabai. So glad that this exists! reply barelysapient 18 hours agoparentCompletely agree. It has its faults; but paired with skhd [1] it makes the mac usable for me.1. https:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;skhd reply sgt 15 hours agoprevI&#x27;ve always wondered why people enjoy tiling window managers. Personally I don&#x27;t really grasp it. If I work with a lot of papers and documents on a desk, I don&#x27;t \"tile\" them, I keep them sort of organically in front of me.And whatever app I need to use should ideally be straight in front of me. But that&#x27;s just me, maybe I haven&#x27;t had a proper tiling experience yet and thus I haven&#x27;t seen the light. reply beautron 2 hours agoparentFor me, it&#x27;s less that I want to \"tile\" windows, than that I want whichever one I&#x27;m using to be fullscreen and distraction-free.I don&#x27;t like visual clutter. If I&#x27;m using a program, I want it to be the only thing on my monitor. But sometimes I want to keep more than one program open, or even be switching between them. Tiling window managers let me give each program its own full screen, and quickly switch between them with a keyboard command. I love it.And then there&#x27;s the exception that proves the rule: Sometimes I do want to have two programs on the same screen, side-by-side. An example would be Vim in the left 2&#x2F;3rds of my screen (where I am programming), and then a browser with some documentation on the right 1&#x2F;3rd. Obviously tiling managers also support this case, and I prefer them here too vs. mousing around windows into place (which still tends to leave behind windowing chrome and other visual clutter).That reminds me of another reason: I prefer keyboard to mouse. I&#x27;ve learned good form and technique for keyboard use, which let me use it frequently and for long periods while avoiding injury. But I just can&#x27;t figure out a way to use a mouse or trackpad that isn&#x27;t harmful, were it repeated enough (a vertical mouse goes a long way though).Tiling managers let me do everything using keyboard (unless the \"tiled\" programs themselves require mouse), whereas traditional windowing managers pretty much require mouse. I find it both more ergonomic and much faster using the keyboard. reply certifiedloud 15 hours agoparentprevI switched to i3 years ago for the workspace management. I can spread different workspaces across my monitors with easy commands and full control. Every OS and linux distro I&#x27;ve tried has inferior workspace management compared to i3. Tiling is secondary for me. reply voisin 15 hours agoparentprevIf you need to work on 2+ apps simultaneously it makes sense, just like if you were copying text from one document to another on your desktop you would put them next to each other. reply sgt 15 hours agorootparentI find your argument strange. Any windowing system can put two windows next to each other. reply deafpolygon 15 hours agorootparentprevmacOS has the ability to put windows side-by-side. reply gigatexal 13 hours agoprevI would love to do this and maybe move back to MacOS but ... this seems like a lot of faffing about for something that was not designed to be done and something that is constantly at risk of being nerfed by some future update or security thing such as Apple disabling the SIP disablement ability or something along those lines.But in Linux the choose-your-own-adventure that is the pluggable desktop model is that something like sway or i3 or other tiling window manager or a plugin for Gnome for example is much more in line with how the system was meant to be used and is very much not trying to shoehorn something in if any of that makes sense.Of course the dev(s) that put this together and it&#x27;s other projects are hackers in the truest form and their efforts are to be applauded. I&#x27;ll continue to show them praise but keep using i3&#x2F;sway on Linux for the time being. (funny enough tiling window managers was why I left MacOS for Linux.) reply dabeeeenster 18 hours agoprevhttps:&#x2F;&#x2F;www.hammerspoon.org&#x2F; is amazing. Not quite the same as Yabai but I cant live without it! reply twp 18 hours agoparentYou can configure Hammerspoon so a hot key combination puts a window in a defined section of the screen. For example, I use Ctrl+Alt+H to put the current window in the left half of the screen, Ctrl+Alt+L for the right half, Ctrl+Alt+Enter for full screen, etc. This makes arranging your windows very fast.Full config: https:&#x2F;&#x2F;github.com&#x2F;twpayne&#x2F;dotfiles&#x2F;blob&#x2F;21d0edcebaeebf0d90e... reply dbalatero 18 hours agoparentprevI have an alpha guide I wrote on scripting macOS with Hammerspoon if anyone&#x27;s interested! There&#x27;s probably some editing issues and I&#x27;ve been meaning to move it to the actual domain&#x2F;clean it up to release, but it&#x27;s got good info in it now:https:&#x2F;&#x2F;learn-hammerspoon-0x8080.netlify.app&#x2F;I have a chapter on window management as part of the guide. reply ckolkey 17 hours agoparentprevIntegrating hammerspoon as the keybinding manager for yabai is a perfect match. You can treat yabai as an api for window management, and script some pretty cool things, like sending the current window to a new space, and either keeping focus or not, swapping windows, all kinds of fun stuff. reply bsnnkv 18 hours agoprevYabai, and chunkwm before it, and kwm before that, are probably the most important pieces of software that fundamentally changed how I interact with computers.I moved over from macOS to Windows and WSL full time back in 2020 and eventually ended up writing my own tiling window manager for Windows, which I never would have done had I not had the experience of using yabai on macOS.I don&#x27;t think it&#x27;s a exaggeration to say that my experience with this one piece of software completely changed my trajectory as a software developer for the better. reply zacte 18 hours agoprevPersonally I&#x27;ve been so hooked to Raycast it&#x27;s taken over my tiling manager needs too now. reply CharlesW 18 hours agoparentI love Raycast for this too. For new and current Raycast users who haven&#x27;t seen this capability, you have to enable the Window Management extension.https:&#x2F;&#x2F;www.raycast.com&#x2F;extensions&#x2F;window-management reply ianyh 8 hours agoprevHey, Amethyst dev here. Love seeing this on the front page.I‚Äôve seen a few comments mention it here and there, but I‚Äôm swooping in with my semi-regular reminder that you don‚Äôt need to disable SIP to use yabai. It‚Äôs a great bsp window manager out of the box, with some optional‚Äîand admittedly very cool‚Äîextra features that require maneuvering around SIP.(Also, minor apology to Amethyst users because I haven‚Äôt managed to get much time to work on it recently.) reply user3939382 12 hours agoprevI like to keep the right 10% or so visible so I can see (and manipulate) the first column of file icons on my desktop behind my otherwise maximized&#x2F;tiled apps. I haven‚Äôt been able to find a window manager that can handle this customization. reply hobofan 12 hours agoparentAmethyst[0] has options for screen padding.[0]: https:&#x2F;&#x2F;ianyh.com&#x2F;amethyst&#x2F; reply handsclean 11 hours agoparentprevYou may have already considered this, but dragging desired folders into the last section of the dock accomplishes some of the same things. reply lsuresh 17 hours agoprevAs a long time XMonad user who had to switch to a mac at a new job, I first used chunkwm and then yabai. Yabai&#x27;s requirement to disable SIP was a problem, and so I finally switched to Amethyst a year ago and have been quite happy with it. It gives me pretty much everything I missed from XMonad. reply gkfasdfasdf 17 hours agoprevIs there any app that has the ability to &#x27;lower&#x27; the focus of a window on mac? E.g. on Ubuntu I used to be able to &#x27;lower&#x27; a window&#x27;s focus by middle-clicking on the title bar, i.e. hide the window behind other open windows (without minimizing it). I really miss that feature on mac. reply lazerlapin 8 hours agoprevI have used yabai for a long time now and never had any big issue. It&#x27;s a set up once and forget solution and honestly I&#x27;m surprises I don&#x27;t see more comment like that. One thing for sure: you got to give it a fair shot to really appreciate what it does to your user experience. reply vietvu 6 hours agoprevIs it working now? I used for about half a year until MacOS Sonoma when it breaks and I had enough. MacOS is just not built for customization, it&#x27;s frustrating. reply wdb 9 hours agoprevI am pretty happy with Swish (https:&#x2F;&#x2F;highlyopinionated.co&#x2F;swish&#x2F;) Use can make gestures to snap&#x2F;tile windows. Pretty cool when you have a trackpad or Magic Trackpad worth the $12 reply oaththrowaway 16 hours agoprevI have used Yabai + skhd previously solely to allow me to \"cycle\" my virtual desktops. Disabling SIP to make this happen was always a fun challenge on my work Macbook. Is there an alternative at this point in time? reply jsf01 16 hours agoprevFor anyone looking to dip their toes into Yabai and tiling windows, I recommend the following setup. Enable full tiling only on a couple workspaces. Desktop 1 can remain in floating layout, just as though you‚Äôd never installed Yabai in the first place. Then tile Desktop 2 and as many others as you want.The one other tip I have is to have an easy hotkey for toggling full screen mode. Toggling an app into full screen mode doesn‚Äôt lose its tiled position or create a new desktop the way native Mac full screen mode does, which is especially helpful when working on a smaller monitor. reply yangikan 15 hours agoparentI usually have too many windows open (upwards of 20). And yabai tries to tile all of them. Is there any way to say \"have a maximum of 3 windows horizontally and but for the left most tile, split vertically into two if needed\"? reply OsrsNeedsf2P 18 hours agoprevI spent more than a year bending over backwards with tools like Amethyst, but ultimately it was too much of a hassle and I ended up going back to my Thinkpad. The ecosystem on MacOS just costs more and never _quite_ works for customizability reply beckhamc 10 hours agoprevI wish I had the ability to toggle PiP for any open window, while I am in full screen mode. For instance, I have both Chrome and Emacs side by side full screen and I can use a hotkey to drop down my iTerm window over both of them. (Basically like a Quake terminal but that feature is specific to iTerm). reply chatmasta 13 hours agoprevI&#x27;m still using Slate, with a .dmg that was built nearly a decade ago and by some miracle continues to work. I pray that each new OS update won&#x27;t be the one to break it, because I really don&#x27;t want to migrate the config for ten years of muscle memory to something new... reply ilikejam 18 hours agoprevAll I want (for christmas) is an &#x27;Always on top&#x27; opton for windows on macos.I&#x27;d pay real actual money, as I suspect would many others. reply harlanlewis 17 hours agoparentPiqued my interest so I just went looking, this relatively recent reddit thread lists a few relevant options: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mac&#x2F;comments&#x2F;q1pgos&#x2F;keep_a_window_o...I hope one works out! reply ilikejam 16 hours agorootparentBetterTouchTool works, but it&#x27;s not pretty - rounded corners end up with blue backgrounds and windows shrink slightly when defocussed.No joy with the rest. reply CharlesW 17 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;jslegendre&#x2F;AfloatX reply shortrounddev2 16 hours agoprevEveryone in my company uses a company provided macbook, but I use my own Windows Desktop. When presenting stuff to my boss, I&#x27;ll use windows shortcuts for tiling windows (Win + Arrow keys) and my boss said that&#x27;s the one feature he thinks is missing from macOS that Windows is really good at reply imbnwa 15 hours agoparentAnother bit is that Explorer&#x27;s Save File modal does autocomplete for file names, which has the side effect of immediately alerting you that a file name is already in use before you submit, but is also massively helpful when you&#x27;re naming consecutive downloads with a naming pattern.That latter case would be supercharged if Finder supported this on macOS as the global text edit bindings are more ergonomic for me than those shared between Windows and Linux. reply neilv 15 hours agoprevFor those who want to support open platforms, there are also tiling window managers for Linux and *BSD.I find XMonad is most effective for me. I sometimes also use i3wm (and there&#x27;s also Sway, for Wayland). There are a few others. reply verdverm 13 hours agoprevAre there recommended 3rd party enhancing apps for cmd-tab?I&#x27;m looking for something that shows all windows for an app, per app, and maybe doesn&#x27;t switch the app to finder when you cancel the cmd-tab reply ferriswil 13 hours agoparentI use AltTab [0] for this. I find using a mac without it horrendous.[0]: https:&#x2F;&#x2F;alt-tab-macos.netlify.app&#x2F; reply verdverm 13 hours agorootparentoh wow, you can have multiple \"alt-tabs\" and set what they showthanks for the link! reply Tyr42 13 hours agoparentprevWitch? It&#x27;s been a while since I used it. reply dustingetz 17 hours agoprevHas anyone tried this with complex layouts (e.g. 4 monitor tie fighter)? My main issue is window state is lost due to race conditions in the display identification logic, though I haven&#x27;t tried dared a Sonoma upgrade yet which contains compositor upgrades reply creakingstairs 10 hours agoprevI cannot use my mac without Yabai because the forced transition delay between spaces is extremely annoying for me. reply gdcbe 14 hours agoprevTbh if there was an app that would allow me to move between spaces on all my monitors at once, instead of each monitor to be done one by one, then that would be sufficient for my needs. reply edanm 14 hours agoparentThere&#x27;s a setting for that.I don&#x27;t remember it but something like separate spaces for displays. reply athorax 16 hours agoprevThe associated hotkey daemon used by yabai from the same author is equally as usefulhttps:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;skhd reply asadm 17 hours agoprevIs there an auto-tiling window manager that just enforces tiling without using keyboard shortcuts? Like if I drag a window with mouse, it snaps it in a tile.I think I saw this in popOS. Does yabai support this? reply very_good_man 18 hours agoprevShiftIt still working basically fine for me after a decade:https:&#x2F;&#x2F;github.com&#x2F;fikovnik&#x2F;ShiftIt reply rcarmo 15 hours agoprevThis still requires disabling SIP, I see. That&#x27;s just not an option for me.(I have done quite a bit of research and testing on these, and keep using a mixture of Moom and Phoenix - here&#x27;s my full list: https:&#x2F;&#x2F;taoofmac.com&#x2F;space&#x2F;apps&#x2F;window_managers ) reply d4l3k 15 hours agoparentIt seems to work just fine with SIP enabled. I just switched and it seems to be a lot better than Amethyst. Amethyst had a lot of issues with focus follows mouse and dropdown dialogs that seems to just work with YabaiSeems like SIP is only needed for system dialogs etc so has the same limitations as Amethyst reply ckolkey 15 hours agoparentprevOnly for a limited subset of features. All the window management works with SIP. reply SJetKaran 13 hours agoprevI&#x27;ve been using Yabai for 2+ years now, super happy that it exists! reply dansteeves68 16 hours agoprevI guess that I just rely on MacOS layout, however random and not great, and use Moom with some hotkeys to handle the exceptions where I want 2 or 3 windows in a specific state.I did try Yabai and Amethyst for a bit, but the overhead was a bit too much to handle.What am I missing? reply nestock 13 hours agoprevI hope there will be support for current Sonoma versions soon reply thomasfl 17 hours agoprevGood job by √Ösmund Vikane and the rest of the team. reply steve1977 17 hours agoprevJust for the sake of correctness, it‚Äôs not a window manager but a window management utility which extends the macOS window manager. reply trenchgun 14 hours agoprevJust use Hammerspoon reply tuananh 18 hours agoprevit&#x27;s either yabai or amethyst. some may find amethyst more user friendly reply rubyissimo 18 hours agopreveverytime I get a new computer it takes me 5 seconds to remember that I can&#x27;t live without sizeup.But I can never remember the name of the app because it&#x27;s just ctr-opt-cmd-left in my head. reply misiti3780 18 hours agoprevmagnet is where it&#x27;s at. reply j45 18 hours agoprevReading the issues with Yabai losing windows and needing a restart..Rectangle is another oss option: https:&#x2F;&#x2F;github.com&#x2F;rxhanson&#x2F;RectangleI bought Moom long before rectangle came out, both are pretty decent. reply poidos 18 hours agoprevI like rectangle [0]. It fits my needs well enough without requiring disabling SIP. I especially like the ‚Äúrepeated key presses resize a window on the same side‚Äù feature, so I can get cycle the size of the window on the right side of the screen from 1&#x2F;2->1&#x2F;3->2&#x2F;3 easily. I miss i3 sometimes and this ain‚Äôt an identical replacement but it gets the job done! Turns out I don‚Äôt move my windows all that often anyways.[0]: https:&#x2F;&#x2F;rectangleapp.com&#x2F; reply betamike 17 hours agoparentJust wanted to point out that disabling SIP is not required to use most of yabai&#x27;s window management features. You do lose some features[0] but I&#x27;ve found the only ones I really care about is focusing spaces, and you can create keyboard shortcuts for that in System Settings.[0]: https:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;yabai&#x2F;wiki&#x2F;Disabling-System-I... reply freeqaz 16 hours agorootparentI want to echo this as well. I use a 2nd tool to help me add additional i3-like keyboard shortcuts as well (I have the ability to \"stack\" windows with Alt-S and rotate through them with Alt-J and Alt-K).It&#x27;s called skhd https:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;skhdI forgot where the script for the stacking is. I can look that up separately, but I&#x27;m on mobile atm. reply urxvtcd 16 hours agorootparentPlease post it if possible. I couldn‚Äôt figure out the stacking API, and I miss that feature from i3. reply zavertnik 16 hours agorootparentThey may be referring to stackline https:&#x2F;&#x2F;github.com&#x2F;AdamWagner&#x2F;stacklineI&#x27;ve used it in the past, it was very aesthetically pleasing but did not work consistently enough for me to use day to day. reply freeqaz 11 hours agorootparentprevHere it is. There is no visualization of the stack, which apparently Stackline in the other comment supports, but I don&#x27;t tend to need that. Just being able to move between the windows is good enough for me.https:&#x2F;&#x2F;github.com&#x2F;koekeishiya&#x2F;yabai&#x2F;issues&#x2F;203#issuecomment... reply poidos 16 hours agorootparentprevThat‚Äôs good to know, thanks for clarifying that! reply jwells89 17 hours agoparentprevI similarly find something like Yabai a bit too heavy-handed for my needs, and instead prefer Moom[0].I find that I only need tiling occasionally, and for that Moom excels since it doesn‚Äôt add any new key shortcuts to memorize and is only ever visibly present when hovering your cursor over a window‚Äôs green button. Its Aero Snap equivalent is optional and turned off by default too, which is great for me (I trigger Aero Snap‚Äôs proposed-window-resize animations unintentionally often enough that they get to be irritating).[0]: https:&#x2F;&#x2F;manytricks.com&#x2F;moom&#x2F; reply asenchi 16 hours agorootparentMoom is one of my main tools and I have used it for years. It is one of the first things I put on my Mac. I use it constantly to move windows around, lay things out for work, and move things between desktops. I can&#x27;t recommend it enough. reply drcongo 15 hours agorootparentprevMoom fan here too. I got as far in the readme for Yabai where it says you need another utility just to set keyboard shortcuts and decided to stick with what I know and love. reply mushufasa 17 hours agoparentprevI find rectangle actually preferable to tiling, which I&#x27;ve tried on my linux boxes. Rectangle has the benefits of keeping the application-defined sizes (e.g. the terminal when opened up is 80x20 instead of whatever space is available on the screen) and playing nicely with the system defaults (like command-tab, command-~ to cycle through applications and windows). I find if I ever end up using a public or someone else&#x27;s device, it&#x27;s easier to cope; I&#x27;m not as helpless without rectangle as I was when I got used to a tiling WM muscle memory.And the name of Yabai -- which translates to \"danger\" -- never sounded appealing to me to introduce into my device. reply l2dy 17 hours agorootparentThe name Yabai can also be used in a positive sense to mean \"great\", and is particularly common among young people. Source: https:&#x2F;&#x2F;web-japan.org&#x2F;trends&#x2F;buzz&#x2F;bz0510.html reply presentation 16 hours agorootparentAs a relatively young Japanese speaker, can confirm, it can be either bad or good in context as slang. The comparison to English ‚Äúwicked‚Äù makes sense - could be like ‚Äúwhoa, that‚Äôs really uncool&#x2F;dangerous&#x2F;awful‚Äù or it could mean ‚Äúwhoa, that‚Äôs amazing! Sick!‚Äù Probably library author is going for the latter! reply Twisol 17 hours agorootparentprevFrom what I understand, a more direct analogue to \"danger\" is \"abunai\". The word \"yabai\" almost seems to be usable as a generic intensifier, like many English speakers might use the s-word. (I&#x27;m extremely far from fluent, though, so take my observations with a pile of salt.) reply Casteil 15 hours agoparentprevBeen using Rectangle pretty much as long as I&#x27;ve had my 14\" M1 Pro - excellent FOSS that holds up extremely well against paid alternatives! Little to no customization necessary for a very well-executed window management experience. I did change one or two of the shortcuts to be one-handed (namely Restore window, to CTRL+OPT+R), but overall the shortcuts seem really well thought out.I might have to grab the Pro version because they really deserve the support.I like it so much that I wish I had the same muscle-memory for similar shortcuts on Windows (I believe PowerToys FancyZones can be set up to snap a window to a given zone or zones with hotkeys, but it&#x27;s not something I&#x27;ve dug into yet). reply nyadesu 17 hours agoparentprevIt&#x27;s amazing, makes MacOS usable when using big displays, I tried to use Yabai on my work macbook but couldn&#x27;t install it due to SIP and work environment restrictions reply bartvk 15 hours agorootparentYou don‚Äôt need to disable SIP reply uberduper 16 hours agoparentprevRectangle and Spectacle before it are really great for managing window placement and sizing. I wouldn&#x27;t use a mac without it.But I don&#x27;t use a mac anymore. I use linux desktops and there&#x27;s just nothing quite like Rectangle. For a long while, I used a tweaked fork of pygrid. These days I use wayland compositors and don&#x27;t have a good option. I&#x27;ve been meaning to hack something together to work with floating windows on Hyprland but I never get around to it.Someday I&#x27;d like to write something that does placement and sizing like Rectangle but snaps adjacent window edges together. Like if I have a bottom left window that&#x27;s 1&#x2F;3rd screen width and assign a new window to bottom right 1&#x2F;3rd width, the bottom left would get resized to 2&#x2F;3rd wide to fill the void. If I place a new window at bottom middle 1&#x2F;3rd, the bottom left would get resized back to 1&#x2F;3rd. If I resized bottom middle to 1&#x2F;2 width, then the bottom left and right would each be resized to 1&#x2F;4 width.Hyprland has a plugin system that I believe would let me do this (someone has created an i3-like tiling plugin) but I don&#x27;t have the skills or motivation to learn c++. reply wharvle 15 hours agorootparentI still just use Spectacle. Still works, has never in years given me any trouble whatsoever‚Äîzero jank, never crashes, just works‚Äîso haven‚Äôt bothered to switch. Being able to get its behavior with same key-combos on any other OS I might seriously use, with similar perfect levels of stability and reliability, is now table-stakes for me. reply poidos 16 hours agorootparentprevIf you‚Äôre willing to use GNOME (and, I believe, KDE Plasma) you can set up keybindings that are a reasonable approximation to Rectangle. That‚Äôs what I did before i3 or on machines that can‚Äôt have i3 for odd reasons. reply seemaze 13 hours agoparentprevThis is the feature that makes rectangle for me. 95% of my use case is just cycling windows sizes (1&#x2F;3 - 1&#x2F;2 - 2&#x2F;3) snapped to either side of my screen with &#x27;[&#x27; and &#x27;]&#x27; as shortcuts. Works wonderfully on both my ultrawide and 13\" mba screen. reply erybodyknows 13 hours agoparentprevI miss rectangle the most after switching from macOS to Linux. Ironically I end up using the mouse more on Linux because I haven‚Äôt found a similar solution for window sizing keyboard shortcuts. reply gigatexal 13 hours agorootparentsway, i3, bswm, awesomewm, so many tiling window managers on Linux that you don&#x27;t need the mouse for. There&#x27;s even tiling making it&#x27;s way into Gnome and KDE with the former having some good extensions and the latter just baking it in and getting better with every release. reply extr 15 hours agoparentprevOne thing I really don&#x27;t like about rectangle (and maybe MacOS in general) is that you can&#x27;t easily multi-key hotkeys with regular keys. So for example CMD + Right Arrow + Up Arrow for upper-right tiling. This is simple&#x2F;trivial with PowerToys on windows. But you can use Karabiner Elements to create a simulation of this using keypress delays. I use Karabiner + BetterTouchTools to accomplish the same functionality since I&#x27;m already running it for other reasons. reply dev_tty01 15 hours agorootparentDoesn&#x27;t this always force delays? If I have CMD + Right Arrow assigned to some other function, the system will have to wait to make sure the Up Arrow isn&#x27;t coming soon before dispatching the Right Arrow function. Of course we want the delay short, but that makes the multi-key shortcut less reliable.I haven&#x27;t tried it, but the tradeoffs don&#x27;t sound great to me. Only allowing multiple modifier keys avoids all of these timing issues. You seem to be comfortable with it, but I think most users would trigger a lot of spurious inputs. reply extr 9 hours agorootparentIt does force delays and your concerns are legitimate. All I will say is that in practice I&#x27;ve never actually noticed any kind of conflict or problem when the delay is on the order of 500ms. reply saagarjha 11 hours agorootparentprevIt‚Äôs not really an issue. I have one of these hot keys set up and what it does is when I hit ‚åò‚Üí it will tile in the right half and when I go to hit ‚Üë it will then shrink to the upper quarter. reply hans-dampf 18 hours agoparentprev+1 for rectangle. was using spectacle before. reply the_svd_doctor 17 hours agoparentprevFantastic app. Thanks for sharing. Already love it after 2 minutes. reply jbverschoor 9 hours agoparentprevRaycast does the same, but I don&#x27;t like it.I like to have margins between my windows, so almost all window tiling &#x2F; snapping managers just make me feel cramped. reply aaronkjones 16 hours agoparentprevThis sounds similar to BetterSnapTool, which I have been using happily for several years. reply yanzhang0219 9 hours agoparentprevSecond this! I am using rectangle pro and it&#x27;s fantastic. reply ar_lan 13 hours agoparentprev> It fits my needs well enough without requiring disabling SIP.This is not a requirement for yabai. reply Icathian 17 hours agoparentprevOn the occasions I&#x27;m using MacOS, this is what I reach for too. It works quite well, I agree. reply teaearlgraycold 18 hours agoparentprevI also use it and the free version is plenty for my needs reply Erratic6576 17 hours agorootparentRectangle is free and open source software, there are no strings attached that I know of reply Bagged2347 16 hours agorootparentThere is pro version you can buy if you want to support the dev and get a few extra features.https:&#x2F;&#x2F;rectangleapp.com&#x2F;proRectangle makes much more sense to me on Mac than a tiling window manager like yabai. I tried to use it but it just felt kludgy and I went back to Rectangle. Suits my needs just fine. reply sergiomattei 17 hours agoparentprevCool thing: you can set a window gap too.Discovered the setting this week, my desktop looks so aesthetic now. reply rewgs 14 hours agoprev [‚Äì] I&#x27;ve tried Yabai multiple times but always bounced off. It&#x27;s just a little too buggy, a little too opinionated. I&#x27;ve never found a window management system I&#x27;ve actually truly liked. On Linux, I&#x27;ve tried em all, and though in theory I love tiling window managers, but for me automatic tiling with the option to go floating on a per-window basis just ultimately ends up annoying me -- I prefer the reverse. On Windows, PowerToys&#x2F;FancyZones is as close to my preferred method as possible, but then I&#x27;m, you know, using Windows.This past week I&#x27;ve come up with what is I think my perfect, dream solution:- Caps Lock is tapped for Escape, held for Cmd+Ctrl+Opt (i.e. \"Hyper\" or whatever). I&#x27;m a Neovim user, so Caps Lock has been mapped to Escape forever already.- Tapping both Shift keys simultaneously engages Caps Lock for the rare times in which I actually need it.- Window snapping is handled entirely with Hammerspoon. I have commands for absolute positions (e.g. half, one third, two thirds, quarters), but also can define grids and call them with key commands, and then use another command to snap all windows or just the focused window to the nearest grid box.It&#x27;s all defined with code, and thus lives with my dotfiles, and is just a `brew install` and symlink away from totally setting up on a brand new machine.Also, I set my tmux prefix to Control+p, which I map to Command in my iTerm profile. So, opening up a new tmux \"window\" (i.e. tab) is Command+t. Navigating to the next \"window\"&#x2F;tab is Control+Tab. And so on. Basically it just feels like a browser. This way, whether I switch terminals, or am ssh-ing into a machine, as long as I have my tmux config, those key commands are set.It&#x27;s an amazing setup, can&#x27;t recommend it enough. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Yabai is a window management utility for Mac that offers users control over windows, spaces, and displays with a command line interface and customizable keyboard shortcuts.",
      "It primarily focuses on tiling window management to enhance concentration and productivity.",
      "Yabai has additional features like focus-follows-mouse and the ability to create spaces beyond the default limit.",
      "Specific operating system versions, accessibility API, and screen recording permissions are required to use Yabai.",
      "Certain system settings and code signing requirements need to be met for Yabai to function properly.",
      "Yabai is released under the MIT License, and users are advised to use it at their own risk."
    ],
    "commentSummary": [
      "Discussion focused on different window management tools for macOS, such as Yabai, Rectangle, Amethyst, Hammerspoon, and more.",
      "Users share their experiences, preferences, and recommendations regarding window management and tiling.",
      "The discussion covers features like fullscreen mode, hotkeys, and multi-monitor support, with some users expressing satisfaction with specific tools while others suggest alternatives and mention drawbacks."
    ],
    "points": 276,
    "commentCount": 196,
    "retryCount": 0,
    "time": 1701354735
  },
  {
    "id": 38477197,
    "title": "Accelerating Generative AI with PyTorch: Faster Text Generation with GPU Optimization",
    "originLink": "https://pytorch.org/blog/accelerating-generative-ai-2/",
    "originBody": "by Team PyTorch This post is the second part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In this blog we‚Äôll focus on LLM optimization. Over the past year, generative AI use cases have exploded in popularity. Text generation has been one particularly popular area, with lots of innovation among open-source projects such as llama.cpp, vLLM, and MLC-LLM. While these projects are performant, they often come with tradeoffs in ease of use, such as requiring model conversion to specific formats or building and shipping new dependencies. This begs the question: how fast can we run transformer inference with only pure, native PyTorch? As announced during our recent PyTorch Developer Conference, the PyTorch team wrote a from-scratch LLM almost 10x faster than baseline, with no loss of accuracy, all using native PyTorch optimizations. We leverage a breadth of optimizations including: Torch.compile: A compiler for PyTorch models GPU quantization: Accelerate models with reduced precision operations Speculative Decoding: Accelerate LLMs using a small ‚Äúdraft‚Äù model to predict large ‚Äútarget‚Äù model‚Äôs output Tensor Parallelism: Accelerate models by running them across multiple devices. And, even better, we can do it in less than 1000 lines of native PyTorch code. If this excites you enough to jump straight into the code, check it out at https://github.com/pytorch-labs/gpt-fast! Note: We will be focusing on latency (i.e. batch size=1) for all of these benchmarks. Unless otherwise specified, all benchmarks are run on an A100-80GB, power limited to 330W. Starting Point (25.5 tok/s) Let‚Äôs start off with an extremely basic and simple implementation. Sadly, this does not perform very well. But why? Looking at a trace reveals the answer - it‚Äôs heavily CPU overhead bound! What this means is that our CPU is not able to tell the GPU what to do fast enough for the GPU to be fully utilized. Imagine the GPU as this super massive factory with a ridiculous amount of compute available. Then, imagine the CPU as some messenger shuttling instructions back and forth to the GPU. Remember, in large scale deep learning systems, the GPU is responsible for doing 100% of the work! In such systems, the only role of the CPU is to tell the GPU what work it should be doing. So, the CPU runs over and tells the GPU to do an ‚Äúadd‚Äù, but by the time the CPU can give the GPU another chunk of work, the GPU has long finished the previous chunk of work. Despite the fact that the GPU needs to perform thousands of computations while the CPU only needs to do orchestration work, this is surprisingly common! There‚Äôs a variety of reasons for this, ranging from the fact that the CPU is likely running some single-threaded Python to the fact that GPUs are just incredibly fast nowadays. Regardless of the reason, we now find ourselves in the overhead-bound regime. So, what can we do? One, we could rewrite our implementation in C++, perhaps even eschew frameworks entirely and write raw CUDA. Or‚Ä¶. we could just send more work to the GPU at once. By just sending a massive chunk of work at once, we can keep our GPU busy! Although during training, this may just be accomplished by increasing your batch size, how do we do this during inference? Enter torch.compile. Step 1: Reducing CPU overhead through torch.compile and a static kv-cache (107.0 tok/s) Torch.compile allows us to capture a larger region into a single compiled region, and particularly when run with mode=‚Äùreduce-overhead‚Äù, is very effective at reducing CPU overhead. Here, we also specify fullgraph=True, which validates that there are no ‚Äúgraph breaks‚Äù in your model (i.e. portions that torch.compile cannot compile). In other words, it ensures that torch.compile is running to its fullest potential. To apply it, we simply wrap a function (or a module) with it. torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True) However, there are a couple of nuances here that make it somewhat nontrivial for folks to get significant performance boosts from applying torch.compile to text generation. The first obstacle is the kv-cache. The kv-cache is an inference-time optimization that caches the activations computed for the previous tokens (see here for a more in-depth explanation). However, as we generate more tokens, the ‚Äúlogical length‚Äù of the kv-cache grows. This is problematic for two reasons. One is that reallocating (and copying!) the kv-cache every time the cache grows is simply expensive. The other one is that this dynamism makes it harder to reduce the overhead, as we are no longer able to leverage approaches like cudagraphs. To resolve this, we use a ‚Äústatic‚Äù kv-cache, which means that we statically allocate the maximum size of the kv-cache, and then mask out the unused values in the attention portion of the computation. The second obstacle is the prefill phase. Transformer text generation is best thought of as a two phase process: 1. The prefill where the entire prompt is processed, and 2. Decoding where each token is generated autoregressively. Although decoding can be made entirely static once the kv-cache is made static, the prefill stage still requires significantly more dynamism, due to having a variable prompt length. Thus, we actually need to compile the two stages with separate compilation strategies. Although these details are a bit tricky, the actual implementation is not very difficult at all (see gpt-fast)! And the performance boost is dramatic. All of a sudden, our performance improves by more than 4x! Such performance gains are often common when one‚Äôs workload is overhead bound. Sidenote: How is torch.compile helping? It is worth disentangling how exactly torch.compile is improving performance. There‚Äôs 2 main factors leading to torch.compile‚Äôs performance. The first factor, like mentioned above, is overhead reduction. Torch.compile is able to reduce overhead through a variety of optimizations, but one of the most effective ones is called CUDAGraphs. Although torch.compile applies this automatically for you when ‚Äúreduce-overhead‚Äù is set, saving the extra work and code you need to write when doing this yourself manually without torch.compile. The second factor, however, is that torch.compile simply generates faster kernels. In the decoding benchmark above, torch.compile actually generates every single kernel from scratch, including both the matrix multiplications and the attention! And even cooler, these kernels are actually faster than the built in alternatives (CuBLAS and FlashAttention2)! This may sound implausible to many of you, considering how hard it is to write efficient matrix multiplication/attention kernels, and how much manpower has been put into CuBLAS and FlashAttention. The key here, however, is that transformer decoding has very unusual computational properties. In particular, because of the KV-cache, for BS=1 every single matrix multiplication in a transformer is actually a matrix vector multiplication. This means that the computations are completely memory-bandwidth bound, and as such, are well within the range of compilers to automatically generate. And in fact, when we benchmark torch.compile‚Äôs matrix-vector multiplications against CuBLAS, we find that torch.compile‚Äôs kernels are actually quite a bit faster! Step 2: Alleviating memory bandwidth bottleneck through int8 weight-only quantization (157.4 tok/s) So, given that we‚Äôve already seen massive speedups from applying torch.compile, is it possible to do even better? One way to think about this problem is to compute how close we are to the theoretical peak. In this case, the largest bottleneck is the cost of loading the weights from GPU global memory to registers. In other words, each forward pass requires us to ‚Äútouch‚Äù every single parameter on the GPU. So, how fast can we theoretically ‚Äútouch‚Äù every single parameter in a model? To measure this, we can use Model Bandwidth Utilization (MBU). This measures what percentage of our memory bandwidth we‚Äôre able to use during inference. Computing it is pretty simple. We simply take the total size of our model (# params * bytes per param) and multiply it by the number of inferences we can do per second. Then, we divide this by the peak bandwidth of the GPU to get our MBU. For example, for our above case, we have a 7B parameter model. Each parameter is stored in fp16 (2 bytes per parameter), and we achieved 107 tokens/s. Finally, our A100-80GB has a theoretical 2 TB/s of memory bandwidth. Putting this all together, we get **72% MBU! **This is quite good, considering that even just copying memory struggles to break 85%. But‚Ä¶ it does mean that we‚Äôre pretty close to the theoretical limit here, and that we‚Äôre clearly bottlenecked on just loading our weights from memory. It doesn‚Äôt matter what we do - without changing the problem statement in some manner, we might only be able to eek out another 10% in performance. Let‚Äôs take another look at the above equation. We can‚Äôt really change the number of parameters in our model. We can‚Äôt really change the memory bandwidth of our GPU (well, without paying more money). But, we can change how many bytes each parameter is stored in! Thus, we arrive at our next technique - int8 quantization. The idea here is simple. If loading our weights from memory is our main bottleneck, why don‚Äôt we just make the weights smaller? Note that this is quantizing only the weights - the computation itself is still done in bf16. This makes this form of quantization easy to apply with very little to no accuracy degradation. Moreover, torch.compile can also easily generate efficient code for int8 quantization. Let‚Äôs look again at the above benchmark, this time with int8 weight-only quantization included. As you can see from the dark blue line (torch.compile + int8), there is a significant performance improvement when using torch.compile + int8 weight-only quantization! Moreover, the light-blue line (no torch.compile + int8) is actually much worse than even the fp16 performance! This is because in order to take advantage of the perf benefits of int8 quantization, we need the kernels to be fused. This shows one of the benefits of torch.compile - these kernels can be automatically generated for the user! Applying int8 quantization to our model, we see a nice 50% performance improvement, bringing us up to 157.4 tokens/s! Step 3: Reframing the problem using speculative decoding Even after using techniques like quantization, we‚Äôre still faced with another problem. In order to generate 100 tokens, we must load our weights 100 times. Even if the weights are quantized, we still must load our weights over and over, once for each token we generate! Is there any way around this? At first glance, the answer might seem like no - there‚Äôs a strict serial dependency in our autoregressive generation. However, as it turns out, by utilizing speculative decoding, we‚Äôre able to break this strict serial dependency and obtain speedups! Imagine you had a senior engineer (called Verity), who makes the right technical decisions but is rather slow at writing code. However, you also have a junior engineer (called Drake), who doesn‚Äôt always make the right technical decisions but can write code much faster (and cheaper!) than Verity. How can we take advantage of Drake (the junior engineer) to write code faster while ensuring that we are still making the right technical decisions? First, Drake goes through the labor-intensive process of writing the code, making technical decisions along the way. Next, we give the code to Verity to review. Upon reviewing the code, Verity might decide that the first 3 technical decisions Drake made are correct, but the last 2 need to be redone. So, Drake goes back, throws away his last 2 decisions, and restarts coding from there. Notably, although Verity (the senior engineer) has only looked at the code once, we are able to generate 3 pieces of validated code identical to what she would have written! Thus, assuming Verity is able to review the code faster than it would have taken her to write those 3 pieces herself, this approach comes out ahead. In the context of transformer inference, Verity would be played by the role of the larger model whose outputs we want for our task, called the verifier model. Similarly, Drake would be played by a smaller model that‚Äôs able to generate text much faster than the larger model, called the draft model. So, we would generate 8 tokens using the draft model, and then process all eight tokens in parallel using the verifier model, throwing out the ones that don‚Äôt match. Like mentioned above, one crucial property of speculative decoding is that it does not change the quality of the output. As long as the time it takes for generating the tokens using the draft model + verifying the tokens is less than it would have taken to generate those tokens, we come out ahead. One of the great things about doing this all in native PyTorch is that this technique is actually really easy to implement! Here‚Äôs the entirety of the implementation, in about 50 lines of native PyTorch. Although speculative decoding guarantees that we have mathematically identical results compared to regular generation, it does have the property that the runtime performance varies depending on the generated text, as well as how aligned the draft and verifier model are. For example, when running CodeLlama-34B + CodeLlama-7B, we‚Äôre able to obtain a 2x boost in tokens/s for generating code. On the other hand, when using Llama-7B + TinyLlama-1B, we‚Äôre only able to obtain about a 1.3x boost in tokens/s. Sidenote: Running this on AMD Like mentioned above, every single kernel in decoding is generated from scratch by torch.compile, and is converted into OpenAI Triton. As AMD has a torch.compile backend (and also a Triton backend), we can simply go through all of the optimizations above‚Ä¶ but on an AMD GPU! With int8 quantization, we‚Äôre able to achieve 102.5 tokens/s with one GCD (i.e. one half) of a MI250x! Step 4: Reducing the size of the weights even more with int4 quantization and GPTQ (202.1 tok/s) Of course, if reducing the weights down from 16 bits to 8 bits allows for speedups by reducing the number of bytes we need to load, reducing the weights down to 4 bits would result in even larger speedups! Unfortunately, when reducing weights down to 4-bits, the accuracy of the model starts to become a much larger concern. From our preliminary evals, we see that although using int8 weight-only quantization has no perceptible accuracy degradation, using int4 weight-only quantization does. There are 2 main tricks we can use to limit the accuracy degradation of int4 quantization. The first one is to have a more granular scaling factor. One way to think about the scaling factor is that when we have a quantized tensor representation, it is on a sliding scale between a floating point tensor (each value has a scaling factor) and an integer tensor (no values have a scaling factor). For example, with int8 quantization, we had one scaling factor per row. If we want higher accuracy, however, we can change that to ‚Äúone scaling factor per 32 elements‚Äù. We choose a group size of 32 to minimize accuracy degradation, and this is also a common choice among the community. The other one is to use a more advanced quantization strategy than simply rounding the weights. For example, approaches like GPTQ leverage example data in order to calibrate the weights more accurately. In this case, we prototype an implementation of GPTQ in the repository based off of PyTorch‚Äôs recently released torch.export. In addition, we need kernels that fuse int4 dequantize with the matrix vector multiplication. In this case, torch.compile is unfortunately not able to generate these kernels from scratch, so we leverage some handwritten CUDA kernels in PyTorch. These techniques require some additional work, but putting them all together results in even better performance! Step 5: Combining everything together (244.7 tok/s) Finally, we can compose all of the techniques together to achieve even better performance! Step 6: Using Tensor Parallelism So far, we‚Äôve been restricting ourselves to minimizing latency while on a single GPU. In many settings, however, we have access to multiple GPUs. This allows us to improve our latency further! To get an intuitive sense of why this would allow us to improve our latency, let‚Äôs take a look at the prior equation for MBU, particularly the denominator. Running on multiple GPUs gives us access to more memory bandwidth, and thus, higher potential performance. As for which parallelism strategy to pick, note that in order to reduce our latency for one example, we need to be able to leverage our memory bandwidth across more devices simultaneously. This means that we need to split the processing of one token across multiple devices. In other words, we need to use tensor parallelism. Luckily, PyTorch also provides low-level tools for tensor-parallelism that compose with torch.compile. We are also working on higher-level APIs for expressing tensor parallelism, stay tuned for those! However, even without a higher-level API, it‚Äôs actually still quite easy to add tensor parallelism. Our implementation comes in at 150 lines of code, and doesn‚Äôt require any model changes. We are still able to take advantage of all the optimizations mentioned previously, which all can continue to compose with tensor parallelism. Combining these together, we‚Äôre able to serve Llama-70B at 55 tokens/s with int8 quantization! Conclusion Let‚Äôs take a look at what we‚Äôre able to accomplish. Simplicity: Ignoring quantization, model.py (244 LOC) + generate.py (371 LOC) + tp.py (151 LOC) comes out to 766 LOC to implement fast inference + speculative decoding + tensor-parallelism. Performance: With Llama-7B, we‚Äôre able to use compile + int4 quant + speculative decoding to reach 241 tok/s. With llama-70B, we‚Äôre able to also throw in tensor-parallelism to reach 80 tok/s. These are both close to or surpassing SOTA performance numbers! PyTorch has always allowed for simplicity, ease of use, and flexibility. However, with torch.compile, we can throw in performance as well. The code can be found here: https://github.com/pytorch-labs/gpt-fast. We hope that the community finds it useful. Our goal with this repo is not to provide another library or framework for people to import. Instead, we encourage users to copy-paste, fork, and modify the code in the repo. Acknowledgements We would like to thank the vibrant open source community for their continual support of scaling LLMs, including: Lightning AI for supporting pytorch and work in flash attention, int8 quantization, and LoRA fine-tuning. GGML for driving forward fast, on device inference of LLMs Andrej Karpathy for spearheading simple, interpretable and fast LLM implementations MLC-LLM for pushing 4-bit quantization performance on heterogenous hardware",
    "commentLink": "https://news.ycombinator.com/item?id=38477197",
    "commentBody": "Accelerating Generative AI with PyTorch II: GPT, FastHacker NewspastloginAccelerating Generative AI with PyTorch II: GPT, Fast (pytorch.org) 267 points by polyrand 15 hours ago| hidepastfavorite60 comments chillee 14 hours agoHey, author of the blog post here. It&#x27;s mentioned in the blog post, but one of the intentions of this repo is that it&#x27;s more of a \"tutorial\" than it is a library&#x2F;framework. My hope is that people will copy-paste and modify it for their own needs :)Code can also be found here: https:&#x2F;&#x2F;github.com&#x2F;pytorch-labs&#x2F;gpt-fastAnd a twitter thread summary here: https:&#x2F;&#x2F;twitter.com&#x2F;cHHillee&#x2F;status&#x2F;1730293330213531844 reply smith7018 14 hours agoparentGreat work! Do you know if it&#x27;s possible to port this over to pytorch&#x27;s Apple Silicon&#x2F;MPS support? reply chillee 10 hours agorootparentUnfortunately it&#x27;s a little bit tricky today. The main issue is that we rely heavily on torch.compile + Triton for performance in this repo, and there isn&#x27;t an Apple Silicon backend either for torch.compile or Triton.For example, there&#x27;s an AMD backend for Triton (and it&#x27;s also integrated into torch.compile), which is why we can mostly do the same optimizations on Nvidia and AMD GPUs.Ideally, there&#x27;d be an Apple Silicon backend for Triton, and then this repo would mostly work out of the box :) reply Dowwie 13 hours agoparentprevWhat kind of workstation would you build&#x2F;buy for local GPT development with a budget of $3000? Is remote dev a viable alternative to local workstations? reply ftufek 13 hours agorootparentLocal workstation is much cheaper in the long run.Even ignoring that, most of the development is running experiments. You&#x27;re gonna be hesitant to run lots of experiments if they each cost money whereas when you pay upfront for the hardware, you&#x27;re gonna have the incentive to fully utilize it with lots of experiments.I&#x27;d go with rtx 4090 and deal with memory limitation through software tricks. It&#x27;s an underrated card that&#x27;s as performant as cards that are magnitude pricier. It&#x27;s great way to get started with that budget. reply biddit 12 hours agorootparentI agree with you but right now RTX 4090 cards are pushing $2000, which doesn&#x27;t leave much budget left. I&#x27;d suggest picking up a used 3090 card from eBay, which are currently around $800. This will still give 24gb of VRAM like the 4090. reply RockyMcNuts 9 hours agorootparenti&#x27;ve seen some blog posts saying if you buy a used 3090 that has been used for bitcoin mining then there is a risk of thermal throttling because the thermal paste on the vram is not great and worse if it was run hot for a long time.any recommendations on how to buy one? e.g. 24GB model, any particular model to run LLMs? what is the biggest baddest LLM you can run on a single card?have been thinking about it but was sticking with cloud&#x2F;colab for experiments so far. reply dharmab 9 hours agorootparentThe good deals are gonna be on local ads. Facebook Marketplace in most of the US. reply two_in_one 8 hours agorootparentprevI remember videos (on youtube likely) of thermal paste replacement, that was upgrade to stock card. So, average person should be able to do it. It&#x27;ll cost a few $$ for the paste. I would go with local workstation, then don&#x27;t have to think much about while running stable diffusion. Plus, if it&#x27;s used from ebay, prices cannot go much lower, you&#x27;ll get something back at the end. Also, for image things training dataset can be quite big for network transfers. reply icelancer 12 hours agorootparentprevStrong endorse here. I pick up used RTX 3090s from Facebook Marketplace and eBay at $800 maximum. Can usually find them locally for $700-750, and typically can test them too, which is fine (though I&#x27;ve had no issues yet). reply Philpax 12 hours agorootparentprevDepending on what you&#x27;re doing, 2x used 3090s are the same price and offer you more VRAM. That&#x27;s what I&#x27;m planning on doing, in any case - being able to run 70B LLMs entirely on the GPU is more useful than being able to run 34B faster. reply icelancer 12 hours agorootparentYeah multiple 3090s is the best budget way to go for sure. Also older server boards with tons of PCIe lanes if you can swing rack mounted hardware and have some technical skills. reply biddit 12 hours agorootparentprevAgreed. I recently completed a new build with two 3090 GPUs and really appreciate being able to run 70b models. reply Dowwie 12 hours agorootparentwhich cpu did you go with? reply biddit 11 hours agorootparenti7-14700kz790 chipset w&#x2F; mobo that supports x8&#x2F;x8 bifurcation96gb ddr5 @5600mhz replyleobg 13 hours agorootparentprevNot OP, but I asked myself that same question two years ago. Then I looked at the energy prices in Germany and knew I had no chance against cloud GPUs. Maybe you live in a country with lower energy prices, like Bermuda (or any other country on earth), in which case this may not be as important to you. A side benefit of going cloud that you can pick and choose the right GPU for whatever project you‚Äôre working on, and you‚Äôre really just paying while you‚Äôre running them. Also, no hardware or Cuda drivers that may divert your attention. reply woodson 13 hours agorootparentprevI‚Äôd go with a remote dev solution. Training&#x2F;finetuning of large models requires much more resources anyway, so the GPUs in the local machine would be unused most of the time. reply modeless 12 hours agorootparentprevI got a 13900k + 4090 workstation for ~$3500. But I hear what people are doing is getting 2x (or more) 3090s instead, because they are cheap used, and having more VRAM and VRAM bandwidth is the important thing at the moment, even if it is split between cards.I&#x27;m happy with my 4090 though. Dealing with splitting between GPUs sounds like a chore and also I like the gaming abilities of the 4090. reply icelancer 12 hours agorootparentprevI would do remote dev using vast.ai and other cheap cloud computing resources to ensure you want to do this and have utility for it, then build your own. 3090s are typically the most budget friendly, and if you have any IT chops (and tolerance for noise), then server rack-mounted hardware, PSUs, and riser cables tend to be the most efficient with tons of PCIe lanes (which is a hidden issue people have with consumer-grade gaming PCs as they scale). reply dharmab 12 hours agorootparentprevI&#x27;m using an AMD 6900XT with ROCm and it&#x27;s fast enough ti be usable, for a fraction of the price of a 3090 or 4090. reply sp332 7 hours agorootparentprevJust make sure you‚Äôre buying something with a warranty. Hardware failures during training are quite common. reply jokethrowaway 1 hour agorootparentprevI made a custom pc in May with a 4090 for playing around. It&#x27;s good and fast but for production workload or serious training you want more ram. It was less than ~3k in components (~2k after considering removing vat and expending it &#x2F; depreciating it in a limited company)If you also want a Mac consider an m3 MacBook with maxed ram. reply LoganDark 6 hours agorootparentprevA regular gaming PC will do, about a grand, then slot in a 3090 or something off eBay. Congratulations, you&#x27;re a grand under-budget, but already have a viable development machine. reply toxik 3 hours agoparentprevFantastic article, great job. One small note: eke and eek are not the same word. reply wolftickets 12 hours agoparentprevJust wanted to share, the charts and gifs are exceptionally well done. Informative, concise, and easy to read. reply chillee 11 hours agorootparentThanks! I&#x27;ve also written a couple other things along a similar vein you might like at https:&#x2F;&#x2F;horace.io&#x2F;writing.html (particularly https:&#x2F;&#x2F;horace.io&#x2F;brrr_intro.html) and also some of the things I&#x27;ve tweeted: https:&#x2F;&#x2F;twitter.com&#x2F;cHHillee&#x2F;highlights reply buildbot 14 hours agoparentprevGreat work and a really useful resource! Comprehensive guides on improving PyTorch performance are pretty hard to come by, and I learned a couple new tricks from this! reply ilaksh 14 hours agoparentprevWhat GPU was used when testing this?Is this faster than HuggingFace&#x27;s Text Generation inference container? reply chillee 14 hours agorootparentWe used an A100-80GB GPU. We didn&#x27;t compare explicitly to Huggingface TGI but I think you should be able to compare the tokens&#x2F;s achieved.One note is that this release is optimized for latency, while I think HF TGI might be more optimized for throughput. reply deepGem 6 hours agoprev\"This may sound implausible to many of you, considering how hard it is to write efficient matrix multiplication&#x2F;attention kernels, and how much manpower has been put into CuBLAS and FlashAttention. The key here, however, is that transformer decoding has very unusual computational properties. In particular, because of the KV-cache, for BS=1 every single matrix multiplication in a transformer is actually a matrix vector multiplication.\"How did they unlock this key ? In retrospect it seems so simple, but without the KV-cache this possibility would not have emerged at all. Hats off ! reply cztomsik 34 minutes agoparentBS=1 also means you can only decode one token. This is not always what you want, and then you&#x27;re back to matrix-matrix multiply again. Most of the from-scratch llama implementations start with matrix-vector because it&#x27;s simpler & easier. But eventually, you will want to serve multiple users, or compute multiple beams together... reply kosolam 1 hour agoprevHey we are exactly dealing now with all that is related to the performance of running models from HF. Yesterday we ran vicuna-13b-q8-gguf using llamacpp on a vast.ai A40 45GB VRAM It gave us 4 tokens&#x2F;s generation rate. This seems a bit slow for that GPU and a 13b model. Does anyone know where the problem could be in llamacpp, gpu, the model, something else?Also‚Ä¶ where are all the people like us that work on applications on top of HF models congregate? reply andy99 14 hours agoprevThis is a great article. Regarding> While these projects are performant, they often come with tradeoffs in ease of use, such as requiring model conversion to specific formats or building and shipping new dependencies.I think it should be acknowledged that (at least IMO) pytorch model formats are not very portable and this is a big part of the problem. It would be nice to see industry move towards a better format (gguf?) that can easily be ported between frameworks and not leave you stuck using torch to load it. Likewise, pytorch is a massive dependency to include with a project, especially for simple inference, so while other projects have new dependencies, they can often be a lot lighter than for a pytorch model, again particularly for inference code. reply chillee 14 hours agoparentYeah, for sure. I think for deployment purposes, many times these model conversions are necessary (such as if you don&#x27;t want to use Python).However, I do think these model conversions are often a significant pain for users.So, in some sense, the goal here is to show that the performance component and the \"convert your model for deployment\" component can be disentangled.We also have work on allowing you to \"export\" an AOT-compiled version of your model with torch.compile, and that should allow you to deploy your models to run in other settings. reply andy99 13 hours agorootparentThanks for the reply. \"show that the performance component and the \"convert your model for deployment\" component can be disentangled\" makes sense.Also, I liked the part of the article about torch.compile producing faster matrix-vector multiplication than cublas. I&#x27;ve seen the same thing on CPU, that it&#x27;s way faster to just write and manually optimize a loop over a bunch of dot products than it is to use BLAS routines because of how simple the \"matmul\" actually is. I don&#x27;t know how widely known that is. reply sva_ 10 hours agoprevOfftopic, but what software do they use to create the benchmark flamegraphs? I&#x27;ve been using cProfile with snakeviz, but am curious to try alternatives. reply chillee 10 hours agoparentAssuming you&#x27;re talking about the one here: https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;accelerating-generative-ai-2&#x2F;#start...it&#x27;s just the pytorch profiler + chrome profiler (chrome:&#x2F;&#x2F;tracing) reply glandium 9 hours agoparentprevThe screenshot in the post looks like https:&#x2F;&#x2F;www.chromium.org&#x2F;developers&#x2F;how-tos&#x2F;trace-event-prof... reply syrusakbary 9 hours agoprevI&#x27;d love to see how this compares against Llama.cpp on speed. Do anyone any benchmarks that compare PyTorch.compile vs Llama.cpp? reply chillee 3 hours agoparentThis person claims to have compared llama.cpp against gpt-fast on a 4090, and found gpt-fast about 20% faster.https:&#x2F;&#x2F;twitter.com&#x2F;zeuxcg&#x2F;status&#x2F;1730450360895242355 reply claytonjy 11 hours agoprevOne of the notable tricks the various LLM serving frameworks provide is a special approaches to batching, e g. continuous, persistent, or in-flight batching depending on the inference framework. At some level they each allow you to start a new generation while in the middle of one or more previous generations.Is that possible with \"just\" pytorch? Could it be added to gpt-fast? reply chillee 11 hours agoparentYeah it&#x27;s certainly possible, but it&#x27;s not the focus of this implementation, which is more latency focused (so BS=1). reply lmeyerov 1 hour agorootparentyeah i&#x27;m curious how this would stack up to vllm in a batch settinglong-term, bodes well as both methodologies should be combineable, just curious for current point-in-time wrt being relevant for production use reply chillee 1 hour agorootparentI wouldn&#x27;t recommend using it for a batch serving setting today. One crucial optimization for batched serving (which you need if you have a large number of requests) is continual batching, which this implementation doesn&#x27;t have. reply dnnssl2 14 hours agoprevWhat are some of the better use cases of fast inference? From my experience using ChatGPT, I don&#x27;t need it to generate faster than I can read, but waiting for code generation is painful because I&#x27;m waiting for the whole code block to format correctly, be available to copy or execute (in the case of code interpreter). Anything else fall under this pattern? reply lmeyerov 6 hours agoparentMost LLM model use shouldn&#x27;t be &#x27;raw&#x27; but as part of a smart & iterative pipeline. Ex:* reading: If you want it to do inference over a lot of context, you&#x27;ll need to do multiple inferences. If each inference is faster, you can &#x27;read&#x27; more in the same time on the same hardware* thinking: a lot of analytical approaches essentially use writing as both memory & thinking. Imagine iterative summarization, or automatically iteratively refining code until it&#x27;s rightFor louie.ai sessions, that&#x27;s meant a fascinating trade-off here when doing the above:* We can use smarter models like gpt-4 to do fewer iterations...* ... or a faster but dumber model to get more iterations in the same amount of timeIt&#x27;s entirely not obvious. For example, the humaneval leaderboard has gpt4 for code being beat by gpt 3.5 for code when run by a LATS agent: https:&#x2F;&#x2F;paperswithcode.com&#x2F;sota&#x2F;code-generation-on-humaneval . This highlights that the agent framework is the one really responsible for final result quality, so their ability to run many iterations in the same time window matters. reply rfw300 13 hours agoparentprevThe main thing is chat is just one application of LLMs. Other applications are much more latency sensitive. Imagine, for instance, an LLM-powered realtime grammar checker in an editor. reply jasonjmcghee 13 hours agoparentprevProgrammatic and multi-step use cases. If you need chain-of-thought or similar, tool use, etc. Generating data.Most use cases outside of classic chat.For example, I made an on-demand educational video project, and the slowest part was by far the content generation. RAG, TTS, Image generation, text rendering, and video processing were all a drop in the bucket, in comparison.It would be an even wider gap now, and TTS is super-realtime, and image generation can be single step. reply ClarityJones 13 hours agoparentprevPerhaps this is naive, but in my mind it can be useful for learning.- Hook LLM to VMs- Ask for code that [counts to 10]- Run code on VM- Ask different LLM to Evaluate Results.- Repeat for sufficient volume.- Train.The faster it can generate results the faster those results can be tested against the real world, e.g. a VM, users on X, other models with known accuracies. reply wedn3sday 14 hours agoparentprevOne obvious use case is that it makes per-token generation much cheaper. reply dnnssl2 14 hours agorootparentThat&#x27;s not so much a use case, but I get what you&#x27;re saying. It&#x27;s nice that you can find optimizations to shift down the pareto frontier of across the cost and latency dimension. The hard tradeoffs are for cases like inference batching where it&#x27;s cheaper and higher throughput but slower for the end consumer.What&#x27;s a good use case for an order of magnitude decrease in price per token? Web scale \"analysis\" or cleaning of unstructured data? reply dnnssl2 14 hours agoprevIf you were to serve this on a datacenter server, is the client to server roundtrip networking the slowest part of the inference? Curious if it would be faster to run this cloud GPUs on better hardware but farther compute, or locally with worse hardware. reply chillee 14 hours agoparentSurprisingly, no. And part of this is that text generation is really expensive. Unlike traditional ML inference (like with, resnets), you don&#x27;t just pass your data through your model once. You need to pass it over and over again (once for each token you generate).So, in practice, a full \"text completion request\" can often take on the order of seconds, which dwarfs the clientserver roundtrip. reply dnnssl2 14 hours agorootparentIs this still the case for sliding window attention&#x2F;streaming LLMs, where you have a fixed length attention window rather than infinitely passing in new tokens for quadratic scaling? You even get better performance due to purposely downsampling non-meaningful attention sink tokens. reply chillee 13 hours agorootparentI cover it a bit in the blog post, but unless you have a really long context length (like 32k+), your primary computational cost doesn&#x27;t come from attention but rather from loading your weights from VRAM into registers.I mean, practically speaking, completions from say, ChatGPT or Claude take seconds to finish :) reply dnnssl2 13 hours agoprevHow does one select a good candidate for the draft model in speculative decoding? I imagine that there&#x27;s some better intuition than just selecting the next parameter count down (i.e 70B -> 13B, 13B -> 7B).Also how does that interact with MoE models? Do you have a mini version of the MoE, with smaller experts? reply chillee 13 hours agoparentThis is indeed a bit of a dark art. Essentially, you want a balance between \"is significantly faster than base model\" and \"generates similar stuff to the base model\".Anecdotally, folks often seem to use say, 70B base + 7B as verifier. But I think there&#x27;s a lot of room for experimentation and improvement here.You could... say, take a 70B model and maybe just chop off the last 90% of layers and then fine-tune. Or perhaps you could use a model that&#x27;s trained to generate 8 tokens at once. Or perhaps you could just use statistical \"n-gram\" predictor. reply xmichael909 14 hours agoprevHoly hotdogs, this look amazing. So ahh. I&#x27;ll jump right to it - where can I run this online without having to do a bunch of work setting it up? I have several python projects that could take advantage of this! (; reply AmazingTurtle 14 hours agoprev240tok&#x2F;s is crazy reply brucethemoose2 13 hours agoprev [‚Äì] This is similar to exllamav2, and exllamav2&#x27;s quantization is also excellent. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The PyTorch team has optimized generative AI models for text generation using GPU quantization and tensor parallelism.",
      "These optimizations improve performance without sacrificing accuracy.",
      "The article discusses the challenges of CPU overhead in deep learning systems and introduces torch.compile as a method to reduce CPU overhead and optimize GPU usage.",
      "The use of compilation strategies, int8 quantization, and speculative decoding greatly enhance the performance of text generation tasks.",
      "Running on multiple GPUs offers benefits such as increased memory bandwidth and better performance.",
      "The code for implementation is available in a GitHub repository and the author acknowledges the support of the open-source community."
    ],
    "commentSummary": [
      "The discussion focuses on an open-source repository aimed at speeding up generative AI using PyTorch.",
      "Topics covered include hardware recommendations for GPT development, GPU choices for machine learning, optimization strategies for transformer decoding, and the advantages of fast inference.",
      "The conversation also delves into various use cases and models for text generation, as well as the potential cost savings in text generation."
    ],
    "points": 268,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1701369300
  },
  {
    "id": 38475926,
    "title": "How to Ripen and Store Avocados: Tips and Tricks",
    "originLink": "https://www.seriouseats.com/how-to-ripen-avocados-7377071",
    "originBody": "How-Tos How to Ripen and Store Avocados We tested popular avocado-ripening methods to see which ways work and which ways don't. By J. Kenji L√≥pez-Alt J. Kenji L√≥pez-Alt Culinary Consultant Kenji is the former culinary director for Serious Eats and a current culinary consultant for the site. He is also a New York Times food columnist and the author of The Food Lab: Better Home Cooking Through Science. Learn about Serious Eats' Editorial Process and Daniel Gritzer Daniel Gritzer Senior Culinary Director Daniel joined the Serious Eats culinary team in 2014 and writes recipes, equipment reviews, articles on cooking techniques. Prior to that he was a food editor at Food & Wine magazine, and the staff writer for Time Out New York's restaurant and bars section. Learn about Serious Eats' Editorial Process Published May 01, 2023 Trending Videos Serious Eats / Amanda Suarez In This Article Expand How to Shop for Avocados How to Know If an Avocado Is Ripe The Best Ways to Ripen Avocados: Tested How to Store Ripe Avocados for Later How to Prevent Avocados From Browning Avocados occupy an interesting place among commercially popular fruits. Unlike most other fruit, avocados do not ripen on the tree‚Äîthey only begin ripening off the tree after reaching full maturity. But then the race to ripeness is off and running, and, in the case of avocados, that race is relatively quick, with limited opportunities for long-term storage. These two facts put the concerns of commercial avocado growers, shippers, and vendors exactly opposite of those of the home cook. Those in the avocado industry want to find as many ways as possible to delay ripening of the picked fruit for as long as possible, buying them as much time as possible to get the avocados from the filed to your kitchen counter in good condition. Through lots of study, they've become pretty good at that‚Äîby lowering temperatures, reducing environmental oxygen and boosting carbon dioxide, shrink-wrapping or waxing the fruit, and other techniques, the avocado industry can manage to keep the fruit in its mature-but-unripe state all the way to the point of sale, despite significant shipping distances and other challenges. The home cook, on the other hand, usually wants to know how the heck to speed up ripening so they can eat the darned thing. This can present a timing challenge when shopping for and eating avocados because, to put it bluntly, perfectly ripe avocados are a pain in the ass to find. The window of time in which they are absolutely perfect‚Äîsoft and tender with no brown spots or streaks‚Äîis notoriously short. It can make planning an avocado-based party a harrowing experience. Will my avocados ripen in time for game day? What if they turn brown? Fortunately, there are a few ways to moderate the rate at which they ripen, and then there are some ways other articles on this topic recommend, but we don't. Read on for more. How to Choose a Quality Avocado When Shopping We've all seen those avocados that seem perfect on the outside, but once you cut into them, a series of deep brown stripes and striations appear. What's up with that? Unfortunately, it's not something that can be predicted or prevented. It's caused by uneven enzymatic action inside the avocado as it's developing and is exacerbated by extreme weather conditions and other seasonal, environmental, and agricultural factors. For Hass avocados, you can expect the likelihood of this phenomenon to increase starting in December and maxing out around February. (Ack, Super Bowl season!) Serious Eats / Amanda Suarez Another common flaw in some avocados are woody strings that stretch through the flesh. These strings are part of the plant's vascular strands, responsible for transporting nutrients into the fruit. They're actually always there, but they only become noticeably woody and stringy later in the season as the avocado becomes more mature. Unfortunately, just as with the brown spots, there's no way to predict which avocados will have these strands until you've cut into it, aside from knowing that the likelihood of encountering them goes up later in the growing season, with fruit harvested in the late summer and early fall (though this will vary depending on the avocado variety and growing region). How to Know If an Avocado Is Ripe While the color of the avocado's skin can change as ripening progresses, that is not a reliable way to determine perfect ripeness, since avocado skin color can vary from variety to variety and fruit to fruit. Better is to gauge ripeness by touch: Using your fingers, very gently press on the avocado near the stem end (that's where the avocado was once attached to the tree). You want to feel a slight tenderness and give. If the avocado is very firm, it's not ready; if it feels soft and mushy, it's gone too far. The Best Ways to Ripen Avocados: Tested Ripening in avocados and many other fruits is regulated by a gas called ethylene. It's produced naturally by the fruit itself and is intended to make sure that all the fruit in one area ripens at the same time. The higher the concentration of ethylene, the faster your fruit ripens. That's why you'll see instructions to leave underripe avocados or bananas in paper bags‚Äîit concentrates ethylene and causes fast ripening. Serious Eats / Amanda Suarez Left on its own, it generally takes an underripe avocado somewhere in the range of three-to-five days when stored in a paper bag at room temperature. If you need your avocados sooner, you will need to turn to other fruit. How to Ripen Avocados Quickly In side-by-side tests tests, avocados from the supermarket showing no softness at all take between three-to-five days to ripen in a brown paper bag. Throw a banana in there (an ethylene powerhouse), and you can bring that range down to two-to-three days. It's not instant, but it does give you a wider range of control over having a ripe avocado when you need it. Serious Eats / Amanda Suarez You could, for instance, buy several avocados, leaving some out in the open kitchen air, some in a brown paper bag, and yet a few more in a second brown paper bag with a banana added. By doing so, you'd stagger their ripeness-reaching days, with the banana-ripened avocados ready in a couple days, the paper bag ones a day or two after that, and the open-air ones ready right on the heels of those, assuming they all started at the same stage of un-ripeness. Can You Ripen an Avocado in the Microwave? This is one of those popular internet and social media tips that sound so good, we all just want to believe it's true. You can rapidly ripen an avocado in the microwave?! Thank you, TikTok! But we're sorry to say, having tried it ourselves, it's a terrible method. The truth is, using the microwave (or a low oven for that matter) does not ripen an avocado at all, it cooks it. Indeed, there are similarities between cooking and ripening: In both cases the fruit's flesh softens as cell walls weaken, leading to a more tender texture. On top of that, new flavors and aromas develop. But the way the texture changes, and the nature of the flavor and aroma changes are not comparable between ripening and (even very, very gentle) cooking. While a properly ripened avocado grows buttery and creamy as time passes, a microwaved one becomes slick and sweaty, and soft like a lit candle‚Äîoverly mushy in the hotter spots and still too firm in others. The flavor of a microwaved avocado, though, is by far its greatest failing. We would describe it as re-warmed, undercooked chicken, a bizarre combination that somehow manages to smell simultaneously of the crudeness of raw poultry and stink of reheated poultry. A microwaved avocado might be easier to chew, but it's not easier to swallow. How to Store Ripe Avocados for Later Once ripened, an avocado will start to produce brown spots and streaks within about two days if left at room temperature. However, refrigerating a ripened avocado can increase that window up to around five days, though avocados are susceptible to chilling damage, so the will begin to show signs of browning if left too long. The best way to guarantee perfect avocados for a Sunday night game? Buy them the Monday before, ripen them at room temperature in a brown paper bag and refrigerate them as soon as they soften. How to Prevent Avocados From Browning What about leftover avocado? Any way to keep it from browning? Oxygen is the enemy of avocados‚Äîit's what causes them to turn that unsightly brown. Plastic wrap works alright on cut avocado halves, but even plastic wrap is oxygen-permeable. In our tests, avocados didn't last more than about eight hours wrapped in plastic before visible browning occurred. The old rub-with-oil-and-place-face-down-on-an-oiled-plate works fine if you've got a perfect half of an avocado with a smooth face, but it doesn't help if you've got, say, 3/4 or 1/4 of an avocado. Serious Eats / Amanda Suarez The better short-term solution in that situation? Just submerge the sucker in water. Simply put any unused avocado pieces in a container filled with water in the fridge to keep oxidation at bay, but do note you can't hold them there for long. After several hours, a ripe avocado will begin to grow mushy where water has managed to penetrate it. As for mashed avocado like guacamole, we think the best solution, aside from preparing it right before serving, is to press a double layer of plastic wrap directly against the surface of the avocado mixture. It's not a perfect solution‚Äîthe avocado will still brown eventually‚Äîbut you can mix in a minimal amount of browning such that diners won't know, buying yourself at least a few hours before it becomes a problem. May 2023 Portions of this article were originally written by Kenji Lopez-Alt as part of the headnote to his guacamole recipe. Those sections were removed from that headnote and added to this article, with additional reporting, testing, and text by Daniel Gritzer. How-Tos The Latest Fruit Guides Food Prep Guides",
    "commentLink": "https://news.ycombinator.com/item?id=38475926",
    "commentBody": "How to ripen and store avocadosHacker NewspastloginHow to ripen and store avocados (seriouseats.com) 266 points by green-eclipse 16 hours ago| hidepastfavorite166 comments stronglikedan 14 hours agoI do the opposite. I buy a bag of green ones, and put them in the fridge. They stay green for a while in there but do start to slowly ripen. Before I put them in the fridge on day 0, I put one on the counter, and one in a brown bag. The brown bag is for day 2, the one on the counter is for day 3 (there is no day 1 after day 0, at first). On day 2, when I take the ripe one out of the bag, I take another out of the fridge and put it on the counter for day 4, and continue this process. When I take the last one out of the fridge, I have another bag ready to go in. With the right timing, this provides a perfectly ripe avocado per day.And for storing a cut avocado, the press & seal plastic wrap works wonders. The trick is to not let air touch the exposed bit. If it does, then just slicing a thin slice off the top exposes the green part again. (This also works for frozen loaves of bread that have been cut - just a thin slice off the end gets rid of the bit that was destroyed by ice.)My memoirs will include all my food storage tips for the perpetually single. reply 4death4 11 hours agoparentI find storing the avocados in the fridge causes them to brown without really ripening. I usually buy a bag of 5 and try to eat them over a 3 or 4 day period starting when the first one ripens. Fresh Direct also sells packs of 2 ripe avocados. So you can buy 2 ripe avocados and a bag of hard avocados, eat the ripe ones, and then the hard ones should be ripe enough to eat. reply kenhwang 11 hours agorootparentMy avocado tree usually drops a dozen or two avocados a week for most of the year, so I&#x27;ve had a lot of time and examples to experiment on for storage.Once they&#x27;re off the tree, ripen at room temperature for about a week. Then transfer into the fridge in an airtight container, they usually last in the fridge for another 2 weeks.Doing fridge first means they never ripen and just dry out before softening. reply gnicholas 9 hours agorootparentMy aunt has an avocado tree. Freshly-picked avocados ripen very differently than store-bought ones. They stay ripe for way longer, since they don&#x27;t have to be picked in advance and shipped. And I say this as someone who lives in CA, where we get produce on the fresher side, in general.When I visited my aunt she gave us several avocados, and I thought, \"we&#x27;re never going to be able to eat these before they go bad\". I was so wrong ‚Äî they stayed perfect for well over a week. reply hutzlibu 6 hours agorootparent\"They stay ripe for way longer, since they don&#x27;t have to be picked in advance and shipped\"... and since they are treated with all kinds of chemicals who mess with the ripening. reply freshpots 6 hours agorootparentWhich kinds of chemicals? Why multiple?Or are they used for other purposes but affect ripening? reply markdown 8 hours agorootparentprevDo you water it with unicorn tears? How did you get it to produce all year round? That&#x27;s unheard of. reply kenhwang 5 hours agorootparentI think it&#x27;s the cooler coastal climate I&#x27;m in that slows down the growth so the regular 3 month harvest period is spread across 9.It still only flowers once a year in spring like a normal tree, but by that following winter (regular harvest time), only the clusters with few siblings would have grown large enough for harvest. Then it flowers again, and then bigger family clusters reach maturity in the summer&#x2F;fall.My avocado tree when I lived in a much hotter climate would drop all the fruit as soon as it started getting hot (they would dry out and wither on the tree). reply civilitty 9 hours agorootparentprevI would trade my third child for your avocado tree. reply Jeff_Brown 9 hours agorootparentKeep them in a bag with a banana and soon your problem child will be like the older two. reply defrost 9 hours agorootparentprevHmmm, we&#x27;ve got a massive fig tree that produces > 4kg a day for over a month each year and a mango tree that&#x27;s similarly exuberant.Sadly we&#x27;re not in the child trade for tree game :-( reply civilitty 9 hours agorootparentLucky! Figs are great for creating homemade energy&#x2F;protein&#x2F;w.e bars as the binder and mangos are great frozen as pulp for mango lassi.What about grandparents? Are you all set on grandparents? In-laws? reply defrost 9 hours agorootparentWe trade figs for lamb, we make glace figs for the year, we make fig jam, the staff from the local patisserie raid the tree a few times and we get half price curry pies for the rest of the year ..The mangoes trade pretty damn well. (Not our mango tree - but same age and same rough geographic location: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=IgBgQQXoUw8 )My father planted the tree (he was born in 1935) and does most of the bulk cookingprep here still; he&#x27;s a great grand parent, I&#x27;m a grand parent ... we&#x27;re doing pretty good on extended family, thank you for the implied offer of surplus humans but they tend to be for life and not just for christmas. replybane 14 hours agoparentprevFactorio-like diagrams of the ripening process would be great to put in your book and likely resonate well with your readership. reply robocat 10 hours agorootparentHopefully in a few years we can buy the Factorio fridge: https:&#x2F;&#x2F;xkcd.com&#x2F;1109&#x2F; reply jl6 11 hours agorootparentprevWell, avocados are good if you need more iron. reply 6th 11 hours agorootparentThe avocados must grow! reply atoav 11 hours agorootparentprevAnd if you wanna drain a lot of water with what you are eating. A single avocado needs 70 liters to grow, a magnitude more than a tomato.Depending on where that farming is being done that might already be an issue. reply andrei_says_ 2 hours agorootparentIf concerned about water it might be a good idea to start with meat. A pound of beef needing about 7000 liters. reply happymellon 1 hour agorootparentprevI&#x27;m in the UK. Unfortunately we don&#x27;t grow avocados, but we do have the water for it. reply civilitty 9 hours agorootparentprevAn avocado also has an order of magnitude more nutritional value. Tomatoes are 95% water with little to show for the other 5%. Avocados have significantly more carbs, fat, and nutrients. reply Terr_ 14 hours agoparentprev> My memoirs will include all my food storage tips for the perpetually single.Why wait? Become a famous author now. :P reply granshaw 11 hours agorootparentWhich might also help with the perpetually single part reply heleninboodler 11 hours agorootparentAnd risk rendering all these single-person food storage techniques useless?! reply zikduruqe 13 hours agoparentprev> And for storing a cut avocadoI just put a thin layer of olive oil on a plate, then put it cut side down on the plate. reply kuchenbecker 13 hours agorootparentOr lime juice reply HankB99 10 hours agorootparentYes, my method too. I&#x27;m a little disappointed the article did not mention this.Since lime juice is an ingredient in my guac, that&#x27;s handled and I don&#x27;t have problems with the guac discoloring unless kept too long. reply osamagirl69 9 hours agorootparentIt does mention it, but later explains why they prefer plastic wrap.Personally I use lemon juice -- not only does it chemically prevent the browning (yay chemistry) but it also makes it taste better.>The old rub-with-oil-and-place-face-down-on-an-oiled-plate works fine if you&#x27;ve>got a perfect half of an avocado with a smooth face, but it doesn&#x27;t help if>you&#x27;ve got, say, 3&#x2F;4 or 1&#x2F;4 of an avocado. reply mindslight 8 hours agorootparentHow exactly do you get half of an avocado with a flat face? Slice through the pit like a cyborg?To save half an avocado for later in the day, I just leave the half with the pit sitting face up. Then later I scrape the tiniest layer of brown off. reply HankB99 6 hours agorootparentPerhaps I can describe my process.Remove the hard bit where the stem attaches (if present.)With a sharp knife slice into the avocado until you hit the pit.Rotate the Avocado to slice around the pit until you get back to the starting point (and remove the knife.)Twist the two halves to separate them. The pit will stick in one half.Chop the pit with the knife (but not enough to cut it in half!) Twist to remove the pit.Or just search Youtube for \"pit avocado Martha Stewart\" who demonstrates this better than I can explain it in about 15s. reply mindslight 6 hours agorootparentI&#x27;ve seen the technique of using the knife to rotate the pit, but I don&#x27;t really do it because I&#x27;m generally happy with radial pieces so cutting the pit half into quarters isn&#x27;t a problem. I also just generally have one avocado at a time, so I&#x27;m not going for speed (as one would say making guac).But my point was that unless you actually slice the pit in half, neither half is going to have a flat face that you can put on a plate of oil or whatever. Rather one half is going to have the pit sticking out, and the other half is going to have a pit shaped indentation that doesn&#x27;t touch the oil. reply planede 25 minutes agorootparentI guess the indentation only interacts with the little oxygen that is trapped there, which might not be enough to substantially brown it. replycrazygringo 16 hours agoprevTrying to change the rate at which avocados ripen is way beyond anything I would ever worry about. Because I can&#x27;t even tell when they&#x27;re ripe! The article is spot-on when it says (emphasis mine):> \"The window of time in which they are absolutely perfect‚Äîsoft and tender with no brown spots or streaks‚Äîis notoriously short.\"But then it claims:> \"...gauge ripeness by touch: Using your fingers, very gently press on the avocado near the stem end (that&#x27;s where the avocado was once attached to the tree). You want to feel a slight tenderness and give. If the avocado is very firm, it&#x27;s not ready; if it feels soft and mushy, it&#x27;s gone too far.\"I have gotten this wrong so many times that it seems like useless advice. The skin of the avocado is so stiff and wrinkly by the stem end, you simply cannot tell. By the time you apply enough force to feel through the skin, you&#x27;re going to explode the avocado.But if you try pressing against the side, where you can sometimes feel the level of hardness&#x2F;softness more accurately (if it is a particularly thin-skinned one), you bruise it.I&#x27;m an expert in the kitchen at basically everything else, but trying to figure out if an avocado is ripe or not just absolutely defeats me. I&#x27;ve routinely cut into an avocado I thought was underripe, only to discover it&#x27;s so over-ripe it&#x27;s inedible, because the skin all over is so darn tough that the whole thing simply felt rock-hard all over. It&#x27;s like fossilized reptile skin.How does anyone do it? I&#x27;m talking about regular Hass avocados bought in the northeast US shipped from Mexico.At the end of the day, I just buy a few, wait 3 days, cut into one, and if it&#x27;s ripe I try to eat the rest quickly. If not, I throw it out, wait another couple days, and repeat. Ugh. reply hobofan 11 hours agoparentIf you want to go for a over-the-top techy solution you could do non-destructive testing with a near-infrared handheld spectrometer. It&#x27;s a pretty neat technique that can determine ripeness of a lot of fruits (and properties of other materials) if that can be detected via abundance of certain molecules.It looks though that Consumer Physics stopped selling their SCiO device that was ~$250 to consumers, and I don&#x27;t know if there is any equivalent current alternative. reply gorgoiler 6 hours agorootparentLots of consumer cameras have near infrared sensitivity, mostly 750nm but with a long exposure I can get photographs through a 900nm filter in my Fuji X-Pro. You lose at least four stops.Could the photography technique work to determine ripeness? Assuming the fruit is fairly motionless of course. reply hobofan 2 hours agorootparentI have to admit I&#x27;m not the biggest expert on it. I mainly know about it because my brother wrote his PhD thesis about using portable Vis&#x2F;NIR spectrometers as food checkers (verifying that they perform on par with expensive desktop spectrometers).He showed me the SCiO in person a few times, and IIRC always had to make direct contact with the solid probe, so I&#x27;d be surprised if a NIR photograph would be a suitable substitute. I think it also mainly measures the area of the fruit where you make contact, so to get an optimal picture about the ripeness of a single fruit it&#x27;s good to take multiple measurements. That also usually makes fruits with tick skins&#x2F;shells (e.g. melons) hard to measure.There seems to be a startup[0] aiming to bring suitable sensors to smartphones so that anyone has access to it - something that the food checker research literature has been hinting at for some time.[0]: https:&#x2F;&#x2F;mantispectra.com reply gorgoiler 1 hour agorootparentThinking about it a little more, it might have some mileage. The IR spectrometer is just measuring the color of the avocado, albeit very precisely and under known lighting conditions.One of the reasons photographers enjoy infrared photography¬π is because foliage shows up brilliant white against pitch black¬≤ skies¬≥. Photosynthesis is inhibited by both green and IR light so plants are bright in IR for the same reason our eyes see it as green.I absolutely think that you could get the same result with a flash photograph from a good modern camera and probably just using the green channel. One of the many things these cameras are designed to be good at is detecting minute subtleties in shades of green‚Å¥.I‚Äôm looking forward to trying it on the weekend.‚Äï1. Traditionally done with a monochromatic black and white film where only deep, bright reds gave the negative any density, rendering as brightness in a positive print. This is in contrast to ‚Äúpanchro‚Äù black and white where a bright object of any color would print as white.2. Unlike blue light, the atmosphere disperses almost no infrared.3. A good example of all of the above is here: https:&#x2F;&#x2F;fineartamerica.com&#x2F;featured&#x2F;infrared-willow-tree-stu...4. Our eye‚Äôs sensitive to green is partly why digital cameras have twice as many green sensors as they do red and blue. It‚Äôs also why high fidelity inkjet printers use more shades of ink for printing photographs. (Epson printers use one yellow, two different cyans, and three different magentas.) reply crazygringo 10 hours agorootparentprevThat would be the best.I already can&#x27;t imagine going back to life before my infrared thermometer -- for food, for liquid temp, for pan temperature, for pan hotspots...And similarly you can pry my sous vide circulator away from my cold dead hands.If there was seriously something guaranteed to tell you ripeness, that would be golden. (Pineapples, mangoes, and melons like canteloupes, are sometimes tricky too -- I&#x27;m usually pretty good at figuring them out, but sometimes they surprise you.) reply dekhn 15 hours agoparentprevI press against the side and it mushes in slightly. this tells you the avocado is ripe or past ripe. When you cut it open, then if it&#x27;s already browning, it&#x27;s gone past ripe.I agree it&#x27;s a bit difficult but I used to make a few pounds of guacamole a day and it always worked great. reply dzolob 7 hours agoparentprevYou just need to press it gently with all your fingers around the principal axis, not on the poles (as if your hand were a spider). If it feels hard, is not ready; if it feels soft, but not too soft, is ready. Also the color and the texture of the skin. Green means green, black and clear&#x2F;bright + soft means ripe, black and opaque&#x2F;wrinkled + too soft means overripe.Source: I‚Äôm from Mexico. reply makingstuffs 4 hours agoparentprevI usually put the tip of my thumb and pinky together and then press the palm flesh beneath my thumb first to get a sort of muscle memory as to what I am looking for.Then I press the avocado around the centre and compare. If it is around the same I open and eat. Seems to work most of the time ‚Äî with the super thick skinned ones I generally press higher up on the avocado (closer to the stem) reply JonChesterfield 11 hours agoparentprevThere&#x27;s a stem of sorts at one end. Prod at it from the side - if it falls off easily, it&#x27;s ripe or too far gone. If it stays attached, not ripe yet.More usefully, you can buy pre-mashed avocado in the same style as pre-made guacamole. If the use case involves (or could be made to involve) mashed avocado this option has a serious shelf life (presumably it&#x27;s irradiated or similar) and lives in the fridge until use time. reply dunham 10 hours agoparentprevI&#x27;m following Rick Bayless&#x27; directions to check the _bottom_ of the avocado and use it or put it in the refrigerator (for 3-7 days) when it gives slightly.https:&#x2F;&#x2F;www.rickbayless.com&#x2F;ingredient&#x2F;avocado-1&#x2F;I&#x27;ve mostly had good luck, but sometimes they&#x27;re just bad to start with, bad because they&#x27;re bruised from mishandling, or have been exposed to too much heat.I don&#x27;t see a lot of the tough leathery ones, but I have seen a few. (Seattle, so I&#x27;m not in the southwest.) I wonder if they are a different varietal or just dried out. reply locallost 15 hours agoparentprevNot really sure why it&#x27;s giving you so much grief. I gently press on the bottom larger side and if I feel it&#x27;s soft and not hard or bouncy it&#x27;s good. I press on the larger sider because in my experience it takes longer to ripen. reply robocat 10 hours agoparentprevApply some sciencey thinking.Test a variety of ideas (perhaps start with what is given here) until you find something that works for you.Challenge yourself: either practice someone else&#x27;s technique, or invent your own.Outcome: satisfaction for the rest of your life every time you use your technique and get a perfect avo! reply bowmessage 15 hours agoparentprev> if it&#x27;s ripe I try to eat the rest quicklyIf it&#x27;s ripe, put the rest in the fridge, and you&#x27;ll have a week or two to eat them at your leisure. reply leobg 13 hours agoparentprevYou know that Hass is German for ‚Äúhate‚Äù?I always find it funny that I didn‚Äôt bother to change the name here. reply labster 7 hours agorootparentSF didn‚Äôt change the name of Haight Street either. People are pretty comfortable with homophones in general. reply ars 14 hours agoparentprevSqueeze the side of the avocado, and if it gives AT ALL it&#x27;s ready to eat.If it&#x27;s soft it&#x27;s way way overripened. Take a piece of cardboard between you fingers and squeeze - that&#x27;s the firmness level that indicates the avocado is ready. Actually even more firm than the cardboard and the avocado is still ready.> where you can sometimes feel the level of hardness&#x2F;softness more accurately (if it is a particularly thin-skinned one), you bruise itYou are squeezing way too hard. reply gooseball92 10 hours agorootparentI think you‚Äôre underestimating the variability in hand strength between people. reply civilitty 15 hours agoparentprevYou must be consistently getting the world&#x27;s worst avocados - do you always buy them from the same store? Though it might just be a variety designed for distant shipping that sucks in general.None of what you described sounds accurate to me but I&#x27;m spoiled in the Southwest. Ripe avocados are definitely soft and when I find the rare one that isn&#x27;t, it&#x27;s usually a bad avocado altogether (most of the flesh is very tough despite being overripe). reply crazygringo 15 hours agorootparentThe avocados I buy look like this:https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;c&#x2F;c9&#x2F;Av...See how the outside is super bumpy, and there&#x27;s a thick rigid brown layer before you get to the green flesh? Rock-hard skin even if the flesh is soft. A serrated knife helps to cut through it without smushing the flesh.While I see other photos that look like this:https:&#x2F;&#x2F;daily.jstor.org&#x2F;wp-content&#x2F;uploads&#x2F;2017&#x2F;05&#x2F;avocado_1...Where the skin is visibly much thinner and smoother and there&#x27;s no brown layer, and I can only imagine that&#x27;s the type of avocado where people can judge its ripeness. But those don&#x27;t seem to be available in NYC at my local store or Whole Foods or anywhere... reply rashkov 15 hours agorootparentThere&#x27;s an avocado delivery service in NYC that claims to deliver perfect avocados every time. There&#x27;s some good press coverage around it: https:&#x2F;&#x2F;davocadoguy.net&#x2F; reply squidsoup 12 hours agorootparentI use a similar service in New Zealand, and no longer have avocados ruined by overzealous squeezers in the supermarket.I don&#x27;t know who these people are, but they exist!https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=38m-wnbHPLA reply kjkjadksj 15 hours agorootparentprevIf its still hard there set it on the counter a day or two and check again. The test works for me with these sorts of avocados so I‚Äôm thinking user error. reply crazygringo 14 hours agorootparentRight but what I&#x27;m saying is it&#x27;s always hard. Because the skin is so thick and hard there&#x27;s literally no way to tell what the flesh is like underneath.Except sometimes it&#x27;s a little thinner&#x2F;softer that you can actually press the outside, but you still have to use so much pressure it bruises the avocado. There&#x27;s zero way of \"gently\" checking.I&#x27;ve opened avocados that are still rock-hard on the outside, after waiting a full week, only to discover they&#x27;re overripe and brown&#x2F;gray on the inside.And I have no problem with any other fruit or vegetable. Just avocados because of their crazy thick&#x2F;hard rind where I live.I just don&#x27;t see how it can be \"user error\". My only suspicion is it&#x27;s a different subvariety or grown for extra-thick skin for longer-distance transport or something. reply Syntonicles 9 hours agorootparentI buy the same Hass avocados you posted, and I&#x27;ve lived in every region in the U.S. For the past 10 years, I have eaten an avocado (minimum 1&#x2F;2 avocado) every single day. That is several thousand avocados.I can say with absolute conviction that it isn&#x27;t the rind giving you trouble. I can pick every ripe Hass avocado out of a bin of 50, and never once be wrong. That said, there are a couple of stores in my current area that have garbage Hass avocados. The ones I buy from this store are consistently gross and fibrous, even when ripe, and have a very short window. I boycott these stores. They either have a second rate supplier, or else somehow ruin the product in transport.By the way, you need a gentle touch and the ability very slowly, smoothly increase pressure. You pick up the avocados that are not yet ripe, the moment one is on the threshold, such that there is a nearly imperceptible give which does not bounce back, you buy the avocado. It should not bruise the fruit, and you won&#x27;t be able to detect it when peeled. After this check, you can visually tell when it has fully ripened, ~1-2 days later. For what it&#x27;s worth, I&#x27;ve never successfully ripened one AFTER cutting into it, and I&#x27;ve never kept an uncut avocado for more than 4 days. reply crazygringo 8 hours agorootparentThank you!I think this is the most helpful comment -- first of all, I&#x27;m now just convinced that the stores I usually go to must be getting garbage avocados for whatever reason, I genuinely never guessed that would be a per-store thing. I&#x27;ll try some different places, there must be different suppliers.And I think:> nearly imperceptible give which does not bounce backCould be the key. I&#x27;ve always been looking for some pretty obvious give -- saying it&#x27;s borderline impercetible seems like it might be the key. Other commenters here seem to be saying something similar.Also good to hear the majority of people advising to feel the side or bottom (not the stem end as the article suggests).I think there&#x27;s hope for me yet! So thanks to you and everyone else who&#x27;s been helping. reply thePhytochemist 10 hours agorootparentprevA durometer, acoustic impulse-response and special UV-VIS spectrophotometer are all instruments used to detect avocado ripeness and usually work well.A durometer is the cheapest at about $40. If you&#x27;re right that it&#x27;s impossible to tell by hardness then it wouldn&#x27;t work. But you might find that it&#x27;s more sensitive than your fingers and so it can work?I would be interested to know the result of a durometer test on your fruits because about 85% of consumers report firmness as a useful measure - a lot but certainly not everyone. So yes maybe there is some granularity to discover there: specific situations where firmness just doesn&#x27;t work well? reply dekhn 13 hours agorootparentprevyou, sir, have a broken avocado reply fiddlerwoaroof 13 hours agorootparentprevI‚Äôve successfully judged the ripeness of those avocados by squeezing all my life reply civilitty 15 hours agorootparentprev> Where the skin is visibly much thinner and smoother and there&#x27;s no brown layer, and I can only imagine that&#x27;s the type of avocado where people can judge its ripeness. But those don&#x27;t seem to be available in NYC at my local store or Whole Foods or anywhere...I get both of those here in SoCal, or at least a varieties that look similar to those photos - the bumpier variety is noticeable when I come across it (it&#x27;s definitely tougher). I never paid attention to the thickness of the skin or kept track of how quality correlates to the bumpiness, though. I&#x27;ll keep an eye out from now on.I wonder if there&#x27;s a single vendor&#x2F;broker supplying the stores in your area. Have you tried buying them from outside the boroughs? reply alhirzel 10 hours agoprevFor our wedding reception (that we self-catered with a taco bar), we ordered a whole box of 100 avocados through our local school. Of course, we received them about 4 days ahead of time with the rest of the food, most of which would go bad if it sat around any longer. The avocados presented a problem: they wouldn&#x27;t be ripe in time. After some asking around, we got the tip mentioned in this article, and we interspersed a dozen bananas in the box so we could JIT-compile some guacamole :) It was $4 to save the day! reply RussianCow 15 hours agoprevIf you&#x27;ve already mashed the avocado, you can keep it from browning by adding a little bit of ascorbic acid (vitamin C). Here[0] is a series of tests someone else did to prove that this works. I&#x27;ve personally had guacamole last several days in the fridge with no noticeable browning, but then again, guacamole doesn&#x27;t last long in my house once it&#x27;s been made. :)[0]: https:&#x2F;&#x2F;cooking.stackexchange.com&#x2F;a&#x2F;53406 reply tsavo 9 hours agoparentLime juice is a staple in my recipe. Have substituted with lemon juice when I&#x27;m surprised to not have lime available. It helps with the browning but not as much with the flavor reply tunapizza 9 hours agoprevHaving toiled in sushi taverns for nigh on a decade, wherein the verdant avocado reigns supreme in North American fare, I have honed my skill in discerning the ripeness of countless avocados each passing annum. Nay, there lies no enchantment in this art, only diligent practice. A keen intuition doth develop with time. Lo and behold, should an avocado err in ripening, fear not. Craft thyself a guacamole or an avocado toast, and the subsequent fruit shall be of greater delight.(I wrote a reply and asked chatGPT to rewrite it in a medieval style) reply vram22 1 hour agoparent>A keen intuition doth develop with timeThe lyf so short, the craft so long to lerne.- ChaucerOur talent goes to naught, by this AI we discern.- Mehttps:&#x2F;&#x2F;wisdominwood.com&#x2F;products&#x2F;chaucer-the-craft-carving reply andrewla 15 hours agoprevI was really hoping that this would be a more detailed breakdown of the different methods, like the egg article [1]. This just presented conclusions rather than trying a bunch of avocados and evaluating (objectively or subjectively) their state of ripeness based on the environment.[1] https:&#x2F;&#x2F;www.seriouseats.com&#x2F;the-secrets-to-peeling-hard-boil... reply bambax 15 hours agoprevAll true, although the article contains no revelations.But one important point that is somehow overlooked is that for taste, brown spot don&#x27;t matter, at all.Depending on your guests, the preparation you&#x27;re making, and your love of perfection, brown spots can maybe alter the looks of your dish. But they taste the same.It would be a great error to prefer an unripe avocado over an over-ripe one for aesthetic reasons only. Unripe avocados are inedible. Over-ripe ones (within reason) taste perfect. reply wizofaus 15 hours agoparentDefinitely not my experience - the flavour definitely deteriorates too, often becoming quite bitter. reply 0xDEADFED5 8 hours agoparentprev> Unripe avocados are inedible. Over-ripe ones (within reason) taste perfect.Grew up with a grove of trees, I always felt the opposite. Over-ripe can have a definite funk. With unripe, it&#x27;s mostly the texture that is wrong. It&#x27;s not ideal, but you can make a perfectly serviceable guac with nearly ripe avos.> brown spot don&#x27;t matter, at all100% agree reply vram22 2 hours agoprevRelated: in India, people put unripe green bananas in raw rice (uncooked rice), well covered by the rice, and leave them there for a few days to ripen. reply koolba 15 hours agoprev> As for mashed avocado like guacamole, we think the best solution, aside from preparing it right before serving, is to press a double layer of plastic wrap directly against the surface of the avocado mixture.Why a double layer? Does oxygen penetrate the single layer? reply shepherdjerred 15 hours agoparentPlastic wrap is oxygen-permeable, so that might be why. reply hamerld 15 hours agoprevI&#x27;m surprised they didn&#x27;t mention storing the avocado with a piece of onion. I just stored one for almost a week with no visible browning thanks to the onion. Just store the 2 cut pieces in a container and store in the fridge. reply azinman2 15 hours agoparentI also find it worthwhile to always store with the seed, even in guac reply andirk 9 hours agorootparentI second keeping the giant pit in the guac. And if I&#x27;m the only one eating it, I don&#x27;t care if it changes color a bit because it has no effect. reply proee 15 hours agoparentprevnext [4 more] [flagged] flextheruler 15 hours agorootparentIs it really necessary to mention ChatGPT assistance with a query so simple?This sort of thing really makes me wonder how much we are trying to persuade ourselves of the hype around ChatGPT. reply Belomolo 14 hours agorootparentIf you no longer need Google but one model can answer it, it is impressive.But different people are going through their chatgpt initial phase, so you will keep seeing this I guess reply flextheruler 15 hours agorootparentprevIn fact I just tested this and I can give a simpler query to Google and get the correct result than ChatGPT.If I search ‚Äúonion avocado store‚Äù on Google it immediately pulls up articles explaining the chemical benefit of the storing method.The same query on ChatGPT causes it to query me for more information about what I‚Äôm asking. Obviously it‚Äôs designed for conversational queries more so than Google which is another reason why simple queries like this are not superior on ChatGPT. reply kruuuder 15 hours agoprevGood to know. Now, could someone please write up a similar guide for mangoes? Too many are hard when I buy them, and they start getting mushy after a week, eventually rotting. However, at no point do they become truly ripe and sweet. reply JamesSwift 14 hours agoparentTheres a tiny limit on the amount of off-the-tree ripening that will occur, and they are really difficult to \"read\" on the tree to know when they are ripe. Each variety is different. The key for good mango is to let them go further than you would think on the tree and hopefully the birds dont beat you to them.Source: moved into a house with several mango trees and am now pretty good at harvesting. reply 0xDEADFED5 8 hours agoparentprevonly buy ones that are almost ripe, they&#x27;ll ripen reply bluGill 15 hours agoparentprevGo to India in mango season. (or I assume other areas where Mangos grow). They export the garbage locals won&#x27;t eat. reply palidanx 7 hours agoprevWhen I used to live in Southern California, there actually are several different varieties of avocadoes with many of them not available in other parts of the country. Some examples are hass, lamb hass, pinkerton, mexicola (where you can eat the skin), and reed avocadoes. Reed avocadoes are my favorite and are the size of 5 hass avocadoes combined with a very buttery mouth taste.. They can take up to 7-12 days to ripe at room temperature, but once cut open, oxidiation is minimal due to the fat content in the avocado. reply gnicholas 9 hours agoprev> It&#x27;s not a perfect solution‚Äîthe avocado will still brown eventually‚Äîbut you can mix in a minimal amount of browning such that diners won&#x27;t know, buying yourself at least a few hours before it becomes a problem.Or you could scrape off the thin layer of oxidized guac, and serve your diners only the fresh parts?One thing they don&#x27;t mention is that when you&#x27;re storing guac in a container, you want to shape it to minimize surface area. This means not using a shallow container, but instead a narrow&#x2F;tall one. That way you oxidize a smaller fraction of it, which means less waste (or less brown stuff you&#x27;re mixing in with the fresh parts, if you follow their instructions). reply mhb 15 hours agoprevAs it&#x27;s Serious Eats, I&#x27;m disappointed that they didn&#x27;t test things like submerging the cut or mashed avocado in CO2 (e.g., from SodaStream, etc.) or nitrogen. reply rootusrootus 11 hours agoparentOr even simple things like storing cut avocado in a container with slice of onion. reply apercu 16 hours agoprevThis is probably sacrilege but after 40 years of being annoyed with avocado ripeness I just routinely buy them and keep them in the fridge. They don&#x27;t go off and keep for weeks. reply jeremiahbuckley 10 hours agoparentI feel my life has improved immensely by the revelation of storing avocados in the fridge.1. Ripeness-window is days, maybe even a week, and they can be super squishy and still not turn brown.2. Because of #1, buy a good number of avocados when you notice you‚Äôre out and immediately toss them in the fridge. Maybe work to find that one in the store that is ready to go for today&#x2F;tomorrow. Sure for the next few days you might not have an avocado ready, but‚Ä¶you probably don‚Äôt have to eat avocado every day so you‚Äôll be ok.3. If you really need avocados and you‚Äôre annoyed at the unripe avocados in your fridge, take one out and put it on the counter for tomorrow.So much less stress to immediately use that purchased avocado. Net-net so much less wasted avocado. reply civilitty 15 hours agoparentprevMy trick is to eat them so frequently (1-3 a day) that I&#x27;m always rotating avocados between the store, kitchen counter, and the fridge. reply lotsofpulp 15 hours agorootparentSame, my family probably goes through 10 to 15 avocados per week, so we buy them every few days at Costco to have a rotating stock. They take about 5 days to ripen, and then once it feels a little soft, we put it in the fridge, and then they last for a while in the fridge (at least a week).The one rule at Costco, however, is to not buy avocados from Peru. For whatever reason, the loss ratio is way too high for those specifically. reply rootusrootus 11 hours agorootparentprevThat&#x27;s impressive. I thought I was a maniac when I was eating one large avocado every day. reply civilitty 7 hours agorootparentAs I love to say: three avocados a day keeps the doctor away** This statement has not been evaluated by the FDA reply Gys 14 hours agoprev> What about leftover avocado? Any way to keep it from browning? Oxygen is the enemy of avocados‚Äîit&#x27;s what causes them to turn that unsightly brown.I am very surprised this article nor the comments mention what my wife does: keep the pit inside the mashed avocado. It will stay green much longer!No idea where she learned this but it works. We always joke to friends this tricks the avocado into thinking it is still complete. reply rootusrootus 11 hours agoparentI&#x27;ve never had much luck with the pit keeping it from browning. Onion, on the other hand, is practically magic. reply Gys 10 hours agorootparentYou put an union in the left over mashed avocado to prevent it from browning? reply rootusrootus 7 hours agorootparentHa, not in it. Just has to share the same airspace in the container. reply Gys 1 hour agorootparentPhewww ;) reply unstatusthequo 14 hours agoparentprev100% this. Keeping the pit in guacamole is the trick. I am happy you commented. I was about to do the same when I realized the article missed this! reply thsksbd 7 hours agoprevWe eat about an avocado a day.When we&#x27;re down to the last two or three, we buy a bag or two at Costco (by far the best in our area), and put most of them in a fridge.Then as we eat the most ripe one we take one out of the fridge.Then whole ripening thing collapses to optimizing the buffer size. reply robg 16 hours agoprevBrown paper bag, two not fully ripe bananas with every three avocados. Leave for two days. reply robg 16 hours agoparentSerious Eats here suggests staggering:- Ripe within 2-3 days - with a banana in a brown paper bag- Ripe within 3-5 days - just avocados in a brown paper bag- Ripe within a week - open air avocados reply soco 16 hours agorootparentMust the paper be brown for some reason? Or why would everybody say \"BROWN paper bags\"? And to the particular solution, has it to be paper, not plastic? reply crazygringo 15 hours agorootparentIt has to be paper to let humidity pass out. Fruit will rot if kept in a plastic bag for too long because of the humidity build-up.It doesn&#x27;t have to be brown, it&#x27;s just that the classic paper bag you buy at the store is always brown. You know, the lunch bags for kids (or adults). I&#x27;ve never seen them sold to consumers in any other color.You could reuse a white bleached paper bag that you sometimes get with food delivery, but they always seem to acquire a grease stain or other liquid along the way... But the food delivery bags are often waxed, and that won&#x27;t let humidity pass either. reply dekhn 13 hours agorootparentprevJust oxygen-permeable. I also keep my cheese in an air-permeable bag in the fridge - if there&#x27;s no bag, it dries too fast, if the bag is not air-permable the cheese gets sweaty and gross. reply richiebful1 16 hours agorootparentprevMaybe the paper bag breathes more? I&#x27;ve speed-ripened avocados in those plastic produce bags they have in the produce section with twist ties and it seems to work okay reply mattkrause 15 hours agorootparentprevThey mean the bags made of uncoated kraft paper, which lets air in very well. \"Brown [paper] bag\" is essentially the name of this type of bag, which is often used for school lunches (small bags) or groceries (bigger). Here&#x27;s an example:https:&#x2F;&#x2F;www.canadiantire.ca&#x2F;en&#x2F;pdp&#x2F;goodtimes-paper-lunch-bag...They come in other colors--we had bleached white ones when I was a kid--and those would be fine too. However, it risks confusion with other paper bags, like the ones used in bakeries. These often have some kind of liner (polyethylene, plastic, foil) or wax coating that would impede airflow.The department store paper bags, even if they are pure paper and colored brown, are often multiple layers that would block airflow too. reply 727374 7 hours agoprevOr if you want to eat it right away, make a carpaccio - https:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;WZ_orsakLG0 reply b2w 15 hours agoprevI learned from a chef a while back to apply a thin layer of lemon juice to the smooth face of the avocado to prevent it from browning quickly.I can usually pick a good hass based off skin tone and feel from the palm of hand if the skin is just beginning to pull away from the face when most ripe.I tend to store it in the fridge to prolong its ripened state.They are a daily use in my household for adding calories while more closely preserving the Keto ratio. reply jechamt 12 hours agoparentThis is what I was looking for, in the article as well as comments. Lemon juice has also worked for me, especially in combination with the sealed plastic wrap, but I was not scientific about it and was hoping for some mention of testing this in the article! reply denton-scratch 15 hours agoparentprevI use lemon juice. It works, inasmuch as anything works.I mainly use avocados to make guacamole, and my recipe includes lemon juice. If there&#x27;s half an avocado left over, I splash lemon juice on, then wrap it in plastic film and refrigerate it. reply mckn1ght 5 hours agorootparentI do this as well, and also with guacamole I‚Äôm going to hold, after putting it in a container I‚Äôll smooth the surface, squeeze lemon&#x2F;lime juice on top and press plastic wrap down onto it to keep a film of juice on the surface. Keeps it from browning and adds to the flavor for next time, when you can just stir it in. reply damontal 7 hours agoprevMy avocado pet peeve are the ones with long strands of fiber in them. Not sure why they develop or how to avoid them. reply sokoloff 14 hours agoprevI&#x27;ve had good outcomes in determining whether an avocado was under-ripe by gently picking at the little nub where the fruit attached to the stem. That is more firmly attached when the fruit is under-ripe and as soon as it easily removes when picked at, I find the avocado is usually near perfect. reply niemandhier 15 hours agoprevHere is the trick that changed my ability to store ripe Avis:Put them into a container and submerge them under water. Than store the container at below 7 degree centigrade. That seems to stop the ripening process.It does not work without water. Vacuum sealing also does not work. reply edmundsauto 14 hours agoparentYou freeze them? How do you then slice them to eat? reply eric-hu 14 hours agorootparentThey shouldn‚Äôt be frozen if stored below 7C and above 0. reply schrodinger 11 hours agorootparentEasy to misread as 7 deg below zero reply ska 11 hours agorootparentprevC not F.Fridge temperature is typically 3-5. reply gweinberg 13 hours agoprevThe avocados we eat are all clones, right? The reason I ask is, it seems to me that avocado sellers ought to be prominently displaying the variety, but in my experience they don&#x27;t. reply c_o_n_v_e_x 11 hours agoparentDepends on where you are in the world. In SE Asia, we get some wacky varieties that greatly vary in size, shape, and flavor. I bought a few at a local market in Indonesia that were border-line zucchini like in shape and had a very watery flavor. reply Meatpropeller 12 hours agoparentprevThe avocados in stores are all Hass variety or almost identical variants (Lamb Hass). These replaced Fuerte variety avocados decades ago as they shipped better, gave more consistent yields, and the darkening skin gave consumers an additional clue to ripeness.If you are interested in trying different varieties, there are many if you live in a growing region and have access to farmer&#x27;s markets. In the US: California, Hawaii, Florida. reply sonicanatidae 12 hours agoprevAvocados are perfectly ripe for about 247 zeptoseconds. After that, they are a black spotted mess.To those that eat them, I wish you luck! reply wizofaus 15 hours agoprevCommercial avocado dips seem to avoid turning brown - admittedly they&#x27;re usually pretty tasteless but I&#x27;d be curious what stops the discoloration. reply dekhn 13 hours agoparentI&#x27;m sure they have a step where they add an oxidation prevention molecule- I bet there&#x27;s some ligand that binds to the oxidation enzyme in avocados. Or maybe high pressure? https:&#x2F;&#x2F;www.hiperbaric.com&#x2F;en&#x2F;the-impact-of-hpp-technology-i... reply poulsbohemian 15 hours agoparentprevCheck the ingredients... a lot of them are heavily adulterated. Not to say they aren&#x27;t tasty, but they aren&#x27;t even necessarily \"avocado\". reply sva_ 11 hours agoparentprevAnd almost all of them are too sour (imo), probably from the antioxidant. reply hulitu 15 hours agoparentprevMaybe like for other fruits ? They take them still green. reply pvaldes 13 hours agoprevAvocados are like oranges. The better way to store them is hanging from the tree. reply JamesSwift 12 hours agoparentAvocados are uncommon in that they _wont_ ripen on the tree. They must finish off the tree. Super convenient that you dont have to deal with them all at once, but inconvenient that you cant just decide to make guacamole and go pick some. reply kubatyszko 11 hours agoprevfinally an article that gives more context on the five minute window between avocados being not ripe and then shortly after they&#x27;re rotten ;) reply 11235813213455 16 hours agoprevMany avocados in stores (at least the lowest price) have almost no taste, not worth it reply voxl 14 hours agoparentTo me avocado&#x27;s are the tuna of fruits, a pinch of salt goes a long way reply init2null 13 hours agorootparentSince avocado is practically vegetable butter, it&#x27;s more of a flavor complement than a flavor itself. reply amelius 15 hours agoprevWhy can&#x27;t supermarkets do this, so you&#x27;ll always have ripe avocados? reply jpeterson 15 hours agoparentYou don&#x27;t always want to use them right away. reply enobrev 15 hours agorootparentI wish my local market had two piles - ripe and unripe. That way I&#x27;d have one ready for use immediately and can ripen the others as needed. reply Hikikomori 14 hours agorootparentSorta have that here. They package 3 ripe ones, slightly more expensive. reply hollerith 15 hours agoprevBecause the ripe ones are very moldy, I only ever eat very green avocadoes, and the way I \"eat\" them is to pay some company to extract the oil from them and sell me the oil. reply ska 11 hours agoparent>Because the ripe ones are very moldyYou are definitely doing something wrong if there is mold. reply hollerith 11 hours agorootparentNo, I&#x27;m not. the black veins are mold. Some edibly-soft avocadoes have much less black veins and black spots than others, but they all have some. Also by the time mold is visible, it is basically everywhere in the avocado.But 5 out of 6 people can detoxify mold toxins, and those people can probably eat avocadoes without long-term problems. reply ska 11 hours agorootparentNone of that is correct. Ripe avocado&#x27;s typically don&#x27;t have any darks spots or \"veins\". Ones that are overripe or poorly stored can, but even then it&#x27;s typically not mold, it oxidization. The \"veins\" are part of the structure, and thicker&#x2F;woodier with late season, but not usually colored at all.There is a fungal (again, not mold) infection that can happen from the stem end down, but that&#x27;s rare and also not a harmful fungus for humans.If there is actual mold on your avocado, you are storing them incorrectly or something. reply locallost 15 hours agoprevApples are another possible source of ethylene. reply jader201 7 hours agoprevHow is this article on the FP, even after 9 hours?While the topic itself is an interesting discussion (and possibly the main reason it‚Äôs being upvoted), the TLDR of this whole article is basically:- There‚Äôs no great way to shop for avocados- You can sort of tell how ripe one is by feeling the ‚Äúgive‚Äù around the stem (though not a guarantee it‚Äôs a ‚Äúgood‚Äù avocado)- You can ripen avocados faster in a bag (though still not a guarantee they‚Äôll be good)- You can‚Äôt really store them for later, once cut open- You can‚Äôt really prevent browningIn other words, according to this article, buying avocados is a crapshoot, and if you happen to get a good one, consume it immediately once you cut it.There is probably better advice in the comments here, but the article itself isn‚Äôt very optimistic or helpful. reply hardwaregeek 13 hours agoprevThis is an extremely niche solution, but when I need a bunch of perfectly ripe avocados, I order them from the Avocado Guy[1]. It&#x27;s not that much more expensive than a grocery store (especially considering the avocados are usually on the large side), and they come perfectly ripe each time.[1]: https:&#x2F;&#x2F;davocadoguy.net reply nemo44x 15 hours agoprevSome higher end refrigerators have various fans&#x2F;vents&#x2F;filters to remove ethylene quickly so as to dramatically slow down the decay of fruits and vegetables. In addition they‚Äôll have separate compressors&#x2F;motors for the fridge and freezer ti keep different humidity levels.My Sub-Zero made this claim and I was astonished how well it delivers on it. They also give you a guide on which fruits&#x2F;vegetables to store together. reply poulsbohemian 14 hours agoparentPossible hack: when I was last shopping for refrigerators, I couldn&#x27;t help but notice that Bosch&#x27;s magic food saver technology appears to be a replaceable plastic ethylene filter that can be purchased from their website or other major retailers. Have been meaning to buy a couple and stick in my much-cheaper-than-a-Bosch refrigerator... reply cmurf 11 hours agoprevAvocado molesters tend to stay away from the bright green ones. At least they aren&#x27;t completely dense. Buy these, fridge all but one, and then stagger removal to room temp each day to (eventually) have a perfectly ripe avocado each day rather than a pile of them all at once. Costco bagged avocados have less of a problem with molesters than the grocery store.3-4 days from bright green will yield a perfectly rip avocado - although on occasion I&#x27;ve had them take 5 days.The rotten soft spots are from the finger and thumb squeeze given by avocado molesters. And then they will tell you \"well how else do you know if they&#x27;re ripe?\" well fuck off, you&#x27;re ruining perfectly good fruit with your goddamn squeezing and impatience! What did the avocado do to you??Impatient people deserve packaged guac. Stay away from the avocados! reply grecy 15 hours agoprevWhile driving around Africa I was always buying avos from the side of the road and working on a system to always have one or two ripe.To make them ripen as fast as possible - stick them in a plastic bag and put it in the full sun. You want them to sweat. They&#x27;ll ripen in a single day if you get enough sun on them.To slow them down, throw them in the fridge. reply OrvalWintermute 16 hours agoprevThis is missing the other avocado trick:You can put citrus in tight proximity to a cut avocado, or guac, or avocado mash to halt oxidation better than oil. It won&#x27;t really impact avocado but it will effect guac&#x2F;mash - it will make your guac mushy and overly acid&#x2F;citrusy, but it is possible to adjust for it, under-adding citrus to your batch when first making it, by reducing the liquid content added to your guac, and straining with a fine mesh strainer it to purge the extra citrus juice.It is also possible to replace the citrus with vinegar if you prefer using vinegar for your guac instead of lime juice. reply richiebful1 16 hours agoparentBy tight proximity, do you mean take a lemon slice and put it on the cut side of the avocado? reply OrvalWintermute 15 hours agorootparentI put a layer of citrus juice on top of a cut avocado, then cover it. It is also possible to do it with solid slices of a citrus too.It works best with narrow lidded jars where you make a batch of guac, put inside a narrow lid container, top with citrus juice. reply 1letterunixname 14 hours agoprevGrow your own from the pits. It will take a long time and some maintenance, but it&#x27;s possible.https:&#x2F;&#x2F;www.youtube.com&#x2F;results?search_query=grow+your+own+a... reply datadrivenangel 8 hours agoparentHow has this gone for you? I&#x27;m 1.5 years into growing a long neck avocado tree from seed, and I feel really worried that it won&#x27;t work right. reply poseva 14 hours agoparentprevAvocado does not grow ‚Äútrue-to-seed‚Äù. It will not work. reply sva_ 11 hours agorootparentYou mean because they&#x27;re cloned from cuttings rather than grown from seeds? Like apples? reply welder 15 hours agoprevnext [3 more] [flagged] orphea 14 hours agoparentHacker News GuidelinesWhat to SubmitOn-Topic: Anything that good hackers would find interesting. That includes more than hacking and startups. If you had to reduce it to a sentence, the answer might be: anything that gratifies one&#x27;s intellectual curiosity. reply welder 14 hours agorootparentThis is something you mom tells you the first time you buy avocados as a child. Wrap them in newspaper to ripen faster. Keep away from bananas to last longer. It&#x27;s definitely not something that \"gratifies one&#x27;s intellectual curiosity\". reply mytailorisrich 12 hours agoprevCounter point: I am from Europe. Local meat is better for the environment than avocados. reply rtsil 9 hours agoparentNot if your local meat is fed with imported foods such as soy from South America, (which accounts roughly to 90% of soys fed to livestocks in the EU), and which is one of the biggest cause of deforestation. reply mytailorisrich 3 hours agorootparentQuite interesting how none of the replies I got engage with the point that it is highly hypocritical to criticise meat but to eat avocados... reply beebeepka 12 hours agoparentprevAlso Europe. Stopped eating meat long time ago. I also don&#x27;t drive. Can I have an avocado and some fancy nuts, please? Do you really want to compare footprints? reply mytailorisrich 10 hours agorootparentYes. Avocados and almonds are extremely bad overall.Please turn off your electronic devices, too. reply amelius 15 hours agoprev [‚Äì] Yeah, I like avocados too, but they&#x27;re not environmentally friendly.https:&#x2F;&#x2F;www.veganfoodandliving.com&#x2F;features&#x2F;is-our-avocado-o... reply sva_ 11 hours agoparent [‚Äì] Look up water use of chocolate if you want to make yourself sad. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The avocado industry aims to delay ripening, while home cooks usually want to speed up the process.",
      "Ripening can be accelerated by storing avocados in a brown paper bag with a banana, which produces ethylene gas.",
      "Microwaving avocados is not recommended as it cooks rather than ripens them, and ripe avocados can be stored in the refrigerator for up to five days.",
      "To prevent browning, plastic wrap is not very effective, but submerging avocado pieces in water or using plastic wrap directly on mashed avocado can help delay oxidation."
    ],
    "commentSummary": [
      "The discussion covers a wide range of topics related to avocados, including methods for ripening and storing them, growing avocado trees, determining ripeness using infrared technology, assessing avocado quality, and preventing browning.",
      "It also touches on various issues, such as the availability of different avocado varieties, the environmental impact of consuming avocados and meat, and the use of ChatGPT assistance.",
      "The conversation offers a valuable resource with a wealth of information and perspectives on avocado-related topics."
    ],
    "points": 266,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1701363948
  },
  {
    "id": 38476482,
    "title": "Generating Character Videos from Still Images: State-of-the-Art Synthesis for Character Animation",
    "originLink": "https://humanaigc.github.io/animate-anyone/",
    "originBody": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, Liefeng Bo Institute for Intelligent ComputingÔºåAlibaba Group Paper video Code arXiv Abstract Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results. Video Method The overview of our method. The pose sequence is initially encoded using Pose Guider and fused with multi-frame noise, followed by the Denoising UNet conducting the denoising process for video generation. The computational block of the Denoising UNet consists of Spatial-Attention, Cross-Attention, and Temporal-Attention, as illustrated in the dashed box on the right. The integration of reference image involves two aspects. Firstly, detailed features are extracted through ReferenceNet and utilized for Spatial-Attention. Secondly, semantic features are extracted through the CLIP image encoder for Cross-Attention. Temporal-Attention operates in the temporal dimension. Finally, the VAE decoder decodes the result into a video clip. Animating Various Characters Human Anime/Cartoon Humanoid Comparisons Fashion Video Synthesis Fashion Video Synthesis aims to turn fashion photographs into realistic, animated videos using a driving pose sequence. Experiments are conducted on the UBC fashion video dataset with same training data. Human Dance Generation Human Dance Generation focuses on animating images in real-world dance scenarios. Experiments are conducted on the TikTok dataset with same training data. BibTeX @article{hu2023animateanyone, title={Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation}, author={Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo}, journal={arXiv preprint arXiv:2311.17117}, website={https://humanaigc.github.io/animate-anyone/}, year={2023} } This page was built using the Template which was adopted from the Nerfies project page.",
    "commentLink": "https://news.ycombinator.com/item?id=38476482",
    "commentBody": "Animate Anyone: Image-to-video synthesis for character animationHacker NewspastloginAnimate Anyone: Image-to-video synthesis for character animation (humanaigc.github.io) 260 points by jasondavies 16 hours ago| hidepastfavorite110 comments crazygringo 12 hours agoJust wow. This is the first time I&#x27;ve seen AI generate convincing human movement. (And the folds of fabric in dresses seem to move realistically too!)Of course, the actual movement skeleton is coming from presumably real motion-capture, but still.I&#x27;m curious what the current state is of generating the movement skeletons, which obviously has a ton of relevance to video games. Where&#x27;s the progress in models where you can type \"a burly man stands with erect posture, then crouches down and sneaks five steps forward, then freezes in fear\" and output an appropriate movement skeleton? reply netruk44 11 hours agoparentThe input poses appear to be generated from OpenPose [0], which uses regular images as input. With the creation of stable diffusion video, you could theoretically prompt it to make a video of what you wanted, then run it through OpenPose.But I think the more realistic approach is to take a video of yourself doing the motions you want the AI to generate, and run that through OpenPose instead.Using just words leaves a lot to the model‚Äôs interpretation. I feel like you might wind up spending a lot of time manually fixing little things, similar to how you might infill the ‚Äúwrong‚Äù parts of an AI generated image. It might be easier to just take a 15 second video to get the exact skeleton animation you want.[0] https:&#x2F;&#x2F;github.com&#x2F;CMU-Perceptual-Computing-Lab&#x2F;openpose reply anonylizard 11 hours agoparentprevThis is already highly, highly relevant for 2d animation.Many complex moves (especially dancing) are filmed in video first, then the movement is traced over by hand. This is called Rotoscoping.This is basically auto-rotoscoping, and I expect it to see commercial usage within popular high budget projects within 2 years. Previously, even the highest budget anime couldn&#x27;t really afford 2d dance scenes due to the insane drawings required. reply Tanoc 7 hours agorootparentRotoscoping is directly tracing from film frame by frame. Most of the time animators instead use what are called reference key poses. For extremely stylized characters it can be difficult if not impossible to use rotoscoping for complex sequences because the proportions make the motions impossible. Instead specific frames where there&#x27;s resting motion in the sequence are pulled, and the animators translate the positions of the joints to the unusual silhouette of the character for the key frames. The Castlevania and Masters Of The Universe: Revelation series from Netflix use this quite a bit for realistic characters -- it was heavily used during the Trevor and Death fight and Skelegod versus Savage He-Man fight -- and Rise Of The Teenage Mutant Ninja Turtles and High Guardian Spice used it for their stylized characters for perspective and fight scenes.If this technology can instead be used to not only find the key poses but also roughly determine the inbetweens for a heavily stylized character like Rocko Wallaby or Blossom, Bubbles, and Buttercup then it is extremely useful. reply sircastor 9 hours agorootparentprevAs I recall, Ralph Bakshi used this to mixed effect back in the 70s with The Lord of The Rings animated films. reply COAGULOPATH 10 hours agoparentprevIt still doesn&#x27;t look ready for primetime to me. When the characters&#x27; hands move, they either gain a billion extra fingers or morph into fleshy shapeless clubs. It&#x27;s just hard to tell because the size of the video samples are small and poor quality. At 1080p, it would look extremely bad. reply sp0rk 8 hours agorootparentThe individual example videos are nearly 1080p (1024 height) and they look pretty good to me. There is some glitchiness on the hands but it&#x27;s very minor and could definitely be fixed easily in post-production or through further refinements. In fact, if you play the high resolution videos and pause at random points, it&#x27;s hard to even find a frame where there are extra fingers, though there are a few. reply lukew3 11 hours agoparentprevI don&#x27;t think that text to movement will be desired. There is a lot of room for interpretation there, and if a creator already has a vision for movement, they would probably choose to have the model mimic a video of them acting rather than trying to get lucky and correctly describe a set of movements. reply crazygringo 10 hours agorootparentTo the contrary, I think text-to-movement is going to be huge for videogames especially.I don&#x27;t see any other way to smoothly link 1,000 possible movements to and from each other, including when there are various fixed distances between you and a ladder or ledge etc.I think models will learn \"movement personalities\" the same way they learn a particular celebrity&#x27;s voice -- everybody moves with different rhythms. So your big burly action-hero character will move with a totally different rhythm from your waif-thin ethereal elf.But there will still be a textual vocabluary that generates the motion -- \"stealthily creeps to the door 2.2 meters away, taking 6.3 seconds, and then suddenly and dramatically opens it with a flourish\". reply spookie 6 hours agorootparentInverse kinematics have been solving most of the animation blending you&#x27;re describing here for years. Not to dismiss completely the potential of this, but there already pretty good, reliable ways to solve these issues. I believe the person you answered to to be correct in most cases, there is a lot of nuance lost if done by text. reply the8472 10 hours agoparentprev> Of course, the actual movement skeleton is coming from presumably real motion-capture, but still.I recall seeing from-scratch learned realistic motion somewhere. I think was meant for character animation that would more realistically interact with the terrain and cross obstacles in video games. So that should be synthesizable too. reply all2 12 hours agoprevI&#x27;ll mention Corridor Crew&#x27;s Rock, Paper, Scissors [0] as the previous state of the art in terms of character animation&#x2F;style transfer&#x2F;etc. using AI tooling.I imagine this will make the barrier to entry for animated stuff very, very low. You literally only need a character sheet.Also, the creep factor for AI girlfriends has ratcheted up another notch.[0] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7QAGEvt-btI reply autoexec 1 hour agoparentI think quality animation will still require a lot of skill. I do think that AI will be the end of animators who do nothing but inbetweens though. Considering the huge amount of skilled work involved and the low pay maybe that&#x27;s a good thing, but it could also make it harder to develop and discover amazing animators. reply tobr 12 hours agoprevThat‚Äôs quite astonishing. In just a few years this might even be generalized to work for characters other than conventionally attractive young women. reply Terretta 10 hours agoparentThere are a couple males (under Human, click 4th dot) and an ironman robot too (first under Humanoid). reply Probiotic6081 10 hours agoparentprevHey, they&#x27;re demonstrating it&#x27;s working for overly sexualised cartoon characters too! reply kaliqt 1 hour agorootparentThere&#x27;s no such thing as \"oversexualized\", stop shaming people for being normal. Your harassment is not welcome here. reply nwoli 12 hours agoparentprevYou really want that to be a comment you make on revolutionary new tech? Think of what you‚Äôd think of finding dismissive comments like that about the invention of the telephone reply simonw 10 hours agorootparentI agree with OP: having a demo video that&#x27;s 90% pretty young ladies is a bad look.I want to show this demo to people, but I&#x27;m honestly slightly embarrassed to do so because it&#x27;s such an obvious example of the male gaze. It&#x27;s kind of tacky, and it distracts from the extremely impressive technology that&#x27;s being demonstrated. reply nmfisher 8 hours agorootparentThese shitty dances are all over TikTok, more than 90% of which are women (in fact, I don‚Äôt think I‚Äôve ever seen a man doing one). reply spiderice 7 hours agorootparentThere are a ton of men doing dances on TikTok. They just don‚Äôt get shown to you because the algorithm knows what keeps your attention. reply all2 1 hour agorootparentI&#x27;d love to see how this imagen works for some of the complex footwork or acrobatics videos that trend. Testing the limits with extreme poses would be quite interesting. reply JSavageOne 5 hours agorootparentprevYea they definitely should&#x27;ve thrown in some ugly ones in there reply redleggedfrog 11 hours agorootparentprevI&#x27;ll go one further. The uncomfortable fixation on attractive women for the models is only an inkling of what is to be the primary application of such technology which will be porn. No matter how amazing that tech underlying these animated stills may be be the race for the lowest common denominator is already shining through on their examples. Don&#x27;t think for one moment they didn&#x27;t choose those on purpose. They know where it&#x27;s heading. reply TeMPOraL 10 hours agorootparentPorn isn&#x27;t the lowest common denominator. Advertising is. Which they more than hint at through the explicit mentions of fashion applications and comparisons to other models in this space.People who thinking porn is anywhere near the worst application are, IMO, letting their sensibilities cloud their judgement. reply dvngnt_ 11 hours agorootparentprevFor ML questioning the data seems okay reply ChatGTP 9 hours agorootparentprevYou‚Äôre arguing that this moment is important in history and the parent is arguing that if true, the a less superficial subject should be presented to demonstrate.When I see all this generative AI stuff, most people are excited about generating manga, computer games and porn.I know it‚Äôs exciting if you‚Äôre into consuming media. But I think the average person would look at this ‚Äúbreathtaking moment‚Äù and be surprised computers couldn‚Äôt do this already, honestly. reply elpocko 12 hours agoprevWhy would you publish your findings on Github of all places, but not release any code? I think this trend is really weird. reply errnoh 3 minutes agoparentAt least based on my observations it&#x27;s been common practice in ML papers for some years already. Usually releasing Github hosted project page and a repository with the same information, then releasing the code on that repo afterwards at some point.I don&#x27;t feel that&#x27;s an issue. A lot more people are able to see what&#x27;s happening on the bleeding edge than if they&#x27;d just release the paper without accompanying demo page, and faster than if they&#x27;d wait for the code to be ready for release? Of course one can argue that \"they should just release whatever code they have instantly\", but that&#x27;s their choice if they want to clean it up, remove secrets etc. reply octagons 11 hours agoparentprevJust guessing based on the authors‚Äô names and their affiliation with Alibaba Group, but I think this research was published by exclusively Chinese citizens.In my experience, it‚Äôs difficult to operate a small, personal website from within China because of their regulations in regards to non-government websites. Because of this, you will often find that Chinese citizens will use approved (or at least unrestricted) services like GitHub pages.Having worked closely with many businesses based in China due to my hobbies, I have noted that services like Google Docs and Drive are favored for this reason.I would guess there are ways to host content like this more easily on platforms that are only accessible within China or are not navigable without the user understanding Chinese language.I would also guess that this is part of the reason why services that target customers in China tend to become ‚Äúsuper apps‚Äù that combine several services that non-Chinese users would expect to find on disparate sites. For example, services may combine a social media style newsfeed&#x2F;interaction API, banking, email, shopping, and food delivery into a single platform. reply nprateem 7 hours agorootparentAI-generated? reply octagons 4 hours agorootparentNope, but I can see why you‚Äôd think that. Honestly, I wanted to choose my words carefully because of the often politically charged tone that can often be interpreted&#x2F;intended when uttering the word ‚ÄúChina.‚ÄùI studied Chinese language, art, history, politics, and culture in university and lived in Shanghai after graduating. While I lived there, I helped some friends setup some domains for hosted GMail&#x2F;e-commerce and learned some interesting things about business there.I also spend some of my spare time finding vulnerabilities in hardware devices and building my own small electronics&#x2F;maker projects. This has given me ample opportunities to work with (i.e. get help from or provide feedback to) many different companies based in China. reply nprateem 2 hours agorootparentNothing to do with China. It seemed to follow the structured tone of chatgpt. This is why attempts to identify AI content through analysis are doomed. reply raincole 6 hours agorootparentprevThe GP is a quite informed and informative comment. Honestly, randomly saying \"AI-generated\" under others&#x27; comments is a much more bot-like behavior. reply crazygringo 12 hours agoparentprevBecause it&#x27;s basically free webhosting and you don&#x27;t need to manage registering a domain?I don&#x27;t know for sure, but that&#x27;s my guess. You could achieve something similar with S3, but you need a credit card attached, and then you need to worry about whose credit card, and what if it gets unexpected traffic and who will pay...You could use Google Sites as well, but then you need to buy a domain, which again means requiring a credit card, and whose responsibility is it to pay and for how many years?I don&#x27;t think it&#x27;s mostly about the cost, I think it&#x27;s mostly about just not having to link a credit card? reply ryanackley 10 hours agoparentprevI&#x27;m fine with them hosting on Github but they have a link that specifically says \"Code\" that takes you to a relatively empty github repo with no code. Hopefully, putting the actual code on github is a work in progress. reply runeblaze 4 hours agoparentprevWhen researchers finish papers often they are too exhausted to choose anything beyond the \"easiest\" path, in this case maybe using an existing template and a gh pages website to attract publicity.(I know people are often too exhausted to even upload the preprint... They take a break and upload later) reply Kiro 11 hours agoparentprevWhat&#x27;s the alternative? I haven&#x27;t found anything that&#x27;s easier to deploy and manage than GitHub Pages. reply elpocko 11 hours agorootparentI always thought of Github as a place to host software projects, but it does work as an image hosting service as well I guess. reply radarsat1 1 hour agoprevI&#x27;m as interested as anyone in the methods behind this and find deep learning just continues to amaze and I really enjoy working in the AI space. Having said that, seeing that we now essentially have the capability to synthesize videos of people doing things they haven&#x27;t done and saying things they haven&#x27;t said, I really do worry about the potential for abuse of this technology once it&#x27;s really perfected. What does it mean for society when can no longer enter recordings into evidence? reply unsupp0rted 1 hour agoparentNet benefit. Before we should have questioned the veracity and applicability of all evidence. Now we have no choice. reply EwanG 12 hours agoprevI&#x27;m just waiting for the tool or toolchain where I can take a manga that I like that doesn&#x27;t have an anime, and get a season or two out to watch when I feel like it rather than wait for it to get an official release.Bonus points if I can let the tool ingest season 1 or an OVA of said material where a season 2 is never going to come (looking at you \"No Game, No Life\") reply sgbeal 1 hour agoparent> Bonus points if I can let the tool ingest season 1 or an OVA of said material where a season 2 is never going to come (looking at you \"No Game, No Life\")At long last, _Firefly_ Season 2 is within our grasp! reply all2 12 hours agoparentprevTo be honest, all the pieces are there to create a pipeline. There&#x27;s still a lot of work on the human side for shot composition, camera movement, etc., but the pieces all exist right now to make this a reality. reply __loam 10 hours agoparentprevThis is so bleak. I hope artists get enough legal protection in the future to stop ghouls from doing this to their work. reply matheusmoreira 8 hours agorootparentI hope not. I want AI to hammer in the final nails into the coffin of intellectual property so we can finally bury it. I want this technology to be so ubiquitous it cannot be controlled. I want them to give up on controlling things with such nonsense \"legal protections\", society should change permanently instead. reply an_aparallel 4 hours agorootparentstem splitting makes me feel that way - i feel like legal music protection is just going to be turned on its head...when people can now sample JUST jimmy&#x27;s guitar, just cobhams drums...JUST elton&#x27;s piano...i wonder if its already happening under our noses? reply __loam 8 hours agorootparentprevIn the absence of a legal framework, I hope enough people recognize that scooping someone else&#x27;s work into some kind of grotesque anime sausage machine is super shitty to do without the input of the author, and the people doing it get ruthlessly mocked. Most anime production involves the input of and express permission from the author.Imagine someone doing this to show us \"their\" ending of berserk, for example. Just utterly disrespectful to Miura&#x27;s legacy. The man dedicated his life to his art and you reinterpreted it like an ass hole because you lack patience and demand more content at the expense of all artistic agency. Complete trash. reply hau 10 minutes agorootparent>disrespectful to Miura&#x27;s legacyYou mean highest form of flattery? Literally participating and reflecting, trying to recreate and reinterpret is how humans integrate, show acceptance, make it part of themselves and culture. His art is not sacred, nor is it absolutely original, nor is it made in isolation from the world. Miura&#x27;s legacy is sharing his reinterpretation of whatever. For it to exist there must be less elitist underground bunkers full of sacred IP which we unable to think about, discuss and share our interpretations of. reply matheusmoreira 7 hours agorootparentprev> someone else&#x27;s workJust a number really. That&#x27;s all intellectual work is: a number. All numbers already exist, humans just discover them.I just can&#x27;t take it seriously, this notion that intellectual work is somehow \"special\" and deserving of \"protections\". To me it&#x27;s as delusional as trying to own numbers.> input of the authorWhy do you need somebody&#x27;s permission to do math with numbers? Makes no sense whatsoever.> Imagine someone doing this to show us \"their\" ending of berserkNo need to imagine it when fanfiction.net exists. AI will make it even easier. reply progman32 6 hours agorootparentTo play devil&#x27;s advocate:The idea being that if we prevent people from doing specific math on specific numbers, the more proficient number-discoverers will have greater incentive to discover more of those pleasing numbers, and we all benefit.... Which to me sounds wishful, given the reality of the system as implemented today.Perhaps someone can help me understand the appeals to creative sanctity. Does making my own fan fiction cheapen the original work? Why is it immoral to change a fiction to suit my own preferences, as long as original authorship is not implied? I mod my single player video games all the time to enjoy them more. I have no qualms about patching out a tech tree if the grinding isn&#x27;t too my taste, is this an affront? How about covering my favorite song? Making a custom cover for a special book in my collection? Skipping the scariest part of a horror movie? Singing a new jazz song imitating Armstrong&#x27;s style? Help me understand. reply apersona 6 hours agorootparentprevThis is so reductionist, it&#x27;s not even funny.Imagine debugging your own code (your code being your intellectual work) and this guy barges into your room and says \"Why are you wasting your time? Why didn&#x27;t you just pick a better &#x27;number&#x27;?\" reply DeIlliad 6 hours agorootparentListen, I&#x27;m on your side of the argument and this example makes no sense at all. reply raincole 6 hours agorootparentprevYeah, if you follow this logic... then humans are just a bunch of molecules. Mostly water. Some protein and fat.Why do humans have any right at all? I can&#x27;t take this seriously. Molecules don&#x27;t have rights. reply erdaniels 6 hours agorootparentprevI&#x27;m thinking of a number, but I&#x27;m definitely not going to tell you it now. It&#x27;s all mine. reply DeIlliad 7 hours agorootparentprevThis \"just math\" argument is so reductionist and unhelpful. reply creata 5 hours agorootparentprev> Why do you need somebody&#x27;s permission to do math with numbers? Makes no sense whatsoever.It&#x27;s part of a very sensible legal framework to incentivize the creation and publication of hard-to-find numbers that you value very much.I hate intellectual property as much as the next guy, but it&#x27;s not nonsense.> Just a number really. That&#x27;s all intellectual work is: a number.You saw that everything could be encoded in a number, and instead of expanding your view of what a number could be, it diminished your view of everything else. That&#x27;s depressing. reply dclowd9901 6 hours agorootparentprevYou sound like a peach.I‚Äôd love to know if you‚Äôve actually ever created something novel that was good. Not code. Not math. Something uniquely original with no foundational basis. reply owenpalmer 7 hours agorootparentprevThere is no amount of legal protection that can prevent 1 individual from generating billions of movies based on an artists style. It&#x27;s time embrace what the act of creating art will mean in the future. You can&#x27;t just regulate away the driving force automation. reply boppo1 7 hours agorootparentprevArtist here. I can hardly protect my work from ghouls as it is. If I want to work for a company as an illustrator, it&#x27;s like pulling teeth to get my personal work separated from their IP. Many contracts I&#x27;ve seen were &#x27;you get salary, we get everything you make&#x27;. Hell I&#x27;ve seen a contract try to claim my knowledge of niche animation tools was company property. reply ChatGTP 9 hours agorootparentprevThey won‚Äôt. I wonder if there will still be demand for original content though because it creates a connection. A communication of what others have to say ? reply dclowd9901 7 hours agorootparentprevIt is. People who say AI won‚Äôt kick people to the curb permanently are either completely ignorant or selling it. reply Pxtl 12 hours agoparentprev\"Hey Bing, can you make me a live action version of the Scouring of the Shire as if it were part of the Peter Jackson Lord of the Rings movies?\". reply thyrox 9 hours agoprevJust imagine in a few years there will be a site like YouTube aka videogpt where all the videos are created on the fly, like chatgpt does for text.From step by step repairing my electronics to learning about science it will all be customised according to my learning level and focus on things I want to know more. And hopefully without that a word from our sponsors section. reply chii 9 hours agoparent> And hopefully without that a word from our sponsors section.it won&#x27;t have this, because the product placement would be seamlessly integrated into the content so naturally that you don&#x27;t notice being subliminally persuaded to think or buy what they want you to! reply corobo 7 hours agorootparentAnd don&#x27;t even think of pausing a TV show now that the paused characters can animate and sell you products!Actually that tech would be kinda cool. The reality would be bleak, but the tech would be cool.If nothing else have the characters look out of the TV and tap their foot impatiently like Sonic used to in the videogame when you stopped playing for a moment reply matheusmoreira 8 hours agorootparentprevLooks like we&#x27;re gonna need AI ad blockers soon. reply mbo 5 hours agoprevThe choice of test images here feels super inappropriate. Surely there&#x27;s a more diverse and standardized dataset for benchmarking this task than what was chosen here.I quote similar criticism from Dianne P. O&#x27;Leary:> Suggestive pictures used in lectures on image processing ... convey the message that the lecturer caters to the males only. For example, it is amazing that the \"Lena\" pin-up image is still used as an example in courses and published as a test image in journals today. reply numpad0 1 hour agoparentI don&#x27;t get this \"suggestive pictures cater to male\" logic. The reality don&#x27;t support it. It&#x27;s almost like saying only men are humans or only men has certain functions so pictures of certain objects are for men only. It&#x27;s just isn&#x27;t how anything works. reply kaliqt 1 hour agoparentprevThere&#x27;s absolutely nothing wrong with it. Most researchers are male, and this is what helps people get through the work day. Stop trying to harass people. reply spondylosaurus 1 hour agorootparentWhat part of their comment is harassment? reply newfriend 3 hours agoparentprevThis appears to be published by a group of folks from China where political correctness hasn&#x27;t infected every aspect of society. It&#x27;s ok to not be offended by everything. reply hiddencost 2 hours agorootparentDiversity makes stuff better, and even if it didn&#x27;t, it would be worth while.That folks like you think advocating for diversity is about \"political correctness\" rather than building a world that is better to live in is very sad. reply KHRZ 5 minutes agorootparentThis is also China&#x27;s justification for being a dictatorship. \"We are just different and it works best for us, not every country must be a democrazy.\" reply vunderba 2 hours agorootparentprevMore diversity is fine (give me some manly men aka the burly Simpson towel man) but you ignored the fact that OP also very much handed down a virtue signaling level of high handed \"judgment\" of the images in question, claiming that their inclusion was \"super inappropriate\"...I mean, give me a break. They&#x27;re completely tame other than being mostly women, but apparently that alone is enough to send people into a fit of hysterics. reply newfriend 2 hours agorootparentprevThat&#x27;s just like.. your opinion manDiversity is fine. Forced diversity is not. Having different opinions and things that are not forcefully diverse is it&#x27;s own form of diversity. Making everyone conform to your ideas is not diversity. reply unsupp0rted 1 hour agorootparentprev> Diversity makes stuff betterCitation needed reply autoexec 1 hour agorootparenthttps:&#x2F;&#x2F;www.oxfordreference.com&#x2F;display&#x2F;10.1093&#x2F;oi&#x2F;authority... reply kaliqt 1 hour agorootparentprevNo, it does not. reply IshKebab 2 hours agoparentprevI wonder what your comment would say if the examples were all men. reply Vinnl 2 hours agorootparentI imagine it might be much the same if those men were mostly underwear models flexing their muscles?Though of course, that would&#x27;ve mostly been super uncomfortable if the field was also dominated by women. reply Peritract 2 hours agorootparentprevThe point is that they aren&#x27;t. reply modeless 5 hours agoprevThis looks impossibly good. I think these samples are cherry-picked and I also think the system is essentially overfit on these datsets and would not generalize to anything even slightly different. I want to see their failure cases! Lack of failure cases is a red flag.Still, though, it could be useful in its current form, and making a more general system might mostly be a matter of collecting appropriate training data. Impressive work, just needs a more realistic presentation. reply sys32768 11 hours agoprevJust imagine when this merges with 3D modeling and VR.The VR pr0n, the video games with dynamic AI characters. Dead actors and historical figures resurrected into movies and education.I&#x27;m not so scared about my future nursing home now. reply BizarroLand 9 hours agoparentLive people will probably make hella money with VR teledildonics in the space between when the products first become available and when AI learns the process well enough to outperform any human at a fraction of the cost. reply numpad0 1 hour agorootparentHumans have always outperformed generators in terms of discriminating live captures against generated data. There is no guarantee that that continues, but there is no sign of that balance of power changing. reply esotericsean 12 hours agoprevPretty huge breakthrough. Hopefully we&#x27;ll be able to access this soon. Between this, SVD, and others, 2024 is going to be the year of AI Video. reply esafak 11 hours agoparentI suppose SVD means something other than singular value decomposition, because that is not new? reply ilaksh 11 hours agorootparentThe tendency to instantly and liberally invent and use acronyms like this is very annoying and a poor communication style.I think they mean Stable Video Diffusion. reply esafak 10 hours agorootparentGood luck to them if they choose to name it SVD; they&#x27;re going to get buried in search engines. reply matheusmoreira 8 hours agorootparentprevSVD could also mean this badass sniper rifle.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SVD_(rifle) reply nmfisher 8 hours agorootparentprevStable Video Diffusion. reply ChatGTP 10 hours agoparentprevI don‚Äôt know why but who cares ? People who like consuming TikTok videos ? reply justanotherjoe 6 hours agoprevwhy is everything from this space so horny. I&#x27;m not sure if I like it or not. Obviously it&#x27;s a bit problematic but on the other hand i welcome when people are honest about their intentions like this. reply numpad0 1 hour agoparentBecause trying to create porn gives a strong and instantest gratification. Things work out, you get more energy to the brain. Things don&#x27;t work, and the brain goes head first into debug logs. When culturally not inhibited, it grants you superpowers. reply siddbudd 5 hours agoparentprevThey made the model learn by watching TikTok videos. WYSIWYG. reply hombre_fatal 11 hours agoprevThis is absolutely insane. The DreamPose output they compare themselves to is less than one year old.It&#x27;s funny to go back to the first avocado chair or deep dream images that wowed me just a couple years ago.I can&#x27;t help but feel lost in the pace of tech. reply johnyzee 11 hours agoparentIt is a massive seismic shift. Almost any project in the works right now, that involves visual content, looks like it will be antiquated in a very short time. Including some that I am working on :(. The new possibilities though... Breathtaking to think about. reply achatham 8 hours agoprevWhat&#x27;s the trigger for all the diffusion video projects this week? Are they all at the same conference, or did one trigger everyone to rush to publish? I&#x27;m grateful but curious. Animating still pictures from my kids&#x27; books is my goal and it seems close to plausible just from this week&#x27;s advances. reply brunorsini 6 hours agoprevApologies for the lazy question... But with so many incredible models being released, how are you all keeping up? Are you mostly trying them all out on the web or are you installing each of them locally? reply EGreg 1 hour agoprevHow did they make the girls smile as she dances in the last ones? reply dartos 5 hours agoprevWhy even have a GitHub? reply kamikaz1k 9 hours agoprevPage is crashing my mobile chrome tab‚Ä¶ reply allanrbo 12 hours agoprevVery impressive quality. reply rvz 7 hours agoprevI predict that Meta will release another open source version of this, given their advanced research in pose estimation. reply bitwize 8 hours agoprevThis is why actors are striking. reply yreg 7 hours agoparentThe strike has ended on Nov 9th and this wasn&#x27;t the main cause.Anyway, they will have to adapt. The diffusion models are not replacing e.g. live theater. reply huytersd 11 hours agoprevHow do you generate the movement data? reply netruk44 11 hours agoparentIt looks like they‚Äôre using OpenPose [0] images fed to a special ‚Äúpose guider‚Äù model. You can make them from just regular video.[0] https:&#x2F;&#x2F;github.com&#x2F;CMU-Perceptual-Computing-Lab&#x2F;openpose reply xrd 9 hours agoprev [‚Äì] Great, this looks perfect as a way to sell more fast fashion, the 2nd worst polluting industry in the world. So many better applications of this technology and instead they showcase dancing boring body stereotypes. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper introduces a framework for generating character videos from still images using diffusion models.",
      "Novel techniques are employed to maintain consistency, control, and continuity in character animation.",
      "The proposed approach outperforms existing methods and achieves state-of-the-art results in fashion video and human dance synthesis."
    ],
    "commentSummary": [
      "\"Animate Anyone\" is an AI model that generates realistic human movement for character animation in video games and 2D animation.",
      "The technology presents a more efficient alternative to traditional animation techniques.",
      "Concerns are raised about the accuracy and readiness of AI-generated movements, as well as the impact on intellectual property rights and creative expression.",
      "Participants share their experiences and opinions on these topics, discussing the future applications and limitations of the technology."
    ],
    "points": 260,
    "commentCount": 110,
    "retryCount": 0,
    "time": 1701366357
  },
  {
    "id": 38482007,
    "title": "Marker: Efficiently Convert PDF to Markdown with Higher Accuracy",
    "originLink": "https://github.com/VikParuchuri/marker",
    "originBody": "Marker Marker converts PDF, EPUB, and MOBI to markdown. It's 10x faster than nougat, more accurate on most documents, and has low hallucination risk. Support for a range of PDF documents (optimized for books and scientific papers) Removes headers/footers/other artifacts Converts most equations to latex Formats code blocks and tables Support for multiple languages (although most testing is done in English). See settings.py for a language list. Works on GPU, CPU, or MPS How it works Marker is a pipeline of deep learning models: Extract text, OCR if necessary (heuristics, tesseract) Detect page layout (layout segmenter, column detector) Clean and format each block (heuristics, nougat) Combine blocks and postprocess complete text (heuristics, pdf_postprocessor) Relying on autoregressive forward passes to generate text is slow and prone to hallucination/repetition. From the nougat paper: We observed [repetition] in 1.5% of pages in the test set, but the frequency increases for out-of-domain documents. In my anecdotal testing, repetitions happen on 5%+ of out-of-domain (non-arXiv) pages. Nougat is an amazing model, but I wanted a faster and more general purpose solution. Marker is 10x faster and has low hallucination risk because it only passes equation blocks through an LLM forward pass. Examples PDF Type Marker Nougat Think Python Textbook View View Think OS Textbook View View Switch Transformers arXiv paper View View Multi-column CNN arXiv paper View View Performance The above results are with marker and nougat setup so they each take ~3GB of VRAM on an A6000. See below for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks. Installation This has been tested on Mac and Linux (Ubuntu and Debian). You'll need python 3.9+ and poetry. First, clone the repo: git clone https://github.com/VikParuchuri/marker.git cd marker Linux Install system requirements Optional: Install tesseract 5 by following these instructions or running scripts/install/tesseract_5_install.sh. Install ghostscript > 9.55 by following these instructions or running scripts/install/ghostscript_install.sh. Install other requirements with cat scripts/install/apt-requirements.txtxargs sudo apt-get install -y Set the tesseract data folder path Find the tesseract data folder tessdata with find / -name tessdata. Make sure to use the one corresponding to the latest tesseract version if you have multiple. Create a local.env file in the root marker folder with TESSDATA_PREFIX=/path/to/tessdata inside it Install python requirements poetry install poetry shell to activate your poetry venv Update pytorch since poetry doesn't play nicely with it GPU only: run pip install torch to install other torch dependencies. CPU only: Uninstall torch, then follow the CPU install instructions. Mac Install system requirements from scripts/install/brew-requirements.txt Set the tesseract data folder path Find the tesseract data folder tessdata with brew list tesseract Create a local.env file in the root marker folder with TESSDATA_PREFIX=/path/to/tessdata inside it Install python requirements poetry install poetry shell to activate your poetry venv Usage First, some configuration: Set your torch device in the local.env file. For example, TORCH_DEVICE=cuda or TORCH_DEVICE=mps. cpu is the default. If using GPU, set INFERENCE_RAM to your GPU VRAM (per GPU). For example, if you have 16 GB of VRAM, set INFERENCE_RAM=16. Depending on your document types, marker's average memory usage per task can vary slightly. You can configure VRAM_PER_TASK to adjust this if you notice tasks failing with GPU out of memory errors. By default, the final editor model is off. Turn it on with ENABLE_EDITOR_MODEL. Inspect the settings in marker/settings.py. You can override any settings in the local.env file, or by setting environment variables. Convert a single file Run convert_single.py, like this: python convert_single.py /path/to/file.pdf /path/to/output.md --parallel_factor 2 --max_pages 10 --parallel_factor is how much to increase batch size and parallel OCR workers by. Higher numbers will take more VRAM and CPU, but process faster. Set to 1 by default. --max_pages is the maximum number of pages to process. Omit this to convert the entire document. Make sure the DEFAULT_LANG setting is set appropriately for your document. Convert multiple files Run convert.py, like this: python convert.py /path/to/input/folder /path/to/output/folder --workers 10 --max 10 --metadata_file /path/to/metadata.json --min_length 10000 --workers is the number of pdfs to convert at once. This is set to 1 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Parallelism will not increase beyond INFERENCE_RAM / VRAM_PER_TASK if you're using GPU. --max is the maximum number of pdfs to convert. Omit this to convert all pdfs in the folder. --metadata_file is an optional path to a json file with metadata about the pdfs. If you provide it, it will be used to set the language for each pdf. If not, DEFAULT_LANG will be used. The format is: --min_length is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you're processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down) { \"pdf1.pdf\": {\"language\": \"English\"}, \"pdf2.pdf\": {\"language\": \"Spanish\"}, ... } Convert multiple files on multiple GPUs Run chunk_convert.sh, like this: MIN_LENGTH=10000 METADATA_FILE=../pdf_meta.json NUM_DEVICES=4 NUM_WORKERS=15 bash chunk_convert.sh ../pdf_in ../md_out METADATA_FILE is an optional path to a json file with metadata about the pdfs. See above for the format. NUM_DEVICES is the number of GPUs to use. Should be 2 or greater. NUM_WORKERS is the number of parallel processes to run on each GPU. Per-GPU parallelism will not increase beyond INFERENCE_RAM / VRAM_PER_TASK. MIN_LENGTH is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you're processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down) Benchmarks Benchmarking PDF extraction quality is hard. I've created a test set by finding books and scientific papers that have a pdf version and a latex source. I convert the latex to text, and compare the reference to the output of text extraction methods. Benchmarks show that marker is 10x faster than nougat, and more accurate outside arXiv (nougat was trained on arXiv data). We show naive text extraction (pulling text out of the pdf with no processing) for comparison. Speed Method Average Score Time per page Time per document naive 0.350727 0.00152378 0.326524 marker 0.641062 0.360622 77.2762 nougat 0.629211 3.77259 808.413 Accuracy First 3 are non-arXiv books, last 3 are arXiv papers. Method switch_trans.pdf crowd.pdf multicolcnn.pdf thinkos.pdf thinkdsp.pdf thinkpython.pdf naive 0.244114 0.140669 0.0868221 0.366856 0.412521 0.468281 marker 0.482091 0.466882 0.537062 0.754347 0.78825 0.779536 nougat 0.696458 0.552337 0.735099 0.655002 0.645704 0.650282 Peak GPU memory usage during the benchmark is 3.3GB for nougat, and 3.1GB for marker. Benchmarks were run on an A6000. Throughput Marker takes about 2GB of VRAM on average per task, so you can convert 24 documents in parallel on an A6000. Running your own benchmarks You can benchmark the performance of marker on your machine. First, download the benchmark data here and unzip. Then run benchmark.py like this: python benchmark.py data/pdfs data/references report.json --nougat This will benchmark marker against other text extraction methods. It sets up batch sizes for nougat and marker to use a similar amount of GPU RAM for each. Omit --nougat to exclude nougat from the benchmark. I don't recommend running nougat on CPU, since it is very slow. Limitations PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address: Marker will convert fewer equations to latex than nougat. This is because it has to first detect equations, then convert them without hallucation. Whitespace and indentations are not always respected. Not all lines/spans will be joined properly. Only languages similar to English (Spanish, French, German, Russian, etc) are supported. Languages with different character sets (Chinese, Japanese, Korean, etc) are not. This works best on digital PDFs that won't require a lot of OCR. It's optimized for speed, and limited OCR is used to fix errors. Commercial usage Due to the licensing of the underlying models like layoutlmv3 and nougat, this is only suitable for noncommercial usage. I'm building a version that can be used commercially, by stripping out the dependencies below. If you would like to get early access, email me at marker@vikas.sh. Here are the non-commercial/restrictive dependencies: LayoutLMv3: CC BY-NC-SA 4.0 . Source Nougat: CC-BY-NC . Source PyMuPDF - GPL . Source Other dependencies/datasets are openly licensed (doclaynet, byt5), or used in a way that is compatible with commercial usage (ghostscript). Thanks This work would not have been possible without amazing open source models and datasets, including (but not limited to): Nougat from Meta Layoutlmv3 from Microsoft DocLayNet from IBM ByT5 from Google Thank you to the authors of these models and datasets for making them available to the community!",
    "commentLink": "https://news.ycombinator.com/item?id=38482007",
    "commentBody": "Marker: Convert PDF to Markdown quickly with high accuracyHacker NewspastloginMarker: Convert PDF to Markdown quickly with high accuracy (github.com/vikparuchuri) 260 points by sebg 8 hours ago| hidepastfavorite26 comments scary-size 5 minutes agoNice work. I tend to do most of my longer reading on an e-reader. PDFs, especially multi-column layouts, are a nightmare with the out-of-the-box offerings from Amazon Kindle or Pocketbook. This looks like something that&#x27;ll improve my experience quite a lot. reply iamflimflam1 4 minutes agoprevHow good is tesseract for OCR nowadays? I tried using it a while back and it was nowhere near as good as the online offerings from AWS, Azure and GCP. reply alsodumb 7 hours agoprevGreat work! I am a bit confused with the comparison with nougat throughout the repo. Nougat was specifically trained for academic documents, and I don&#x27;t think anyone ever claimed Nougat was the best OCR model out there. That&#x27;s kinda clear in your benchmark too where you mention that nougat has higher accuracy on arxiv documents. You also mention that marker will convert fewer equations when compared to nougat, and yet compare with nougat in terms of speed? (again, only complaining because it&#x27;s a model designed for academic documents).For anyone trying to do OCR on any pdf with math in it, definitely do try nougat. It&#x27;s very easy to install (just a python package), and extracts the math, text, tables and beyond (in a .mmd file) with a single command line command. It also runs reasonably fast for personal uses - it takes about 30 seconds to convert a 6 page document using CPU only on my 4 year old i5 laptop. reply fshr 5 hours agoparent> I don&#x27;t think anyone ever claimed Nougat was the best OCR model out thereComparing two things doesn&#x27;t inherently imply the previous thing was touted about with superlatives. It&#x27;s just a way to juxtapose the new thing with something that may be familiar. As you said, nougat is easy to install&#x2F;run so it makes sense they&#x27;d compare it. Would it be better if they could add more libraries in the comparison? Absolutely; that&#x27;d be helpful. reply defsectec 5 hours agoparentprevHow do you think nougat would handle RPG rulebook PDFs?I&#x27;m looking for a food OCR model to help me transcribe sections of RPG books to markdown. Ideally, I&#x27;d like the emphasis such as bold or italics to be transcribed.The combo of text, numbers, and math symbols seems similar to technical and academic writing, but often has weird formatting, text boxes in the margins, and many diagrams. reply alsodumb 3 hours agorootparentI&#x27;m not completely sure to be honest, but you should try it yourself with a sample page! I believe hugging face hosts it online on their demo pages so you don&#x27;t even have to install the package to test on one page. reply KeplerBoy 12 minutes agoprevGreat stuff!I have a question regarding the output of Nougat: Where do the \"hallucinations\" come from (just scroll through the Nougat output of the Think Python example to see what I mean)? reply defsectec 6 hours agoprevThis looks amazing, I&#x27;ll have to play around with this over the weekend.I regularly hand transcribe RPG PDFs scans from dubious sources that have not always been run through OCR to have selectable text. If it has, it wasn&#x27;t always done very well.It&#x27;s literally faster to type it all myself than fix all the errors from copy-pasting (or after using OCR to turn it into text).Even if the file was an official PDF the formatting would often get screwed up with lots of double or triple spaces and even tabs included between words.This would save so much time if I can get it to work. Thanks for sharing! reply crooked-v 4 hours agoparent> I regularly hand transcribe RPG PDFs scans from dubious sourcesHeh, that was my immediate thought too. There&#x27;s a ton of RPG stuff that never had any kind of physical release and is totally orphaned as IP. reply danofsteel32 44 minutes agoprevI have an odd usecase that I&#x27;ve yet to find a good solution to: Reading construction documents (Blueprints are always PDF). I&#x27;ve had much better luck parsing DXF (AutoCAD) files but it&#x27;s not always easy to get an architect to send them to me even if I&#x27;m the GC on the job. reply potatoman22 7 hours agoprevThis seems like a great tool to help migrate my notes out of OneNote reply smusamashah 25 minutes agoparentHow can it help with OneNote? reply mlhpdx 7 hours agoprevNice. This would have been very helpful when I was building an e-discovery document processing engine. Back then we could get text out (OCR, so kind of) but it was a bear to present. Markdown would have been a whole lot easier. reply airstrike 6 hours agoprevReally interesting stuff... it might be worth adding some before-and-after examples to the repo.What kind of PDF are you tweaking it for? How does it handle handwritten annotations? reply lgats 5 hours agoprevIt&#x27;d be really great if there was something like this that also supported image extraction reply afandian 14 minutes agoparentMy current workflow (for getting a magazine onto a website) is Calibre&#x27;s HTMLZ export, then through Pandoc to markdown. It produces good enough Markdown to feed in to Hugo, and extracts images.I&#x27;ve been through a number of options in the past and this is what I&#x27;ve settled on. reply prmoustache 1 hour agoparentprevEspecially for those that want to move out of Confluence. It is rather easy to obtain a docx or pdf from the API as well as the raw, uncompressed attachements, a bit more complicated to convert said files to markdown with full quality attachements and no formatting errors on every pages. reply dr_kiszonka 6 hours agoprevNice! The only missing feature is conversion of plots to ASCII art ; ) reply ploum 5 minutes agoparentThis could be achieved with chafa.py :https:&#x2F;&#x2F;chafapy.mage.black&#x2F;https:&#x2F;&#x2F;hpjansson.org&#x2F;chafa&#x2F; reply jnathsf 6 hours agoprevAre there any other libraries or online services that does this well? I have a large number of PDFs from government agencies. I‚Äôve tried AWS Textract and works fairly well. reply wriggles 0 minutes agoparentHandwritingocr.com is aimed specifically at handwriting, and will do that better than Textract and co, but works well for printed text too.Disclaimer: I launched handwritingocr.com recently. reply nicodjimenez 5 hours agoparentprevTry Mathpix reply nicodjimenez 5 hours agoprev [‚Äì] Mathpix does this and also supports handwriting, in addition to capturing all figures, and also supports over 50 languages. reply phi-go 2 hours agoparentYou should mention that you are the CEO of Mathpix. reply pk-protect-ai 55 minutes agorootparentIt actually doesn&#x27;t matter. For my cases, I found Mathpix to be much more reliable than Nougat, for example. So, when you have hundreds of documents to convert a year and little time for manual labor on the results, paying a yearly \"pro\" subscription fee is worth it. However, it will really hit your pocket when you need to prepare datasets from thousands of PDFs... That&#x27;s what you can&#x27;t afford without a budget allocation from your project. reply tomr75 4 hours agoparentprev [‚Äì] $$ replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Marker is a tool that converts PDF, EPUB, and MOBI files to markdown.",
      "It is faster and more accurate than the previous tool nougat, with a lower risk of errors.",
      "Marker supports a range of PDF documents, removes headers/footers/artifacts, converts equations to latex, and formats code blocks and tables."
    ],
    "commentSummary": [
      "Marker is a tool that efficiently converts PDF files to Markdown format, enhancing the reading experience on e-readers, particularly for PDFs with multi-column layouts.",
      "Users have provided positive feedback about the tool and have compared it to other OCR models like Nougat.",
      "Marker is beneficial for tasks such as transcribing RPG PDFs, reviewing construction documents, and transferring notes from OneNote. Users have suggested additional features like image extraction and support for other file formats such as docx."
    ],
    "points": 259,
    "commentCount": 25,
    "retryCount": 0,
    "time": 1701395625
  },
  {
    "id": 38474696,
    "title": "Large Language Models Lack Deep Insights or a Theory of Mind",
    "originLink": "https://arxiv.org/abs/2311.16093",
    "originBody": "Computer Science > Machine Learning arXiv:2311.16093 (cs) [Submitted on 27 Nov 2023] Title:Have we built machines that think like people? Authors:Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, Eric Schulz Download PDF Abstract:A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. The models exhibit a rudimentary understanding of physical laws and causal relationships, but their performance is hindered by a lack of deeper insights-a key aspect of human cognition. Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models, and point out the importance of cognitively-inspired benchmarks. Subjects: Machine Learning (cs.LG) Cite as: arXiv:2311.16093 [cs.LG](or arXiv:2311.16093v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2311.16093 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Luca M. Schulze Buschoff [view email] [v1] Mon, 27 Nov 2023 18:58:34 UTC (4,181 KB) Full-text links: Access Paper: Download PDF PostScript Other Formats Current browse context: cs.LGnewrecent2311 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=38474696",
    "commentBody": "Large language models lack deep insights or a theory of mindHacker NewspastloginLarge language models lack deep insights or a theory of mind (arxiv.org) 257 points by mnode 18 hours ago| hidepastfavorite238 comments tinco 16 hours agoI think that if they would, that would be very surprising and indicative of a lot of wastefulness inside the model architecture. All these tests are simple single prompt experiments, so the LLM&#x27;s get no chance to reason about their responses. They&#x27;re just system 1 thinking, the equivalent of putting a gun to someone&#x27;s head and asking them to solve a large division in 2 seconds.I bet a lot of these experiments would already solvable by putting the LLM in a simple loop with some helper prompts that make it restructure and validate its answers, form theories and get to explore multiple lines of thought.If an LLM would be able to do that in a single prompt, without a loop (so the LLM always answers in a predictable amount of time), then it would mean its entire reasoning structure is repeated horizontally through the layers of its architecture. That would be both limiting (i.e. limit the depth of the reasoning to the width of the network) and very expensive to train. reply Androider 16 hours agoparentThe equivalent for a human would be an reflexive response to a question, the kind you could immediately answer after being woken up at 3am in the morning. That type of answer has been deeply trained into the human networks and also requires no deep insight.But if a human is allowed time and internal reasoning iterations, so should the LLM when determining if it has deep insight. Right now we&#x27;re simply observing input -> output of LLMs, the equivalent of snap answers from a human. But nothing says it couldn&#x27;t instead be an input -> extensive internal dialogue, maybe even between multiple expert models for seconds, minutes or hours, that are not at all visible to the prompter -> final insightful answer. Maybe future LLMs will say, \"let me get back to you on that\". reply lukev 13 hours agorootparentCompletely agree.From a computer science point of view: a single prompt&#x2F;response cycle from a LLM is equivalent to a pure function; the answer is a function of the prompt and the model weights and is fundamentally reducible to solving a big math equation (in which each model parameter is a term.)It seems almost self evident that \"reasoning\" worthy of the name would involve some sort of iterative&#x2F;recursive search process, invoking the model and storing&#x2F;reflecting&#x2F;improving on answers methodically.There&#x27;s been a lot of movement in this direction with tree-of-thought&#x2F;chain-of-thought&#x2F;graph-of-thought prompting, and I would bet that if&#x2F;when we get AGI, it&#x27;s a result of getting the right recursive prompting pattern + retrieval patterns + ensemble models figured out, not just making ever-more-powerful transformer models (thought that would certainly play a role too.)The LLM isn&#x27;t the whole brain. Just the area responsible for language and cultural memory. reply kaba0 13 hours agorootparentIdeally a recursive execution would also be a pure function - maybe a better way to put it about current LLMs is that they are a single mathematical expression being built up from a fix number of nodes and only addition and multiplication. reply lukev 12 hours agorootparentyes, except the \"reasoning\" process should also be able to look up facts (retrieval) and invoke external tools, making it non-pure. reply Scubabear68 9 hours agorootparentAgree. I am not an expert on AGI, but I suspect this route is a dead end because I believe true AGI requires interaction with an external environment to learn eg there needs to be a feedback loop to external things that it can interact with and learn from.‚ÄúYou learn best from your mistakes‚Äù comes to mind here.And not just shallow feedback loops but bigger, more profound ones eg ‚ÄúI didn‚Äôt go to work for a month and so I was fired and and now I have no money and it will now be hard to be hired again‚Äù. reply I_Am_Nous 14 hours agorootparentprevThis reminds me of The Last Question by Isaac Asimov. I also think if we stopped expecting all LLMs to have an immediate answer, it would be relatively easy to shim some kind of \"conscience\" to direct the output in different ways. Similar to the safeties already in place in LLMs, but instead of it just saying \"NO DON&#x27;T SAY THAT\" it can dialog internally to change what the output is until it reaches what it believes to be the agreed upon best answer. reply mr_toad 11 hours agorootparent> I also think if we stopped expecting all LLMs to have an immediate answer, it would be relatively easy to shim some kind of \"conscience\" to direct the output in different ways.If the shim was just another AI, then how do you align that AI? Who watches the watchers? But if it was a deterministic algorithm it would probably fail for the same reasons that algorithmic AI never went anywhere. reply I_Am_Nous 11 hours agorootparentA great point! A smaller AI with a rather limited parameter count could be trained for individual needs so some things (chat moderation) might be easier to do than other things (fact check peer reviewed papers in a verifiable way). For some use cases it would be overkill to have a conscience but an AI spokesperson for a company will probably have a company-aligned conscience for obvious reasons. reply lifeisstillgood 14 hours agorootparentprevIt would have an emotional reaction to certain \"thought constructs\" and would be guided by that.Or we could just give them three laws reply I_Am_Nous 13 hours agorootparentWith current LLMs the three laws might be tough to implement in a way that can&#x27;t be prompt injected around. That&#x27;s why I described the extra bits as a \"conscience\" which could enforce the three laws. Maybe the three laws are the internal conscience&#x27;s context prompt while the main LLM is more able to think anything in general and then the output is tuned down by the conscience?Otherwise the laws will have to be implemented as weights or during training so the model explicitly knows the laws and would never even be capable of doing something against them. reply pixl97 13 hours agorootparentI mean, the whole purpose of the I, Robot story was to show you that the 3 laws didn&#x27;t work. We had the first story on prompt injection decades ago and we just didn&#x27;t realize it. reply ShamelessC 9 hours agorootparentIndeed. Asimov says as much in interviews - the laws are flawed from the get go. It is the naive belief that the laws work by various scientists that creates these Sherlock-style mysteries&#x2F;crimes to be solved in the first place.I suppose the real wisdom there is that humans are doomed to fail at alignment if we create sentience and expect it only to serve us. reply TeMPOraL 11 hours agorootparentprevObligatory reminder that the \"three laws\" were invented to be deconstructed, with Asimov spending a lot of pages showing many ways in which they completely fail, illustrating that AI alignment is a hard problem. reply jvanderbot 11 hours agorootparentprevI think on of the reasons we require so much data is that we try to bake all that \"simulated experience and internal dialogue\" into the snap responses. I bet if you could do an efficient sim&#x2F;test retraining, you&#x27;d do data-driven responses on the fly. reply two_in_one 14 hours agoparentprevJust note that loop doesn&#x27;t have to be visible from outside. It can be internal, with another driving thread asking right questions. Inner monologue. Then the summary is given back to user. This will give the model space for &#x27;thinking&#x27; with internally generated text much large than the visible prompt + output. This way multi-step logic can be implemented. reply chii 9 hours agorootparent> with another driving thread asking right questions. Inner monologue.spoilers warning:Is that basically the plot to westworld? reply two_in_one 7 hours agorootparentInner monologue idea was around for a while. That&#x27;s interesting that it actually materializes. It was thought that AGI will be operating with some abstracts, logic rules, probability calculations, etc. Not thinking in plain English. reply creer 5 hours agoparentprevAt the same time, if LLMs are based on all or enough human writings then don&#x27;t they necessarily contain a theory of mind? A rather general, smoothed out and still neurotic one probably. But still just like an LLM can&#x27;t be expected to have a specific knowledge of hydraulics, it also has read more hydraulics than even experts might be expected to. That&#x27;s the entire issue about it, right? This issue of \"is most of our mind basically just mixing and matching stuff we have seen, read, heard?\" Do humans have some magical theory of mind that somehow stands ASIDE from all the \"normal\" learned stuff?Of course, yes, we do know one thing that&#x27;s missing in LLMs, which is \"loop and helpers\" like you describe. Which I&#x27;m sure many people are currently hacking at - one way being for the LLM to talk to itself.But as for \"a theory of mind\", if enough writings served as input, then LLMs do have plenty of that.Another question is whether LLMs are raised to behave like humans (which might be where they most NEED some theory of mind). Of course not. The ones we know most about are only question answerers. The theory of mind they might have (that is not negated by the lack of loop and internal deliberation) may be overwhelmed by the pre- and post-processing: \"no sex, no murder plots, talk to the human like they are 5, bla bla bla\". And yet you can ask things like \"Tell it like you are speaking to 5 year olds who want to have a fun time\". Some theory of mind makes it through. reply lixy 14 hours agoparentprevYep, prototype exactly that this past week. With a strong instruction spec prompt from the start, you can have an AI come up with a much better answer by making sure it knows it has time to answer the questions and how it should approach the problem in stages.The great part is with clear enough directions it also knows how to evaluate whether its done or not. reply twobitshifter 13 hours agoparentprevThere are plenty of models that use introspection and check answers, that‚Äôs the idea behind let‚Äôs think about it step by step. reply tinco 13 hours agorootparentI feel the \"let&#x27;s think about it step by step\" is a bit of a hack. To circumvent the fact that there&#x27;s no external loop you use the fact that it gets re-run on every token so you can store a bit of state in the tokens that it&#x27;s already generated.Or am I misunderstanding something about that technique? reply twobitshifter 13 hours agorootparentYou are right, it‚Äôs sometimes called zero shot chain of thought, but it‚Äôs a way of getting the type of thing you are describing to happen. The LLMs somehow process things in a perceived step by step to get a much improved answer. Whether the external loop or an llm imposed internal loop, does it matter? Are our own minds looping or just adding tokens? reply tinco 12 hours agorootparentYeah that&#x27;s true, and I do believe there&#x27;s a good chance our minds are perpetually adding tokens. But our minds also have an efficient&#x2F;effective way of dealing with the context cut off. We don&#x27;t have (or we don&#x27;t experience) a hard cut off of our memory context. Instead the tokens are increasingly lossily compressed as they age out of our memory, the lossiness amount being based both on time passed but also on some fancy value function. And that combined with a \"system\" (or trained&#x2F;fine-tuned in) prompt that motivates the LLM to reason in a way that is conducive to working with that kind of memory would be a sort of single-shot AGI system. Where single-shot is lying a bit because it&#x27;s just infinitely looping.I guess from that perspective it might make sense to test if such a thing is already happening in current LLM&#x27;s and my dismissive attitude stems from the fact that I&#x27;ve played with them enough to know that they currently don&#x27;t. reply vidarh 11 hours agorootparentprevThe main thing that may matter, at least in the short run, is that an external loop allows us to inject additional steps by applying heuristics and allowing tool use. E.g. we can let the LLM \"realise\" there errors in its code and have it continue from an injected \"thought\" about making sure to fix the errors from the compiler before presenting it&#x27;s output, or \"remembering\" that it needs test cases etc.We can also potentially add longer term memory - summarise the context, and judge which parts are important and stuff them in a vector store, and now and again swap in similar pieces of past context.But of course it&#x27;s not either or - better prompting to get the LLMs to do better from the start doesn&#x27;t compete with then feeding that into an external loop as well. reply uoaei 16 hours agoparentprev> They&#x27;re just system 1 thinking, the equivalent of putting a gun to someone&#x27;s head and asking them to solve a large division in 2 seconds.No, it&#x27;s the equivalent of putting a gun to someone&#x27;s head and asking them \"what are my intentions?\" Which is readily available to any being with a theory of mind. reply choudharism 15 hours agorootparentI don‚Äôt think LLMs have theory of mind, but your point is not very strong. You can literally query ChatGPT right now and see that it can figure out intentions (both superficial and deep) of a gun is held to a head quite easily.Because, obviously, training data probably includes a decent amount of motivation breakdowns as a function of coercion.It doesn‚Äôt know why, but it knows what to say. reply og_kalu 16 hours agorootparentprevLLMs don&#x27;t fail those kind of tasks though. 4 is very good at keeping track of who knows what and why in a story. You can test this yourself. reply bongodongobob 13 hours agorootparentprevWhat? My answer would be \"I have no idea!\"Are they faking it? Will they murder me? Are they just trying to scare me?I don&#x27;t understand the point you&#x27;re trying to make. reply FrustratedMonky 15 hours agorootparentprevDon&#x27;t think so.Put gun to persons ahead.Ask them to do a division.Then screaming at them \"HOW DID YOU DO THAT, TELL ME NOW, OR YOU&#x27;RE TOAST\".Even most humans would splutter and not be able to answer. reply uoaei 15 hours agorootparentThe division problem is arbitrary and irrelevant to any notion of theory of mind. If you have a better example, you can feel free to offer one. I already did so above. reply FrustratedMonky 12 hours agorootparentThe point is not about the example of division, it is about &#x27;tell me your intentions&#x27;.A human also can&#x27;t explain how their neurons calculated a division, or anything else, or &#x27;intention&#x27;, even if a gun is pointed at them.So, why is that a fault of AI if it can&#x27;t explain how itself is working? It is not a requirement for AGI. reply uoaei 11 hours agorootparentEvery point you made here either reiterates something I&#x27;ve already said or misinterprets the nature of the discussion altogether. replyrf15 16 hours agoparentprevthey can&#x27;t reason though, sadly - the premise does not hold. reply TeMPOraL 11 hours agorootparentThat&#x27;s literally GP&#x27;s point though - they don&#x27;t reason because reasoning requires a loop, which you deny them and then say \"see, it can&#x27;t reason\". reply FrustratedMonky 15 hours agorootparentprevSo your premise is correct? Please back up the opposite. If you can, you should publish.These responses are logically the same as \"No You\". replymenssen 17 hours agoprevI appreciate this paper for relatively clearly stating what \"human-like\" might entail, which in this case involves \"reasoning about the causes behind other people&#x27;s behavior\" which is \"critical to navigate the social world\" as outlined in this citation:https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S00100...I get frustrated often when people argue \"well, it isn&#x27;t really intelligent\" and then give examples that are clearly dependent on our brain&#x27;s chemical state and our bodies&#x27; existence in-the-physical-world.I get the feeling that when&#x2F;if we are all enslaved by a super-intelligent AI that we do not understand its motives, we will still argue that it is not intelligent because it doesn&#x27;t get hungry and it can&#x27;t prove to us that it has Qualia.This paper argues that gpts are bad at understanding human risk&#x2F;reward functions, which seems like a much more explicit way to talk about this, and also casts it in a way that could help reframe the debate about how human evolution and our physical beings might be significantly responsible for the structure of our rational minds. reply swatcoder 16 hours agoparentThe underlying problem is that \"intelligence\" is itself a crappy, poorly defined word with a fraught and inconsistent history.It doesn&#x27;t appear until the early 20th century, in the shadow of compulsory education and the challenges it presented, first as a technical label for attempts to sort students -- and later soldiers -- into the tracks in which they&#x27;re most likely to succeed, and then being haphazardly asserted (but not scientifically evidenced) as some general measure of mental aptitude.At that point it shifts from something qualitative (which mental tasks might someone be good at) to something quantitative (how much more might one personal excel at all mental tasks than another), and the burgeoning field of modern American psychology goes \"Aha! A quantitative measure! Here&#x27;s our meal ticket to being recognized as a science instead of those quacks from Vienna\", with far too much at stake to either question the many assumptions at play or the inconsistent history of usage.Momentum takes hold and the public takes the word into its everyday vernacular, even while it&#x27;s still not a clear and sound concept in its technical domain. [Most of this is history is more academically covered in Danziger&#x27;s 1987 \"Naming the Mind\" which is excellent, and critical foundational reading to contextualize recent hot discussions in AI]The way you&#x27;re using it when you worry about \"super-intelligence\" is in the sense of intelligence being some universal, unbounded, quantitative independent variable along the lines of \"the more intelligent something is, the more cunningly it can pursue some rationalized goal\" -- some master strategist.That&#x27;s fine, and you&#x27;re not alone in that, but there&#x27;s not really any sound scientific groundwork to establish that there exists some quality of the world that scales like that. You&#x27;re fear, and what you try to distinguish conceptually from what the paper addresses, is an inductive leap made from highly unstable ground. It&#x27;s in the same invented, purely abstract idea-space of \"omnipotence\" or \"omniscience\" where one takes a practical idea like \"power to influence\" or \"ability to know fact\" and inductively draws a line from these practical senses towards some abstract infinite&#x2F;incomprehensible version of that thing. But that inductive leap a Platonic logician&#x27;s parlor trick and ends up raising all kinds of abstract paradoxes, as well countless physical impracticalities about how such things could exist.So a lot of people (academic and lay) just aren&#x27;t with you in taking that framing of intelligence very seriously. For many, an \"super-intelligent\" software whose \"motives\" we don&#x27;t understand is just a program that produces incorrect outputs and ought to be debugged or retired, and the more interesting questions around machine \"intelligence\" are practical ones like \"what tasks are these programs well-suited for\". Here, the authors point out that the current batch of programs are not good at tasks that benefit from a theory of mind.Knowing the answer to that kind of question reaches back to the earliest and least disputable sense of the word, where we saw that some new students and soldiers excelled at certain tasks and struggled with others, and wanted to understand how best to educated&#x2F;assign them. And likewise, as we look at these tools, the pressing question for engineers and businesses is \"what are they good for and what are they not good for\" rather than the fantastical \"what if we make a broken program and it wants to kill everyone and we don&#x27;t notice and forget to shut it off\" reply thewakalix 15 hours agorootparent> and it wants to kill everyoneIt wouldn&#x27;t have to want to kill everyone. As long as it doesn&#x27;t want to not kill everyone, the side effects of it getting what it wants could be catastrophic.> and we don&#x27;t noticeHow well do we understand what&#x27;s going on inside ChatGPT? How well will we understand the next?> and forget to shut it offEarlier I would have argued that sufficiently advanced AI could prevent itself from being shut off via Things You Didn&#x27;t Expect, and would instrumentally want to preserve its existence. But these days, people are giving ChatGPT not just internet access but even actively handing it control over various processes. At this rate, the first superhuman AI will face not an impermeable box but a million conveniently labeled levers! reply TeMPOraL 11 hours agorootparent> Earlier I would have argued that sufficiently advanced AI could prevent itself from being shut off via Things You Didn&#x27;t ExpectThere&#x27;s a good argument along these lines that I keep reposting when someone asks if we can&#x27;t just shut the AI off. \"All you gotta do is push a button, sir?\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ld-AKg9-xpM&t=30s reply joe_the_user 12 hours agorootparentprevThe way you&#x27;re using it when you worry about \"super-intelligence\" is in the sense of intelligence being some universal, unbounded, quantitative independent variable along the lines of \"the more intelligent something is, the more cunningly it can pursue some rationalized goal\" -- some master strategist.I appreciate the highlighting of the term intelligence being ill-defined. Moreover, it&#x27;s certainly true that \"AI safety analysts\" takes intelligence as a sort magic wand term and this seems to drive their arguments.All that said, since both computers and human brains are material artifacts, it doesn&#x27;t seem impossible to create a device that combines their properties. It seems plausible that such a thing could have a variety of dangers.For many, an \"super-intelligent\" software whose \"motives\" we don&#x27;t understand is just a program that produces incorrect outputs and ought to be debugged or retired, and the more interesting questions around machine \"intelligence\" are practical ones like \"what tasks are these programs well-suited for\".We saw early Bing Chat behave, not in ways we couldn&#x27;t understand but like a deranged and vengeful human. Certainly, it was merely simulating human behavior but if today&#x27;s methods produce artifacts that unselectively amplify human behaviors, it&#x27;s not hard to imagine problems appearing.We can hope that there&#x27;s a fundamental difference between programs that simulate human language and programs able to plan and carry out long term goals (and carrying out long term goals is something people do so there&#x27;s no good reason some kind of program couldn&#x27;t do that).I think you&#x27;re right that particular weirdness of the \"doomers\" makes some other portion of the population dismiss concerns. But that isn&#x27;t an argument that the doom isn&#x27;t possible - it should be an argument to clarify how we talk of computation and human capacities (see, I don&#x27;t to say \"intelligence\" unless I want to). reply kens 15 hours agorootparentprev> It [the word \"intelligence\"] doesn&#x27;t appear until the early 20th centuryI&#x27;m not sure what you mean here, since the word dates back to the late 14th century with roughly the same meaning as now. Perhaps you&#x27;re thinking of \"intelligence quotient\"?https:&#x2F;&#x2F;www.etymonline.com&#x2F;word&#x2F;intelligence reply swatcoder 15 hours agorootparentSummary etymology can provide interesting reference points when looking a the history of ideas, but isn&#x27;t sufficient because adjacent concepts change their meaning over time as well. It&#x27;s good for showing when a word was attested and where to start looking for an understanding of how it was used and considered.Where you say \"roughly the same meaning as now\" you seem to mean that \"the highest faculty of the mind, capacity for comprehending general truths;\" is how we think of intelligence now, but the meanings of \"mind\", \"truth\" \"comprehending\" and \"faculties of mind\" have all had their own radical shifts over the last 600 years. That quoted phrase conveys an entirely different perspective and set of assumptions&#x2F;implications in the context of its time, and is not at all analogous to how we read it today.Raymond Williams&#x27; \"Keywords\" collects a very interesting and accessible collection of examples of this phenomenon, although it focuses more on the language of politics and society more than the language of psychology.The modern use of intelligence, and the conceptual constellation it represents, is essentially isolated from what&#x27;s described in that article, but it&#x27;s re-introduction in modern psychology does borrow from its prior existence in the lexicon. reply og_kalu 15 hours agorootparentprevThere does seem to be a general factor of intelligence in humans though that is the single biggest indicator of performance. Yes there are other factors too.>Here, the authors point out that the current batch of programs are not good at tasks that benefit from a theory of mind.Not good at tasks that benefit from a theory of mind extracted from visual data. reply throwanem 15 hours agorootparent\"Seem\" is doing a lot of work here. So is the implicit claim that theory of mind in general can be demonstrated by current-gen foundation models, and only those aspects dependent on vision cannot. reply og_kalu 15 hours agorootparentI say seem but it&#x27;s stronger than that. all evidence and testing points towards a general factor of intelligence. The better you perform at one \"kind\" of intelligence task, the better you will perform at them all. The shift in defining intelligence didn&#x27;t come from nowhere. Yes, It&#x27;s easy to think that there are multiple different mutually exclusive-ish kinds of intelligences and that you can excel in one and it has no bearing on performance on the other but that&#x27;s not really true. all indication point otherwise. I&#x27;m not saying there aren&#x27;t other factors but generally, that&#x27;s what you can expect.Yes theory of mind can be demonstrated. Make up whatever bespoke story you can with characters having varying levels if intention and knowledge. Then query GPT-4 about the state of the characters.What i&#x27;m saying is that the vision component introduces another point of error, is it a lack of theory of mind ? or being yet unable to extract the necessary features from visual data ? They rapidly learn to but Blind people who could recognize squares by feel do not have the ability to recognize squares by sight upon gaining vision. https:&#x2F;&#x2F;www.projectprakash.org&#x2F;_files&#x2F;ugd&#x2F;2af8ef_5a0c6250cc3... reply pegasus 14 hours agorootparentprev\"Seem\" only seems to be doing lots of work. Here&#x27;s a relevant article: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;G_factor_(psychometrics) reply menssen 15 hours agorootparentprevFor what it&#x27;s worth, I don&#x27;t take that framing of intelligence seriously either. It&#x27;s useful to have a word to describe the far-future state of the increasing capabilities of Computers.I&#x27;m just saying that I don&#x27;t think there&#x27;s any point on that line where we will be comfortable admitting that the machine is \"intelligent\" or \"conscious\" or \"AGI,\" or whatever, and that I appreciate attempts to quantify (or at least qualify) what we MEAN when we say that, rather than just goalpost-moving. reply pixl97 15 hours agorootparentprevMost of what you&#x27;re saying here is describing the alignment issue.We (mostly) don&#x27;t want unaligned A(G|S)I. The outcomes of that could be extenstential. reply swatcoder 15 hours agorootparentOnly for those mundane senses of alignment where we say \"This system is reliable in tasks that look like X and unreliable in tasks that look like Y, so let&#x27;s craft hard boundaries to avoid naive use for Y\"But it&#x27;s skeptical of the other sense alignment, where a potential Master Strategist needs to be trained or crippled before it outsmarts us. It sees that perspective as comparable to logicians debating whether we might live in the domain of a benevolent or evil omnipotence: \"if an ant is more powerful than a rock, and I&#x27;m more powerful than an ant, then perhaps there is something so powerful that it encompasses all opportunities to influence the universe including the power to hide itself from me.\" -- which comes from taking a concrete measure, assuming that it&#x27;s an independent variable, and then inductively extending it to an infinite or otherwise unevidenced scale. This technique is undisprovable and so it&#x27;s easy for \"rational\" people to mine work from it for a very long time, but history and analysis give room for skeptics to be like \"WTF you going on about; let&#x27;s have some tea\" reply pixl97 13 hours agorootparent\" but history and analysis give room for skeptics to be likeWhen the skeptic is correct. The problem with skeptics is when incorrectness is not terminal, they can&#x27;t hear you over the sound of pushing the goal posts farther to give a reasonable rebuttal for their originally incorrect statements. reply mistermann 8 hours agoparentprev> I get frustrated often when people argue \"well, it isn&#x27;t really intelligent\" and then give examples that are clearly dependent on our brain&#x27;s chemical state and our bodies&#x27; existence in-the-physical-world.A big part of the problem is that \"is\" has a wide variety of inconsistent meanings, and that this fact is sub-perceptual, and that it is culturally very inappropriate to comment on aspects of our culture like this, preventing knowledge of the problem from spreading.&#x2F;u&#x2F;Swatcoder makes essentially the same point but in much more detail, though regarding less important words. reply fredliu 15 hours agoprevI have small kids, toddlers, who can already speak the language but still developing their \"sense of the world\" or \"theory of mind\" if you will. Maybe it&#x27;s just me, but talking to toddlers often reminds me of interacting with LLMs, where you would have this realization from time to time \"oh, they don&#x27;t get this, need to break down more to explain\". Of course LLM has more elaborate language skills due to its exposure to a lot more text (toddlers definitely can&#x27;t speak like Shakespeare if you ask them, unless, maybe, you are the tiger parents that&#x27;s been feeding them Romeo and Juliet since 1.), but their ability of \"reasoning\" and \"understanding\" seems to be on a similar level. Of course, the other \"big\" difference, is that you expect toddlers to \"learn and grow\" to eventually be able to understand and develop meta cognitive abilities, while LLMs, unless you retrain them (maybe with another architecture, or meta architecture), \"stay the same\". reply TeMPOraL 11 hours agoparent> Maybe it&#x27;s just me, but talking to toddlers often reminds me of interacting with LLMsIt&#x27;s not just you. It hit me almost a year ago, when I realized my then 3.5yo daughter has a noticeable context window of about 30 seconds - whenever she went on her random rant&#x2F;story, anything she didn&#x27;t repeat within 30 seconds would permanently fall out of the story and never be mentioned again.It also made me realize why small kids talk so repetitively - what they don&#x27;t repeat they soon forget, and what they feel like repeating remains, so over the course of couple minutes, their story kind of knots itself in a loop, being mostly made of the thoughts they feel compelled to carry forward. reply Terretta 10 hours agorootparentAnd, if you change their context, the story unspooling will change. reply TeMPOraL 10 hours agorootparentYes. And if they&#x27;re looped enough in their original story, this feels like the spring from a mechanical watch rapidly unwinding. reply passion__desire 15 hours agoparentprevIt&#x27;s not just true about toddlers but also for adults in particular time frame. Maturity of thought is cultural phenomenon. Descartes used to think animals are automaton while they behaved exactly like humans in almost all aspects in which he could investigate animals and humans during those times and yet he reached illogical conclusion. reply fredliu 14 hours agorootparentThat&#x27;s a great point. Just thinking out loud, if we can time travel back to the cavemen time, and assuming we speak their language, there would still be so much that we couldn&#x27;t explain or they wont&#x27; be able to understand even for the smartest cavemen adults. Unless, of course we spend significant time and effort to \"bring them up to speed\" with modern education. reply kbelder 12 hours agorootparentIn Jayne&#x27;s &#x27;The Origin of Consciousness in the Breakdown of the Bicameral Mind&#x27;, there&#x27;s some interesting investigation into some of our oldest known tales... Beowulf, The Iliad, etc.In those texts, emotional and mental states are almost always referred to with analogs to physical sensation. &#x27;Anger&#x27; is the heating of your head, &#x27;fear&#x27; is the thudding of your heart. He claims that at the time, there wasn&#x27;t a vocabulary that expressed abstract mental states, and so the distinction between the mind and body was not clear-cut. Then, over time, specialized terms to represent those states were invented, passed into common usage, which enabled an ability to introspect that didn&#x27;t exist before.(All examples are made up, I read it more than 20 years ago. But it made an impression.) reply nomel 9 hours agorootparentprevI don&#x27;t think it has anything to do with brain development. I think it&#x27;s entirely related to the development of an individual concept, whenever the structure of ideas that make the concept is too simple.I would claim that most people use intuition&#x2F;assumptions rather than internal chain-of-thought, when communicating, meaning they will present that simplified concept without second thought, leading to the same behavior as the toddler. It&#x27;s actually trivial to find someone that doesn&#x27;t use assumptions, because they take a moment to respond, using an internal chain-of-thought type consideration to give a careful answer. I would even claim that a fast response is seen as more valuable than a slow one, with a moment of silence for a response being an indication of incompetence. I know I&#x27;ve seen it, where some expert takes a moment to consider&#x2F;compress, and people get frustrated&#x2F;second guess them. reply joduplessis 15 hours agoprevFor me, the entire AGI conversation is hyperbolic &#x2F; hype. How can we infer intelligence to something when we, ourselves, have such a poor (none) grasp of what makes us conscience? I&#x27;m associating intelligence with consciousness - because it seems correlated. Are we really ready to associate \"AGI\" with solving math problems (\"new Q algo.\")? That seems incredibly naive & reinforces my opinion that LLM&#x27;s are much more like crypto, than actual progress. reply poulsbohemian 15 hours agoparentCompletely agree, and while we are at it... look I&#x27;m just a guy, not an expert, but I can&#x27;t understand why there&#x27;s so much focus on AGI. It feels like there are so many niche areas where we could apply some kind of analytical augmentation and by solving problems in the small, might learn something that would help figure the larger question of intelligence. I don&#x27;t need the AI to replace everything I do, I need it to solve 10,000 micro problems I solve every day - each of which is a business opportunity for someone. reply tivert 13 hours agorootparent> ...but I can&#x27;t understand why there&#x27;s so much focus on AGI.Lots of software engineers have spent their lives reading sci-fi that features AGI, and they&#x27;re excited by&#x2F;lost in that fantasy.It&#x27;s interesting to see that in people who often view themselves as hyper-rational. reply tim333 12 hours agorootparentAGI as in computer intelligence that out does humans would be a huge deal in practical terms. Chat GPT and similar are kind of like handy toys. With proper AGI you could link it to a robot body and tell it to go off, design a better version of itself, make a billion more robots and take over the world. It&#x27;s a different category of thing. And if you think that&#x27;s just sci-fi I think you&#x27;ll get a surprise at some point during your life. reply tivert 11 hours agorootparent> With proper AGI you could link it to a robot body and tell it to go off, design a better version of itself, make a billion more robots and take over the world.^^^^^ literally a plot ripped from the pages of science fiction used to reason about the real world.> And if you think that&#x27;s just sci-fi I think you&#x27;ll get a surprise at some point during your life.Such faith that fantasy can be made real. Wake me up when you have my hyperdrive ready. reply tim333 11 hours agorootparentI&#x27;ve never actually read that in fiction. It&#x27;s just logical really. I mean I&#x27;m sure it is in fiction somewhere because the idea is obvious. reply tivert 11 hours agorootparent> I&#x27;ve never actually read that in fiction.I find that hard to believe. Ever watch Terminator?But even if true, that science-fictional plot is so pervasive it would be easy to pick up from the millions who have the software engineer&#x27;s blurry line between fantasy and reality.> It&#x27;s just logical really.OK, then. You&#x27;re a GI, go off and build an army of better yous and take over the world. reply TeMPOraL 11 hours agorootparentThe idea is indeed logical and stupidly obvious, once you learn the basics of what \"optimization\" means, or what \"recursion\" is.> I find that hard to believe. Ever watched Terminator?Terminator has fuck all to do with recursive self-improvement. Don&#x27;t confuse people who grew up on sci-fi with people who casually went to see Terminator or some other pop-culture artifact featuring some kind of \"AI\".> OK, then. You&#x27;re a GI, go off and build an army of better yous and take over the world.What do you think the drama with eugenics, genetic engineering and designer babies is around? It&#x27;s literally humans trying to make better humans in the only way that is available - reproduction.AI made in silica would be more malleable, easier and cheaper to replicate. Self-improving software isn&#x27;t even a fantasy; it exists in many forms - though it&#x27;s far from open-ended like a self-improving GI would be. reply tivert 10 hours agorootparent>>> I&#x27;ve never actually read that in fiction.>> I find that hard to believe. Ever watched Terminator?> Terminator has fuck all to do with recursive self-improvement. Don&#x27;t confuse people who grew up on sci-fi with people who casually went to see Terminator or some other pop-culture artifact featuring some kind of \"AI\".You&#x27;re not following the thread. The future timeline in Terminator does involve something like an AI making \"a billion more robots [to] take over the world.\" The popularity of that and similar sci-fi makes that claim that someone has never encountered it hard to believe.> What do you think the drama with eugenics, genetic engineering and designer babies is around?So how has that been going? Those things should also probably be labeled \"science fiction.\"> AI made in silica would be more malleable, easier and cheaper to replicate. Self-improving software isn&#x27;t even a fantasy; it exists in many forms - though it&#x27;s far from open-ended like a self-improving GI would be.Fantasies based on squishy assumptions. How do you know it would have an easier job optimizing itself than humans have? How do you know there isn&#x27;t some fundamental contradiction in the concept of \"superintelligence\" that these fantasies are based on? Or even just some practical resource limits that makes the fantasy impossible? reply TeMPOraL 10 hours agorootparent> The future timeline in Terminator does involve something like an AI making \"a billion more robots [to] take over the world.\"Yes. That&#x27;s distinctly different from Skynet iterating on itself a billion times to make itself smarter, which AFAIK (I&#x27;m not up to date with full Terminatorverse, but then, most people aren&#x27;t either), isn&#x27;t something that happened in that story.> The popularity of that and similar sci-fi makes that claim that someone has never encountered it hard to believe.Again, there&#x27;s very little in mass-market sci-fi of what we&#x27;re discussing here. And most people, including many in tech, have a hard time wrapping their heads around the idea of a feedback loop, so no, I don&#x27;t think it&#x27;s something readily available from mass-market sci-fi.(But the more niche, better thought-out works, will teach you feedback loops, and this is just one of the ways recursive self-improvement becomes an obvious idea.)> So how has that been going? Those things should probably be labeled \"science fiction.\"Eugenics? We had to ban it and create such a strong cultural (and legal) repulsive field around it, that it impedes biotech and medical research.Designer babies? Weren&#x27;t attempts made in China recently? And in the West, we&#x27;re already correcting congenital defects, so all in all, it&#x27;s less \"science fiction\", and more \"science someone is going to apply soon, if they haven&#x27;t already\".> How do you know it would have an easier job optimizing itself than humans have?Because it was created by us, using processes and media that are strongly optimized for malleability. Software, algorithms, digital data, optimization models. All well-defined (and comprehensible to an AGI, by definition) - unlike our own minds, which were not made by us but by a dumb, random process, and that the brains are made of stupidly complex nanotech instead of simple transistors is not helping.Also because the kind of models we&#x27;re now worried about gain capability through an optimization process that&#x27;s open-ended, and limited only by availability of training data and compute. So if e.g. a successor of GPT-4 were to become AGI, it would be set up for recursive self-improvement from day one.> How do you know there isn&#x27;t some fundamental contradiction in the concept of \"superintelligence\" that these fantasies are based on? Or even just some practical resource limits that makes the fantasy impossible?Maybe, but what makes you think this is the case? We know of some fundamental limits to compute, but we&#x27;re very, very far from hitting them. Otherwise, I don&#x27;t know of anything that would put a cap on intelligence at around human level. Remember: by the very nature of evolution, we&#x27;re the dumbest possible beings capable of learning and building a technological civilization. There may be better brain designs than ours, but ours \"took off\", and we took over the world. reply mistermann 8 hours agorootparentprevIt is not technically logical to think one can see the future, but it is colloquially logical.Judging reality by how it appears is a bad strategy, this should be common knowledge by now.What&#x27;s concerning to me is that I suspect LLM&#x27;s will be able to learn and remember thousands of basic facts like this, and ~reason on top of them. Perhaps they won&#x27;t figure this out on their own, but what if all it takes is one individual to point them in this direction? I bet there are numerous people who know much more about this than me working for our various three letter agencies. reply tim333 11 hours agorootparentprevSadly I can&#x27;t build a better me as I&#x27;m not of robotic construction. And I was a being a bit flippant with the world takeover. But as soon as AI reached human level it would quickly go beyond it given the rate these things improve, allowing it to get to to work on improved models. As something along those lines in the real world think the Tesla robots but improved with far better AI.Actually thinking about it I wouldn&#x27;t rule out Musk&#x2F;Tesla going for the world takeover thing;) reply tivert 11 hours agorootparent> Sadly I can&#x27;t build a better me as I&#x27;m not of robotic construction.Why the fuck not? You literally have all the code to manufacture a person. reply8note 11 hours agorootparentprevSo I link this to my Roomba, and in a few days time it&#x27;ll design and build a much better Roomba, while confined to my apartment with only wheels and a vacuum to actuate?I think a much more realistic outcome is that you put it into a fancier robot body that can go outside, and by the end of the week it&#x27;s scrap in a homeless camp chop shop reply TeMPOraL 11 hours agorootparentprev> It&#x27;s interesting to see that in people who often view themselves as hyper-rational.It&#x27;s perhaps because they are rational enough to realize, thanks to the same knowledge&#x2F;skill that put them on the software engineer careers, that AGI isn&#x27;t a fantasy but a possibility and a potentially very big deal. reply pixl97 12 hours agorootparentprev>I need it to solve 10,000 micro problems I solve every day - each of which is a business opportunity for someone.Because you have to solve 10,000 different problems. And a huge number of those problems are going to have significant overlap, but sharing lessons between them is going to be difficult unless you have a generalized algorithm.Hence AGI is the trillion dollar question. reply Avicebron 14 hours agorootparentprevI&#x27;ve thought about this a bit as well, and I think it&#x27;s almost like this toxic concoction of incentive (how can \"we\" hype this until and make boatloads of money off of it, coupled with a genuine (if sub-conscious) desire to be seen as a visionary&#x2F;great engineer who \"created artificial life.\" I mean, at least on HN, I see lots of this aspirational attitude for living the sci-fi future circa. Star trek, ex-machina, etc, while couching their language in professions of expertise now that the firehose of cash has turned on.Also there is the general hubris in all this to only look at the new and shiny, I remember when there was that pizza robot (some multi-dimension axis hand thing) that cost whatever in building and research, when the costco pizza \"robot\" is pretty darn good, but doesn&#x27;t sell as \"futuristic&#x2F;cool\" because its a spigot on a servo. reply sgregnt 15 hours agorootparentprevMany of the seemingly small problems do require a good model of the world for context and edge case solving, so they still get very close to general intelegence. reply pixl97 13 hours agorootparentYep, at least in my eyes you&#x27;ll never be able to \"solve\" self driving without solving the G in AGI. You require a world model for predictions in order to have enough time to avoid many bad outcomes. Avoiding an empty soda can and avoiding a brick are similar problems, but one can easily lead to critical failures if you miss it. reply RGamma 14 hours agoparentprevA(G)I models don&#x27;t need higher order thinking or somesuch to be impactful. For that they just need to increase productivity with or without job loss (be Good Enough), which they are on a good track for.The real impacts will come when they are properly integrated into the current computational fabric, which everyone is racing to do as we write this. reply nyrikki 14 hours agoparentprevA particular subset of Connectivism have a philosophical belief that the mind IS a neutral net, not that it is a reductive practical model.Hinton is one of these individuals and with no definition of what intelligence is it is an understandable of dogmatic position.This whole problem of not being able to define what intelligence is pretty much allows us all to pick and choose.In my mind BPP is the complexity class solvable by ANNs and it is a safe and educated guess that most likely BPP=P.BPP being one of the largest practical complexity classes makes work in this area valuable.But due to many reasons that I won&#x27;t enumerate again AGI simply isn&#x27;t possible and requires a dogmatic position to believe in for people who have even a basic understanding of how they work and the limits from the work of G√∂del etc...But many of the top scientists in history have been believers of numerology etc...Associating math with LLMs is a useful too to avoid wasted effort by those who don&#x27;t believe AGI is close, but it won&#x27;t convince those who are true believers.LLM&#x27;s are very useful for searching very large dimensional spaces and for those problems that are ergotic with the Markov property they can find real answers.But for most of what is popular in the press will almost certainly be a dead end for generalized use of the systems are not extremely error tolerant.Unfortunately it may take another AI winter to break the hype train but I hope not.IMHO it will have a huge impact but overconfident claims will cause real pain and misapplication for the foreseeable future. reply corethree 12 hours agoparentprevIt&#x27;s not hype. It&#x27;s a language problem that makes people like you think this way.The problem is consciousness is a vocabulary word that establishes a hard boundary where such a boundary doesn&#x27;t exit. The language makes you think either something is conscious or it is not when the reality is that these two concepts are actually extreme endpoints on a gradient.The vocabulary makes the concept seem binary and makes it seem more profound then it actually is.Thus we have no problem identifying things at the extreme. A rock is not conscious. That&#x27;s obvious. A human IS conscious, that&#x27;s also obvious. But only because these two objects are defined at the extremes of this gradient.For something fuzzy like chatGPT, we get confused. We think the problem is profound, but in actuality it&#x27;s just poorly defined vocabulary. The word consciousness, again, assumes the world is binary that something is either&#x2F;or, but, again, the reality is a gradient.When we have debates about whether something is \"conscious\" or not we are just arguing about where the line of demarcation is drawn along the gradient. Does it need a body to be conscious? Does it need to be able to do math? Where you draw this line is just a definition of vocabulary. So arguments about whether LLMs are conscious are arguments about vocabulary.We as humans are biased and we blindly allow the vocabulary to mold our thinking. Is chatGPT conscious? It&#x27;s a loaded question based on a world view manipulated by the vocabulary. It doesn&#x27;t even matter. That boundary is fuzzy, and any vocab attempting to describe this gradient is just arbitrary.But hear me out. chatGPT and DALL-E is NOT hype. Why? Because along that gradient it&#x27;s leaps and bounds further than anything we had just even a decade ago. It&#x27;s the closest we ever been to the extreme endpoint. Whichever side you are on in the great debate both sides can very much agree with this logic. reply nprateem 8 hours agorootparent> A rock is not conscious. That&#x27;s obvious.But it&#x27;s not obvious at all. It may possess consciousness in a way we can&#x27;t relate to or communicate with.This is the whole problem with consciousness and has been discussed by philosophers for centuries. We each appear to be conscious but can&#x27;t be certain anything else is or isn&#x27;t. reply corethree 7 hours agorootparentYou&#x27;re not reading my argument. You responded to one part of it. As a whole my argument is not about the problem you&#x27;re describing.My argument is saying that the problem is a sham. An illusion. The problem doesn&#x27;t even exist. Read my whole write up. reply nprateem 2 hours agorootparentYeah I stopped reading since it&#x27;s based on a false predicate. I&#x27;ve read the rest and it&#x27;s still undermined by the same unprovable assumption.Perhaps consciousness is binary and the rock is just as conscious as a human. We don&#x27;t know. But consciousness != intelligence, and it&#x27;s reasonable to assume humans are more intelligent than rocks of course, but we can&#x27;t say anything about each one&#x27;s level of consciousness. reply upghost 14 hours agoparentprevCouldn&#x27;t agree more. How about this -- I think we&#x27;ve already reached AGI. Let me know if this tracks: Pick a set of tasks that can be considered AGI tasks. Provided the task sequences can be compared as closer to AGI or further from AGI, we can create a reward model using the same techniques as were used by ChatGPT via RLHF. Thus, for any definition of AGI that is meaningful and selectable, even if subjectively selectable or arbitrarily preferential, we can create a reward model for it.You might say, well thats not AGI, AGI must also do such and such. Well, we can get arbitrarily close to that definition as well via RLHF.Another objection might be: well, if thats the definition of AGI, that seems really underwhelming compared to the hype train. This says nothing about autonomy, sentience, free will -- exactly. Those concepts can or should be orthogonal to doing productive work.l, IMHO.So, there it is. We can now make a reward model for folding socks, and use gradient descent with RL to do the motion planning.Maybe thats AGI and maybe its not, but I&#x27;d really love it if we had a golden period between now and total enshittification that involved laundry folding robots. reply pshc 10 hours agorootparentIt tends to boil down to semantics, but as I interpret it, a threshold for AGI has to involve some breakthrough in generalizable, adaptable intelligence. So, yeah, I would invoke the \"that&#x27;s not AGI\" move.An AGI should be able to solve any creative problem a human could, with drive and knowing purpose and coherent vision. The LLMs are still narrowly focused and require human supervision.We might well get there with chained AIs automatically training new reward models for each new problem, or by some other paradigm, but I don&#x27;t feel like we&#x27;re past the threshold yet. reply FrustratedMonky 12 hours agorootparentprevExactly. 5 years ago, we would have said what GPT-4 is doing now would be AGI.Now it is here and it&#x27;s like \"No, what we really meant is it has to be the next Einstein\".People are forgetting how stupid people are.GPT is already better than average human.Most people can&#x27;t do what we claim GPT must be capable of to qualify as AGI.The only logical conclusion is that many people are also not conscious and don&#x27;t qualify as being able to reason. reply krainboltgreene 12 hours agorootparent> Exactly. 5 years ago, we would have said what GPT-4 is doing now would be AGI.okay, and 500 years ago we would have said it was magic, that doesn&#x27;t make it magic. people who don&#x27;t understand the thing often are confused about the thing. as soon as you explain how the whole mechanism works it&#x27;s obvious that it&#x27;s not that thing.> People are forgetting how stupid people are. GPT is already better than average human. Most people can&#x27;t do what we claim GPT must be capable of to qualify as AGI. The only logical conclusion is that many people are also not conscious and don&#x27;t qualify as being able to reason.citation massively needed, this sounds like it was written by someone who thinks idiocracy was a documentary and not a comedy. reply mistermann 7 hours agorootparentprev> The only logical conclusion is that many people are also not conscious and don&#x27;t qualify as being able to reason.Most humans aren&#x27;t \"able to\" juggle 3 balls, but most humans are physically and mentally capable of learning to be able to juggle 3 balls, it&#x27;s just not a common thing for most people to learn. The same used to be true of reading and basic math, but look where we are now with some good planning and hard work! reply AnimalMuppet 13 hours agorootparentprevWell... humans have different mental \"spaces\" (not intended as a technical term).Let&#x27;s say I&#x27;m deep in a coding problem. A co-worker comes by and says \"How did your team do in the game yesterday?\". I say, \"Um, uh... sorry, my head&#x27;s not there right now.\" It takes us time to swap between mental \"spaces\".So, if I have an AGI (defined as having a trained model for almost everything, even if that turns out to be a large number of different models), if it has to load the right model before it can reason on that topic, then that&#x27;s pretty human-like. (As long as it can figure out which model to load...)The one thing missing is that (at least some) humans can figure out linkages between different mental \"spaces\", to turn them into a more coherent holistic mental space, even if they don&#x27;t have (all of) each space at front-of-mind at any moment. I&#x27;m not sure if this flavor of an AGI could do that - could see the connections between different models. reply pixl97 12 hours agorootparentThe power of analogy is one of the most important things that humans seem to have.Humans typically use the toolset they&#x27;ve seen along the way to solve problems (hence if you have a hammer all problems become nails statement). When you get people that are multi-disciplinary they commonly can solve a complex problem in one field by bringing parts of solutions from other fields.Hence if you have more life experiences (especially positive&#x2F;learning ones) you are typically better off then a person who does not.Also I think this is where a lot of interest in Q* learning after the OpenAI thing occurred, as this would be a means of allowing an AI to explore problem spaces and enlist specialist AI and tools for it to do so. reply upghost 11 hours agorootparentsolid points. As a D&D nerd, might I offer that this is more along the lines of AGW (Artifical General Wisdom) than Artificial General Intelligence? Intelligence seems mode closely related to \"IQ\" as in (mechanical) \"ability to solve a problem\". But wisdom is knowing when to solve the problem and maybe when not to solve it, or which problem to solve. And of course, those times when instead of solving it, way better to talk about it with your fellow nerds on HN!! reply33a 15 hours agoprevLooking at their data and their experiments, I&#x27;d actually come to the opposite conclusion of the title. It&#x27;s true that current LLMs are probably not quite at human level performance for these tasks, they&#x27;re not that far off either and clearly we see as models increase in size and sophistication their performance on these tasks are improving.So it seems like maybe a better title would be \"LLMs don&#x27;t have as advanced a theory of mind as a human does... for now...\" reply og_kalu 15 hours agoparentIndeed. Not sure what i was expecting reading the title but \"GPT-4V is close to or matching human median performance on most of these tasks\" was not it. reply hilux 12 hours agoprev> A chief goal of artificial intelligence is to build machines that think like people.Maybe that&#x27;s their goal.But for many users of AI, the goal is to have easy and affordable access to a machine that, for some input (perhaps in a tightly constrained domain), gives us the output that we would expect from a high-functioning human being.When I use ChatGPT as a coding helper, I really don&#x27;t care about its \"theory of mind.\" And its insights are already as deep (actually more deep) as I get from most humans I ask for help. Real humans, not Don Knuth, who is unavailable to help me. reply dontupvoteme 8 hours agoparentTelling it to write like (or outright that it is) Knuth might just get it to write more efficient algorithms as it were.IIRC one of the prompting techniques developed this year was to ask the model who are some world class experts in a field and then have it write as if it was that group collaborating on the topic. reply marmaduke 12 hours agoparentprev> insights are already as deep (actually more deep) as I get from most humans I ask for helpThis was my thought as well. But then I figured if I can&#x27;t get someone to give me thoughtful feedback, I might have bigger problems to solve. reply szundi 12 hours agorootparentOr rather 30 different people on 30 different topics reply krainboltgreene 12 hours agoparentprevLook this is the only time I&#x27;ll engage in this sort of discussion on HN[1], but first Donald Knuth is a real Human and it&#x27;s extremely weird to position world class experts as something otherworldly. Second, suppose you got what you wished for (you used the \"us\" pronoun), is that not a sentient mind that you&#x27;re forcing to do your labour? Does that not raise a ton of red flags in your ethics?[1] normally I find HN discussions about what if chatGPT is human or \"humans are just autocompletes\" to be highschool-level scifi and cringe respectively reply hilux 12 hours agorootparentI don&#x27;t understand your objection about Don Knuth. I&#x27;m well aware who he is. My point is that I don&#x27;t have access to that kind of insightful human helper. So I \"settle\" for ChatGPT.And by \"us\" I mean \"those of us who choose to use ChatGPT,\" and not that I was forcing you to use ChatGPT.It&#x27;s true, I don&#x27;t morally object to asking ChatGPT to \"do my labour.\" It raises no red flag for me. (Okay, there&#x27;s the IP red-flag about how ChatGPT was trained, but I don&#x27;t think that&#x27;s what you mean.) reply mr_toad 9 hours agorootparentprev> is that not a sentient mind that you&#x27;re forcing to do your labour? Does that not raise a ton of red flags in your ethics?Maybe and maybe not. Being sentient or even sapient doesn‚Äôt mean it has human emotions, feelings or motivations. You have to be very careful when ascribing naturally evolved emotions and motivations to something that is not a naturally evolved intellect. reply creer 6 hours agorootparentDoes it need \"human emotions, feelings etc\" to \"deserve\" protection and status? Human society answered no a long time ago. But still it will be interesting to see what protections are granted to entities that are unarguably also machines. reply hiddencost 17 hours agoprevAnother paper in a long series that confuses \"our tests against currently available LLMs tuned for specific tasks found that they didn&#x27;t perform well on our task\" with \"LLMs are architecturally unsuitable for our task\". reply marcosdumay 16 hours agoparentOur tests against current cars found that they didn&#x27;t perform well on transatlantic flights... But who knows what the future holds? Maybe we should test them again next year.LLM names an specific product, aimed at solving an specific problem. reply LesZedCB 14 hours agorootparentour tests of combustion engine driven crankshafts connected to a spinning mechanism can perform well in both transatlantic flights and cross country road trips. reply AnimalMuppet 13 hours agorootparentThat&#x27;s not the hard part about building a working airplane, though... reply TeMPOraL 11 hours agorootparentYes, but as the saying goes, anything can fly if you strap a powerful enough engine to it... so we&#x27;ve demonstrated basic capability, and the rest is now optimizing its performance a couple orders of magnitude. reply AnimalMuppet 7 hours agorootparentNot at all. \"Anything can fly\", but not everything can fly without killing you. The fundamental problem is three-axis control with stability. A source of power does exactly nothing to solve that. reply TeMPOraL 2 hours agorootparentNo, that just means you need a bigger engine. (And, perhaps, an inertial dampener, to survive the gees.)Think of it this way: the more thrust you get, the farther you&#x27;ll go before control problems start to manifest. Keep pushing, and eventually the rest of the craft becomes a rounding error in the math :). replydsr_ 16 hours agoparentprevThere is no reason to believe (evidence) that any meaning ascribed to an LLM&#x27;s utterances comes from the LLM rather than being pareidolia.If you&#x27;ve found some, please let everyone know. reply Tadpole9181 15 hours agorootparentThere is no reason to believe (evidence) that any meaning ascribed to anyone but me&#x27;s utterances comes from the person rather than being pareidolia.If you&#x27;ve found some, please let everyone know. reply mistermann 7 hours agorootparentIt&#x27;s worse: there is technically no way to know if there is in fact no evidence, it is a colloquial phrase that people cannot think twice about. reply kkzz99 16 hours agorootparentprevYou would first have to define what you mean with \"meaning\" and \"pareidolia\" in this context. reply LesZedCB 14 hours agorootparentprevall text ever written has only ever had meaning imbued by the reader (including this text). reply og_kalu 17 hours agoparentprevIt&#x27;s a weird title anyway. I was expecting worse results but GPT-4V is close to or matching Human median performance on most of the tests besides the multimodal \"Intuitive Psychology\" tests. reply fnordpiglet 17 hours agoprevIn Buddhism there‚Äôs the idea that our core self is awareness, which is silent - it doesn‚Äôt think in a perceptible way, it doesn‚Äôt feel in a visceral way, but it underpins thought and feeling, and is greatly impacted by it. A large part of meditation and ‚Äúrelease of suffering‚Äù is learning to let your awareness lead your thinking rather than your thinking lead your awareness.To be clear, I think this is in fact a correct assessment of the architecture of intelligence. You can suspend thought and still function throughout your day in all ways. Discursive thought is entirely unnecessary, but it is often helpful for planning.My observation of LLMs in such a construction of intelligence is they are entirely the thinking mind - verbal, articulate, but unmoored. There is no, for lack of a better word, ‚Äúsoul,‚Äù or that internal awareness that underpins that discursive thinking mind. And because that underlying awareness is non articulate and not directly observable by our thinking and feeling mind, we really don‚Äôt understand it or have a science about it. To that end, it‚Äôs really hard to pin specifically what is missing in LLMs because we don‚Äôt really understand ourselves beyond our observable thinking and emotive minds.I look at what we are doing with LLMs and adjacent technologies and I wonder if this is sufficient, and building an AGI is perhaps not nearly as useful as we might think, if what we mean is build an awareness. Power tools of the thinking mind are amazingly powerful. Agency and awareness - to what end?And once we do build an awareness, can we continue to consider it a tool? reply pixl97 15 hours agoparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moravec%27s_paradoxWhile you&#x27;re adding a bunch of eastern philosophy to it, we need to take a step back from &#x27;human&#x27; intelligence and go to animal and plant intelligence to get a better idea of the massive variation in what covers thought. In animal&#x2F;insects we can see that thinking is not some binary function of on or off. It is an immense range of different electrical and chemical processes that involve everything from the brain and the nerves along with chemical signaling from cells. In things like plants and molds &#x27;thinking&#x27; doesn&#x27;t even involve nerves, it&#x27;s a chemical process.A good example of this at the human level is a reflex. Your hand didn&#x27;t go back to your brain to ask for instructions on how to get away from the fire. That&#x27;s encoded in the meat and nerves of your arm by systems that are much older than higher intelligence. All the systems for breath, drink, eat, procreate were in place long before high level intelligence existed. Intelligence just happens to be a new floor stacked hastily on top of these legacy systems that happened to be beneficial enough it didn&#x27;t go extinct.Awareness is another one of those very deep rabbit hole questions. There are &#x27;intelligent&#x27; animals without self awareness, but with awareness of the world around them. And they obviously have agency. Of course this is where the AI existentialists come in and say wrapping up agency, awareness, and superintelligence may not work out for humans as well as we expect. reply thegiogi 13 hours agorootparent> A good example of this at the human level is a reflex. Your hand didn&#x27;t go back to your brain to ask for instructions on how to get away from the fire.Is this actually true? I thought it just involved a different part of the brain. Is there actually no brain involvement? Sure it does not need your awareness or decision making, but no brain? I find that hard to believe. reply pixl97 13 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reflex_arcSimple answer: No, it does not go to the brainDetailed answer: We are complex as all hell.>A reflex arc is a neural pathway that controls a reflex. In vertebrates, most sensory neurons do not pass directly into the brain, but synapse in the spinal cord. This allows for faster reflex actions to occur by activating spinal motor neurons without the delay of routing signals through the brain. The brain will receive the input while the reflex is being carried out and the analysis of the signal takes place after the reflex action. reply TeMPOraL 11 hours agorootparentOne way of seeing a subset of reflex behavior is speculative execution - as in, you&#x27;ll start executing on stimulus before the brain has a chance to evaluate it, but when it eventually does, it may cancel the reflexive action. This is absurdly efficient if your reflexes are well-calibrated. reply danenania 16 hours agoparentprevAnother idea from Buddhism is that this core of awareness you&#x27;re talking about is nothingness. So when you stop all thought (if such a thing is really possible), you temporarily cease to exist as an individual consciousness. \"Awareness\" is when the thoughts come back online and you think \"whoa, I was just gone for a bit\".If that&#x27;s how it works, then the \"soul\" is more like an emergent phenomenon created by the interplay between the various layers of conscious thought and the base layer of nothingness when it&#x27;s all turned off. That architecture wouldn&#x27;t necessarily be so difficult to replicate in AI systems. reply fnordpiglet 12 hours agorootparentI think what you mean is that in Buddhism there is no self beyond the self implied by your thinking mind. The nothingness you refer to is the eschewing of attachment to what isn‚Äôt and being simply what is. It doesn‚Äôt mean a void, it means that all existence is within the awareness, which isn‚Äôt directly observable and is constantly changing. As such, it‚Äôs effectively nothing - except it is literally all you are. Your past and memories are just crude encodings, the future is a delusion. Your self identity has almost nothing to do with who you actually are right now. Your dissatisfaction with your situation isn‚Äôt meaningfully different from your delight in some experience - they‚Äôre both transient, and are just experiences of the present. You can avoid unpleasantness, and enjoy pleasure, but holding onto and seeking or avoiding entangles your awareness in what isn‚Äôt to the determinant of what is. As you continue releasing the various attachments and let the awareness take hold, and actions come naturally without thought or attachment, you cease suffering and cease causing suffering.But to my understanding the idea of nothingness being some objective in Buddhism isn‚Äôt the case - but it‚Äôs often described as such because that state of pure awareness without encumbering thought and attachment in many ways to an unpracticed person feels like nothingness. After all, the awareness is silent, even if it is where all thought and feeling spring from.Finally, awareness isn‚Äôt that moment you snap back to thought. You‚Äôre always aware. We just tend to be primarily aware of our thoughts and emotions. We walk around in a haze of the past and future and fiction as the world ticks by around us, and we tend to live in what isn‚Äôt rather than what is. You don‚Äôt disappear in the sense that you cease to be as an individual mind, you are always yourself - that‚Äôs a tautology. What you lose is the sense of some identity that‚Äôs separate from what you ARE in this very moment. You aren‚Äôt a programmer, you aren‚Äôt a Democrat, you aren‚Äôt a XYZ. You are what you are, and what that is changes constantly, so can‚Äôt be singularly defined or held onto as some consistent thing over time with labels and structure. You just simply are. reply danenania 11 hours agorootparentI&#x27;m not an expert in Buddhism, but from what I&#x27;ve read I think your interpretation may be a bit reductive of the many strains of thought that exist within Buddhism.\"You don‚Äôt disappear in the sense that you cease to be as an individual mind, you are always yourself - that‚Äôs a tautology. What you lose is the sense of some identity that‚Äôs separate from what you ARE in this very moment.\"This assumes that there&#x27;s any concept of \"you\" that exists independent of your thoughts whatsoever. I think you&#x27;re right that some Buddhist thinkers believe in this kind of essential awareness underlying conscious thought that you&#x27;re describing, but others would say there is literally nothing underneath. \"The self is an illusion\", \"all is emptiness\", etc. If you believe in those ideas, then you have no awareness independent of conscious thought because there is no you independent of conscious thought. reply fnordpiglet 10 hours agorootparentNote, the interpretation I‚Äôm presenting is the Theravada Buddhism view interpreted into a western rational framework. There are indeed many forms of Buddhism and I‚Äôll admit I‚Äôm unfamiliar with all of them other than Theravada Buddhism.The self as an illusion though refers to the concept that we as we exist now is ever changing and can‚Äôt be kept as some identity as that identity is necessarily not who you actually are. Our memories are not real - just shadows of an experience, and our aspirations are fiction. That does not however mean you don‚Äôt exist. You do exist, and as you move beyond the basics of mindfulness and releasing of attachments our role in a broader human experience becomes crucial - loving kindness (metta), and other aspects, are what are suppose to fill that void left by detaching from self and what isn‚Äôt. You move from being obsessed with self to becoming a force of goodness in the world by virtue of your existence. This couldn‚Äôt possibly be the case if the goal were nothingness and that‚Äôs it. This is where karma and other concepts come into play. reply nprateem 7 hours agorootparentprevYou&#x27;ve misunderstood. Awareness absolutely is not \"when thoughts come back online\".This is simple to experience for yourself since it&#x27;d mean we stop being aware when listening so intently thoughts stop. Obviously we don&#x27;t cease to be aware at such times.You&#x27;ve also misunderstood what is meant by nothingness (\"no thingness\"). reply danenania 6 hours agorootparentOk? I‚Äôm not an expert, but I‚Äôve read enough to know that there are many differing takes on these topics within Buddhist thought. Your appeals to dogma are not convincing. reply NoMoreNicksLeft 16 hours agorootparentprev> So when you stop all thought (if such a thing is really possible),It&#x27;s not. They don&#x27;t realize it, they&#x27;re merely referring to stopping your internal monologue. There are dozens of other mental processes going on in any given waking moment. Even actual top shelf cognition is going on, it just occurs in a \"language of thought\". reply astrange 13 hours agorootparent> It&#x27;s not. They don&#x27;t realize it, they&#x27;re merely referring to stopping your internal monologue.They certainly have realized that. It&#x27;s one of the first things you notice doing awareness meditation; thoughts appear from nowhere even if you didn&#x27;t try to think them. reply NoMoreNicksLeft 13 hours agorootparent> thoughts appear from nowhere even if you didn&#x27;t try to think them.This isn&#x27;t a joke? Like, that actually happens to people? I thought it was just to exaggerate how poorly the minds of dumb people work. reply fnordpiglet 12 hours agorootparentAre you saying every thought you‚Äôve ever had is a logical consequence of all prior thoughts with a definite traceable lineage? (Assuming your mind was too dumb as some point and a thought emerged that originated all future thoughts) reply NoMoreNicksLeft 11 hours agorootparentI&#x27;m saying that I&#x27;m extremely confident that...1. Most people (including Buddhists) think that their internal monologue comprises their thoughts, in majority or even in total.2. Can&#x27;t conceive of the possibility of a thought existing other than expressed in their spoken language3. Find it difficult or impossible to suppress their inner monologue.4. When successful at suppressing it believe that their thoughts have ceased.5. Are apparently unaware of the paradox that belief entails... if they are no longer thinking, how is the decision made to initiate thinking once more? That&#x27;s a conscious decision, or else successful Buddhists would turn into vegetables and die of starvation in their little meditation pose.This isn&#x27;t a matter of speculation anymore. We can see inside the brain, non-invasively, while these things occur. Any mystical element is an artifact of people poorly defining words, or being completely ignorant of how a brain must operate in principle. You&#x27;re all very confused. reply astrange 11 hours agorootparentI didn&#x27;t say anything mystical.> Most people (including Buddhists) think that their internal monologue comprises their thoughtsAnd they don&#x27;t think this, in fact they think the opposite.You seem to be offended I used the expression \"from nowhere\" instead of \"from your unconscious\" or something. reply Trasmatta 10 hours agorootparentprev> Are apparently unaware of the paradox that belief entails... if they are no longer thinking, how is the decision made to initiate thinking once more?It doesn&#x27;t need to be, and isn&#x27;t, a conscious decision. reply fnordpiglet 10 hours agorootparentprevI would say you‚Äôre completely wrong and we have thousands of years of Buddhist experience that contradicts everything you said, including that Buddhist think their inner monologue comprises their inner thoughts.First, not everyone even has an inner monologue. Just like many people can‚Äôt close their eyes and see things while others can, many people don‚Äôt think in words.Second, you absolutely can suspend your thoughts. I do regularly. In fact the entire point of Buddhist meditation is to practice the act of suspending observable thoughts entirely in a controlled environment to carry the practice over into every day life. I like you didn‚Äôt think it was possible until after several years of practice I could. With that I lost my anxiety and depression, and found a way to be generally happy. I still think, but it‚Äôs in service towards a goal. Otherwise I let myself simply live. In so, I have a happy marriage and a successful career. Much more so than before.I don‚Äôt feel compelled to convince you I‚Äôm not lying or otherwise deluded if you choose to not believe me.Now, how do you choose to begin thinking again? The fact is you‚Äôre always thinking when you are conscious and aware, but it‚Äôs happening in an aspect of your mind you can‚Äôt perceive directly. While I‚Äôm not thinking I still make decisions and take actions and initiate things, even carry on conversations and form intentions. But they aren‚Äôt thoughts that are articulating in my mind like we typically think of as thinking. I usually drop into thinking when I need to analyze something rationally or methodically plan, particularly if it has to be verbalized or written in some way, or recorded on a schedule or something. How do I begin that? I can‚Äôt observe the process and isolate it as that‚Äôs just the way things are. I‚Äôm definitely aware of my intentions, but I‚Äôm not thinking about them or articulating some thoughts in my mind. They just flow naturally from the present. When my mind needs to think, it does. Because, my thinking facilities in my mind are merely a tool of my mind - the crucial insight is that we are not our tools.I don‚Äôt think any of this is mystical in the least. I think people who think their thoughts are really important are the mystical thinkers. We are meat brains, and our thinking facility is a late evolutionary feature that we‚Äôve become ensnared in much to our own suffering as it distracts us from our more natural states of being because they‚Äôre so seductive in its constructs. I think Buddhism in its most raw form is almost entirely devoid of mystical thinking and most mysticism associated with it is an accretion of animistic and ancestor worship indigenous societies where it developed. reply akomtu 9 hours agorootparentprevThis looks like AI-buddhism, a \"fork\" of buddhism rewritten by an uber-LLM of the future, in which it will discard everything it cannot relate to. To such an AI, the only internal process is a sequence of words, and nothing is thinkable outside such a sequence.However, we can imagine images, shapes and sounds, indescribable in words, and even higher there are abstract ideas that can&#x27;t be expressed with images or sounds, let along with words.The \"thoughts from without\" may appear for a multitude of reasons, because our minds aren&#x27;t isolated systems. The source may be some sensory input that triggered a memory, and I wouldn&#x27;t even reject the possibility, that brains can react to background electromagnetic noise and form thoughts based on it. reply astrange 12 hours agorootparentprevThat seems like an unusually rude thing to say about Buddha. You might be experiencing dukkha. replyjacobsimon 16 hours agoparentprevThis is a profound question but I also wonder if this non-thinking ‚Äúawareness‚Äù you‚Äôre referring to is largely defined by quieting the thinking mind and listening to the senses more directly. A lot of meditation is about tuning out thoughts and focusing on proprioception like breathing, the feelings of the body, etc. reply vjerancrnjak 16 hours agorootparentFundamentally, this \"awareness\" isn&#x27;t defined by quieting the thinking. It is a description of fundamental reality. No individual should be able to experience it, and the \"glimpses\" are just forms of brain dysfunction.Meditation techniques that focus on breath or the body are an attempt to make you do the breathing&#x2F;sensing consciously. If you film yourself and later look at what you did, you&#x27;ll notice you aren&#x27;t breathing well when you&#x27;re breathing consciously, so you&#x27;re probably depriving yourself of oxygen, lowering blood concentration in certain brain regions and you hope it will be the brain region associated with conceptualizing, language etc.You can do the same with sleep. You can try to consciously fall asleep, and just like breathing, you will have a hard time because there&#x27;s a reason why falling asleep is not conscious (or in other words it does not go through the regions of the brain that conceptualize). You can experience the balance center shutting down (feels like falling or turning) and you can go even deeper and feel the fear of the \"ego\" dying (temporarily). What remains is definitely much different than waking or dreaming state. But it is still not that \"awareness&#x2F;nothingness\". reply fnordpiglet 12 hours agorootparentI think this is entirely incorrect. Vipassana meditation, the type focused on breathing, require intense awareness of your breathing and physical body. It‚Äôs a similar state to when you intensely focus on what‚Äôs around you and everything gets brighter and more vibrant and you pick out a lot of details you normally don‚Äôt notice because you‚Äôre distracted by your thoughts.If you‚Äôre doing it the way intended you would 100% be aware of your irregular breath or pausing. In fact beginners vipassana often advises counting the breaths individually in a cycle 1..10, and resetting the count when you lose track of your breathing. You intensely focus on the sensation of the air moving through your nostrils, the muscles contracting, your clothing shifting.However it‚Äôs not about controlling your breathing, so it‚Äôs not the same as breathing consciously. It‚Äôs observing passively. Often you‚Äôll notice that you are breathing irregularly, not because of the meditation, but because you typically are stressed and tight in your musculature due to the way you‚Äôre thinking. You can then loosen and reset your patterns of breath to be more natural, deep, and complete.A goal isn‚Äôt to stop with observing the breath though, and you work towards having a total awareness of the entire body at once, shifting your center of existence from your head to the rest of your body. You then incorporate sounds and events in your environment. This requires an intense amount of mental power, and is entirely different from your description of oxygen deprivation. Thought ceases because it interferes with being aware, not because you are experiencing brain death. reply vjerancrnjak 9 hours agorootparent> It‚Äôs observing passively.This is a byproduct of the process. You cannot observe passively if you still have a very strong imprint of a doer. When you start observing the tingles and the sensations, you still observe them from the center and there&#x27;s a distance from the center and the sensations. The only way this center disappears is if the regions of the brain that create imprints lower their activity.I&#x27;d say it is very easy to accomplish it when you&#x27;re sleepy (especially after you wake up during the night and go back to sleep). There&#x27;s no need to shut anything down through high focus, brain does it naturally to paralyze the body and lose the waking state.\"My Stroke of Insight\" by Jill Bolte Taylor goes through many side-effects experienced by the meditators, but through a lens of a brain stroke.> Thought ceases because it interferes with being aware, not because you are experiencing brain death.Not sure about that. The verbal&#x2F;visual thinking can be a strong feedback loop to the underlying unconscious processes. The only way I can see that it stops is if it is suppressed to unconscious, you&#x27;ve learned to not have it in focus. Also, many people have no visual and verbal thoughts. They don&#x27;t have an imprint of standard modes of thinking at all. I&#x27;d say the definition of thought needs to expand to sensing. Any sensation is a thought. Focus on verbal&#x2F;visual thoughts is misplaced.I do agree that people learn to eventually do this without the side effects of practice. They can probably transition to a different brain activity without dealing with the breath ever again.I think a much better standard for meditators is to ask them if they feel like their whole body is alien, or if they have something like alien hand syndrome. Losing thoughts, yet still being the body (or even the space around the body) is just another form of thoughts.The imprint of a doer gives you the feeling of someone controlling the body or doing the thinking. For example, I can get lost in thoughts so hard that I no longer have imprints of controlling the body. Somehow I leave the shower and dress myself yet I know I don&#x27;t remember doing any of it.To also move the conclusion further, it sounds like the ultimate goal is to feel like you&#x27;re not separate but that you&#x27;re whole, so in that case it must be that this body, from that perspective, is fully without individual imprint, behaving completely like an input&#x2F;output machine, no inner subjective experience present. reply sdwr 16 hours agoparentprevMaybe the soul is social, and oriented towards others? I believe it can be constructed.If you assume that \"the eyes are the window to the soul\", you notice some interesting properties.1. It is far more observable from the outside (eyes open&#x2F;lidded&#x2F;closed, emotion read in eyes)2. It affects behavior in a diffuse way3. It pays attention but does not dictate reply munificent 16 hours agorootparent> Maybe the soul is socialMy pet theory about human consciousness is that is that consciousness is simply recursive theory of mind. Theory of mind [1] is our ability to simulate and reason about the mental states of others. It&#x27;s how we predict what people are thinking and how they will react to our actions, which is critical for choosing how to act in a social environment.But when you&#x27;re thinking about what&#x27;s in someone&#x27;s head, one of the things might be them thinking about you. So now you&#x27;re imagining your own mind from the perspective of another mind. I believe that&#x27;s entirely what our sense of consciousness is. It&#x27;s our social reasoning applied to ourselves.If my pet theory is correct, it implies that the level of consciousness of any species would directly correlate to how social the species is. Solitary animals with little need for theory of mind would have no self awareness in the way that we experience it. They&#x27;d live in a zen-like perpetual auto-pilot where they do but couldn&#x27;t explain why they do what they do... because they will never explain it to anyone anyway.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Theory_of_mind reply crdrost 16 hours agorootparentTheory of mind is interesting but one wouldn&#x27;t want to hinge consciousness upon it.That direction would likely contain weird outcomes if the science progressed, something like \"Dogs are barely-conscious due to their pack structure, they have a couple levels of recursive theory of mind but they can&#x27;t sustain it as deep as we can. But cats didn&#x27;t have that pack structure, they&#x27;re not conscious at all.\" Or, \"this person has such severe autism that he cannot fundamentally understand others&#x27; minds or what others interpret his mind to be, so we&#x27;ve downgraded his classification to unconscious. He&#x27;ll talk your ear off about the various cars produced in a golden age between 1972 and 1984, but because he doesn&#x27;t really know what it means for you to be listening we regard it as sleep-talking.\"It also just kind of doesn&#x27;t sound right. \"What happens when we go to sleep? Well, we stop thinking about what others think we think, and we simply accept what they think about us.\" That doesn&#x27;t sound like any sleep I experience -- it might describe some of my dreams, but of course dreams are anomalous conscious experiences that happen during sleep so that also misses the mark. reply munificent 12 hours agorootparent\"Consciousness\" is one of those loaded words that means a few different things in different contexts. When I say consciousness is about recursive theory of mind, I&#x27;m not trying to say that when you&#x27;re asleep you&#x27;re unable to do social reasoning. That&#x27;s a different use of the same word.I mean \"conscious\" in the sense of self-awareness or sentience.I&#x27;m also not ascribing any moral or cognitive superiority or inferiority to different levels of it. The fact that a cat might be less self-aware of its suffering because it think about how it would explain its pain to other cats does mean imply that I&#x27;m saying it should be OK to torture cats.I&#x27;m just interested in what is going in human brains when we \"feel self-aware\". What are we doing when we&#x27;re thinking about what we&#x27;re doing? Where does that sense of perceptual distance come from when we are aware of ourselves? And my pet theory is that the distance comes from imagining how we look through others&#x27; eyes and developed from our highly advanced social reasoning. reply sdwr 13 hours agorootparentprevSounds about right to me, honestly. In my experience, if the theory is predictive, insulting, and has societal ramifications, it&#x27;s usually correct.Let&#x27;s say the soul is exclusively located in eye-to-eye contact. Theres a lot of information in how that contact is broken, how long its broken for, and what happens in between.(Enemy&#x27;s-gate-is-down-style reorientation) reply pshc 10 hours agorootparentprevI think people who seldom engage in direct socialization repurpose their social machinery, diverting it toward internal dialogues with themselves, and toward artifacts that were generated by other minds. An indirect, conscious channel into the world, a simulacra theory of mind. reply jacobsimon 15 hours agorootparentprevYou might find this book interesting! This is essentially the theory put forward. https:&#x2F;&#x2F;www.google.com&#x2F;books&#x2F;edition&#x2F;Consciousness_and_the_S... reply munificent 12 hours agorootparentAh, that looks perfect! Thank you! I knew other people smarter than me must have stumbled onto this idea as well. reply crdrost 6 hours agorootparentFWIW if you&#x27;re looking for book recommendations this is similar to the thesis of G√∂del, Escher, Bach: An Eternal Golden Braid. reply Davidzheng 14 hours agorootparentprevIf this is correct, do you think GPT5 will be conscious because its training data will include a lot of itself (albeit GPT4 not 5) reply munificent 12 hours agorootparentI&#x27;m sorry, but I honestly don&#x27;t find philosophical questions about the intelligence or sentience of generative AI interesting at all. reply passion__desire 15 hours agorootparentprevA Possible Evolutionary Reason for Why We Seem to Have Continuity of Consciousness and Personality.Thesis : The very thing (a brain module) which allows for outside object continuity, that same brain module maintains inside self &#x2F; personality &#x2F; identity continuity.Reasoning :Evolution found out modelling the outside world is helpful for survival. Some eons later, it figured out modelling yourself (self &#x2F; agent) modelling the outside world is also helpful. In the outside world, we keep track of continuity of objects through a brain module which hones in on the essence of objects (e.g. tracking a prey or predator.) so that EXACT matching algorithms aren&#x27;t applied but ONLY approximate ones. As soon as a high enough approximate match (>95%) is found, we \"register\" it to be an exact match. i.e. the Brain bumps up the confidence level to 100%. This is also the reason why we consider our friend Bob to be the same childhood Bob even though he has different hairstyle, clothes, and other such properties. We don&#x27;t call Bob who looks different than yesterday as an Imposter. The damage to this brain module could lead us to call Bob today an imposter. This same module also tracks continuity of self in a similar manner. Even though our \"self\" changes from childhood to adult we \"register\" \"changing self\" to be the same thing. i.e. internally we bump up the confidence to 100% when memories, etc. match and provide a coherent picture of the self. A multiple personality disorder is just different stable states of neural attractor states. Continuity is local to a personality but not global and hence transient. This local continuity of information could link up globally, giving rise to coherent single personality.Capgras Syndrome :Ramachandran Capgras Delusion Case : https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=3xczrDAGfT4 reply ctoth 13 hours agorootparentprev> If you assume that \"the eyes are the window to the soul\", you notice some interesting properties.And people say LLM output is nonsense.I&#x27;m blind with glass eyeballs. Does this mean my soul is easier to access than yours? Or is it harder because there&#x27;s something specific about the eyeball that makes it the window? reply Merrill 13 hours agoparentprevDecision making seems fundamental to intelligence, is done by animals and humans, and can be done without the use of language or logic. This is the case when someone \"decided without thinking\".Decision making requires imagination or the ability to envision alternative future states that may result from various choices.Imagination is the start of abstract thinking. Consciousness results from the individual thinking abstractly about itself and how it interacts with the world. reply dimal 16 hours agoparentprevI‚Äôve been thinking along similar lines. It‚Äôs like with LLMs, they‚Äôve created the part of the mind that is endlessly chattering, generating stories, sometimes true, sometimes false, but there‚Äôs no awareness or consciousness that ever steps back and can see thoughts as thoughts. And I don‚Äôt see how awareness or consciousness would arise from just more of the same (bigger models). It seems to be a fundamentally different part of the mind. I wonder if AGI is possible without this. AGI under some definition (good enough to replace most humans) may be possible. But it wouldn‚Äôt be aware. And without awareness, I don‚Äôt see how it could be aligned. It may appear to be aligned but then eventually it would probably get caught in a delusional feedback loop that it has no capacity to escape, because it can‚Äôt be aware of its own delusion. reply hotpotamus 15 hours agorootparent> It may appear to be aligned but then eventually it would probably get caught in a delusional feedback loop that it has no capacity to escape, because it can‚Äôt be aware of its own delusion.I believe this is more or less the definition of human mental illness. I have to say that while I know it&#x27;s really not possible, I wish people would stop pulling on these threads. I got into this line of work because I thought video games were cool, not because I wanted to philosophize about theories of mind and what intelligence is. I really don&#x27;t like thinking about whether I&#x27;m just some sort of automaton made out of meat rather than metal and silicon. reply pixl97 15 hours agorootparentAh, the first releases of the Bing AI were fun here as they plunged into feedback loops of madness that were scarily human sounding. Thank you humanity for making artificial insanity. reply melenaboija 15 hours agoprevFew weeks ago I did an experiment after a discussion here about LLMs and chess.Basically inventing a board game and play against ChatGPT and see what happened. It was not able to do a single move, even having provided all the possible start moves in the prompt as part of the rules.Not that I had a lot of hope about it, but it was definitely way worst than I expected.If someone wants to take a look at it:https:&#x2F;&#x2F;joseprupi.github.io&#x2F;misc&#x2F;2023&#x2F;06&#x2F;08&#x2F;chat_gpt_board_g... reply golergka 15 hours agoparentYou haven&#x27;t specified what model did you use, and the green ChatGPT icon in the shared conversation usually signifies GPT-3.5 model.Here&#x27;s my attempt at similar conversation ‚Äî it seems GPT-4 is able to visualise the board and at least do a valid first move.https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;98427e21-678c-4290-aa8f-da8e93... reply melenaboija 15 hours agorootparentInteresting.The model was whatever was up that that time, so probably was 3.5 if you say so. reply xcv123 13 hours agorootparentIf you were using the free ChatGPT then you were playing with an obsolete LLM. GPT-4 has an estimated 10x parameters of GPT-3.5 (1.8 trillion vs 175 billion), and other improvements. reply golergka 14 hours agorootparentprevYour conversation is from June, GPT-4 was available for almost half a year at that point. reply melenaboija 12 hours a",
    "originSummary": [
      "Vision-based large language models show proficiency in processing and interpreting visual data.",
      "However, they still lag behind human capabilities in intuitive physics, causal reasoning, and intuitive psychology.",
      "These models have a basic understanding of physical laws and causal relationships but lack deeper insights and fail in tasks requiring an intuitive theory of mind.",
      "The study highlights the importance of integrating more robust mechanisms for understanding causality, physical dynamics, and social cognition into language models."
    ],
    "commentSummary": [
      "The discussion explores various topics surrounding artificial general intelligence (AGI) and language models.",
      "Participants discuss the limitations of current language models and the need for recursive prompting patterns and ensemble models for better reasoning and understanding.",
      "The debate also covers the challenges of AI alignment, the distinction between intelligence and consciousness, the risks and benefits of AGI, and the relationship between language meaning and cognitive processes."
    ],
    "points": 257,
    "commentCount": 238,
    "retryCount": 0,
    "time": 1701358286
  }
]
