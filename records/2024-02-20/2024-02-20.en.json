[
  {
    "id": 39429370,
    "title": "htmz: Crafting Modular Web UIs with Lightweight HTML Microframework",
    "originLink": "https://leanrada.com/htmz/",
    "originBody": "=>htmz> a low power tool for html htmz is a minimalist HTML microframework that gives you the power to create modular web user interfaces with the familiar simplicity of plain HTML. [GitHub] plainüç¶ Use straight up HTML. No supersets. No hz- ng- hx- v- w- x-; no special attributes. No DSLs. No . Just vanilla HTML. lightweightü™∂ 166 bytes in total. Zero dependencies. Zero JS bundles to load. Not even a backend is required. Just an inline HTML snippet. nofilter‚ö° No preventDefaults. No hidden layers. Real DOM, real interactions. No VDOM, no click listeners. No AJAX, no fetch. No reinventing browsers. In a nutshell, htmz lets you swap page fragments using vanilla HTML code. Imagine clicking a link, but instead of reloading the whole page, it only updates the relevant portion of the page. htmz is an experiment inspired by htmx, Comet, ‚ÄòHTML As The Engine Of Application State‚Äô[1][2], and other similar web application architectures. Demos Check out these demos to get an idea of what htmz can do! Tabs Greeting Edit inline Dialog More examples# üêô Select an example above! Installing Simply copy the following snippet into your page: document.querySelector(contentWindow.location.hash||null)?.replaceWith(...contentDocument.body.childNodes))\"> For npm enjoyers, the following npm commands automate the process of copying the snippet into your page: npm install --save-dev htmz npx htmzify ./path/to/my/index.html For hackers, you may start with the development version (deminified): htmz.dev.html Basic usage To invoke htmz, you need a hyperlink (or form) having these attributes: href (or action) pointing to the resource URL href=\"/flower.html‚ãØ Continuing within the href: destination ID selector ‚ãØ#my-element\" And a target attribute with this value target=htmzFlower While this looks like an abuse of the URL fragment (it is), there is no other use for the URL fragment in this context, so it was repurposed as the destination ID selector. And it already looks like a CSS ID selector. ‚ö† Important note: The loaded content replaces the selected destination. It may not be intuitive at first, but htmz does not insert the content into the destination. The rationale is that replacement is a more powerful operation. With replacement, you can replace, delete (replace with nothing), and insert-into (replace with the same container as original). What does it do exactly? htmz does one thing and one thing only. Enable you to load HTML resources within any element in the page. Think tabbed UIs, dual-pane list-detail layouts, dialogs, in-place editors, and the like. This idea is not new. Dividing web pages into independently reloading parts has been a thing since mid-1990s. They were called frames, namely, s, s, and s. htmz is a generalisation of HTML frames. ‚Äî Load HTML resources within any frame any element in the page. Read more on how it works in a section below. Examples More example applications, componentization approaches, and code in different languages can be found in the /examples directory. To start the example server: cd examples ./run_servers.sh Then load http://localhost:3000/. Advanced usage Naturally, onlyandelements can target and invoke htmz (as of current HTML5). This is fine; it‚Äôs semantic, after all. However, HTML offers a couple more features that work well with htmz. Per-button action & target If you want to override the form‚Äôs action on a per-button basis, use the ‚Äôs formaction attribute.Default form actionDifferent button action Another action Base target value Tired of adding target=htmz to every link and form? Using the base element, set htmz as the default target for all relative links. Add this at the top of your page.Clean target values Don‚Äôt like the look of target=htmz at all? Prefer using the real target as the value? We can do a hack that enables you to write the target ID selector in the target attribute itself! Like this:Flower The key is to add an iframe with a matching name, and modify the htmz snippet accordingly.function htmz(frame) { document.querySelector(frame.name) // use the iframe's name instead of the URL hash ?.replaceWith(...frame.contentDocument.body.children); } You can even automate the generation of matching target iframes. Scripting / interactivity If you need something more interactive than the request-response model, you may try the htmz companion scripting language: javazcript. Sorry, I meant JavaScript, a scripting language designed to make HTML interactive. htmz does not preclude you writing JS or using UI libraries to enhance interaction. You could, say, enhance a single form control with vanillaJS, but the form values could still be submitted as a regular HTTP form with htmz. That said, htmz is extensible! Extensibility Need advanced selectors? Need error handling? Multiple targets? Fear not; the hero is here to save the day. The hero is you. Here‚Äôs the development version of the snippet. Feel free to hack and extend according to your needs. You‚Äôre a programmer, right? function htmz(frame) { // Write your extensions here // Remove setTimeout to let the browser autoscroll content changes into view setTimeout(() => document .querySelector(frame.contentWindow.location.hash || null) ?.replaceWith(...frame.contentDocument.body.children) ); }A number of extensions will be available in the custom builder (coming soon!). FAQ How does it work? htmz is an iframe named \"htmz\". You invoke htmz by loading a URL into the iframe via target=htmz. By using an iframe, we lean on the browser‚Äôs native capability to fetch the URL and parse the HTML. After loading the HTML resource, we take the resulting DOM via an onload handler. htmz is essentially a proxy target. Like how a proxy server forwards requests to some specified server, proxy target htmz forwards responses into some specified target.#my-element --> Flower#my-element --> Flower When you load a URL into the htmz iframe, the onload handler kicks in. It extracts your destination ID selector from the URL hash fragment and transplants the iframe‚Äôs contents (now containing the loaded HTML resource) into your specified destination. htmz only runs when you invoke it. It does not continually parse your DOM and scan it for special attributes or syntax, nor does it attach listeners in your DOM. It‚Äôs a proxy not a VPN. So it‚Äôs just another JavaScript framework? Oh my! Not the f-word!!! On a more serious note, I would say that rather than a JS one, it‚Äôs more of an HTML micro-f*******k. It does use JS, but only the minimum necessary. Is htmz a library or a framework? htmz is a snippet. ‚úÇ What does htmz mean? HTMZ stands for Html with Targeted Manipulation Zones. Is this a joke? This started as a ‚ÄúDo I really need htmx? Can‚Äôt I do the load-link-into-target thing with current web? Sounds a lot like frames.‚Äù and ended up with this. So, it isn‚Äôt quite a joke, but a response to htmx. I wanted to try htmx. The premise sounded great (Why should you only be able to replace the entire screen?), then I saw that it was 16kB of JavaScript. Huh. Then there‚Äôs special syntax everywhere. Huh. I don‚Äôt want to learn a whole new set of instructions and Turing-complete DSLs specific to those instructions. Regardless of joke status, htmz seems fine as a library. It feels kinda powerful for its tiny size. (But really it‚Äôs the browser that‚Äôs doing the heavy lifting!) Nonetheless, there are limitations. What are the limitations? The main direct limitation is having only one destination per response. However, this can be fixed by writing an extension. ;) A more general but classic limitation is the request-response model. The Web 1.0 model, and the baggage that comes with it. Like a roundtrip delay on every interaction, a browser history entry on every click, etc. The Web 1.0 model might also mean putting more UI logic in the web server. This can be a good thing or a bad thing, as it could lead to either consolidation or fragmentation of UI logic, which respectively decreases or increases complexity. It really depends on your goal and style. =htmz>",
    "commentLink": "https://news.ycombinator.com/item?id=39429370",
    "commentBody": "htmz ‚Äì a low power tool for HTML (leanrada.com)848 points by Kalabasa 20 hours agohidepastfavorite206 comments keepamovin 18 hours agoThis is great. I had an idea to use named iframes and targeted forms for simple, server-rendered pages with built-in style-scoped widgets, without leaning into complex JS client-side. But, I never simplified it well nor expressed a polished and elegant realization of that idea, as this htmz looks to me to be. A reminder to never give up good ideas, focus on excellence, and focus on refinement to a completion of an idea, and communicate well! Also the comments here: - This is a great hack and shows how close the browser is to offering SPA natively. - This is a glorious demonstration of someone really understanding the platform. - Simple and powerful, as the vanilla web should be. Thank you for this (small) gem :) - This is a neat hack, I like it :). Thanks for sharing. are exactly what I hoped to hear reflected about my creation, and are totally on point for what this type of thing should be. Close to the web-grain, using the given material of the web in the best way possible. Fuck yeah! :) Thank you for being a genius! :) And for inspiring about what's possible! :) P.S - also your communication and marketing skills are top notch! I think the way you have communicated this elegant technical thing, from the choice of name, API, examples, copy -- is just awesome. I learn from it! :) reply cschep 16 hours agoparentthanks for a high quality top comment! reply safety1st 14 hours agoparentprevI am a little bit confused because your comments seem to imply initially that htmz is written by someone other than you, and then later that you wrote htmz. Who are you and what is your relationship with htmz and its creators? Please be honest and refrain from violating federal law and FTC guidelines in your response. reply scubbo 12 hours agorootparentYou seem to have misinterpreted \"[those comments] are exactly what I hoped to hear reflected about my creation\". They weren't saying \"I enjoyed hearing those comments about htmz, which is a thing I created\" - they were saying \"those comments are what I _had_ hoped to hear about my (unreleased and unnamed) creation, which indicates that htmz is a good implementation of a similar idea that I had\" reply jkingsman 14 hours agorootparentprevI don't think this person is implying that at all (nor do I think that anyone needs to be trotting out federal law and FTC guidelines here :). \"I had an idea to use named iframes [...] But, I never simplified it well nor expressed a polished and elegant realization of that idea, as this htmz looks to me to be. [...] the comments here [...] are exactly what I hoped to hear reflected about my creation, and are totally on point for what this type of thing should be.\" Seems pretty clear to me. This person had a similar idea but didn't complete it, and finds the flavor of appreciative \"nice hack\" energy (as opposed to \"this is enterprise\" or \"this is a revolution\" energy, I guess) appropriate for the project and the type of feedback they had wanted to hear had they completed their project. reply RamblingCTO 1 hour agorootparentprevWho the f are you then? reply TimTheTinker 13 hours agorootparentprev> Who are you and what is your relationship with htmz and its creators? Please be honest and refrain from violating federal law and FTC guidelines in your response. My name is John Galt and I wrote this library. When you learn who I really am, then you'll understand. reply MikeTheGreat 11 hours agorootparentYou're a convenient fiction? ba-dump-ching! No, no don't bother, I'll show myself out :) reply tambourine_man 19 hours agoprevThat's a great hack and it shows how close the browser is to offering SPA natively. Just a few attributes and we could avoid the iframe. It's probably more useful to prove a point than an actual day to day tool. And the point seems to be: htmx is too much trouble for what it offers. We just need HTML native ajax. reply recursivedoubts 17 hours agoparentI'm the creator of htmx and think this is a great library/snippet. Much closer to what htmx-like functionality in HTML would/should look like in that it is following existing norms (iframes, the target attribute) much more closely than htmx. From a practical perspective, a lot of the bulk of htmx is bound up in things like history support, collecting inputs, a lot of callbacks/events to allow people to plug into things, etc. I expect a lot of htmx-like libraries will come out now that it has some traction: it's not a super complicated idea, and many of them will pick smaller, more targeted subsets of functionality to implement. That's a good thing: the ideas of hypermedia are more important than my particular implementation. reply harryvederci 14 hours agorootparentI'm not the creator of htmx and you are wrong. I am now officially emotionally invested in htmx so I have to justify investing my time/energy in htmx instead of something else, so all alternatives to htmx are stupid and their existence make me very angry. reply recursivedoubts 13 hours agorootparenthow sweet it is reply 1propionyl 7 hours agorootparentDulce et decorum est pro patria mori. reply tambourine_man 16 hours agorootparentprevSorry if I came across as dismissive, htmx is a much needed counterpoint to React's dominance and a great contribution to the industry. And I hope its core idea (Ajax from HTML with results loading inside a target element) will be adopted as a Web standard. reply recursivedoubts 13 hours agorootparentnot at all, i understand there are going to be different takes on what the right balance of functionality & configuration are and htmx has been hamstrung to an extent by being IE compatible, which, coupled w/ my commitment to backwards compatibility, means some obvious things are off the table (e.g. using fetch()) i don't begrudge other people having differing opinions on this stuff and agree w/the spirit of your original comment: HTML should have a native mechanism for doing what htmx does. hopefully htmx (and other takes on the idea) contribute to that future. reply liendolucas 11 hours agorootparentprevHi Carson! I've been using htmx sprinkled with hyperscript and I find using your tools very enjoyable (though I found myself fighting a bit with hyperscript at the beginning but past that and once you get the mindset and the hs way, things are easier). Thanks for these tools! I wanted to also take the opportunity to ask you something you either mentioned, commented or heard in a podcast. You said that htmx might not be the tool to solve all problems (or certain kind of apps). Just asking because I think is also great to hear when not to use something to solve a problem. So, in your opinion what kind of webapps do you consider that maybe htmx is not a good fit? And in that case what alternate tools/approaches do you suggest? Once again thanks a lot for htmx & hyperscript! reply recursivedoubts 9 hours agorootparentI wrote an essay on that topic here: https://htmx.org/essays/when-to-use-hypermedia/ Things that involve a lot of events that need to be handled quickly are not a good fit for hypermedia. Sometimes you can have a rich island inside hypermedia though (e.g. a rich text editor w/ autocomplete, etc) so long at is triggers events and offers form participation. Two common examples I give are google maps and google sheets. Those would be difficult to do well in htmx. (But the settings pages for them might be a good use case for it) reply bb88 11 hours agorootparentprevThanks for HTMX, I think it's showing the world what the web browser could be. I'm going to be doing a lot of web pages in HTMX in the next couple of years, and it will be much easier to develop/debug than javascript. reply Exoristos 9 hours agorootparentIt is JavaScript. reply gorjusborg 9 hours agorootparentThat's like saying Linux is C. True, but missing the point. reply felixguendling 12 hours agorootparentprevRegarding the size I would guess that if htmz would be extended to have the same features as htmx, it would also be similar in size? Would it make sense to modularize htmx in order to only pay for what you really use to support adding features without necessarily increasing the downloaded size? reply recursivedoubts 9 hours agorootparentI think you could do a smaller htmx by dropping a lot of the event and config stuff & adopting modern tools that produce smaller javascript. Clever use of JavaScript features could probably knock it down as well, but i'm anti-clever in most cases. As it stands, htmx is ~17ms to download over slow 4G and should be cached after the first download, so, while I wish I could make it smaller, every time I've tried to it ends up not moving the needle too much. We are going to get a chance to remove all the IE-related syntax in 2.x which should help a bit. reply DoesntMatter22 15 hours agorootparentprevLove the fact that you care more about the tech than HTMX winning. I like htmx a lot though reply eproxus 15 hours agorootparentprevKind of makes me wish there was a htmx-lite of sorts. Like an 80/20 solution at 20% of the size, which could be extended to the full blown package with extensions. Have you thought about moving more features to extension, like was done with the web sockets feature? reply recursivedoubts 13 hours agorootparentI've looked at the codebase and there isn't much I feel would do well as an extension. As I said upthread, most of the code is AJAX, input gathering, history support and then book-keeping/event stuff around that. There will be a bunch of htmx-like libraries coming out though and I expect many of them to take different design perspectives. Two that I'm aware of are https://data-star.dev/ and https://ajaxial.unmodernweb.com/, both were created by people on the htmx discord. I know of at least two other minimalist rewrites htmx discord users have done, but they haven't published their stuff yet. If you'd like to create your own, I do an overview of how htmx works here: https://www.youtube.com/watch?v=javGxN-h9VQ I think a minimal, clean room, fetch() based implementation could be done in a weekend. reply larodi 58 minutes agoparentprevYou know... we've been doing something very similar 20 years ago for a stats dashboard web app - reloading only DIVs that need with new content server-generated. We didn't even bother to recreate the DOM, but directly innerHTML = content loaded. Perl on the back-end and some very tiny JS on the front-end. Do I need to tell U that this worked as absolute charm and was blazing fast. The only considerable downside was that indeed lots of traffic was going back and forth. But then 20 years later latency is much lower, traffic much cheaper, CPUs also, and I am very happy to see more and more people realize this bare-bones approach was actually a good thing to consider (the author lists the downsides). To me such approach is much more web-native in comparison to abomination UI frameworks, that try to reinvent presentation marginalizing the browser to be nothing more than a drawing surface. But guess what - their primary goal is to save on this network latency, that is anyway going down down down down with every year. The htmlz/htmlx approach is indeed much simpler and easy to live with in a large project, it is really sad that we put so much logic in the front-end in recent years... reply Kalabasa 18 hours agoparentprevYes this was a response to htmx. It was a half-parody half-I wanna make it work project. Like https://github.com/vilgacx/aki I would fear if anyone wants to use this in production BUT I would love someone to get inspired and use the concepts rather than the actual code. Hmm maybe i should write a disclaimer... reply noduerme 15 hours agorootparentIt's funny, I stumbled on a similar use for iframes a few years ago that I did put into production. I needed to have a rather large SPA for employees hosted locally - that is, on a server on a local network in retail stores, not accessible from the web or unless you're on the same network as the server. The employee had to be able to load it on a tablet (but wouldn't necessarily know the server's local, dynamically allocated IP address without checking). And it had to be deployable without any complicated local DNS setups, router changes, etc. I solved it by writing a discovery service... a wrapper that's accessible on a public (non-SSL) web page that basically opens a pile of hidden iframes and uses them to war dial local IP addresses until it finds the app and replaces the page's content with the winning frame's. Amazingly this janky thing has held up ;) Anyway, nice work, and a cool idea! reply o11c 12 hours agorootparentPretty sure recent browser security features will break that. reply aeontech 15 hours agorootparentprevThat's a truly brilliant/horrible way to solve it. Love it! reply IggleSniggle 15 hours agorootparentprevI don't see why you wouldn't want someone to use this in production. This is pretty close to the way it should have been all along. reply bachmeier 17 hours agorootparentprev> I would fear if anyone wants to use this in production Why? How would it be different from using htmx? reply yawaramin 3 hours agorootparentFor one thing, it seems like it wouldn't gracefully degrade in functionality if JavaScript was disabled or unavailable. With htmx you can use its HX-Request header to check whether a request is from htmx or not, and serve a partial HTML or a full page accordingly. So no matter the circumstances, you can maintain a good user experience. reply Kalabasa 12 hours agorootparentprevIt's just a weird feeling for someone to use a hack / experiment as a foundation or something. I know 'the software is provided \"as is \", without warranty'. The difference with htmx is that they are more polished. reply Exoristos 9 hours agorootparentprevI would caution against using HTMX in production, either, from personal experience. reply recursivedoubts 9 hours agorootparentI've been looking for someone to write a negative htmx experience analysis to host on the essays page: https://htmx.org/essays i don't want people to pick htmx if it's going to be a bad choice for their particular application, would be happy to work w/ you to get something put together reply NelsonMinar 17 hours agorootparentprevVery nicely done. Your intent was quite clear reading the site and code. reply briantakita 18 hours agorootparentprevLove it! I think this idea has some legs in that a programmer can build their own f****k. Bundling only the pieces that they actually use. I don't see why it should not be used in a production environment...other than someone in the internet disapproves...a fear many of us suffer from. It's a simple idea & can be easily managed in a codebase. In he spirit of breaking apart HTMX piece by piece, I created hyop (Hypermedia/Hydration Operation). It weighs in at 61 B but uses the \"hyop\" attribute. I know, I know, I'm bad...but at least I save bytes. https://github.com/hyopjs/hyop I'm going to use some of your concepts, with credit of course...like the snippetware idea among others. reply zestyping 14 hours agorootparentWhat does single__hyop actually do? I read the whole README and couldn't find a definition of it. Is it essentially the same as onload=\"...\" ? reply briantakita 9 hours agorootparentI released it a few days ago & I appreciate the feedback. A single__hyop is a strategy to run a hyop mapped to a key...opposed to a multi__hyop which runs multiple hyops on keys delimited by whitespace. Other strategies are possible, like implementing an HTMX-like api with tree-shaking...but I havn't yet run into a use case for such sophisticated behavior. TBH, I only use single__hyop but wanted to leave the api space open for more sophistication. I mainly use this pattern instead of the overly-bloated hydration feature that come with the large UI frameworks. It is like onload for all elements, it works with tree-shaking, & the hyops can keep local scope (do not need to be assigned to globalThis/window). I didn't use the onload attribute because hyop has different semantics than onload. eval would need to be used & the functions would need to in global scope to preserve onload semantics. There is a DEBUG preprocessor which ensures that all of the hyops are loaded in the browser build & there are no unused hyops as well. It's only 61 bytes, so it's basically snippet-ware at this point. There may be more built on this foundation as I mentioned earlier. I wanted to express this pattern b/c it's been useful for me. reply bee_rider 14 hours agorootparentprevWhat is f****k? Fuuuck? Or maybe framework but HN has eaten some of your stars? reply zubairshaik 14 hours agorootparentIt's supposed to be framework, the same way it was censored in TFA reply detritus 18 hours agorootparentprevIt sounds like 'probably, yes' to adding a disclaimer, if only because I rather took it at face-value seeing it posted here on QBN so bookmarked for investigation later... . reply naasking 18 hours agoparentprevHTML native ajax is the right approach, and what the htmx devs fully support if I understand correctly, but I don't think this demonstrates htmx is too much trouble for what it offers. It offers considerably more than what's possible here, eg. DOM morphing, animations, graceful degradation, rich event model, server side events, etc. reply alexpetros 18 hours agoparentprev\"Just a few attributes and we could avoid the iframe\" could be the htmx/z motto reply koliber 17 hours agoprevIn 2001 or so, I was building an HTML based email client. We used a hidden iframe to execute JS to load data from the server and manipulate the DOM with the result. It was not quite as neat and elegant as this‚Äîthe browser support was not there. However, the basic mechanism was the same. It warms my heart to see the basic mechanism in such a compact package, without libraries upon libraries and machinations to make things work. This is probably perfect for 90% or so use cases where react or similar FE frameworks are used at the moment. We later used to joke that we used Ajax before Ajax was a thing. reply xutopia 7 hours agoparentOmg! Same story for me! I was working in a billing company and we used an iframe that we'd reload with JS that would run to change the DOM. Around the same time as well! reply LispSporks22 16 hours agoparentprevThis brings back memories. We did the same thing with a giant Fortran 77 app turned CGI app. It was my first job, fun times. reply smrtinsert 15 hours agoparentprevI think I posted it before, but I had SPA using Spring Webflow running in dom nodes (no iframes required) with animation event handling, csrf, all back around 2006 or so. The calling html was also pure save for a jQuery include at the top and my app include. The backend used JSP. No back button issues, no client or server state (I used a db instead of some options webflow gives you), it was a dream. Completely lost on the company I worked for at the time. I was up and running with a new user flow, in half a day or so. Static blocks suddenly would \"do stuff\" from the perspective of the business team in about half a day. This is the problem with technology. When it works well and really solves the problem, it is invisible. No one gets promoted for invisible. reply azornathogron 19 hours agoprevThis is a glorious demonstration of someone really understanding the platform. I don't expect I would ever use it, but I think it's excellent. reply simplify 16 hours agoparentUnderstanding or not, certain decisions like overriding the semantic meaning of a hash in a url doesn't seem to be working with the platform. A better version would be adding a target to a `data-` attribute. It's a fun project overall, though. reply yawaramin 3 hours agorootparentBut the hash doesn't have a semantic meaning when it is in an href or action attribute, because it is stripped out before the request is sent to the server. The hash only has a meaning when it's in the URL bar. That's kinda the point of this hack. reply sam2426679 15 hours agorootparentprevOP addresses this at ‚ÄúWe can do a hack that enables you to write the target ID selector in the target attribute itself!‚Äù reply simplify 15 hours agorootparentI'm only addressing the comment on \"a glorious demonstration\" of understanding the platform, where hacks like this go against such a take (IMO) reply neoberg 10 hours agorootparentI'd say hacking the platform in a way that's elegant like this would require a certain level of understanding of it, reply mirekrusin 12 hours agorootparentprevJust use target, if it starts with #, it's replacement, otherwise behaves as is. reply err4nt 11 hours agoprevFurther size reduction, you don't need the `this.` on the inline event listener, so it can be: `contentWindow.location.hash` and `contentDocument.body.childNodes` instead of `this.contentWindow.location.hash` or `this.contentDocument.body.childNodes`. This will shave another 10 bytes off the snippet :D reply ericlewis 5 hours agoparentWe can bring that down to 16 since query selector can handle undefined. 12% reduction. setTimeout(()=>document.querySelector(contentWindow.location.hash)?.replaceWith(...contentDocument.body.childNodes)) reply Kalabasa 3 hours agorootparentYeah, querySelector(undefined) works Although location.hash defaults to the empty string '' when there is no hash, which gives a SyntaxError, so we still need the fallback selector to select none. reply spankalee 17 hours agoprevI think there's a pretty strong argument at this point for this kind of replacing DOM with a response behavior being part of the platform. I think the first step would be an element that lets you load external content into the page declaratively. There's a spec issue open for this: https://github.com/whatwg/html/issues/2791 And my custom element implementation of the idea: https://www.npmjs.com/package/html-include-element Then HTML could support these elements being targets of links. reply pier25 14 hours agoprevThis seems snappy from the US but I doubt someone in say NZ will have a good experience. Going back and forth between the client and the server on every interaction can result in terrible UX. Users will be happy waiting 1-2 seconds after submitting a form but waiting that much to switch a tab is not gonna fly. Plus there's internet weather etc which might result in unpredictable latencies over long distances. Yes, you can move the compute layer of your app close to the user in multiple ways. Moving the data to the edge is much harder. reply jdthedisciple 1 hour agoparentThis is my biggest gripe with this approach as well as HTMX. Avoiding delays for what should be purely client-side actions is a non-negotiable to me. reply mhd 11 hours agoparentprevI don't think every solution in the webdev space is supposed to be a one size fits all panacea. Especially this, which seems more like an incentive to think about structuring webapps. Sometimes it's interesting to see how far you can go without going the \"let's download the web in a JS blob\" way. One of these days, I'm going to see whether I can still build something reasonably modern with Seaside, the continuation-based framework from the olden days, where basically the whole UI state is stored in the server-side session and everything is a request. Worked quite alright, way back when not everything had a local CDN and the internet tubes were a bit narrower. reply mariusseufzer 14 hours agoparentprevTexting from NZ: I don‚Äôt know how snappy your experience is but all I can say is that we‚Äòre used to slower websites here in New Zealand. Switching between the tabs and having aMoving the data to the edge is much harder. Which means its gonna be the same thing if you try to render on the client, since you still need a round trip for data... reply pier25 6 hours agorootparentWith some JS you might not need to do multiple roundtrips though. For example to open a tab or a dialog. reply flaburgan 14 hours agoparentprevBut would the response be cached in the browser after the first call? reply withinboredom 13 hours agoparentprevTotally disagree as a regular user of 2g internet on a phone. If you are downloading a 1mb bundle to show me your home page, I'm going to give up within 30s or go elsewhere. I'm more than happy to put up with laggy feedback. That being said, usually people writing downloads using fetch will implement a promise-based timeout on top of the network timeout, which causes all kinds of issues on 2g internet. One of my favorite side-effects are implementations that \"retry after 2s\" that don't actually cancel the in-flight request, causing my entire bandwidth to be hogged up with dozens of these requests \"retrying\" when the initial request eventually returned successfully. Javascript developers are unintentionally evil to non-5g internet. reply pier25 6 hours agorootparent1MB bundle of JS is definitely terrible. I'm not advocating that by any means :) reply withinboredom 2 hours agorootparentIt happens more than anyone wants to admit, but even at 2g speeds, 256kb takes almost 20 seconds to download (I get about 120-180 kbps). When sites provide the HTML and CSS, I can usually stop the page from loading and at least read the content within a few minutes. When there's a js bundle, there is usually a fairly large initial bundle, then that loads a couple of other bundles ... and I'm still looking at a white screen, and not even a loading animation (from the browser or the developer) because as far as the browser is concerned, it has loaded the HTML and most developers don't even realize they can edit the base-line HTML sent to browsers because they never even see it. reply cmgriffing 13 hours agoprevI think the demo section needs work. Clicking a \"tab\" to change the example code to Greeting, or anything else adds a history event but doesn't update the url. I probably would have done the exact opposite in both aspects. Use replace to prevent extra navigation entries, but still update the url for bookmarking etc. For something that claims to \"just be html\", it seems to be breaking some fundamental rules of the web/UX. Whether it's a simple mistake or not and easy to fix, it does not inspire confidence in the framework. reply phinnaeus 11 hours agoparentSo that's why my back button didn't work the first time when I tried to return to HN... reply hwillis 11 hours agoparentprev...You realize this \"framework\" is 181 characters long, right?elements add to the history, it's what they're supposed to do. If you don't want them to do that, don't useelements for UI. Use a button and change the iframe src with javascript. That's not really the library's problem, and it's not really hypertext's problem what the browser is putting into history. > For something that claims to \"just be html\", it seems to be breaking some fundamental rules of the web/UX. Yes, obviously. HTML+CSS are meant to put boxes and words and images on a screen. If you want to show a spreadsheet with cell formulas, use javascript. Fundamental rules of web/UX are not the same as fundamental rules of hypermedia. > Whether it's a simple mistake or not and easy to fix, it does not inspire confidence in the framework. There's a section on the page titled \"Is this a joke?\" (not to mention, \"Is htmz a library or a framework?\"- its a snippet). Confidence is not the point. The point is to demonstrate how little it takes to turn HTML into a stateful UI. As the author says: > If you need something more interactive than the request-response model, you may try the htmz companion scripting language: javazcript. Sorry, I meant JavaScript, a scripting language designed to make HTML interactive. > htmz does not preclude you writing JS or using UI libraries to enhance interaction. You could, say, enhance a single form control with vanillaJS, but the form values could still be submitted as a regular HTTP form with htmz. reply spankalee 18 hours agoprevReusing theelement like this is a bad idea - it has very specific behavior in browsers. In a shadow root it'll be replaced by the children of the host element, no matter what the library does. HTML already has an inertelement for things like this. reply KatrinaKitten 18 hours agoparentSlight correction -is not inert for users who use screen readers, though in this case it shouldn't cause issues. If you need an actual inert element, use a div. reply Kalabasa 18 hours agoparentprevThanks for the suggestion, I'll update the examples. Edit: that is, after I wake up... reply Kalabasa 12 hours agorootparentWoke up... It's at the top of HN... Anyway I chose div instead thanks to the other commenter https://github.com/Kalabasa/htmz/commit/153a5a448b60a0898604... reply jallasprit 16 hours agoprevIt's quite good. I noticed that my back button had to be repeatedly pressed to go back to write this comment, after interacting with the examples a few times. I'm sure that's simple enough to fix. reply Arch-TK 19 hours agoprevGiven that this uses `target`, doesn't it mean that unlike htmx you can't easily make this gracefully degrade when JS isn't enabled? And, yes, I know, saying \"when JS isn't enabled\" in 2024 is a bit like saying \"when the user is on Mars and has a 10 minute RTT\" but forgive me for being an idealist. reply Kalabasa 19 hours agoparentYeah it breaks without JS. You could add the iframe behind JS, so the target would default to a new tab. But the server would still be designed to return HTML fragments. I never found a way for a server to check if the originating request is for an iframe or a new tab. It's not quite a graceful degradation. reply simpaticoder 18 hours agorootparent> I never found a way for a server to check if the originating request is for an iframe or a new tab. There is no such technique. One way to distinguish is to pick a URL convention and modify the URL (before the hash) of the iframe URL. For example, add ?iframe=true to the URL, and then have the server check for that. Perhaps more usefully you could include information about the parent URL, e.g. url += '?parent=${document.referrer}'. Or something. reply sesm 13 hours agorootparentCan we add a cookie instead of modifying URLs? reply simpaticoder 12 hours agorootparentNo. The same cookies are added to both the host and guest pages. reply naasking 19 hours agorootparentprevYou could intercept the clicks with JS and add a special header, like htmx does, to return fragments, otherwise fall back to full documents. Edit: rather than a header, dynamically adding a query parameter to the URL of the element that was clicked would probably fit better with htmz's approach. reply beretguy 19 hours agoparentprevI use Disable JavaScript extension with js disabled by default and only enable it if website is broken. https://addons.mozilla.org/en-US/firefox/addon/disable-javas... reply efilife 19 hours agorootparentYou should use uMatrix so you can only enable the scripts necessary to unbreak the site reply samtheprogram 15 hours agorootparentI use NoScript for this as well, I‚Äôll have to check out uMatrix. reply SushiHippie 15 hours agorootparentprevOr uBlock origin (from the same author) which is still maintained reply pmontra 15 hours agorootparentIt is maintained but the UI for dealing with JS is horribly time consuming and overly complex compared to uMatrix. I'll never really understood it and I keep using uMatrix on my laptop. I switched to NoScript on my phone. Maybe I can install uMatrix now if Mozilla really unblocked many extensions. If uMatrix stops working, I'll switch to NoScript everywhere for JS and uBO for all the other issues. reply samtheprogram 15 hours agorootparentThis is what I do and it works great - uBlock Origin defaults, and NoScript. It‚Äôs surprising how well the internet works without JavaScript. reply SushiHippie 13 hours agorootparentprevI've never used uMatrix what can uMatrix do what uBlock can't? I have set all sites to no js, and if something does not work I click the JS toggle in the menu, reload the page and thats it. And yes uMatrix should work on Firefox on Android. ( At least it is installable) reply pmontra 12 hours agorootparentuMatrix has a spreadsheet like matrix of features and sites. It's immediately clear what happens when I click a cell, a column or a row. With uBO I don't even understand where to click to toggle JS for one of the sites a page got scripts from. I learned it years ago but it was too cumbersome. I kept using uMatrix and I forgot it. I attempted to use it again now and I couldn't find anything. I didn't google for it. I installed uMatrix on my phone and disabled NoScript. Of course I keep using uBO for filtering out ads and hiding annoying parts of sites with the element picker. reply dormento 13 hours agoparentprevI had a dream yesterday, that scientists managed to create a new kind of EMP bomb. This bomb was unusual in that, by varying the level of EM discharge in the payload (dreamy-sciency explanation), it could target all hardware created after a certain point in time. I had access to their facility (dreams being as dreams often are), and managed to set it for 1992, and right when I was about to press the button, I woke up. It RUINED my day. reply MatthiasPortzel 18 hours agoparentprev> being an idealist Could you describe your ideals for why websites should gracefully degrade without JS enabled? It‚Äôs not an unpopular view on HN, but from my perspective as a web developer, JS is a part of browser application just like HTML, and there‚Äôs no reason for the website to work if you‚Äôve disabled a part of the browser. I suspect ‚Äúdoesn‚Äôt have JavaScript‚Äù is being used as a proxy for a lot of other ideals that I do understand, like ‚Äúshould work on as many devices as possible‚Äù but that‚Äôs a rough correlation and doesn‚Äôt make the use of JS inherently bad. reply Arch-TK 17 hours agorootparentSo I've been in numerous situations where having JavaScript enabled was either undesirable or impossible, granted, it's my own fault for using strange platforms like a Nokia N900 or whatever, with strange web browsers. But it's still nice when interactive websites continue to work even in contexts where JavaScript isn't a thing. I always thought of JavaScript as something you use to \"upgrade\" the web experience, not to build it. Although obviously there are some things you simply can't do without JavaScript and for which there exists literally no alternative. There's also situations where JavaScript is a liability. See, for example, the Tor browser. Especially my ideal is that all functionality which can work without JavaScript should work without JavaScript. So, for example, I am not expecting someone to implement drag-and-drop calendars without JS, but there's no reason why the editing function of a calendar item should fundamentally require JS. That being said, I know this is an idealist position, most companies which work on developing web-applications simply don't care about these niche use-cases where JS isn't an option and as such won't design their web-applications to accommodate those use-cases to save on development costs and time. But, while I am not really a web-developer, whenever I do deal with the web, I usually take a plain-HTML/CSS first approach and add JavaScript later. reply tambourine_man 15 hours agorootparentprevBecause there's a case for a very useful Web without running a Turing-complete language on the visitor's end. If you just want to consume text, images, audios and videos, follow links and fill in forms (and that's quite a lot and pretty awesome already), you shouldn't need JavaScript. reply J_Shelby_J 17 hours agorootparentprevChances are I‚Äôm on your website for information, mostly text content. Which really doesn‚Äôt require JavaScript. So then, most JavaScript on the web is enabling revenue generation rather than improving the user experience. So yeah, disabling JS is a proxy for, ‚Äúdon‚Äôt waste my time.‚Äù But I agree that it‚Äôs not inherently bad, but just mostly bad (for the user.) reply SigmundurM 17 hours agorootparentprevA reason people might want to have JavaScript disabled, is because of the immense tracking possibilities that JavaScript has, which can't easily be safe-guarded against. The people who do disable JavaScript completely are admittedly few and far between, but are, I would assume, more common among the Hacker News crowd. reply smaudet 14 hours agorootparentprevSupporting as many devices as possible, breaking RESTful APIs, etc. A JS engine pre-supposes many, many things (too many) about a client, stuff like implicit assumptions that \"this device has enough power and bandwidth to process myjavascript, perform appropriate security checks on it, handle this fast enough to service other requests in a timely manner, and also not take over the user's entire computer\". Accessibility means you should presume the least number of things possible. There's no sound, no visuals, no powerful cpu (maybe there isn't even a cpu), the device is the only working device from 20 years ago performing some critical function (e.g. government poverty assistance), there's only a couple minutes of power left, the internet is not available currently or quickly, etc. You should never assume you have JS, period, and features should gracefully degrade in the presence of JS engines. reply jamespo 11 hours agorootparentprevPart of this is backend devs and admins like telling frontend devs what to do reply superkuh 14 hours agorootparentprevIf you would've told anyone in the year 2000 that it'd become standard practice to blindly execute programs sent to you by anonymous people you don't know you'd get a lecture on why that's stupid. But in 2024 it's standard accepted practice. And that standard has made it so browser developers have to literally prevent the user themselves from having control over their browser because it's too dangerous to do otherwise. The problem with the entire commercial web application ethos, despite it being a perfect fit for for-profit situations, is that it forces the rest of the web stack to gimp itself and centralize itself, CA TLS only, etc, just to keep the auto-code executing people secure. The one horribly insecure user behavior (auto executing random code) takes over from all other use cases and pushes them out. So, we end up with very impressive browsers that are basically OSes but no longer functions as browsers. And that's a problem. Design your own sites so that they progressively enhance when JS is available. When work requires you to make bad JS sites, do so, but only in exchange for money. reply smaudet 13 hours agorootparentAgreed. And treat all JS engine-requiring sites like viruses that need to be executed in protected VMs. reply jerbear4328 11 hours agorootparentOf course! Actually, for any website that runs JS, in my opinion, we should just automatically forward the JS execution into a virtual machine, it is horrible to allow any random website to just run code directly on our machine. What if we built this virtual sandbox directly into the browser, that way no code could run on our machine, but all websites still work fine? That's revolutionary! Hang on... reply smaudet 7 hours agorootparentIf you could actually control the virtualization in the browser, that would be good. Unfortunately you are just left trusting that it is even virtualized (properly). reply nashashmi 15 hours agorootparentprevI browse the web without JS. It is a fast easy way to load websites. And some sites with heavy app interaction features need JS. And that is fine. It is the other sites that use JS to figure out if their users have read more than 2 articles that are the problem. Degrade gracefully is a required development skill. Sites need to allow for their pages to work in limited fashion without JS. JS should only be a layer added for interactivity, animation, and app construction. Otherwise, workarounds are great. Is there a way to make this gracefully work? YES!! Instead of using hash tag names, use '?id=example'. And let the script in frame figure out the real destination of the output. Otherwise, the page will load the full site. Also use script to add \"target\" attribute to links. reply Kalabasa 12 hours agorootparent`?id=thing` is a great idea! reply pmontra 15 hours agoparentprevIt breaks without JS but many JS blocker extensions can be configured to always allow JS from the host serving the page. For example NoScript on my phone has the \"Temporarily set top-level sites to TRUSTED\" option. With only 181 bytes it could even be included in the page. It's much less than the sum of the meta tags on many sites. reply kevincox 13 hours agoparentprevNot even just without JS. If you middle-click to open a link in a new tab you get just get content that was expected to be swapped in. I think that abusing links is a far bigger sin than adding a custom attribute. reply yawaramin 3 hours agorootparentThis is why htmx sends requests by default with an HX-Request header so that the server can distinguish between them and serve different content if need be. reply jdp 19 hours agoprevReminds me of pjax [1], except pjax works over XHR instead of an iframe and uses pushState by default to keep the back button working. [1]: https://github.com/defunkt/jquery-pjax reply darylteo 8 hours agoparentNon jQuery version (and relatively newer ) https://github.com/MoOx/pjax reply captn3m0 11 hours agoparentprevGitHub itself used pjax heavily and I liked those interactions far more than the newer React ones, the HTML was much more semantic for one, with middle click always being respected. reply synergy20 17 hours agoprevI happened to spend a little more time on htmx this weekend which htmz was inspired by. htmx/htmz does do well for simple use cases, htmx does well for SSR heavy cases(e.g. django). in the end I returned to vue.js, with a few lines code(put in a library) I can mimic htmx easily plus all the extra stuff vue.js brings, and no I do not need use a build tool for that, vuejs works fine via CDN inside a html for simple use cases the way htmx does, but vuejs can do more, e.g. draw live chart from data returned via ajax calls where vuejs is json by default, htmx is html by default instead. reply angra_mainyu 17 hours agoparentThis. I think Vue.js \"scales down\" excellently. You can just load it via the cdn and write some widgets and go about your day. In the past I've tried to make use of \"microframeworks\" like alpine.js and such but often found myself returning to Vue.js. reply synergy20 15 hours agorootparentsame here, spent lots of time poking around, even tried svelte and react.js(heavily), and now firmly back to vue.js. vue.js does not mix SSR with SPA into one, make it much simpler compare to what React.js is doing today, and it provides way more than alpine.js and htmx etc, it's the best one in practice for me now. reply buremba 10 hours agorootparentI also started with Vue and switched to React for work but still miss the simplicity of Vue. Sadly, it didn't take off as React. reply synergy20 9 hours agorootparentReact has Meta behind it, Vue.js is a non profit project by individuals, very different, good news is that vue.js is good enough for serious projects nowadays and I especially like the fact that it does not mix SSR into SPA to make things complicated. reply sublinear 18 hours agoprevThis seems likely to have issues with most \"Content-Security-Policy\" rules because of the inline script in \"onload\" and the iframe. Makes it a non starter in real world production environments. reply morbicer 13 hours agoparentCan be imho solved with nonce or hash based policy reply tommica 14 hours agoparentprevBut isnt it the same exact domain? reply internet2000 19 hours agoprevApparently it breaks your back button too. reply rnmmrnm 19 hours agoparentIf you're talking about the tabs demo, I think it's reasonable for some use cases to respect back button as previous tab. as long as it's easy to opt out. reply troupo 19 hours agorootparentThe inline editing demo also has a separate URL for the \"we're editing\" state reply Kalabasa 18 hours agorootparentYeah it's terrible in some use cases. If only HTML provided a way to navigatewithout adding a history entry, like . If this was a real product i would market it as \"Native time travel debugging! Go through your application state as you would go through your browser history!\" reply ComplexSystems 14 hours agorootparentCould you perhaps modify the iframe to clear the new URL from history using JavaScript? You could also make it optional; perhaps there'd be syntax like ... or something. Probably would be better to make the no history default though. reply devmor 18 hours agorootparentprevThat would be terrible UX. The real answer is that web developers should stop using anchor tags to target non-stateful links and should properly handle navigation to pages that are short lived. But that battle is a bit like asking people to use their turn signals when merging. reply socketcluster 11 hours agoprevInteresting. I wrote a similar plain HTML/WebComponent-based front end for my new no-code/low-code serverless platform https://saasufy.com/ It lets you build just about any data-driven application using only a handful of declarative generic HTML components: https://github.com/Saasufy/saasufy-components?tab=readme-ov-... I built a chat app with both group chat and private chat (with access control) using only plain HTML; only ~250 lines of HTML markup for the entire thing, no front end or back end code was used: https://github.com/Saasufy/chat-app/blob/main/index.html You can use it here - All hosted on GitHub pages: https://saasufy.github.io/chat-app/ It would be great to add support for other front end tech like this. I kind of like HTMX (especially as it's declarative). This HTMZ looks interesting. I'd like something with a big community and more components to handle more advanced cases (e.g. higher level components like calendars). reply torgoguys 11 hours agoparent>I built a chat app with both group chat and private chat (with access control) using only plain HTML; only ~250 lines of HTML markup for the entire thing: https://github.com/Saasufy/chat-app/blob/main/index.html I think I know what you mean, but we clearly have different definitions of \"plain HTML.\" Mine wouldn't involve 9 javascript files. :-D Either way, I'm interested and will check out what you made! reply socketcluster 8 hours agorootparentAh yes good point. By plain HTML, I meant apps which can be assembled without custom JavaScript logic needed. reply account42 1 hour agoprevZero noscript fallback. Please don't use this. reply intrasight 14 hours agoprevVery cool snippet. This is what I now wished existed. A flowchart/wizard that let you choose a development framework based on some questions and answers. So that a minimum framework (HTMZ) is used if it can satisfy. Or HTMX if one of your answers indicates that it's needed. Or Vue, etc. - getting \"heavier\" platforms as needed. Of course we don't always know ahead of time the answers to the question. But being given the questions and the flowchart would be beneficial for the up front analysis. reply Xeoncross 7 hours agoprevWhere did those basic ( This looks neat! I've never really been in to web development, but I'm curious... is it possible to create a standalone .html file for a browser-delivered app? Like, not just PWA or SPA, but... a single HTML App? Yes, you can include your JavaScript and CSS directly inside the [0] and[1] tags, so you don't need to include any other file. Images like PNG, JPEG, ... can be either embedded with a base64 data URL [2] or an SVG with the SVG tag [3]. > what's the barrier to something interesting, say... implementing a minimal spreadsheet or maybe just a sortable/filterable table? Well, you could go the easy route and use an already existing JavaScript library for this. Libraries are normally included via a URL, but you can just copy the contents of this library into the script tag as mentioned above. Otherwise, I think it's manageable to do develop it yourself (sortable/filterable tables) much knowledge, but frontend development can be a PITA very fast [0] https://developer.mozilla.org/en-US/docs/Web/HTML/Element/sc... [1] https://developer.mozilla.org/en-US/docs/Web/HTML/Element/st... [2] https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_... [3] https://developer.mozilla.org/en-US/docs/Web/SVG/Element/svg reply eproxus 15 hours agorootparentThat‚Äôs how TiddlyWiki works for example: https://classic.tiddlywiki.com/ reply h43z 13 hours agoprevWhy the \"||:not(*)\" in the iframe onload handler of htmz and not just \"null\"? reply naasking 10 hours agoprevVery neat use of existing HTML. One thing this highlighted for me is that the MDN docs on the target attribute are incomplete, because I was recently reading them and while they mention that you can target iframes, they didn't actually describe how to do so. I was reading the docs because I had some thoughts on my own htmx-like take. You've used some of the same attributes I was going to use but slightly differently than how I was going to approach it. Some good food for thought! reply h43z 13 hours agoprevHere a version that works for cross origin requests. document.querySelector(x.data.target).innerHTML=x.data.content\"> but website you are requesting has to embed the below script tag. just some other text parent.postMessage({ target:location.hash, content:b.outerHTML },'*') You would still set the DOM destination on the main page. dogreply chatmasta 13 hours agoparentCareful with that snippet, targetOrigin of * is dangerous. I could embed your iframed content on my own site and then you'd happily send me the entire HTML inside the iframe. reply sodimel 19 hours agoprevSimple and powerful, as the vanilla web should be. Thank you for this (small) gem :) reply buremba 11 hours agoprevI wonder how feasible it is to create htmz / htmx -like lightweight library with the support for React/Vue/Stelve using web components. I agree that 90% of the use-cases you don‚Äôt need React but for the last 10%, most of us are stuck with these bloated frameworks. Astro has a similar idea but it‚Äôs working as a full framework instead of being a library. Considering the limitations with Astro, I guess the biggest bottleneck is state management. reply lelandfe 14 hours agoprevnext [‚Äì]` logo in code if you order the target attribute last :D reply yellowapple 17 hours agoprevOne downside of this approach is that it fills my browser history with a bunch of extra entries, which ain't ideal (especially for my Back button). I'd guess that's probably fixable as an htmz \"extension\"? That aside, I love the concept. reply p4bl0 19 hours agoprevThis is a neat hack, I like it :). Thanks for sharing. reply flanbiscuit 17 hours agoprevWas curious to see the code so found the GitHub. Posting it here in case anyone is interested since the author doesn't link to it on the site and also the npm page doesnt link to it either https://github.com/Kalabasa/htmz reply atahanacar 17 hours agoparentIt is right at the end of the first sentence. reply Uptrenda 1 hour agoprevHTML is already lightweight. It takes a very low IQ to make it seem complex. reply franky47 17 hours agoprevIf anyone is wondering, `htmy` is available on NPM, and `htmx` there is not what you think. reply atum47 17 hours agoprevI'll take a deeper look in the code later, but it seems useful. If been using the window location hash to do some simple navigation on my SPA, but i use JS. (Just hide all sections and shows the one that matches the hash i.e.: #main reply Kalabasa 12 hours agoparent> hide all sections and shows the one that matches the hash Oh there is a good hack you can do here! I've been meaning to write a blog post about this exact thing! See CSS `:target` selector section { display: none; } section:target { display: block; } reply tambourine_man 9 hours agorootparentYou‚Äôre full of great hacks. That one I may actually use. Looking forward to that article. reply d0utone 15 hours agoprevRefreshed 10 times to see the neat animation on top again! reply hiccuphippo 14 hours agoprevI think you can use null instead of ':not(*)' in the query selector, if only to make it a bit shorter and maybe a bit faster. reply Kalabasa 12 hours agoparentNice, it works! I was a bit scared here because the querySelector() MDN docs say > A string containing one or more selectors to match. This string must be a valid CSS selector string; if it isn't, a SyntaxError exception is thrown. I'll test it out. If this works good, we shave off 5 bytes towards a 176-byte snippet! reply qwertygnu 17 hours agoprevha, I'm working on a dev-side version similar to this (mostly just for me, but hopefully publishable). I opted for an entirely pre-deplopyment build tool, where you just putin your HTML, run the build and it outputs the filled in file somewhere else. I know its functionality is very similar to many web frameworks (e.g. React, handlebars) but it does one thing. reply tommica 13 hours agoprevCan it trigger JS returned from the proxy? Something like alert(123); reply agp2572 13 hours agoprevThis is very similar to how Hotwire does replacing of HTML generated on server side in Ruby on Rails. reply Alifatisk 19 hours agoprevKind of impressive that the installation script was that small. reply turnsout 17 hours agoparentThat‚Äôs not the installation script, it‚Äôs the entire ‚Äúframework!‚Äù reply fractaledmind 17 hours agoprevI cannot believe how simple and elegant this is. Using an iframe as a request/response proxy to enable targeted replacement is just reply naasking 11 hours agoparentYou accidentally the reply moritzwarhier 18 hours agoprevIt's a fun one liner, but what is the use case? When I want to replace some element using JS as the user clicks a link, it is progressive enhancement. Usually links enable history navigation. If you do stuff like this, you need to code it in a way that uses the history API to fix it for users with JS enabled (majority of users). If you don't want history navigation and URLs, why do you use links? This breaks history navigation and bfcache for no good reason, or am I missing something? bfcache already provides SPA-like speed, or even better. No need to avoid regular links if you e.g. link from a list to a detail page. Also: > No preventDefaults. No hidden layers. Real DOM, real interactions. No VDOM, no click listeners. No AJAX, no fetch. No reinventing browsers. So many words saying nothing, just to cater to some sentiment. fetch is part of browsers by the way. If I need to replace an element as the user clicks a link, I can code it myself (without using this abstraction layer, however thin it is). I also don't need an iframe for doing this. And preventDefault is aptly named and a good reminder for what progressive enhancement should do. If it's not meant to be a link, don't use a link. And if you want to react to clicks, you know, use click listeners (event handlers). Where's the problem? It is understandable to developers and uses native browser functionality as intended. As opposed to this hack, which I'd find pretty glaring, bug-prone and hard to understand, would I have to debug an issue on some page that uses this snippet. To me this seems like useless indirection and technical debt. If you really need low-code AJAX \"links\" (who says you need that, if you don't want an SPA?), code yourself some understandable JS that is properly tailored to your site and respects history navigation, or use a library that is a good fit for your concrete use case. As a joke, I like it though‚Ä¶ reply sesm 15 hours agoparentThe use case is to show that you can do core htmx functionality in a one-liner. reply scwoodal 17 hours agoparentprevThe use case is a fun one liner. reply moritzwarhier 14 hours agorootparentAs a fun hack / code golf it's great. I was being tonedeaf I guess, woosh as they say. The marketing pitch on the landing page is written so well that I took it too seriously probably. I know htmx but wasn't able to see this as a parody, so my fun capabilities were failing :) reply Kalabasa 11 hours agorootparentI have changed it to explicitly state within the first paragraphs that \"htmz is an experiment\" now. I started this as a joke but turned into a fun working solution - I myself am not sure if this is just a joke or a thing. Maybe I'll use it in some smaller projects, or maybe not! reply moritzwarhier 10 hours agorootparentAll power to you! No need to decide if it is a joke or not! ;) In fact I clicked on it just because the name was so hilarious and already indicated pretty well what might be to expect (I didn't expect your particular solution though, kudos) reply oneshtein 15 hours agoparentprevIt can be used in Markdown with HTML enabled but JavaScript disabled. reply moritzwarhier 14 hours agorootparentHaha I hope not ^^ reply germandiago 16 hours agoprevHow it compares to htmx? reply mixmastamyk 11 hours agoparentIt's a subset. reply lyxsus 16 hours agoprevThis is incredible. I really want it to take off. reply woah 16 hours agoprevWaiting for htmz on rails, the 100k loc batteries included htmz framework to implement full featured UX patterns and make development delightful reply sesm 15 hours agoparentWe need to keep the naming convention, so: - htmz on handcar (still on rails but you have more manual control) - htmz on horseback (if you want to reject modernity) reply fuzztester 8 hours agoprevhtmz is pronounced hit and miss. reply begueradj 13 hours agoprevwow ! Useful interactions in plain HTML ! reply kickofline 19 hours agoprevAt what point does it cease to be a framework and become just a regular line of code? reply p4bl0 19 hours agoparentFrom the FAQ: > * Is htmz a library or a framework? > htmz is a snippet. reply vcg3rd 19 hours agorootparentFrom the README: > htmz is a minimalist HTML microframework So, some ambiguity. reply Kalabasa 19 hours agorootparentAuthor here. to be honest I have no idea reply joshfarrant 19 hours agorootparentFinally, a Hacker News comment I can relate to. reply alexpetros 18 hours agorootparentprevHah that's amazing reply gardenhedge 11 hours agoprevBetter than HTMX but it's a JS framework/tool at the end of the day reply d0utone 15 hours agoprevRefreshed 10 times to see the neat animation on top! reply andrewmcwatters 17 hours agoprevExcept using a URL fragment refers to a ‚Äúresource that is subordinate to another, primary resource‚Äù not a destination. They point out the URL abuse, so why do it? Slot is also a part of the custom elements standard, but they say no custom elements. Why use only web standards and then use them incorrectly? reply chuckadams 16 hours agoparent> Why use only web standards and then use them incorrectly? Because it's a clever hack, not a standards proposal. Lighten up, Francis. reply hanniabu 18 hours agoprev> Not even a backend is required. > In a nutshell, htmz lets you swap page fragments with HTML from the server using vanilla HTML code. So a backend is needed.... reply Kalabasa 18 hours agoparentOops, it's just a mistake on word choice in the nutshell summary. This works on static files like on a local filesystem (in that case the \"server\" is the local filesystem that serves me files that happen to be html) Edit: on second thought, direct filesystem access has different origins which would mess with iframes. I'm not on the computer now to test. But at the very least you need a basic web server that serves files. reply hanniabu 17 hours agorootparentThanks for the clarification, so it would work the same as a static site where you just need a place to retrieve the files from reply turnsout 17 hours agorootparentYes, aka a webserver reply pwdisswordfishc 2 hours agoprevOh great, yet another way to break \"Copy link target\". reply lelanthran 16 hours agoprevJesus Christ, this is all at once simple, powerful and useful. So little code, achieving so much!!! reply aitternh 15 hours agoprevfdnl.kakjd reply ulrischa 17 hours agoprevMe so: What? I looked at the code and it is really hacky. But so great reply overstay8930 18 hours agoprevhtmx in shambles /s reply recursivedoubts 17 hours agoparentalways has been reply sunshinerag 16 hours agoprevHi, No babel plugin? Npm module‚Ä¶ sheesh /s reply chuckadams 16 hours agoprev [‚Äì] It's not my style, and I'll stick with Vue SPAs thanks, but still... damn this is elegant. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "htmz is a minimalist HTML microframework empowering users to build modular web interfaces with plain HTML, free from backend dependencies.",
      "Inspired by htmx and other web architectures, htmz enables seamless swapping of page parts using basic HTML, updating only specific sections on link clicks.",
      "It is not a conventional JavaScript framework but a compact code snippet harnessing the browser's innate features to retrieve and interpret HTML, offering extensibility for enhanced functionality."
    ],
    "commentSummary": [
      "The discussion delves into various web development tools like htmz, htmx, Vue.js, and hacks such as htmy, highlighting their benefits, limitations, and potential applications.",
      "Users debate on balancing functionality and configuration, the use of iframes, HTML standards, JavaScript dependency, and the importance of websites gracefully degrading without JavaScript for accessibility.",
      "Some users admire the simplicity and effectiveness of the tools, while others raise questions about their practicality, compliance with web standards, security, accessibility, and the effects of large JavaScript bundles on user experience."
    ],
    "points": 848,
    "commentCount": 206,
    "retryCount": 0,
    "time": 1708348338
  },
  {
    "id": 39428880,
    "title": "Groq Demonstrates High Performance with Mixtral 8x7B-32k",
    "originLink": "https://groq.com/",
    "originBody": "Groq .container { width: 100vw; height: 100vh; display: flex; justify-content: center; align-items: center; background-color: #ea4c31; }// The value below is injected by flutter build, do not touch. const serviceWorkerVersion = \"2784836587\";/*0) { return false; }; } return true; } function parseJwt(token) { // terminate operation if token is invalid if (!token) { return; } // Split the token and taken the second const base64Url = token.split(\".\")[1]; // Replace \"-\" with \"+\"; \"_\" with \"/\" const base64 = base64Url.replace(\"-\", \"+\").replace(\"_\", \"/\"); return JSON.parse(window.atob(base64)); } /* ]]> */window.addEventListener(\"load\", function (ev) { var loading = document.querySelector(\"#loading_indicator\"); // loading.textContent = \"Loading entrypoint...\"; // Download main.dart.js _flutter.loader.loadEntrypoint({ serviceWorker: { serviceWorkerVersion: serviceWorkerVersion, }, onEntrypointLoaded: async function (engineInitializer) { // loading.textContent = \"Initializing engine...\"; let appRunner = await engineInitializer.initializeEngine(); loading.textContent = \"\"; await appRunner.runApp(); }, }); });",
    "commentLink": "https://news.ycombinator.com/item?id=39428880",
    "commentBody": "Groq runs Mixtral 8x7B-32k with 500 T/s (groq.com)704 points by tin7in 21 hours agohidepastfavorite423 comments eigenvalue 16 hours agoI just want to say that this is one of the most impressive tech demos I‚Äôve ever seen in my life, and I love that it‚Äôs truly an open demo that anyone can try without even signing up for an account or anything like that. It‚Äôs surreal to see the thing spitting out tokens at such a crazy rate when you‚Äôre used to watching them generate at one less than one fifth that speed. I‚Äôm surprised you guys haven‚Äôt been swallowed up by Microsoft, Apple, or Google already for a huge premium. reply tome 15 hours agoparentReally glad you like it! We've been working hard on it. reply jonplackett 10 hours agorootparentIs this useful for training as well as running a model. Or is this approach specifically for running an already-trained model faster? reply tome 10 hours agorootparentCurrently graphics processors work well for training. Language processors (LPUs) excel at inference. reply frozenport 10 hours agorootparentprevIn principle, training is basically the same as running inference but iteratively, in practice training would use a different software stack. reply robrenaud 7 hours agorootparentTraining requires a lot more memory to keep gradients + gradient stats for the optimizer, and needs higher precision weights for the optimization. It's also much more parallelizable. But inference is kind of a subroutine of training. reply lokimedes 15 hours agorootparentprevThe speed part or the being swallowed part? reply tome 15 hours agorootparentThe speed part. We're not interested in being swallowed. The aim is to be bigger than Nvidia in three years :) reply jonplackett 10 hours agorootparentIs Sam going to give you some of his $7T to help with that? reply dazzaji 15 hours agorootparentprevGo for it! reply nurettin 12 hours agorootparentprevCan you warn us pre-IPO? reply tome 12 hours agorootparentI'm sure you'll hear all about our IPO on HN :) :) reply FpUser 10 hours agorootparentprevYes please reply elorant 12 hours agoparentprevPerplexity Labs also has an open demo of Mixtral 8x7b although it's nowhere near as fast as this. https://labs.perplexity.ai/ reply vitorgrs 9 hours agorootparentPoe has a bunch of them, including Groq as well! reply larodi 11 hours agoparentprevwhy sell? it would be much more delightful to beat them on their own game? reply brcmthrowaway 15 hours agoparentprevI have it on good authority Apple was very closing to acquiring Groq reply baq 15 hours agorootparentIf this is true, expect a call from the SEC... reply sheepscreek 13 hours agorootparentTIL that SEC has authority over private company dealings wrt sale of shares[1]. [1] https://www.sec.gov/education/capitalraising/building-blocks... reply belter 11 hours agorootparentprevNot if poster is in a crashing plane... reply 317070 15 hours agorootparentprevEven if it isn't true. Disclosing inside information is illegal, _even if it is false and fabricated_, if it leads to personal gains. reply KRAKRISMOTT 14 hours agorootparentYou have to prove the OP had personal gains. If he's just a troll, it will be difficult. reply frognumber 14 hours agorootparentYou also have to be an insider. If I go to a bar, and overhear a pair of Googlers discussing something secret and overhear it, I can: 1) Trade on it. 2) Talk about it. Because I'm not an insider. On the other hand, if I'm sleeping with the CEO, I become an insider. Not a lawyer. Above is not legal advice. Just a comment that the line is much more complex, and talking about a potential acquisition is usually okay (if you're not under NDA). reply jazzyjackson 5 hours agorootparentjust so you know no one's ever been taken to court for discussing the law, it doesn't matter that you're not a lawyer, it's basically a meme reply p1esk 5 hours agorootparentAre you a lawyer? reply throwawayurlife 12 hours agorootparentprevIt doesn't matter if you overheard it at a bar or if you're just some HN commenter posting completely incorrect legal advice; the law prohibits trading on material nonpublic information. I would pay a lot to see you try your ridiculous legal hokey-pokey on how to define an \"insider.\" reply jahewson 9 hours agorootparentNo, a bar is a public place so this counts as a public disclosure. The people having the conversation would be in trouble with the SEC for making a disclosure in this manner. reply kergonath 9 hours agorootparentprev> the law prohibits trading on material nonpublic information. Isn‚Äôt it public information the moment it‚Äôs said audibly in a public space? reply frognumber 9 hours agorootparentNo. It's not. However, as pointed out elsewhere, you can trade on many types of non-public information. Indeed, hedge funds engage in all sorts of surveillance in order to get non-public material information to trade on which gives them a proprietary edge. You just can't trade on insider information. That's a very complex legal line. reply windexh8er 10 hours agorootparentprevFeel free to share some legal precedence where this situation has fared poorly for someone who \"overheard it at a bar\". It'd also be a good time to watch you lose all that money on your hokey-pokey assumption. reply frognumber 12 hours agorootparentprevYou're an idiot. https://www.kiplinger.com/article/investing/t052-c008-s001-w... Case #1. reply hnfong 4 hours agorootparentUnless you earn enough money to retain good lawyers and are prepared to get into complicated legal troubles, getting sued isn't a great outcome even if you win. The prudent thing to do is to stay away from anything that might make you become a target of investigation, unless the gains outweigh the risk by a significant margin. reply programmarchy 12 hours agorootparentprevIf you did hear it in a bar, could you tweet it out before your trade, so the information is made public? reply jahewson 9 hours agorootparentIf you hear it in a bar it‚Äôs already public. reply sofixa 12 hours agorootparentprevHad insider trading training, and yes, that's the gist of it. If you know or presume that the information is material (makes a difference) and not public, it's illegal to act on it. reply tripletao 12 hours agorootparentRoughly, it's illegal only if you have some duty not to trade on it. If you acquired the information without misappropriating it (like overhearing it from strangers in a normal public bar), then you're free to trade. https://corpgov.law.harvard.edu/2017/01/18/insider-trading-l... There's no reason for normal corporate training to discuss that element, because an employee who trades their employer's stock based on MNPI has near-certainly misappropriated it. The question of whether a non-employee has misappropriated information is much more complex, though. reply frognumber 12 hours agorootparentprevTraining is designed to protect the corporation, not to provide accurate lega ladvice. That's true of most corporate trainings, for that matter, be that bribes / corruption, harassment, discrimination, or whatnot. Corporations want employees very far from the line. That's the right way to run them. If you want more nuance, talk to a lawyer or read case law. Generally, insider trading requires something along the lines of a fiduciary duty to keep the information secret, albeit a very weak one. I'm not going to slice that line, but you see references in-thread. reply timomaxgalvin 15 hours agoparentprevSure, but the responses are very poor compared to MS tools. reply treesciencebot 17 hours agoprevThe main problem with the Groq LPUs is, they don't have any HBM on them at all. Just a miniscule (230 MiB) [0] amount of ultra-fast SRAM (20x faster than HBM3, just to be clear). Which means you need ~256 LPUs (4 full server racks of compute, each unit on the rack contains 8x LPUs and there are 8x of those units on a single rack) just to serve a single model [1] where as you can get a single H200 (1/256 of the server rack density) and serve these models reasonably well. It might work well if you have a single model with lots of customers, but as soon as you need more than a single model and a lot of finetunes/high rank LoRAs etc., these won't be usable. Or for any on-prem deployment since the main advantage is consolidating people to use the same model, together. [0]: https://wow.groq.com/groqcard-accelerator/ [1]: https://twitter.com/tomjaguarpaw/status/1759615563586744334 reply matanyal 16 hours agoparentGroq Engineer here, I'm not seeing why being able to scale compute outside of a single card/node is somehow a problem. My preferred analogy is to a car factory: Yes, you could build a car with say only one or two drills, but a modern automated factory has hundreds of drills! With a single drill, you could probably build all sorts of cars, but a factory assembly line is only able to make specific cars in that configuration. Does that mean that factories are inefficient? You also say that H200's work reasonably well, and that's reasonable (but debatable) for synchronous, human interaction use cases. Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia. reply pbalcer 16 hours agorootparentJust curious, how does this work out in terms of TCO (even assuming the price of a Groq LPU is 0$)? What you say makes sense, but I'm wondering how you strike a balance between massive horizontal scaling vs vertical scaling. Sometimes (quite often in my experience) having a few beefy servers is much simpler/cheaper/faster than scaling horizontally across many small nodes. Or I got this completely wrong, and your solution enables use-cases that are simply unattainable on mainstream (Nvidia/AMD) hardware, making TCO argument less relevant? reply tome 16 hours agorootparentWe're providing by far the lowest latency LLM engine on the planet. You can't reduce latency by scaling horizontally. reply nickpsecurity 15 hours agorootparentDistributed, shared memory machines used to do exactly that in HPC space. They were a NUMA alternative. It works if the processing plus high-speed interconnect are collectively faster than the request rate. The 8x setups with NVLink are kind of like that model. You may have meant that nobody has a stack that uses clustering or DSM with low-latency interconnects. If so, then that might be worth developing given prior results in other low-latency domains. reply tome 15 hours agorootparentI think existing players will have trouble developing a low latency solution like us whilst they are still running on non-deterministic hardware. reply WanderPanda 15 hours agorootparentWhat do you mean by non-deterministic hardware? cuBLAS on a laptop GPU was deterministic when I tried it last iirc reply tome 14 hours agorootparentNon-deterministic timing characteristics. reply frozenport 14 hours agorootparentprevTip of the ice-berg. DRAM needs to be refreshed every X cycles. This means you don't know the time it takes to read from memory. You could be reading at a refresh cycle. This circuitry also adds latency. reply LtdJorge 9 hours agorootparentOP says SRAM, which doesn't decay so no refreshing. reply ndjdbdjdbev 2 hours agorootparentTiming can simply mean the FETs that make up the logic circuits of a chip. The transition from high to low and low to high has a minimum safe time to register properly... reply nickpsecurity 13 hours agorootparentprevWhile you‚Äôre here, I have a quick, off-topic question. We‚Äòve seen incredible results with GPT3-176B (Davinci) and GPT4 (MoE). Making attempts at open models that reuse their architectural strategies could have a high impact on everyone. Those models took 2500-25000 GPU‚Äôs to train, though. It would be great to have a low-cost option for pre training Davinci-class models. It would great if a company or others with AI hardware were willing to do production runs of chips sold at cost specifically to make open, permissive-licensed models. As in, since you‚Äôd lose profit, the cluster owner and users would be legally required to only make permissive models. Maybe at least one in each category (eg text, visual). Do you think your company or any other hardware supplier would do that? Or someone sell 2500 GPU‚Äôs at cost for open models? (Note to anyone involved in CHIPS Act: please fund a cluster or accelerator specifically for this.) reply tome 12 hours agorootparentGreat idea, but Groq doesn't have a product suitable for training at the moment. Our LPUs shine in inference. reply huac 13 hours agorootparentprev> 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia. I believe that this is doable - my pipeline is generally closer to 400ms without RAG and with Mixtral, with a lot of non-ML hacks to get there. It would also definitely be doable with a joint speech-language model that removes the transcription step. For these use cases, time to first byte is the most important metric, not total throughput. reply qeternity 9 hours agorootparentIt‚Äôs important‚Ä¶if you‚Äôre building a chatbot. The most interesting applications of LLMs are not chatbots. reply chasd00 6 hours agorootparent> The most interesting applications of LLMs are not chatbots. What are they then? Every use case I‚Äôve seen is either a chatbot or like a copy editor which is just a long form chatbot. reply jasonjmcghee 5 hours agorootparentObviously not op, but these days LLMs can be fuzzy functions with reliably structured output, and are multi-modal. Think about the implications of that. I bet you can come up with some pretty cool use cases that don't involve you talking to something over chat. One example: I think we'll be seeing a lot of \"general detectors\" soon. Without training or predefined categories, get pinged when (whatever you specify) happens. Whether it's a security camera, web search, event data, etc reply nycdatasci 5 hours agorootparentprevComplex data tagging/enrichment tasks. reply throwaway2037 6 hours agorootparentprev> The most interesting applications of LLMs are not chatbots. In your opinion, what are the most interesting? reply jrflowers 4 hours agorootparentprev>Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia. Is your version of that on a different page from this chat bot? reply treprinum 13 hours agorootparentprev> Show me a 30b+ parameter model doing RAG as part of a conversation with voice responses in less than a second, running on Nvidia I built one, should be live soon ;-) reply tome 13 hours agorootparentExciting! Looking forward to seeing it. reply startupsfail 7 hours agorootparentprevI have one, with 13B, on a 5-year-old 48GB Q8000 GPU. It‚Äôs also can see, it‚Äôs LLaVA. And it is very important that it is local, as privacy is important and streaming images to the cloud is time consuming. You only need a few tokens, not the full 500 tokens response to run TTS. And you can pre-generate responses online, as ASR is still in progress. With a bit of clever engineering the response starts with virtually no delay, the moment its natural to start the response. reply yaknh 6 hours agorootparentDid you find anything cheaper for local installation? reply mlazos 12 hours agorootparentprevYou can‚Äôt scale horizontally forever because of communication. I think HBM would provide a lot more flexibility with the number of chips you need. reply fennecbutt 7 hours agorootparentprevAre there voice responses in the demo? I couldn't find em? reply tome 1 hour agorootparentHere's a live demo of CNN of Groq plugged into a voice API https://www.youtube.com/watch?v=pRUddK6sxDg&t=235s reply trsohmers 16 hours agoparentprevGroq states in this article [0] that they used 576 chips to achieve these results, and continuing with your analysis, you also need to factor in that for each additional user you want to have requires a separate KV cache, which can add multiple more gigabytes per user. My professional independent observer opinion (not based on my 2 years of working at Groq) would have me assume that their COGS to achieve these performance numbers would exceed several million dollars, so depreciating that over expected usage at the theoretical prices they have posted seems impractical, so from an actual performance per dollar standpoint they don‚Äôt seem viable, but do have a very cool demo of an insane level of performance if you throw cost concerns out the window. [0]: https://www.nextplatform.com/2023/11/27/groq-says-it-can-dep... reply tome 15 hours agorootparentThomas, I think for full disclosure you should also state that you left Groq to start a competitor (a competitor which doesn't have the world's lowest latency LLM engine nor a guarantee to match the cheapest per token prices, like Groq does.). Anyone with a serious interest in the total cost of ownership of Groq's system is welcome to email contact@groq.com. reply trsohmers 14 hours agorootparentI thought that was clear through my profile, but yes, Positron AI is focused on providing the best performance per dollar while providing the best quality of service and capabilities rather than just focusing on a single metric of speed. A guarantee to match the cheapest per token prices is sure a great way to lose a race to the bottom, but I do wish Groq (and everyone else trying to compete against NVIDIA) the greatest luck and success. I really do think that the great single batch/user performance by Groq is a great demo, but is not the best solution for a wide variety of applications, but I hope it can find its niche. reply Aeolun 10 hours agorootparentprevI think that just means it‚Äôs for people that really want it? John doe and his friends will never have a need to have their fart jokes generated at this speed, and are more interested in low costs. But we‚Äôd recently been doing call center operations and being able to quickly figure out what someone said was a major issue. You kind of don‚Äôt want your system to wait for a second before responding each time. I can imagine it making sense if it reduces the latency to 10ms there as well. Though you might still run up against the ‚Äògood enough‚Äô factor. I guess few people want to spend millions to go from 1000ms to 10ms, but when they do they really want it. reply nickpsecurity 5 hours agorootparentprevWhat happened to Rex? Did it hit production or get abandoned? It was also on my list of things to consider modifying for an AI accelerator. :) reply trsohmers 3 hours agorootparentLong story, but technically REX is still around but has not been able to continue to develop due to lack of funding and my cofounder and I needing to pay bills. We produced initial test silicon, but due to us having very little money after silicon bringup, most of our conversations turned to acquihire discussions. There should be a podcast release (https://microarch.club/) in the near future that covers REX's history and a lot of lessons learned. reply tome 16 hours agoparentprevIf you want low latency you have to be really careful with HBM, not only because of the delay involved, but also the non-determinacy. One of the huge benefits of our LPU architecture is that we can build systems of hundreds of chips with fast interconnect and we know the precise timing of the whole system to within a few parts per million. Once you start integrating non-deterministic components your latency guarantees disappear very quickly. reply pclmulqdq 16 hours agorootparentI don't know about HBM specifically, but DDR and GDDR at a protocol level are both deterministic. It's the memory controller doing a bunch of reordering that makes them non-deterministic. Presumably, if that is the reason you don't like DRAM, you could build your compiler to be memory-layout aware and have the memory controller issue commands without reordering. reply tome 16 hours agorootparentThat could be possible. It's out of my area of expertise so I can't say for sure. My understanding was HBM forces on you specific access patterns and non-deterministic delays. Our compiler already deals with many other forms of resource-aware scheduling so it could take into account DRAM refreshes easily, so I feel like there must be something else that makes SRAM more suitable in our case. I'll have to leave that to someone more knowledgeable to explain though ... reply johntb86 15 hours agorootparentprevPresumably with dram you also have to worry about refreshes, which can come along at arbitrary times relative to the workload. reply pclmulqdq 11 hours agorootparentYou can control when those happen, too. reply Temporary_31337 6 hours agorootparentnot without affecting performance though? If you delay refreshes, this lowers performance as far as I remember... reply frognumber 14 hours agorootparentprevFrom a theoretical perspective, this is absolutely not true. Asynchronous logic can achieve much lower latency guarantees than synchronous logic. Come to think of it, this is one of the few places where asynchronous logic might be more than academic... Async logic is hard with complex control flows, which deep learning inference does not have. (From a practical perspective, I know you were comparing to independently-clocked logic, rather than async logic) reply foundval 11 hours agorootparent(Groq Employee) You're right - we are comparing to independently-clocked logic. I wonder whether async logic would be feasible for reconfigurable \"Spatial Processor\" type architectures [1]. As far as LPU architectures go, they fall in the \"Matrix of Processing Engines\"[1] family of architectures, which I would naively guess is not the best suited to leverage async logic. 1: I'm using the \"Spatial Processor\" (7:14) and \"Matrix of Processing Engines\" (8:57) terms as defined in https://www.youtube.com/watch?v=LUPWZ-LC0XE. Sorry for a video link, I just can't think of another single reference that explains the two approaches. reply frognumber 10 hours agorootparentCuriously, almost all of this video is mostly covered by computer architectures lit in the late 90's early 00's. At the time, I recall Tom Knight had done most of the analysis in this video, but I don't know if he ever published it. It was extrapolating into the distant future. To answer your questions: - Spatial processors are an insanely good fit for async logic - Matrix of processing engines are a moderately good fit -- definitely could be done, but I have no clue if it'd be a good idea. In SP, especially in an ASIC, each computation can start as soon as the previous one finishes. If you have a 4-bit layer, and 8-bit layer, and a 32-bit layer, those will take different amounts of time to run. Individual computations can take different amounts of time too (e.g. an ADD with a lot of carries versus one with just a few). In an SP, a compute will take as much time as it needs, and no more. Footnote: Personally, I think there are a lot of good ideas in 80's era and earlier processors for the design of individual compute units which have been forgotten. The basic move in architectures up through 2005 was optimizing serial computation speed at the cost of power and die size (Netburst went up to 3.8GHz two decades ago). With much simpler old-school compute units, we can have *many* more of them than a modern multiply unit. Critically, they could be positioned closer to the data, so there would be less data moving around. Especially the early pipelined / scalar / RISC cores seem very relevant. As a point of reference, a 4090 has 16k CUDA cores running at just north of 2GHz. It has the same number of transistors as 32,000 SA-110 processors (running at 200MHz on a 350 nanometer process in 1994). TL;DR: I'm getting old and either nostalgic or grumpy. Dunno which. reply foundval 8 hours agorootparentSweet, thanks! It seems like this research ecosystem was incredibly rich, but Moore's law was in full swing, and statically known workloads weren't useful at the compute scale of back then. So these specialized approach never stood a chance next to CPUS. Nowadays the ground is.. more fertile. reply frozenport 10 hours agorootparentprevThis was sort of the dream of KNL but today I noticed Xeon Phi CPUs support (a.k.a. Knight Landing and Knight Mill) are marked as deprecated. GCC will emit a warning when using the -mavx5124fmaps, -mavx5124vnniw, -mavx512er, -mavx512pf, -mprefetchwt1, -march=knl, -march=knm, -mtune=knl or -mtune=knm compiler switches. Support will be removed in GCC 15. the issue was that coordinating across this kind of hierarchy wasted a bunch of time. If you already knew how to coordinate, mostly, you could instead get better performance you might be surprised but we're getting to the point that communicating over a super computer is on the same order of magnitude as talking across a numa node. reply frognumber 9 hours agorootparentI actually wasn't so much talking from that perspective, as simply from the perspective of the design of individual pieces. There were rather clever things done in e.g. older multipliers or adders or similar which, I think, could apply to most modern parallel architectures, be that GPGPU, SP, MPE, FPGA, or whatever, in order to significantly increase density at a cost of slightly reduced serial performance. For machine learning, that's a good tradeoff. Indeed, with some of the simpler architectures, I think computation could be moved into the memory itself, as long dreamed of. (Simply sticking 32,000 SA-110 processors on a die would be very, very limited by interconnect; there's a good reason for the types of architectures we're seeing not being that) reply frozenport 9 hours agorootparentTruth is that there is another startup called graph core that is doing exactly that, and also a really big chip reply frognumber 7 hours agorootparentThey do what you were talking about, not what I was. They seem annoying. \"The IPU has a unique memory architecture consisting of large amounts of In-Processor-Memory‚Ñ¢ within the IPU made up of SRAM (organised as a set of smaller independent distributed memory units) and a set of attached DRAM chips which can transfer to the In-Processor-Memory via explicit copies within the software. The memory contained in the external DRAM chips is referred to as Streaming Memory‚Ñ¢.\" There's a ‚Ñ¢ every few words. Those seem like pretty generic terms. That's their technical documentation. The architecture is reminiscent of some ideas from circa-2000 which didn't pan out. It reminds me of Tilera (the guy who ran it was the Donald Trump of computer architectures; company was acquihired by EZchip for a fraction of the investment which was put into it, which went to Mellanox, and then to NVidia). reply SilverBirch 12 hours agorootparentprevSurely once you're scaling over multiple chips/servers/racks you're dealing with retries and checksums and sequence numbers anyway? How do you get around the non-determinacy of networking beyond just hoping that you don't see any errors? reply tome 12 hours agorootparentOur interconnect between chips is also deterministic! You can read more about our interconnect, synchronisation, and error correction in our paper. https://wow.groq.com/wp-content/uploads/2023/05/GroqISCAPape... reply pclmulqdq 16 hours agoparentprevGroq devices are really well set up for small-batch-size inference because of the use of SRAM. I'm not so convinced they have a Tok/sec/$ advantage at all, though, and especially at medium to large batch sizes which would be the groups who can afford to buy so much silicon. I assume given the architecture that Groq actually doesn't get any faster for batch sizes >1, and Nvidia cards do get meaningfully higher throughput as batch size gets into the 100's. reply foundval 11 hours agorootparent(Groq Employee) It's hard to discuss Tok/sec/$ outside of the context of a hardware sales engagement. This is because the relationship between Tok/s/u, Tok/s/system, Batching, and Pipelining is a complex one that involves compute utilization, network utilization, and (in particular) a host of compilation techniques that we wouldn't want to share publicly. Maybe we'll get to that level of transparency at some point, though! As far as Batching goes, you should consider that with synchronous systems, if all the stars align, Batch=1 is all you need. Of course, the devil is in the details, and sometimes small batch numbers still give you benefits. But Batch 100's generally gives no advantages. In fact, the entire point of developing deterministic hardware and synchronous systems is to avoid batching in the first place. reply nabakin 12 hours agorootparentprevI've been thinking the same but on the other hand, that would mean they are operating at a huge loss which doesn't scale reply frozenport 11 hours agorootparentprevI assume given the architecture that Groq actually doesn't get any faster for batch sizes >1 I guess if you don't have any extra junk you can pack more processing into the chip? reply foundval 10 hours agorootparent(Groq Employee) Yes! Determinism + Simplicity are superpowers for ALU and interconnect utilization rates. This system is powered by 14nm chips, and even the interconnects aren't best in class. We're just that much better at squeezing tokens out of transistors and optic cables than GPUs are - and you can imagine the implications on Watt/Token. Anyways.. wait until you see our 4nm. :) reply londons_explore 15 hours agoparentprev> more than a single model and a lot of finetunes/high rank LoRAs I can imagine a way might be found to host a base model and a bunch of LoRA's whilst using barely more ram than the base model alone. The fine-tuning could perhaps be done in such a way that only perhaps 0.1% of the weights are changed, and for every computation the difference is computed not over the weights, but of the output layer activations. reply kcorbitt 11 hours agorootparentThis actually already exists! We did a writeup of the relevant optimizations here: https://openpipe.ai/blog/s-lora reply azeirah 10 hours agorootparentprevI recall a recent discussion about a technique to load the diff in weights between a lora and base model, zip it and transfer it on a per-needs basis. reply moralestapia 16 hours agoparentprev>The main problem with the Groq LPUs is, they don't have any HBM on them at all. Just a miniscule (230 MiB) [0] amount of ultra-fast SRAM [...] IDGAF about any of that, lol. I just want an API endpoint. 480 tokens/sec at $0.27 per million tokens? Sign me in, I don't care about their hardware, at all. reply treesciencebot 16 hours agorootparentthere are providers out there offering for $0 per million tokens, that doesn't mean it is sustainable and won't disappear as soon as the VC well runs dry. Am not saying this is the case for Groq, but in general you probably should care if you want to build something serious on top of anything. reply foundval 10 hours agorootparent(Groq Employee) Agreed, one should care, and especially since this particular service is very differentiated by its speed and has no competitors. That being said, until there's another option at anywhere that speed.. That point is moot, isn't it :) For now, Groq is the only option that can let you build an UX with near-instant response times. Or a live agents that help with a human-to-human interaction. I could go on and on about the product categories this opens. reply bethekind 4 hours agorootparentWhy go so fast? Aren't Nvidias products fast enough from a TPS perspective? reply mike_hearn 14 minutes agorootparentOpenAI have a voice powered chat mode in their app and there's a noticeable delay of a few seconds between finishing your sentence and the bot starting to speak. I think the problem is that for realistic TTS you need quite a few tokens because the prosody can be affected by tokens that come a fair bit further down the sentence, consider the difference in pitch between: \"The war will be long and bloody\" vs \"The war will be long and bloody?\" So to begin TTS you need quite a lot of tokens, which in turn means you have to digest the prompt and run a whole bunch of forward passes before you can start rendering. And of course you have to keep up with the speed of regular speech, which OpenAI sometimes struggles with. That said, the gap isn't huge. Many apps won't need it. Some use cases where low latency might matter: - Phone support. - Trading. Think digesting a press release into an action a few seconds faster than your competitors. - Agents that listen in to conversations and \"butt in\" when they have something useful to say. - RPGs where you can talk to NPCs in realtime. - Real-time analysis of whatever's on screen on your computing device. - Auto-completion. - Using AI as a general command prompt. Think AI bash. Undoubtably there will be a lot more though. When you give people performance, they find ways to use it. reply imtringued 16 hours agoparentprevI honestly don't see the problem. \"just to serve a single model\" could be easily fixed by adding a single LPDDR4 channel per LPU. Then you can reload the model sixty times per second and serve 60 different models per second. reply treesciencebot 16 hours agorootparentper-chip compute is not the main thing this chip innovates for fast inference, it is the extremely fast memory bandwith. when you do that, you'll loose all of that and will be much worse off than any off the shelf accelerators. reply QuadmasterXLII 13 hours agorootparentload model, compute a 1k token response (ie, do a thousand forward passes in sequence, one per token), load a different model, compute a response, I would expect the model loading to take basically zero percent of the time in the above workflow reply karpathy 17 hours agoprevVery impressive looking! Just wanted to caution it's worth being a bit skeptical without benchmarks as there are a number of ways to cut corners. One prominent example is heavy model quantization, which speeds up the model at a cost of model quality. Otherwise I'd love to see LLM tok/s progress exactly like CPU instructions/s did a few decades ago. reply bsima 13 hours agoparentAs tome mentioned we don‚Äôt quantize, all activations are FP16 And here are some independent benchmarks https://artificialanalysis.ai/models/llama-2-chat-70b reply xvector 12 hours agorootparentJesus Christ, these speeds with FP16? That is simply insane. reply throwawaymaths 12 hours agorootparentAsk how much hardware is behind it. reply modeless 11 hours agorootparentAll that matters is the cost. Their price is cheap, so the real question is whether they are subsidizing the cost to achieve that price or not. reply throwawaymaths 8 hours agorootparentThe point of asking how much hardware is to estimate the cost? (Both capital and operational, i.e. power) reply tome 17 hours agoparentprevAs a fellow scientist I concur with the approach of skepticism by default. Our chat app and API are available for everyone to experiment with and compare output quality with any other provider. I hope you are enjoying your time of having an empty calendar :) reply mr_luc 10 hours agorootparentWait you have an API now??? Is it open, is there a waitlist? I‚Äôm on a plane but going to try to find that on the site. Absolutely loved your demo, been showing it around for a few months. reply tome 10 hours agorootparentThere is an API and there is a waitlist. Sign up at http://wow.groq.com/ reply Gcam 13 hours agoparentprevAs part of our benchmarking of Groq we have asked Groq regarding quantization and they have assured us they are running models at full FP-16. It's a good point and important to check. Link to benchmarking: https://artificialanalysis.ai/ (Note question was regarding API rather than their chat demo) reply sp332 16 hours agoparentprevAt least for the earlier Llama 70B demo, they claimed to be running unquantized. https://twitter.com/lifebypixels/status/1757619926360096852 Update: This comment says \"some data is stored as FP8 at rest\" and I don't know what that means. https://news.ycombinator.com/item?id=39432025 reply tome 15 hours agorootparentThe weights are quantized to FP8 when they're stored in memory, but all the activations are computed at full FP16 precision. reply youssefabdelm 14 hours agorootparentCan you explain if this affects quality relative to fp16? And is mixtral quantized? reply tome 14 hours agorootparentWe don't think so, but you be the judge! I believe we quantize both Mixtral and Llama 2 in this way. reply a_wild_dandan 13 hours agorootparentIs your confidence rooted in quantified testing, or just vibes? I'm sure you're right, just curious. (My reasoning: running inference at full fp16 is borderline wasteful. You can use q7 with almost no loss.) reply monkmartinez 6 hours agorootparentI know some fancy benchmark says \"almost no loss\", but... subjectively, there is a clear quality loss. You can try for yourself, I can run Mixtral at 5.8bpw and there is an OBVIOUS difference between what I have seen from Groq and my local setup beside the sound barrier shattering speed of Groq. I didn't know Mixtral could output such nice code and I have used it A LOT locally. reply doctorpangloss 3 hours agorootparentYes, but this gray area underperformance that lets them claim they are the cheapest and fastest appeals to people for whom qualitative (aka real) performance doesn‚Äôt matter. reply tome 13 hours agorootparentprevWhat quantified testing would you like to see? We've had a lot of very good feedback from our users, particularly about Mixtral. reply bearjaws 12 hours agorootparentprevNothing really wrong with FP8 IMO, it performs pretty damn well usually within 98% while significantly reducing memory usage. reply losvedir 16 hours agoparentprevMaybe I'm stretching the analogy too far, but are we in the transistor regime of LLMs already? Sometimes I see these 70 billion parameter monstrosities and think we're still building ENIAC out of vacuum tubes. In other words, are we ready to steadily march on, improving LLM tok/s year by year, or are we a major breakthrough or two away before that can even happen? reply binary132 17 hours agoparentprevThe thing is that tokens aren't an apples to apples metric.... Stupid tokens are a lot faster than clever tokens. I'd rather see token cleverness improving exponentially.... reply behnamoh 16 hours agoparentprevtangent: Great to see you again on HN! reply tome 21 hours agoprevHi folks, I work for Groq. Feel free to ask me any questions. (If you check my HN post history you'll see I post a lot about Haskell. That's right, part of Groq's compilation pipeline is written in Haskell!) reply michaelbuckbee 17 hours agoparentFriendly fyi - I think this might just be a web interface bug but but I submitted a prompt with the Mixtral model and got a response (great!) then switched the dropdown to Llama and submitted the same prompt and got the exact same response. It may be caching or it didn't change the model being queried or something else. reply tome 17 hours agorootparentThanks, I think it's because the chat context is fed back to the model for the next generation even when you switch models. If you refresh the page that should erase the history and you should get results purely from the model you choose. reply michaelbuckbee 15 hours agorootparentAppreciate the quick reply! That's interesting. reply tome 15 hours agorootparentYou're welcome. Thanks for reporting. It's pretty confusing so maybe we should change it :) reply pests 14 hours agorootparentI've always liked how openrouter.ai does it They allow you to configure chat participants (a model + params like context or temp) and then each AI answers each question independently in-line so you can compare and remix outputs. reply xanderatallah 7 hours agorootparentopenrouter dev here - would love to get Groq access and include it! reply itishappy 17 hours agoparentprevAlright, I'll bite. Haskell seems pretty unique in the ML space! Any unique benefits to this decision, and would you recommend it for others? What areas of your project do/don't use Haskell? reply tome 16 hours agorootparentHaskell is a great language for writing compilers! The end of our compilation pipeline is written in Haskell. Other stages are written in C++ (MLIR) and Python. I'd recommend anyone to look at Haskell if they have a compiler-shaped problem, for sure. We also use Haskell on our infra team. Most of our CI infra is written in Haskell and Nix. Some of the chip itself was designed in Haskell (or maybe Bluespec, a Haskell-like language for chip design, I'm not sure). reply mechagodzilla 18 hours agoparentprevYou all seem like one of the only companies targeting low-latency inference rather than focusing on throughput (and thus $/inference) - what do you see as your primary market? reply tome 17 hours agorootparentYes, because we're one of the only companies whose hardware can actually support low latency! Everyone else is stuck with traditional designs and they try to make up for their high latency by batching to get higher throughput. But not all applications work with high throughput/high latency ... Low latency unlocks feeding the result of one model into the input of another model. Check out this conversational AI demo on CNN. You can't do that kind of thing unless you have low latency. https://www.youtube.com/watch?v=pRUddK6sxDg&t=235s reply vimarsh6739 14 hours agorootparentMight be a bit out of context, but isn't the TPU also optimized for low latency inference? (Judging by reading the original TPU architecture paper here - https://arxiv.org/abs/1704.04760). If so, does Groq actually provide hardware support for LLM inference? reply tome 14 hours agorootparentJonathan Ross on that paper is Groq's founder and CEO. Groq's LPU is an natural continuation of the breakthrough ideas he had when designing Google's TPU. Could you clarify your question about hardware support? Currently we build out our hardware to support our cloud offering, and we sell systems to enterprise customers. reply vimarsh6739 14 hours agorootparentThanks for the quick reply! About hardware support, I was wondering if the LPU has a hardware instruction to compute the attention matrix similar to the MatrixMultiply/Convolve instruction in the TPU ISA. (Maybe a hardware instruction which fuses a softmax on the matmul epilogue?) reply tome 14 hours agorootparentWe don't have a hardware instruction but we do have some patented technology around using a matrix engine to efficiently calculate other linear algebra operations such as convolution. reply mirekrusin 12 hours agorootparentprevAre you considering targeting consumer market? There are a lot of people throwing $2k-$4k into local setups and they primarily care about inference. reply tome 12 hours agorootparentAt the moment we're concentrating on building out our API and serving the enterprise market. reply jart 15 hours agoparentprevIf I understand correctly, you're using specialized hardware to improve token generation speed, which is very latency bound on the speed of computation. However generating tokens only requires multiplying 1-dimensional matrices usually. If I enter a prompt with ~100 tokens then your service goes much slower. Probably because you have to multiply 2-dimensional matrices. What are you doing to improve the computation speed of prompt processing? reply tome 15 hours agorootparentI don't think it should be quadratic in input length. Why do you think it is? reply johndough 11 hours agorootparentYou can ask your website: \"What is the computational complexity of self-attention with respect to input sequence length?\" It'll answer something along the lines of self-attention being O(n^2) (where n is the sequence length) because you have to compute an attention matrix of size n^2. There are other attention mechanisms with better computational complexity, but they usually result in worse large language models. To answer jart: We'll have to wait until someone finds a good linear attention mechanism and then wait some more until someone trains a huge model with it (not Groq, they only do inference). reply tome 1 hour agorootparentOK, thanks, that's useful to know. Personally I'm not involved directly in implementing the model, so I don't know what we do there. reply jart 8 hours agorootparentprevChanging the way transformer models works is orthogonal to gaining good performance on Mistral. Groq did great work reducing the latency considerably of generating tokens during inference. But I wouldn't be surprised if they etched the A matrix weights in some kind of fast ROM, used expensive SRAM for the the skinny B matrix, and sent everything else that didn't fit to good old fashioned hardware. That's great for generating text, but prompt processing is where the power is in AI. In order to process prompts fast, you need to multiply weights against 2-dimensional matrices. There is significant inequality in software implementations alone in terms of how quickly they're able to do this, irrespective of hardware. That's why things like BLAS libraries exist. So it'd be super interesting to hear about how a company like Groq that leverages both software and hardware specifically for inference is focusing on tackling its most important aspect. reply jart 14 hours agorootparentprevall I know is that when I run llama.cpp a lot of the matrices that get multiplied have their shapes defined by how many tokens are in my prompt. https://justine.lol/tmp/shapes.png Notice how the B matrix is always skinny for generating tokens. But for batch processing of the initial prompt, it's fat. It's not very hard to multiply a skinny matrix but once it's fat it gets harder. Handling the initial batch processing of the prompt appears to be what your service goes slow at. reply ppsreejith 21 hours agoparentprevThank you for doing this AMA 1. How many GroqCards are you using to run the Demo? 2. Is there a newer version you're using which has more SRAM (since the one I see online only has 230MB)? Since this seems to be the number that will drive down your cost (to take advantage of batch processing, CMIIW!) 3. Can TTS pipelines be integrated with your stack? If so, we can truly have very low latency calls! *Assuming you're using this: https://www.bittware.com/products/groq/ reply tome 21 hours agorootparent1. I think our GroqChat demo is using 568 GroqChips. I'm not sure exactly, but it's about that number. 2. We're working on our second generation chip. I don't know how much SRAM it has exactly but we don't need to increase the SRAM to get efficient scaling. Our system is deterministic, which means no need for waiting or queuing anywhere, and we can have very low latency interconnect between cards. 3. Yeah absolutely, see this video of a live demo on CNN! https://www.youtube.com/watch?t=235&v=pRUddK6sxDg reply ppsreejith 20 hours agorootparentThank you, that demo was insane! Follow up (noob) question: Are you using a KV cache? That would significantly increase your memory requirements. Or are you forwarding the whole prompt for each auto-regressive pass? reply tome 20 hours agorootparentYou're welcome! Yes, we have KV cache. Being able to implement this efficiently in terms of hardware requirements and compute time is one of the benefits of our deterministic chip architecture (and deterministic system architecture). reply ppsreejith 20 hours agorootparentThanks again! Hope I'm not overwhelming but one more question: Are you decoding with batch size = 1 or is it more? reply tome 20 hours agorootparentThat's OK, feel free to keep asking! I think currently 1. Unlike with graphics processors, which really need data parallelism to get good throughput, our LPU architecture allows us to deliver good throughput even at batch size 1. reply gautamcgoel 15 hours agorootparentprevCan you talk about the interconnect? Is it fully custom as well? How do you achieve low latency? reply tome 15 hours agorootparentYou can find out about the chip to chip interconnect from our paper below, section 2.3. I don't think that's custom. We achieve low latency by basically being a software-defined architecture. Our functional units operate completely orthoganal to each other. We don't have to batch in order to achieve parallelism and the system behaviour is completely deterministic, so we can schedule all operations precisely. https://wow.groq.com/wp-content/uploads/2023/05/GroqISCAPape... reply WiSaGaN 18 hours agorootparentprevHow much do 568 chips cost? What‚Äôs the cost ratio of it comparing to setup with roughly the same throughput using A100? reply benchess 18 hours agorootparentThey‚Äôre for sale on Mouser for $20625 each https://www.mouser.com/ProductDetail/BittWare/RS-GQ-GC1-0109... At that price 568 chips would be $11.7M reply fennecbutt 7 hours agorootparentI presume that's because it's a custom asic not yet in mass production? If they can get costs down and put more dies into each card then it'll be business/consumer friendly. Let's see if they can scale production. Also, where tf is the next coral chip, alphabet been slacking hard. reply bethekind 4 hours agorootparentI think Coral has been taken to the wooden shed out back. Nothing new out of them for years sadly reply tome 18 hours agorootparentprevYeah, I don't know what the cost to us is to build out our own hardware but it's significantly less expensive than retail. reply WiSaGaN 18 hours agorootparentprevThat seems to be per card instead of chip. I would expect it has multiple chips on a single card. reply renewiltord 18 hours agorootparentFrom the description that doesn't seem to be the case, but I don't know this product well > Accelerator Cards GroqCard low latency AI/ML Inference PCIe accelerator card with single GroqChip reply WiSaGaN 18 hours agorootparentMissed that! Thanks for pointing out! reply andy_xor_andrew 17 hours agoparentprevare your accelerator chips designed in-house? or they're some specialized silicon or FPGPU or something that you wrote very optimized code for inference? it's really amazing! the first time I tried the demo, I had to try a few prompts to believe it wasn't just an animation :) reply tome 17 hours agorootparentYup, custom ASIC, designed in-house, built into a system of several racks, hundreds of chips, with fast interconnect. Really glad you enjoyed it! reply UncleOxidant 17 hours agoparentprevWhen will we be able to buy Groq accelerator cards that would be affordable for hobbyists? reply tome 16 hours agorootparentWe are prioritising building out whole systems at the moment I don't think we'll have a consumer level offering in the near future. reply frognumber 14 hours agorootparentI will mention: A lot of innovation in this space comes bottom-up. The sooner you can get something in the hands of individuals and smaller institutions, the better your market position will be. I'm coding to NVidia right now. That builds them a moat. The instant I can get other hardware working, the less of a moat they will have. The more open it is, the more likely I am to adopt it. reply tome 14 hours agorootparentDefinitely, that's why we've opened our API to everyone. reply frognumber 13 hours agorootparentI don't think that quite does it. What I'd want -- if you want me to support you -- is access to the chip, libraries, and API documentation. Best-case would be something I buy forHowever, the hardware requirements and cost make this inaccessible for anyone but large companies. When do you envision that the price could be affordable for hobbyists? For API access to our tokens as a service we guarantee to beat any other provider on cost per token (see https://wow.groq.com). In terms of selling hardware, we're focused on selling whole systems, and they're only really suitable for corporations or research institutions. reply pwillia7 9 hours agorootparentDo you have any data on how many more tokens I would use with the increased speed? In the demo alone I just used way more tokens than I normally would testing an LLM since it was so amazingly fast. reply tome 1 hour agorootparentInteresting question! Hopefully being faster is so much more useful to you that you use a lot more :) reply dsrtslnd23 6 hours agorootparentprevHow open is your early access? i.e. likelihood to get API access granted right now reply tome 1 hour agorootparentWe are absolutely slammed with requests right now, so I don't know, sorry. reply stormfather 12 hours agoparentprev>>50 t/s is absolutely necessary for real-time interaction with AI systems. Most of the LLM's output will be internal monologue and planning, performing RAG and summarization, etc, with only the final output being communicated to you. Imagine a blazingly fast GPT-5 that goes through multiple cycles of planning out how to answer you, searching the web, writing book reports, debating itself, distilling what it finds, critiquing and rewriting its answer, all while you blink a few times. reply dmw_ng 14 hours agoparentprevGiven the size of the Sindarin team (3 AFAICT), that mostly looks like a clever combination of existing tech. There are some speech APIs that offer word-by-word realtime transcription (Google has one), assuming most of the special sauce is very well thought out pipelining between speech recognition->LLM->TTS (not to denigrate their awesome achievement, I would not be interested if I were not curious about how to reproduce their result!) reply SeanAnderson 14 hours agoprevSorry, I'm a bit na√Øve about all of this. Why is this impressive? Can this result not be achieved by throwing more compute at the problem to speed up responses? Isn't the fact that there is a queue when under load just indicative that there's a trade-off between \"# of request to process per unit of time\" and \"amount of compute to put into a response to respond quicker\"? https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/rel/do... This chart from NVIDIA implies their H100 runs llama v2 70B at >500 tok/s. reply MasterScrat 14 hours agoparentScaling up compute can improve throughput, but can't easily improve latency between tokens. Generation is usually bottlenecked by the time it takes to go through the network for each token. To speed that up, you need to perform these computations faster, which is a hard problem after you've exhausted all the obvious options (use the fastest accelerator you can find, cache what you can etc). reply qeternity 8 hours agorootparentAt batch size 1 LLMs are memory bandwidth bound, not compute bound‚Ä¶as in you spend most time waiting for model weights to load from vram. At higher batch sizes this flips. But this is why Groq is built around large numbers of chips with small amount of very fast sram. reply SeanAnderson 14 hours agorootparentprevYeah. That makes sense, thank you for clarifying. I updated my original post with a chart from NVIDIA which highlights the H100's capabilities. It doesn't seem unreasonable to expect a 7B model to run at 500 tok/s on that hardware. reply snowfield 12 hours agorootparentThis is a 50B model. (Mixtral 8x7b) reply SeanAnderson 12 hours agorootparentOh, sorry, I assumed the 8 was for quantization. 8x7b is a new syntax for me. Still, the NVIDIA chart shows Llama v2 70B at 750 tok/s, no? reply tome 12 hours agorootparentI guess that's total throughput, rather than per user? You can increase total throughput by scaling horizontally. You can't increase throughput per user that way. reply tome 14 hours agoparentprevLLM inference is inherently a sequential problem. You can't speed it up by doing more in parallel. You can't generate the 101st token before you've generated the 100th. reply NorwegianDude 11 hours agorootparentTechnically, I guess you can use speculative execution to speed it up, and in that way take a guess at what the 100th token will be and start on the 101st token at the same time? Though it probably has it's own unforeseen challenges. Everything is predictable with enough guesses. reply jsmith12673 10 hours agorootparentPeople are pretty cagey about what they use in production, but yes, speculative sampling can offer massive speedups in inference reply Aeolun 9 hours agorootparentprevThey‚Äôre using several hundred cards here. Clearly there is ‚Äòsomething‚Äô that can be done in parallel. reply nabakin 11 hours agoparentprevThere's a difference between token throughput and latency. Token throughput is the token throughput of the whole GPU/system and latency is the token throughput for an individual user. Groq offers extremely low latency (aka extremely high token throughput per user) but we still don't have numbers on the token throughput of their entire system. Nvidia's metrics here on the other hand, show us the token throughput of the whole GPU/system. So, in reality, while you might be able to get 1.5k t/s on an H100, the latency (token throughput per user) will be something much lower like 20 t/s. The really important metric to look for is cost per token because even though Groq is able to run at low latency, that doesn't mean it's able to do it cheaply. Determining the cost per token can be done many ways but a useful way for us is approximately the cost of the system divided by the total token throughput of the system per second. We don't have the total token throughput per second of Groq's system so we can't really say how efficient it is. It could very well be that Groq is subsidizing the cost of their system to lower prices and gain PR and will increase their prices later on. reply frozenport 11 hours agorootparenthttps://wow.groq.com/artificialanalysis-ai-llm-benchmark-dou... Seems to have it. Looks cost competitive but a lot faster. reply nabakin 10 hours agorootparentPeople are using throughput and latency differently in different locations/contexts. Here they are referring to token throughput per user and first token/chunk latency. They don't mention the token throughput of the entire 576-chip system[0] that runs Llama 2 70b which would be the number we're looking for. [0] https://news.ycombinator.com/item?id=38742581 reply SushiHippie 13 hours agoparentprevI guess it depends on how much the infrastracture from TFA costs, as the H100 only costs ~$3300 to produce, but gets sold for ~$30k on average. https://www.hpcwire.com/2023/08/17/nvidia-h100-are-550000-gp... reply neilv 15 hours agoprevIf the page can't access certain fonts, it will fail to work, while it keeps retrying requests: https://fonts.gstatic.com/s/notosansarabic/[...] https://fonts.gstatic.com/s/notosanshebrew/[...] https://fonts.gstatic.com/s/notosanssc/[...] (I noticed this because my browser blocks these de facto trackers by default.) reply sebastiennight 14 hours agoparentSame problem when trying to use font replacements with a privacy plugin. This is a very weird dependency to have :-) reply tome 14 hours agorootparentThanks, I've reported this internally. reply rasz 14 hours agoparentprevHow to show Google how popular and interesting for acquisition you are without directly installing google trackers on your website. reply Gcam 14 hours agoprevGroq's API performance reaches close to this level of performance as well. We've benchmarked performance over time and >400 tokens/s has sustained - can see here https://artificialanalysis.ai/models/mixtral-8x7b-instruct (bottom of page for over time view) reply eurekin 16 hours agoprevJaw dropping. Both groq and mixtral. I used following prompt: Generate gitlab ci yaml file for a hybrid front-end/backend project. Fronted is under /frontend and is a node project, packaged with yarn, built with vite to the /backend/public folder. The backend is a python flask server reply logtempo 10 hours agoparentAnd yet, it made a simple mistake in some python code :'( > particles = np.zeros((2, 3)) # position, velocity, and acceleration particles[:, 0] = [0.0, 0.0, 0.0] # initial position reply sebzim4500 18 hours agoprevSo this has nothing to do with `Grok`, the model provided by x.ai? EDIT: Tried using it, very impressed with the speed. reply tome 18 hours agoparentYeah, it's nothing to do with Elon and we (Groq) had the name first. It's a natural choice of name for something in the field of AI because of the connections to the hacker ethos, but we have the trademark and Elon doesn't. https://wow.groq.com/hey-elon-its-time-to-cease-de-grok/ reply terhechte 18 hours agorootparentCan't Chamath (he's one of your investors, right), do a thing there? Every person I pitch Groq to is confused and thinks its about Elons unspectacular LLM. reply tome 18 hours agorootparentYeah the confusion has happened a lot to me too. All I know is that it's in the hands of our legal team. reply fragmede 16 hours agorootparentprevI mean it sucks that Elon went and claimed Grok when you want Groq, plus you were there first, but getting stuck on the name seems like it's going to be a distraction, so why not choose something different? When Grok eventually makes the news for some negative thing, so you really want that erroneously associated with your product? Do you really want to pick a fight with the billionaire that owns Twitter, is that a core competency of the company? reply int_19h 5 hours agorootparentIf anything, getting in a very public fight with Musk may well be beneficial wrt brand recognition. Especially if he responds in his usual douchy way and it gets framed accordingly in the media. reply fragmede 5 hours agorootparentit'll be great for publicity, for sure reply kopirgan 11 hours agoparentprevThanks for asking just the question I wanted to ask! reply mtlmtlmtlmtl 17 hours agoparentprevThere's also a children's toy named Grok which uses LLMs to talk to the kid. reply matanyal 9 hours agoprevHey y'all, we have a discord now for more discussion and announcements: https://discord.com/invite/TQcy5EBdCP reply zmmmmm 6 hours agoprevAs a virtual reality geek, this is super exciting because although there are numerous people experimenting with voicing NPCs with LLMs, they all have horrible latency and are unusable in practice. This looks like the first one that can actually potentially work for an application like that. I can see it won't be long before we can have open ended realistic conversations with \"real\" simulated people! reply blackoil 1 hour agoprevIf Nvidia adds L1/2/3 cache in next gen of AI cards, will they work similar or is this something more? reply CuriouslyC 18 hours agoprevThis is pretty sweet. The speed is nice but what I really care about is you bringing the per token cost down compared with models on the level of mistral medium/gpt4. GPT3.5 is pretty close in terms of cost/token but the quality isn't there and GPT4 is overpriced. Having GPT4 quality at sub-gpt3.5 prices will enable a lot of things though. reply emporas 15 hours agoparentMixtral's quality is definitely up there with Gpt3.5. Specifically for coding, i consider them almost equivalent in quality. In fact Mixtral 8x7 is starting to be my go-to coding assistant instead of Gpt. It is fast, it is accurate, and i think i like his responses better than Gpt. Reducing LLM size almost 10 times in the span of a little more than a year, that's great stuff. Next step i think is 3 billion parameters MoE with 20 experts. reply int_19h 5 hours agoparentprevYou seem to be implying that Mistral Medium is on the same level as GPT-4? reply MuffinFlavored 17 hours agoparentprevWhat's the difference in your own words/opinion in quality between GPT-3.5 and GPT-4? For what usecases? reply CuriouslyC 17 hours agorootparentGPT3.5 is great at spitting out marketing babble, summarizing documents and performing superficial analysis but it doesn't take style prompts as well as gpt-4 and its reasoning is significantly worse when you want it to chain of thought follow a complex process while referencing context guidance. reply ukuina 17 hours agoparentprevI wonder if Gemini Pro 1.5 will act as a forcing function to lower GPT4 pricing. reply ComputerGuru 17 hours agorootparentIs that available via an API now? reply sp332 16 hours agorootparentKind of, it's in a \"Private Preview\" with a waitlist. reply sturza 16 hours agorootparentAnd in non EU countries. reply ComputerGuru 15 hours agorootparentprevVia GCP only? reply Zpalmtree 9 hours agoparentprevGPT-4 is overpriced vs what? reply 165 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The code establishes the width and height of a container to fill the entire viewport, utilizing flexbox for content centering and a background color.- It contains a function for parsing a JSON web token and employs an event listener to launch a Flutter entrypoint utilizing a service worker version.- The implementation combines responsive design techniques with web token handling and service worker utilization for an interactive and dynamic user experience."
    ],
    "commentSummary": [
      "Groq showcased the Mixtral 8x7B-32k tech demo, aiming to outperform Nvidia without requiring sign-up, sparking discussions on insider trading legality and Groq LPU scalability limits.",
      "Various topics covered hardware efficiency, challenges with large language models, cost-effectiveness, DRAM vs. SRAM in memory controllers, model quantization, compiler usage, low latency inference, self-attention mechanisms, and hardware support for attention matrices and efficient matrix multiplication.",
      "The discussion also touched on Groq's LPU architecture, API access, pricing, comparisons to Nvidia's hardware, and strategies for gaining industry recognition."
    ],
    "points": 705,
    "commentCount": 423,
    "retryCount": 0,
    "time": 1708344361
  },
  {
    "id": 39433451,
    "title": "Big Media Dominates Web with Subpar Product Recommendations",
    "originLink": "https://housefresh.com/david-vs-digital-goliaths/",
    "originBody": "How Google is killing independent sites like ours And why you shouldn‚Äôt trust product recommendations from big media publishers ranking at the top of Google By Gisele Navarro and Danny Ashton ‚óè Published on February 19, 2024 Google regularly launches updates to its algorithm to continuously improve search results quality. Think of these updates as a refresh of the system where rankings change: some websites see an improvement while others see a decline. At HouseFresh, we keep an eye on Google‚Äôs news and documentation because these updates can literally make or break our website. That said, we don‚Äôt write for Google‚Äôs robots and always make editorial decisions with our readers in mind. We know that at the end of the day, Google will reward us if our readers find our articles useful. Or that‚Äôs what we thought. You might have noticed that no matter what you google, there‚Äôs always a selection of the same publishers showing up at the top of the results: Source: How 16 Companies are Dominating the World‚Äôs Google Search Results What do BuzzFeed, Rolling Stone, Forbes, Popular Science, and Better Homes & Gardens have in common? They all know which are the best air purifiers for pet hair: Another thing they‚Äôve got in common is that they all also seem to know the best cooling sheets for hot sleepers: You could play this game yourself. Other searches you could try are: best gifts for mom, best home saunas, best beard products, best gifts for teens, best cocktail kits‚Ä¶ the list goes on. The problem is, for the most part, these publishers recommend products without firsthand testing and simply paraphrase marketing materials and Amazon listing information. In the last year, we have waited patiently for the many, many, MANY Google algorithm updates to impact these results. We were hopeful when Google introduced its reviews system with the Products Review Update back in 2021. It seemed they were finally doing something about one of the worst aspects of the modern internet: searching for information about products only to have to wade through countless reviews from people who had never even seen the thing. Two years later, SEO professional Lily Ray mentioned that (big media) publishers were hit hard by Google‚Äôs Product Review updates, prompting a response from Google itself: In our experience, each rollout of the Products Review Update has shaken things up, generally benefitting sites and writers who actually dedicated time, effort, and money to test products before they would recommend them to the world. That said, most searches for specific product models don‚Äôt just magically start with users searching for specific devices off the top of their heads. There is an immediate step before this: the hours of research reading through lists of product recommendations. If you have been reading HouseFresh for a while, your first encounter with us was likely a list like this one or this one recommending the best devices for a specific issue you were trying to solve. That is how most of our readers find us. Unfortunately, we‚Äôre getting less and less traffic from those pages, and it‚Äôs endangering the future of our site. That‚Äôs why we‚Äôre writing this article. Big media publishers are inundating the web with subpar product recommendations you can‚Äôt trust Uncovering the cookie-cutter system well-known magazines and newspapers use to trick Google Savvy SEOs at big media publishers (or third-party vendors hired by them) realized that they could create pages for ‚Äòbest of‚Äô product recommendations without the need to invest any time or effort in actually testing and reviewing the products first. All they had to do was say what they needed to say to pass a manual check if it came to that. So, they peppered their pages with references to a ‚Äòrigorous testing process,‚Äô their ‚Äòlab team,‚Äô subject matter experts ‚Äòthey collaborated with,‚Äô and complicated methodologies that seem impressive at a cursory look. Sometimes, they even added photos of ‚Äòtests‚Äô with products covered in Post-it notes, someone holding a tape measure, and people with very ‚Äòscientific‚Äô clipboards. There‚Äôs nothing wrong with wanting to show you‚Äôre doing the thing you‚Äôre supposed to be doing, but what happens when that‚Äôs as far as you go? Let‚Äôs look at one example. These are the current top 10 results on Google.com for a query we have completely given up on ‚Äî Best Air Purifier for Pets: Right now, the magazine Better Homes & Gardens is ranking at the top of the first page of results. At a glance, the article shows all the right things: If you were to keep scrolling, you‚Äôd also find photos of an air purifier inside of a tent, two more mentions of the expert Kenneth Mendez, and four mentions of their lab in Des Moines, Iowa. They say all the right things on the page and are a perfect example of a big media publisher with 40 different pages of ‚Äòbest of‚Äô product recommendations in the house cleaning section alone without a single in-depth product review: They mention that they have tested 67 air purifiers in their lab in Des Moines, Iowa, but somehow, they have published zero product reviews and they don‚Äôt make their test data available anywhere. They do have photos, with the majority of them being credited to Henry Wortock. Remember that name. ‚ÄúBut how do you know they didn‚Äôt test these devices?‚Äù Better Homes & Gardens never mentioned conducting tests prior to the Google Product Review Update in July 2022. You can see clearly here how, on July 6th, there were no mentions of air purifiers being tested. Fast forward to July 26th (one day before the announcement of the Google update), and they‚Äôre now saying they‚Äôve tested 38 air purifiers. Zero to over 30 devices in just a few weeks without any prior mention of any sort of testing. This is also the first time we see some original photos. That‚Äôs not all. Their air purifier recommendations are generally plagued by high-priced and underperforming units, Amazon bestsellers with dubious origins (that also underperform), and even subpar devices from companies that market their products with phrases like ‚Äòthe Tesla of air purifiers.‚Äô Any actual product testing would show these air purifiers to be a bad pick. What you hardly ever see in their recommendations are truly affordable and high-performing options, which should be a priority if you‚Äôre trying to help people clean the air in their homes. For example, Better Homes & Gardens recommends the Molekule Air Mini+ as their best option for small rooms: We have no idea how this device made the list considering that Molekule recently filed for bankruptcy, has active class action lawsuits for false advertising, has been recognized by Wirecutter as the worst air purifier they tested, and received the honor of being labeled as ‚Äúnot living up to the hype‚Äù by Consumer Reports. When we reviewed this device, we also found it to be one of the worst air purifiers we have ever tested for multiple reasons: It took 3x as long to clear our test room of smoke compared to units sold for a third of its price It‚Äôs incredibly noisy, generating 88 dB when running at its top fan speed ‚Äî equivalent to a leaf blower, which people use while wearing hearing protection! The filter replacement costs A LOT ($99.99), and you need to change it every six months, so you need to spend an additional $199.98 to use this air purifier for a year It pulls 61 watts at its top speed, which might not seem like a lot but it is considering you could buy units for a third of the price that won‚Äôt consume more than 21 watts We could go on forever. But we can also show you actual firsthand data to back up each of our reasons not to recommend this air purifier. Even some shallow desk research powered by Google would show that this product shouldn‚Äôt be recommended, but, hey, it‚Äôs $360, so it comes with a juicy commission compared to other better quality yet budget products. But let‚Äôs go back to the search results for Best Air Purifier for Pets and give Google the chance to rank a truly reliable list of recommendations that live up to their guidelines. Ranking below Better Homes & Gardens, we have Real Simple, another magazine most people would immediately trust due to their longstanding brand. When landing on Real Simple, the first thing you‚Äôll notice is how similar the site looks to Better Homes & Gardens. It uses the same template and has all the right signals to communicate that they test products for real: Another interesting thing is that the photos on this page are credited to the same photographer, Henry Wortock (remember him?) It even looks like the photos were taken in the same space: Now, Real Simple doesn‚Äôt mention a lab in Des Moines, Iowa, but they do say they acquired 56 air purifiers to test AND they named the same expert: Kenneth Mendez. Similarly to Better Homes & Gardens, there are no air purifier reviews on the entire site. That‚Äôs 56 devices that we just have to trust they actually tested and assessed. Many of you won‚Äôt be surprised by all this because you‚Äôll know that both companies are owned by the same media giant: Dotdash Meredith. That‚Äôs probably why both sites have the same design and feature photos from the same freelance photographer (Henry Wortock). It‚Äôs also probably why they couldn‚Äôt really be bothered with sourcing another expert to satisfy that specific point in their E-E-A-T checklist (more on that later). A deeper look inside Google Images shows how Dotdash Meredith is using photos clearly taken at the same time across different publications: Perhaps Dotdash Meredith did pay some lab to test all those devices across all those different websites, and it‚Äôs not just a collection of photoshoots their commerce writers can tap into when writing best-of lists. Whatever the case, it‚Äôs clear the team at Dotdash Meredith has worked out what Google needs to see in order to rank best-of lists in top positions without the need to actually publish insightful product reviews or share any evidence of original test data. But it‚Äôs not just Dotdash Meredith that we need to outrank if we want to recommend the actual best air purifiers for pets. Let‚Äôs scroll down to position number six and see who‚Äôs ranking right below Amazon and Reddit: Hello, BuzzFeed. There‚Äôs a lot wrong with this list, starting with the fact that it includes a whopping 22 air purifiers and clearly hasn‚Äôt even been curated. Similarly to most big media publisher recommendations, BuzzFeed also lists the Molekule Air Mini+. Reading through the list, we found the BuzzFeed team doesn‚Äôt even pretend to test the air purifiers. There‚Äôs no firsthand research other than curating a list of devices and images from Amazon.com: And then pulling in some reviews from Amazon.com as the bulk of their reasoning behind why they picked one air purifier over another: Some of you might be shocked to see BuzzFeed recommending air purifiers, considering this is far from what BuzzFeed is all about. Others might rightfully think this list must be some sort of joke, expecting to see AI-generated images of air purifiers hugging and feeding cats and dogs. The truth is that BuzzFeed has been struggling to maintain the ranking of this particular page, so it will probably drop off the first page of Google results eventually: But until that day comes, searchers will continue to land on that Amazon.com copy-and-paste page when searching for an air purifier to help with pet odor and dander. But hey, it‚Äôs BuzzFeed. They‚Äôre publicly listed and are the parent company of Huffington Post and Complex, so they obviously deserve to be there. Right after BuzzFeed, we‚Äôve got a Reddit thread that someone opened four months ago and has good discussions between Redditors: There are also lots of spammy replies, with this specific one at the top right now for those landing on that page without being logged into Reddit: When clicking on that link, users will then land on this incredibly sketchy website: Those of you with high attention-to-detail will notice that this introduction is a word-by-word copy of Real Simple‚Äôs article: Clicking on the profile of the Redditor behind this ‚Äòrecommendation‚Äô will lead you to a suspended account page: Somehow the user has been banned from Reddit, but their comment is still at the top of the thread ‚Äî we wonder how many other comments this user has published across different subreddits. Tip After their latest Helpful Content Update, Reddit and LinkedIn started ranking heavily in Google search results. If you want to find out more about Reddit specifically, you should read this article from Glen Allsopp. Private equity firms are utilizing public trust in long-standing publications to sell every product under the sun In a bid to replace falling ad revenue, publishing houses are selling their publications for parts to media groups that are quick to establish affiliate marketing deals At position #8, we have Popular Science, a magazine from 1872 that was sold to a private equity firm, North Equity LLC, in 2020. A year later, North Equity introduced Recurrent Ventures, a new arm of their business that runs all the media brands they acquired. A few months later, PopSci switched to an all-digital format. Two years later, in 2023, PopSci stopped being a magazine altogether. Of course, most people won‚Äôt know that because the site still feels like the PopSci we all know and trust: It doesn‚Äôt help that they have a ‚ÄòWhy trust us‚Äô section at the end of all their ‚ÄòBest of‚Äô lists that says: The vast majority of readers don‚Äôt know that the teams behind these product recommendations are far from the team of journalists and editors who built the brand behind the site. It‚Äôs unfortunate because many people will click on that Popular Science article expecting a trustworthy list of products only to find a list of units that haven‚Äôt even been tested by PopSci‚Äôs team: After reading the article, it quickly becomes clear that every air purifier on the list was selected (and ranked) based on anecdotal experience. Another thing worth mentioning about this PopSci list of air purifiers is that it‚Äôs a completely new page on their site that went live on December 29th, 2023. This makes it all the more strange for them to be recommending not one but two (2!) Molekule air purifiers. Surely, the writer who researched each device ran into the news of Molekule going bankrupt ‚Äî or the ridiculous reasons why they supposedly filed Chapter 11. Shouldn‚Äôt ‚Äòbest of‚Äô lists be treated as product reviews? Hint: yes, they should, but somehow they‚Äôre not In theory, ‚Äòbest of‚Äô product recommendation lists should be treated the same as any product review. Google‚Äôs documentation clearly states that: ‚ÄúReviews can be about a single thing, or head-to-head comparisons, or ranked-lists of recommendations.‚Äù So, shouldn‚Äôt Google be rewarding ranked lists of recommendations ‚Äúthat provide insightful analysis and original research [‚Ä¶] written by experts or enthusiasts who know the topic well‚Äù? Shouldn‚Äôt the reviews system ensure that people don‚Äôt end up landing on ‚Äúthin content that simply summarizes a bunch of products, services, or other things‚Äù? Perhaps we‚Äôre mistaken here, but we think the documentation says it plainly and clearly. Sadly, these are just empty words because Google has a clear bias towards big media publishers. Their Core and Helpful Content updates are heavily focused on something they call E-E-A-T, which is an acronym that stands for Experience, Expertise, Authoritativeness, and Trustworthiness. The SEO world has been obsessed with E-E-A-T for a few years now, to the point where there is always someone on X (formerly Twitter) discussing how to show experience, expertise, authoritativeness, and trustworthiness. Many of the examples come from dissecting big media publishers like the ones we‚Äôve been discussing in this article. The reason why SEOs look up to these sites is that Google rewards those sites: Interesting to see US News and Forbes benefit from this core update. US News is earning top positions for its \"cars\" and \"college\" pages, sometimes even outranking the brand itself for its own branded KWs. pic.twitter.com/qNkQy1HlT8 ‚Äî Lily Ray üòè is on vacation üå¥üßò (@lilyraynyc) June 6, 2022 And people have been pointing this out for months: Genuine question ‚Äì why are large sites like New York Magazine, CNN, Tom‚Äôs Guide, CNET, Forbes, and Business Insider not deemed unhelpful in the extreme when they write unrelated content purely targeting search engines. Let me give you my logic. Presumably Google ranks these‚Ä¶ ‚Äî Sean Kaye (@SeanDoesLife) October 24, 2023 Every single big media company (a.k.a. Digital Goliaths) is currently pumping up their bottom line with affiliate earnings. The strategy of Recurrent Ventures for Popular Science seems to be to squeeze as much money as possible from this type of ranked list of recommendations before things start to go south. You can see here how they have ramped up the pages inside of their ‚ÄòGear‚Äô section, which is where they house the bulk of their ‚Äòbest of‚Äô articles ‚Äî you can also see how their traffic has been declining since the latest Product Review update: The strategy of Dotdash Meredith for their publications seems to be to optimize resources and maximize profit. We might one day see the first page of Google results full of copycat recommendations once they roll out their hacks across all their websites, including Verywell, People.com, Health.com, Travel + Leisure, Byrdie, MyDomaine, The Spruce, Lifewire, Southern Living, TreeHugger, Parents.com‚Ä¶ and so many other top tier publications. Oh, wait, that‚Äôs already happening: So, where do we go from here? Google is killing independent sites like ours through inaction While this happens, investment firms and ‚Äòinnovative digital media companies‚Äô are selling you bad products HouseFresh has published over 60 hands-on reviews that were written on the basis of multiple performance tests. We can tell you that testing and reviewing products takes a lot of time, money, and effort. But if our small team can publish real reviews, then these big publishers and private equity surely have the resources to do the same. Unfortunately, right now, these companies are using all their resources to publish more and more pages peppered with the right ‚Äòingredients‚Äô to dish up a tasty E-E-A-T meal for Google. We have no doubt that these big publications could build their own labs, where they could run actual tests in order to make product recommendations backed by actual firsthand data. CNET bought an entire smart home back in 2015 in order to test products. That was before they were acquired by Red Ventures and found themselves selling the house after reporters told The Verge they were feeling pressured to change their reviews to be more favorable to brands that were being advertised on CNET. How many other ‚Äòparent companies‚Äô are using commerce or shopping editors to pass off promotional articles as editorial content? It turns out that what we‚Äôre seeing now is a result of the brainchild of Alicia Navarro, the founder and CEO of Skimlinks, a content monetization platform for online publishers. Navarro wrote a series of opinion articles and provided quotes to industry publications, who started introducing the concept of ‚Äòcomtent‚Äô in 2016: Someone who voiced their discomfort with the idea of e-commerce editors publishing ‚Äòcomtent‚Äô at the time was Brian Lam, founder of The Wirecutter. Months before being acquired by The New York Times, Lam explained that the Wirecutter had grown to become one of the most successful independent tech sites by publishing 20 to 30 articles a month that would take 30 to 200 hours to research and produce. He said he didn‚Äôt believe in a role like that of an e-commerce editor who would be pushing products. ‚ÄúI believe in talented editors on a beat who help people find related gear, not someone specifically meant to find things to push to readers,‚Äù Lam added. We are big fans of The Wirecutter, but we wonder whether their content would be as successful in the eyes of Google today if they were still an independent website. Luckily, they don‚Äôt need to worry about it because their site now sits inside the nytimes.com domain ‚Äî a trusted and well-established media brand. And it‚Äôs not just newer independent sites like HouseFresh that are losing traffic to big media publishers and their e-commerce editors. Long-standing websites such as GearLab have also seen their traffic decline in recent months even though they publish product reviews based on objective, independent testing: We can‚Äôt speak for the GearLab team, but this situation just isn‚Äôt sustainable. Many independent sites will go out of business if this trend continues. We hope to still be here to see things change That‚Äôs the reason why we‚Äôre writing this article A few months ago, Futurism uncovered how Sports Illustrated was publishing ‚Äòbest of‚Äô articles by fake, AI-generated writers. The magazine‚Äôs publisher, The Arena Group, washed their hands of it by stating that the ‚Äúarticles in question were product reviews and were licensed content from an external, third-party company, AdVon Commerce.‚Äù We wonder how many of these big media companies and investment firms are hiring intermediaries to develop their ‚Äòaffiliate marketing‚Äô arm, whatever the cost. These Digital Goliaths are utilizing their websites‚Äô authority and the public‚Äôs trust in their brands to sell every product under the sun. They‚Äôre buying magazines we love, closing their print operations, turning them into digital-only, laying off the actual journalists who made us trust in their content in the first place, and hiring third-party companies to run the affiliate arm of their sites. And while they do all this, they‚Äôre telling you to buy: Products from brands that are bankrupt and have class action lawsuits for false advertising against them Outdated, inefficient, and underpowered air purifiers that won‚Äôt actually clean the air in your home Overpriced devices powered by fancy marketing tactics that will perform as well as units half the price We appreciate how much information Google has shared about what a high-quality review is and about helpful content in general, but these guidelines need to be applied to everyone. These Digital Goliaths shouldn‚Äôt be able to use product recommendations as their personal piggy bank, simply flying through Google updates off the back of ‚Äòthe right signals,‚Äô an old domain, or the echo of a reputable brand that is no longer. As a team that has dedicated the last few years to testing and reviewing air purifiers, it‚Äôs disheartening to see our independent site be outranked by big-name publications that haven‚Äôt even bothered to check if a company is bankrupt before telling millions of readers to buy their products. That isn‚Äôt helpful content. Especially considering the work of air purifiers can‚Äôt be assessed by the naked eye. Users won‚Äôt be able to tell if their air purifier is actually working without subject-matter knowledge and the help of tools to measure air quality. That‚Äôs when actual testing and firsthand data become indispensable. If a magazine they trust tells them the Molekule Air Mini+, the PuroAir HEPA 14 240, and the Okaysou AirMax 10L Pro will help with their pet allergies, their asthma flare-ups, the air pollution that gets through the windows, the wildfire smoke blowing in their direction, the mold spores in their damp apartment, or recurring flu outbreaks in their school, then they‚Äôll go and buy one of those useless, overpriced units. Everybody loses but the investment firm. We‚Äôre talking about only one product here, but we imagine the issues highlighted in this article are rampant across every consumer tech product being recommended by these big media sites. Google won‚Äôt be the gatekeeper forever, but they are the gatekeeper now. The ball is in their court. About the author Gisele Navarro As the Managing Editor of HouseFresh, Gisele runs the day-to-day operations of our editorial team. She supports Danny and Ted with product tests, and works with the writers and producers to continually improve our content. She is also the human behind our X (formerly Twitter). Table of Contents Big media publishers are inundating the web with subpar product recommendations you can‚Äôt trust Private equity firms are utilizing public trust in long-standing publications to sell every product under the sun Shouldn‚Äôt ‚Äòbest of‚Äô lists be treated as product reviews? Google is killing independent sites like ours through inaction We hope to still be here to see things change Read More READ MORE Do air purifiers help with smells? The U.S. real estate markets most vulnerable to climate change Coway air purifier red light: What it means and how to fix it 16 ways to get rid of cooking smells The most humid cities in America and the world Can you use an air purifier with the windows open?",
    "commentLink": "https://news.ycombinator.com/item?id=39433451",
    "commentBody": "Big media publishers are inundating the web with subpar product recommendations (housefresh.com)599 points by beavershaw 14 hours agohidepastfavorite287 comments codexon 14 hours agoGoogle has been killing all but the most widely known domains for a very long time. I've mentioned this repeatedly on ycombinator multiple times, but only people who have made their own website 15 years ago and tried to grow it know what I mean. https://news.ycombinator.com/item?id=38923627#38933675 My recommendation is to start moving to some other closed platform that is not part of Google search like Facebook, Twitter, Youtube (yes I know Google owns it but its still not part of the search ecosystem). Tying your entire business to how high you rank on Google search is always going to eventually end up in disaster like this. reply granzymes 13 hours agoparentRead past the provocative title, and Google actually seems to be doing the right things here. They cracked down on product reviews that aren‚Äôt actually testing the product in 2021, and the article says big media companies (presumably with lower quality review content) suffered as a result. But then those media companies found a loophole with \"The Best X\" lists that weren't subject to the 2021 Products Review Update changes, which lets them continue spamming affiliate links while avoiding the new requirements. So now independent sites with actual reviews are in a holding pattern for these search terms, waiting for Google to bring the hammer down again on sites that are evading its quality metrics. This article is pretty clearly an open letter trying to bring attention to this issue. If the team at Google working on ranking for product reviews is reading this, I hope you have another update in the works to close this loophole. H1 planning just wrapped up! -- Edit: The title on HN has changed to be less click-baity. The original title was \"How Google is killing independent sites like ours\". Title aside, the article is quite excellent and does a great job of explaining the product review niche of SEO. Kudos to the authors. reply codexon 13 hours agorootparentI still believe the original title is warranted. The fact that Google has to manually step in to intervene or else the big domains get all the top rankings tells you that they are very heavily biased towards big media domains. reply permo-w 10 hours agorootparentdoes it? I would happily believe that Google is corrupt in this manner, but the reason big domains have the advantage here is because they can afford to pay teams of people with the express purpose of gaming the system. this is true in all industries, everywhere, and it can only be fixed with society-wide change, which, short of a world war (or, more likely, two), isn't going to happen reply codexon 4 hours agorootparentIt's not about being corrupt, it is about doing the lazy thing in order to fight spam. No one ever got fired for suggesting New York Times or Better Housekeeping. To me it feels like a big brand website hardly needs to try in order to rank #1 even without doing a bunch of SEO. reply lelanthran 18 minutes agorootparent> It's not about being corrupt, it is about doing the lazy thing in order to fight spam. No one ever got fired for suggesting New York Times or Better Housekeeping. > To me it feels like a big brand website hardly needs to try in order to rank #1 even without doing a bunch of SEO. To rub salt into the wound, any forum will happily let anyone and everyone post in support of any big brand, but instantly spam-block some lone developer trying to showcase their product (HN is different in this regard). Microsoft/Jetbrains/Apple releases a new paid product, available as a paid subscription only - dozens of people will get upvoted telling the forum about it. Some lone indie dev releases a free-tier search tool with optional paid tier, and they're banned for spamming. Now I understand why you'd want to block people who self-advertise, but there's gotta be a middle ground. Why is advertising on behalf of a company that has a larger ad budget than all the current readers salaries combined okay, but advertising for your own product is a bannable offence? There really should be a middle ground where (like on HN) the audience understands that someone who posts something that took them 6 months to create is not the same as someone selling love potions or stock tips. reply shostack 5 hours agorootparentprevWhat is the loop hole? reply cogman10 14 hours agoparentprevGoogle search has become worthless for me. I use bing instead because of how horrible google results are. On most searches, especially with my phone, the results are almost all sponsored and rarely what I'm actually looking for. Google search has gone from being one of the best to being ask jeeves at it's worst. reply FirmwareBurner 14 hours agorootparent>Google search has become worthless for me. Ditto. My most recent example, I asked Google what the thickness of the Pixel 8 is including the camera visor, which was something not listed in the spec sheet since the official dimensions sneakily only list the thinnest point on the phone, not the thickest. And Google proudly and confidently gave me the answer at the top ... but it was the thickness without the visor, something I already knew since that's in the specs everywhere. I looked through the other results lower on the page and nada, no correct answer. So I asked Bing and it gave me the exact answer I was looking for at the top measured by some Android review site. And man is that phone a tick boy in that spot. You can probably put your weed in there. Sure, that's sample size=1 so probably not an accurate test, but still, to me it feels like Google sucks for anything but the easiest context searches where it works because it knows a lot of info about me like where I live and where I work so it can correctly deduct the context, but for other shit not related to me, it's like you're drowning in SEO junk. reply politelemon 2 hours agorootparentFWIW I just tried \"thickness of the Pixel 8 including the camera visor\", and got a snippet from Google: > The actual dimensions of the Google Pixel 8 are apparently 150.5 x 70.8 x 8.9mm, with the thickness rising to 12mm at the camera bar. And nothing on Bing. reply mschuster91 13 hours agorootparentprev> So I asked Bing and it gave me the exact answer I was looking for at the top. And man is that phone a tick boy in that spot. You can probably put your weed in there. And the most annoying thing is, your phone will not. sit. flat. on a table, because the damn camera will always be unbalanced in height, which makes it an excellent attraction for feline companions. Tap on it and it wiggles. Tap harder, it wiggles more, and eventually the phone will fall to the floor, your feline will look at you with big round eyes and ask for f...ing treats. reply FirmwareBurner 13 hours agorootparentI went today to a carrier showroom where they have the Pixel 8 on display to mess around with, and I though the reviewers were exaggerating, but that damn visor is nearly as thick as the phone itself. It almost doubles in height of the phone at that spot. The Pixel 7a next to it had a much much thinner visor despite sharing a similar design. What the hell did Google put in that thing, lenses from the Hubble space telescope? It's not like they have a 100x zoom lens in that thing or a camera sensor so large it makes a Hasselblad wet itself. And that's before we get to the visible PCB screw heads poking through the OLED panel. On a phone that costs 600+ Euros. I feel like Google is at least 5 years behind the competition when it comes to HW and industrial design. Or they just culturally as a company don't give a shit about HW, thinking their SW is gonna be the main selling point and the HW is treated like some last minute \"who cares, just ship it, it's gonna sell anyway\" afterthought. reply whatshisface 11 hours agorootparentIt's still the only real option, all of the Chinese brands lock their bootloaders, and so does Samsung. reply mschuster91 9 hours agorootparentSamsung phones can still be rooted, at least outside of the US (the only place I see complaints about is when people buy Samsung phones in the US from a carrier). It is a bit annoying though that you have to link your phone to a Google and a Samsung account and keep it active and connected to the Internet for at least 7 consecutive days because of Samsung's anti-theft system. reply whatshisface 8 hours agorootparentIt's quite difficult to obtain EU SKUs for Samsung phones in the US, even without buying through a carrier. (If anyone knows an easy way to get them, let me know!) reply FirmwareBurner 2 hours agorootparenteBay? reply CarVac 5 hours agorootparentprevThe recent Pixels specifically don't rock, since the visor goes all the way across. reply Solvency 13 hours agorootparentprevThis is honestly a terrible example even if it is completely valid. You can't even get Google to find the most basic possible content about, say, oranges, without it being some SEO ad-infested fandom.com page about fruit, let alone product specifications. reply FirmwareBurner 13 hours agorootparent>This is honestly a terrible example I know, I'm not saying it was scientific, I was just sharing an anecdotal mainstream search query which I though was very relevant today for me and maybe others as well and also not super difficult for Google. reply JumpCrisscross 14 hours agorootparentprev> Google search has become worthless for me. I use bing I‚Äôve been thrilled with Kagi. It‚Äôs the first time in over a decade that searching became fun again. The Quick Answer feature (Kagi‚Äôs LLM) filters through SEO better than Copilot, and the results are noticeably higher quality than ad-based engines. At $5/month for 300 searches, it‚Äôs cheap to try out (both for experience and if you actually notice the search limit). reply notpachet 7 hours agorootparent> At $5/month for 300 searches Wow, that's a lower search count than I would have thought. I am pretty sure I'd blow through that in a day or two... reply reddalo 13 hours agorootparentprevThe main problem with Kagi is that it's a paid service with no free tier. I get their reasons for this, and it totally makes sense -- but that's also a big problem for their growth. I know very few people who would pay for a search engine. reply JumpCrisscross 13 hours agorootparent> that's also a big problem for their growth I agree, but it‚Äôs a good early filter for conversion. The difference in quality, for me and everyone I‚Äôve gifted a month to, is stark enough to make paying for search for the first time worth it. Given the absolute cost (for the cheapest tier, paid annually, less than $50) it‚Äôs a psychological hurdle more than a financial one for most Americans. Also, drawing those eyeballs from the ad-driven engines has a disproportionate effect on their marginal ad prices (in the long run). So if you need a sense of vengeance to get you over the hill, there you go. reply hedora 7 hours agorootparentRegarding ad margins: If kagi saturates the market of people that can afford to spend $50/year for a decent search engine, then Google ads will only reach people that cannot. This would greatly reduce the value of their ad inventory (far more than the percent market share they‚Äôd lose). reply swatcoder 13 hours agorootparentprevIf they can keep is sustainable and profitable without eat-the-world \"growth\", that's not a bad thing. There are few consumer products that have held up against the competing demands of billions users in thousands of different markets and cultures. I'd say there's maybe even been none. The kind of \"growth\" you're talking about is a bad but understandable habit among founders and cold financiers, but it's not a requisite part of running a business and generally runs counter to having a good product that serves a specific need well. reply lolinder 13 hours agorootparentprev> that's also a big problem for their growth. Frankly, I see this as a good thing. Maybe someone else will come along and solve the universal-search-engine-that-stays-good problem, but Kagi's best hope at being useful for me into the future is for them to stay where they are: tiny and used only by a small cohort of extremely savvy and skeptical geeks that aren't worth the effort to SEO-jack. They just need to be sustainable‚Äîgrowing large would actually be counterproductive. reply smsm42 3 hours agorootparentprevI didn't think I would. Then I tried it. Then I paid for the cheapest option because I really liked it. Then I paid for unlimited plan because I can't go back to crappy search after I tried the non-crappy one. And, thinking about it, why not pay for a good service? It costs less than cofee+pastry per month, and it improves the quality of my life. I think it makes sense. Some people may disagree, but as long as the service itself works, why would I care? reply OJFord 11 hours agorootparentprev> I know very few people who would pay for a search engine. It's actually maybe ChatGPT et al. that have done most to warm me up to the idea. I've tried Plus for a few months, basically using it like better search. I don't think I'll stick with it mainly because it's a pretty steep cost (enough that I want to go back to not having it for a bit at least, see how much of a problem it really is) - but it does make me wonder if perhaps Kagi can get me a lot of the way for half the price (the non-LLM tier). reply mrweasel 13 hours agorootparentprev> a big problem for their growth But do they need to grow to the size of Bing, Google or just DuckDuckGo? If they just want to grow a sustainable business, then it's a feature of their business model. reply reddalo 13 hours agorootparentNo I'm sorry. I don't mean \"growth\" as in infinite and unsustainable growth like VC-founded startups. I mean \"growth\" as in adopting a bigger market share. reply eszed 12 hours agorootparentBut, again, do they need to? It seems to me like \"market share\" is a metric relevant to companies pursuing VC-founded, unicorn lottery-ticket scale. If they generate enough revenue to pay competive wages, cover their operating costs, and make a reasonable (real-world, not VC-world) return on investment, they're a gosh-darn success. It's only within tech, where valuations and evaluations sailed off into ZIRP-ified bizarro-world, that people think of that as a failure of ambition or execution. I think it's time to re-assess our mental models. reply JumpCrisscross 11 hours agorootparent> do they need to? I don‚Äôt know what Kagi‚Äôs minimum sustainable size might be, but it‚Äôs probably bigger than what it is now. Particularly if they want to stay competitive with LLMs. reply eszed 10 hours agorootparentI suspect you're correct, and am rooting for them to hit sustainable, as soon as possible. I don't know what their maximum sustainable size is, either - that's equally important, though always a moving target. I only wanted to point out that neither inflection point, for a paid-service business model, has to do with \"market share\" - that's a VC thing, to which I'm increasingly allergic. reply internet101010 9 hours agorootparentprevI use Kagi for finding things and LLM for asking questions. Two different use cases. I want them stay separate but I am probably in the minority. reply carlosjobim 11 hours agorootparentprev> But, again, do they need to? Why not? There are tens of millions of people who need/want a high quality search engine and can pay for it. Kagi deserves to be successful for having made a better search engine than Google. And their success can inspire other entrepreneurs to start delivering quality information products, so that maybe we can get out of this ad/scam fuelled quagmire once and for all. Good products and ideas should be successful, that's progress. reply s1artibartfast 11 hours agorootparentThat is a complete non-sequitur from the question of if Kagi will die if it doesn't grow. This is orthogonal to questions of morality and justice. reply carlosjobim 10 hours agorootparent> This is orthogonal to questions of morality and justice. What? If Kagi doesn't grow, I fully expect the owner to eventually shut it down and move on to more fruitful ventures. Nobody owes anybody to keep a business running. So yes, it would die. reply s1artibartfast 9 hours agorootparentSorry If I wasn't clear. But I think the question is pretty clearly stated in the post you responded to. The question is: Does Kagi needs to grow to be sustainable, and if so, how much. >If Kagi doesn't grow, I fully expect the owner to eventually shut it down and move on to more fruitful ventures. If you had a business that made you a $1 million per year profit, would you shut it down just because it wasnt growing? Companies need to make a profit or they go out of business. However, most businesses don't need to continually increase users/ revenue to stay afloat. The coffee shop down my street is 100 years old, and didn't need to double in size every year. I agree that nobody is owed anything. I also think that Kagi is \"owed\" or \"deserves\" tells us nothing about how many users they need to keep staff paid and the lights on. reply carlosjobim 8 hours agorootparent> If you had a business that made you a $1 million per year profit, would you shut it down just because it wasnt growing? Yeah, I would. We have to remember that these are guys who beat Google at their own game. They beat a company of a monstrous size and revenue at their own game. With that kind of capacity, I don't expect them to be satisfied with a million a year in profit to share. I expect them to go as far as they can. If you're nobody special doing nothing special, then you can be happy with just needing to pay staff and keep the lights on. Like the coffee shop down your street, or my day job. But Kagi is clearly in a different category as a business. Last I heard Kagi needs to grow a little bit more from current user base to break even. reply s1artibartfast 8 hours agorootparentWell if their goal is to make money, they haven't beaten Google yet. Also, if you have a business that makes a million dollars a year and that's not enough, the typical solution is to sell it to someone else for 20 million or so instead of Burning It to the Ground reply akudha 4 hours agorootparentprevI know very few people who would pay for a search engine If Google Search continues its downward trajectory, people will start to pay for Kagi or some other similar search engine. 10-20$ per month for unlimited search is nothing, at least in the western world. We just haven‚Äôt reached that point yet reply chrischen 7 hours agorootparentprevThat‚Äôs honestly their loss. As long as Kagi can sustain itself with its paying members then it can silently retain and grow its users forever. reply seventytwo 9 hours agorootparentprevProblem? No. It‚Äôs a feature. reply zer00eyz 13 hours agorootparentprevOddly as a Kagi user, it does have a fault. It actually sucks at finding the low cost product. Want the cheapest esp32 c3... google is a better place to start. I can quickly find the \"price to beat\" and go deeper elsewhere. reply hedora 7 hours agorootparentGoogle and Kagi give the same top hit for ‚Äúcheapest esp32 c3‚Äù for me ($2.50, ali express). If I add a ? to the end of the query, Kagi additionally suggests an $11 reference board and a redit forum on the topic. reply EA-3167 13 hours agorootparentprevSame, I finally gave up and tried it after Google just stopped being remotely useful, and DDG is just a reskinned bing. A week on Kagi and I signed up for an annual plan, and never looked back. reply plumeria 13 hours agorootparentprevSomething as basic as: \"2,5 cm to mm\" won't show up the unit conversion widget if it's not formatted as \"2.5 cm to mm\", at least for me. WolframAlpha also fails at this query. However, ChatGPT understands it and gives the right answer. reply quatrefoil 9 hours agorootparentprev> Google search has become worthless for me. I use bing instead because of how horrible google results are. Although Bing is generally OK at dealing with general queries, it's far, far worse at surfacing niche content, no? My non-commercial, hobby homepage fares reasonably OK on Google (although some queries are dominated by SEO spam). But on Bing, it ranks below a good number of spam websites, including ones that simply copied my content and serve it with ads... reply a_wild_dandan 14 hours agorootparentprevI use an LLM for 80% of my queries now. Fighting Google isn't worthwhile, unless I need a trusted source. reply web3-is-a-scam 13 hours agorootparentHow do you know when your LLM is bullshitting you reply bmurphy1976 13 hours agorootparentYou don't, but at least you have one answer you need to verify vs 100 listings of random garbage to wade through. reply evilduck 13 hours agorootparentprevHow do you know when a Google result is bullshitting you, or if their pre-LLM AI summaries of results were bullshitting you? reply stevage 13 hours agorootparentprevWhat do you use? reply bkandel 13 hours agorootparentI use the Azure GPT-4 offering. It's not always 100% correct, but for technical questions in areas I'm not very familiar with, it's close enough. I can get much more done in a given amount of time than I would have been able to reading docs and SO. I know lots of people will point to examples where it's wrong, but I'd suggest trying it out yourself. If you're not intentionally trying to trip it up, it really works quite well. reply gotbeans 13 hours agorootparentprevI second this. The quality of google search has reached a point that's only good to search things that can't be bought. reply Scoundreller 13 hours agorootparent... and uncontroversial. Searches for \"coronavirus\" seem to be hard-coded, or interfered with. I get pages and pages of Covid-19 results, but that's not what I searched for. I even get a wikipedia link to its covid-19 page, but no wikipedia link to its coronavirus page within the first several pages of results. reply fortran77 13 hours agorootparentprevI use Bing, too. People are suprised when I recommend it, but for most general searches, it's quite a bit better. reply seventytwo 9 hours agorootparentprevI subscribe to Kagi, and it‚Äôs great. reply yazzku 12 hours agorootparentprevIn general, I find the same is true for all mainstream search engines, including DDG and Bing. You can't even search for things to buy anymore, ironically; it'll just dump you into Amazon or some \"top 10\" shitpost on CNN fake news or Forbes, much like the trash pit of websites shown in the header image of this article. Like others, I also find myself searching on Reddit or HN directly. What the point of a search engine is at that point, I don't know. reply aniftythrifrty 13 hours agoparentprevI am a small business owner who started their site and SEO and within three months I was beating multi-million dollar competition on the most important keyword google search terms for our market and industry. I did this with no budget, no adspend, just basic SEO and good keyword research. It's totally possible for mom and pop websites to get traction with google, even easy. You just have to be halfway decent at SEO. reply dazc 2 hours agorootparentI would guess you're working in a niche that has not been targeted by the big media sites alluded to here? Being halfway decent at SEO will come to nothing when you have 20 or so competing sites that can rank overnight for anything. reply whatamidoingyo 13 hours agorootparentprevSame. Numerous 1st page top results, and even snippets. Honestly no idea how... I do know SEO basics, but didn't know I knew them well enough for this. Within a year I had the first result for a very, very popular search term. Granted it was a lot of hard work (18 hour days, sometimes). reply Solvency 13 hours agorootparentYou spent 18 hours a day on SEO? Doing what?? reply whatamidoingyo 13 hours agorootparentNo, no. I spent 18 hour days posting on social media, writing articles, designing cover images, researching, etc. I honestly didn't do much work in regards to SEO, at least I don't think so. (There were a few times I knew something was going to be released soon, so I wrote about it before anyone else did.) But I do believe all of this contributes to SEO. But yeah, this brought my site to having every article I write today to be listed on page 1 (top 5 results, at the very least) almost within a day, numerous snippets, etc. reply kjkjadksj 12 hours agorootparentI wonder how easy this will continue to be in the chatgpt era. What takes you 18 hours can probably be generated in 18 seconds with enough fidelity to get traction. reply throwaway2037 2 hours agorootparentprev> It's totally possible for mom and pop websites to get traction with google, even easy. You just have to be halfway decent at SEO. If this is really true, then you should be running your own SEO consulting business. You would undoubtedly make much more money than your existing (presumably not SEO consulting), small business Comments like this are similar to people (ahem, Internet randos) talking about their investment portfolio returns, where they wildly exceed the very best professionally managed hedge funds. I always say: \"If you are so good, why don't you run your own hedge fund? It will easy to get funding.\" > reply 15457345234 13 hours agorootparentprevWhat's the site and what keywords are you targeting? reply dazc 2 hours agorootparentParent poster is, I hope, wise enough to not answer. reply carlosjobim 13 hours agorootparentprevSame here. I'm right up there competing with billion dollar companies with decades of presence and I don't know how many backlinks. While my small business has no backlinks from others and only relies on content for ranking, being entirely dependent on Google to be honest. reply kjkjadksj 12 hours agoparentprevThe same issues you have with google search engine optimization are present in every other closed platform too. Welcome to the attention economy, you better learn how to go viral. reply rahidz 13 hours agoprevIt feels like sometime in the past decade, Google search results went from \"Here's what most people click on\" to \"Here's the most trusted sources, handpicked by Google\". WebMD, Wikipedia, CDC, etc. for health results, the NYT, CNN, BBC, etc. for news, major magazines/newspapers for reviews. Which makes sense from a corporate perspective, you don't want your users searching for something controversial and stumbling upon something that doesn't line up with the mainstream POV. Maybe \"Bob's 10 best mattresses\" is a thorough and exhaustive article that easily beats the rest, but what if Bob is antivax, or thinks Bush did 9/11? It's safer to just ignore small blogs like Bob's and not risk any controversy. And here's the side effect. Some of these organizations realized \"Wait, we rank really high on Google for anything! So let's pump out shitty listicles about the top 10 air purifiers, even though we're a tech company, and fill them up with expensive affiliate links. We're 'trusted', after all.\" reply nostromo 12 hours agoparentYes, exactly -- Google is the new Yahoo. It's no longer about training a great algorithm to find great results -- but hand-selecting the most anodyne, least interesting results for everything using a small army of human and AI reviewers. Not to mention how it ignores half of your query terms for no appreciable reason. The ultimate irony now is that Google's ads are usually more relevant than their organic search results -- because they actually care about the ad experience. reply rockskon 9 hours agorootparentWhat? Malware ads masquerading as legitimate websites are common and Google hasn't visibly done much to combat them. reply jstarfish 10 hours agoparentprevJust use Yandex. When your \"trusted\" intelligence can't be trusted, you might as well see what the other side has to say. reply cowpig 10 hours agorootparentSeeing the world in \"sides\" makes it hard to discern truth from propaganda. reply jstarfish 9 hours agorootparentOnly ever hearing a single one of them is propaganda. reply RamblingCTO 1 hour agorootparentGoogle is not a side though. It's a tool and what you see depends on how you use it, doesn't it? Odd thinking you have there. reply ado__dev 14 hours agoprevFinding trustworthy reviews and recommendations via Google is useless. The first few pages are always littered by the lowest quality, highest SEO-spam content, and the recommendations on these pages are so shallow and inauthentic that I know the person that wrote the article has never even looked at the product they're shilling. And so often these lists are literally the same list of 20 products slightly re-arranged. Reddit is also really hit and miss, depending on the community. TikTok has been ruined by TikTok Shop. Small YouTube channels seem to be where it's at for now - but even then it's sometimes hard to tell if it's an honest review, or a paid video, and YouTubers do a terrible job disclosing paid promotion/free products. There surely must be a better option. reply sharkweek 14 hours agoparent> Reddit is also really hit and miss It took savvy SEO folk about .3 seconds to figure out that Google was ranking Reddit for almost any informational query and start trying to game the system there too. I love using Reddit for information but be wary of any new Reddit thread ranking well in Google search that's only a few months old, in a small community, with very few other responses besides a strangely specific answer to the question. reply jstarfish 11 hours agorootparentReddit is highly subjective. In shopping for flashlights, the respective subreddit recommends only obscure AliExpress brands. The community are retiree collectors who obsess over specs and cannot possibly use them in the field. Availability of parts and removable/disposable batteries are never a consideration in their recommendations, for example. What throws the most lumens is the only factor they concern themselves with; at a certain point you can't even see anything outside your own beam. They shit on all \"American\" brands (but Coast is shit). It's hilarious watching them drive off clueless gift-givers seeking advice. reply Panzer04 7 hours agorootparentAre we looking at the same subreddit? Perhaps it's changed since I last looked, but most recommended lights use a replaceable, rechargeable cylindrical lithium battery. A common requirement/desirable feature of lights is good light output controls (such as ramp firmwares), waterproofness, etc. There's plenty of recognition that different lights are not ideal for everyone, and if anything the brightest lights are seen as too much for some (given they can literally burn holes in your pocket..) I can't say every post is legit, and I'm sure there's a fair share of marketing posts posing as real users, but there's good reason IMO to recommend the lights they do. There's a lot of stuff that comes out of China that's just as good if not better than the American brand for half the price, if not even less. That being said, it's an enthusiast subreddit, as all such things are, so if you're looking for a \"casual\" recommendation it's likely to be more than you need XD reply Liftyee 5 hours agorootparentprev\"American business-hating lumen chasers\" is an incredibly shallow portrayal of that flashlight community. I've found flashlights to be one of the few product areas where \"obscure AliExpress brands\" actually outperform Western equivalents. > Availability of parts and removable/disposable batteries are never a consideration in their recommendations That's because it's taken for granted that nearly every recommendation uses one of a few standard cylindrical battery sizes, which are trivially user replaceable. Only a few brands use non-standard or built-in batteries. Often the electronics or LED emitters can be modified or replaced, since most lights fit a simple \"cylindrical tube\" formula. > What throws the most lumens is the only factor they concern themselves with I guess all those discussions about tint (subtle colours of the beam), CRI (how well colours are represented), beam shape, optics type, user interface, etc. were nothing then. (\"Big number of lumens\" is far from the only factor - sometimes not even the most significant one.) Granted, not all that information is useful for a newcomer who just wants a decent light. But the buying guide on the wiki sums it up simply enough. What is hilarious are the people selling those same AliExpress lights at huge markups (search \"Goonbeam\"). Quality and price are often linked, but far from the same. reply cogman10 14 hours agoparentprevI've noticed that a large portion of reviews are literally just rehashing reviews on amazon. And, if I were to guess, a good number of them are just these review sites pumping in \"top 10 x reviews from amazon\" into chatgpt and having it write their review for them. > YouTubers do a terrible job disclosing paid promotion/free products. The trick I think I've found for this (which isn't fool proof) is to find videos where the youtuber is actually physically interacting with the product. Doesn't work for everything, but in a lot of cases the paid promotional reviewers aren't getting their hands on the product in question and instead they are putting up stock images and reading the marketing material. The bigger the youtuber, the harder it is to know if it's a paid promotional thing. reply michaelt 13 hours agorootparent> The trick I think I've found for this (which isn't fool proof) is to find videos where the youtuber is actually physically interacting with the product. IDK, there's a long tradition of shill reviewers being given free products \"for testing\" on the unspoken agreement that if the review is bad, they won't get more free products in the future. reply singron 12 hours agorootparentYeah if they have no negative reviews, that's a bad sign. A particularly scrupled YouTuber I follow typically won't do a paid video if the product isn't good and instead does an unpaid tear down video. That probably limits his opportunities to brave marketing teams with high quality products, but it also makes his reviews quite valuable. reply callmelalo 11 hours agorootparentWhat Youtuber is that? reply class3shock 14 hours agoparentprevIt depends on what you are looking for. I found looking at the BIFL subreddit, sites that cater more towards industry (McMaster Carr as an example), and companies based in Europe (Fjallraven as an example) can help find higher quality products faster (or finding items on there and then searching reddit/forums for \"alternatives\"). Sometimes it just feels impossible though E.g. trying to find various items for the kitchen that are better than the crappy import stuff sold everywhere but not ludicrously expensive for a low use item. reply evilduck 12 hours agorootparent> trying to find various items for the kitchen that are better than the crappy import stuff sold everywhere but not ludicrously expensive for a low use item. If you're wanting BIFL kitchen items for low use try looking for commercial foodservice versions. That stuff is generally priced between plastic throwaway versions and Williams Sonoma but if it's built to survive at least a month in a busy professional kitchen, it'll probably serve me for life. Alternatively, head over to your nearest ethnic grocers. I have some Asian and Mexican grocery stores near me that have kitchen supply sections that stock no-frills but reasonable quality versions of kitchen tools. My nearby standard American grocery stores stock much lower quality items by comparison. reply radicality 5 hours agorootparent+1 on buying from commercial suppliers for even home kitchen stuff. I‚Äôm a fan of webstaurantstore.com . Prices are good, and you can buy stuff that will last you forever in a home kitchen which usually isn‚Äôt available in normal stores (eg Cambro containers). reply mietek 13 hours agorootparentprevDo you happen to know a McMaster-Carr equivalent based in Europe? reply class3shock 10 hours agorootparentI do not but if you have a STEM club at a local school or a nearby university with a mechanical engineering program they would be able to tell you (assuming one exists). reply kjkjadksj 12 hours agoparentprevEvery review on youtube is paid placement for the most part. The exception is if you find a real user who will post some crappily shot video and never step in frame themselves, those are always the highest quality reviews yet its rare and below the fold because people do it out of their own freetime and goodwill and aren‚Äôt trying to make a hustle out of it (which means accepting paid review offers). reply quatrefoil 9 hours agorootparentYeah, YouTube is absolutely dominated by paid product placement, especially for stuff like power tools. That said, YouTube reviews at least tend to be real in the sense that at least the reviewer is actually using and demonstrating the product, which is a huge step up from the \"we summarized some Amazon reviews for you\" SEO spam. reply Quothling 13 hours agoparentprevI know I buy sort of expensive products, but most of the things I've bought recently like my christiania bike all have youtube channels detailing their products. I think that is frankly the only real way for brands to advertise to people like me who'll maybe look at reddit threads or similar, but these days you can barely even trust many of those. We bought a Baby Brezza based on recomendations, they have a semi decent youtube with a mix of useful information and advertisement. A good example of the reddit bit is the robock s8 we bought. 95% of the reviews on reddits tell you to buy the big version with the huge dock... But then there was this one person in one thread who posted about how it was easy to just empty it without the station and that the station was known to rot or mold (not sure how you say that in english). So we bought the smallest s8 version we could and whoever that redditor was, they were absolutely right that it was so easy to maintain it without any of the addons. Roborock doesn't have a good youtube channel, they do have one, but it's really just advertisement. Anyway, I agree with you. I don't even really use google anymore. I switched to ecosia (it also sucks) out of spite, but it's been as good as google for anything except for when I want to do site:blabla.com in which case I'll !g. Before you recommend it I've used the duck before and it doesn't work for me. Likely because I'm Danish. reply Solvency 13 hours agorootparentReddit is 99% schill bots. Heaven help you if you're researching baby products. The entire scandalous baby product market has commandeered Reddit with accounts like this one that I found just because I kept seeing the name pop up relentlessly hawking the same products: https://www.reddit.com/user/ErinElizabeth1187/ reply encom 13 hours agorootparent99,9% of the time, a username in the format NameNameNumber is a bot. The probability goes up as the value of Number increases. reply crote 10 hours agorootparentThis used to be the case, but at some point in the last few years Reddit started suggesting usernames like that to new human users. It'll generate stuff like \"Fine_Ad6357\", \"Unhappy-Benefit8521\", and \"SuccessfulShape2454\". I believe they started doing this because Reddit got really popular and all the normal usernames were eventually taken. I wouldn't be surprised if they implemented this after noticing that a significant number of signup attempts were aborted due to multiple attempted usernames already being taken. reply AtlasBarfed 14 hours agoparentprevIt's called \"Consumer Reports\" / Consumers Union. That's what it look like. The only thing that could enhance or replace it would be official government testing of products. reply brucethemoose2 14 hours agorootparentBut people need to use it. Critical internet/app browsing should be taught in school, like critical reading. I feel lucky to have been a nerd in the 2000s where people picked up this skill, but honestly I have no idea how kids, older folks just getting into tech and such are acquiring sources/skills trapped inside of Discord, YouTube, Facebook or whatever. reply nebula8804 14 hours agoparentprev>There surely must be a better option. Maybe Consumer Reports? Only complaint I hear about them is Tesla fanboys complaining that the cars are not getting perfect scores and that its a conspiracy. Not sure if there is any truth to that(probably not). Other than that I haven't heard much bad to say but who knows, they could also be compromised. reply JoshTriplett 13 hours agorootparentOr rtings for any categories they review. reply ado__dev 13 hours agorootparentrtings is really good and in-depth. I have used them as a gut check many times and they haven't let me down yet. reply ado__dev 13 hours agorootparentprevI did actually buy a year long subscription of CR when I moved into my new house a few years ago and I found their reviews to be generally more helpful and have bought a few products based on their recommendations. reply Solvency 13 hours agoparentprevPeople like to shit on Nextdoor but once I embraced it as a homeowner it's my go-to for everything. Fuck Google/Yelp for reviews. It's refreshing getting local first-hand reviews and recommendations from neighbors about plumbers, roofers, electricians, solar panel experiences, tax stuff, home security camera questions, etc. Having a local authenticated community is so refreshing compared to the corporate bot infested internet. reply Scoundreller 13 hours agorootparentJust don't ask for realtor suggestions. Your inbox will never be the same again. And everyone is a realtor or related to the one that does the best job... reply kjkjadksj 12 hours agorootparentIt blows my mind why sellers would even need a realtor in hot markets. Your home will get a dozen offers in a week as soon as its put up for sale, you don‚Äôt need to burn 5% and do all the bullshit ritualism like staging or aerial photography that people are paying for. reply zeroonetwothree 6 hours agorootparentStaging can certainly increase the offer amount by more than it costs reply frankish 4 hours agorootparentMore than 5%? reply 01HNNWZ0MV43FF 13 hours agorootparentprevAh and there's the example of a service that's local, organic, home-grown, small business, crunchy, and also thoroughly paid-off. To complement my sibling comment that \"local\" is not the deciding factor reply 01HNNWZ0MV43FF 13 hours agorootparentprevKnowing there's an unpaid human writing the review is about the only thing that matters. I guess for repair services it has to be local, but the real point is, if I get a recommendation from friends or family, I can trust that they aren't affiliates, because they're staking the relationship on their review.> in dealing with advertisers you must remember they are professional liars. I don‚Äôt mean this to offend. I mean it as a job description. An advertiser's job is to convince you to do stuff you would not otherwise do. reply cableshaft 13 hours agorootparentprevThere's a couple Facebook groups for residents for the city I live in and I've found them useful for the same reason. I should also start checking NextDoor more, thanks for the mention. reply ryandrake 14 hours agoprevIt's always struck me as very risky to have a business that is utterly dependent on the actions/policies of a separate business with whom you have no formal business relationship. This is just a risk that one acknowledges when they decide to go for it. If I ran an eBay store, I'm totally dependent on the whims of eBay, and my business plan should include the risk that they can do anything they want--up to and including kicking me out. Same if I had a business that ran off of Facebook. Not taking sides here or saying anyone is right or wrong, but it's reality of operating on the Internet that small businesses probably just have to go into with both eyes open. Personally, I wouldn't want to be in the situation where my revenue could dramatically go up and down purely because Some Company X making some kind of routine algorithm change. I wouldn't be able to sleep. reply cptaj 14 hours agoparentThis is a serious problem. Internet marketplaces are so big now that its really hard to even have a business without them at all. I think that after a certain size, these marketplaces should be regulated to insure due process between the parties. That way the whims of the marketplace owner can't destroy thousands of prosperous businesses at the push of a button. We have similar regulations for utilities. The power company can't kick you out on a whim. I think the same rationale applies here. reply sofixa 14 hours agorootparentThat's exactly the thinking that led to the Digital Markets Act in the EU. Those marketplaces are effective monopolies or oligopolies in their space, so access to them needs to be regulated to ensure a level playing field. reply bemusedthrow75 14 hours agorootparentAhh yes, the Digital Markets Act. That thing that HNers rail against as the uncompetitive doodlings of quaint groups of people who insist on speaking different languages, paying decent wages and self-governing in the face of the obvious virtues of limited government, low regulation, and sweet, sweet disruptive libertarianism. reply sjwhevvvvvsj 13 hours agorootparentAhh yes, another premature declaration of success by the EU, who haven‚Äôt properly enforced ePrivacy, GDPR, or a host of other regulations. The EU is all talk and posturing, you can write any law you want but the tech companies already figured out compliance is optional. reply sofixa 13 hours agorootparentApple have already started using USB-C in iPhones, have already announced how they'll allow apps to be installed on iPhones without their App Store. Google have stopped shoving Google Maps on Search results. Many companies, including Google, Facebook and similar, have changed how they do things because of the GDPR, and have been fined for not complying. reply sjwhevvvvvsj 13 hours agorootparentThe FTCs fine on Facebook from a few years ago exceeds the combined value of all GDPR fines levied to date. The Irish government fills its coffers with the largesse of tech companies that are headquartered there specifically to dodge taxes and regulation. Likewise, while GDPR has some initial changes in privacy after many months of inaction tracking levels started to rebound because everybody figured out nobody was going to enforce it. reply crote 10 hours agorootparent> The FTCs fine on Facebook from a few years ago exceeds the combined value of all GDPR fines levied to date. Which is quite ironic, because when the GDPR was introduced just about everyone on HN adamantly insisted that one-person startups would be getting a ‚Ç¨20M fine every time they made the slightest mistake. reply bemusedthrow75 13 hours agorootparentprev> The FTCs fine on Facebook from a few years ago exceeds the combined value of all GDPR fines levied to date. This isn't anywhere near as significant a point as you think. reply sjwhevvvvvsj 11 hours agorootparentThe fun thing about hacker news is that for all you know I may be directly involved in these issues and speak from direct first-hand, but highly confidential, knowledge. Or I‚Äôm just some moron, posting opinions with absolutely no basis in fact. Only I can be sure! reply bemusedthrow75 11 hours agorootparentAnd yet either way it still would not, IMO, be as significant a point as you think. reply sjwhevvvvvsj 10 hours agorootparentAh, but you are be very wrong. reply smoldesu 10 hours agorootparentGiven that we have to take you on your merits as an entirely anonymous source, I think they're actually very right. reply bemusedthrow75 53 minutes agorootparentI am assuming that the parent poster worked in Google‚Äôs ad tech or analytics unit. Perhaps another FAANG. Perhaps Facebook. Still: that doesn‚Äôt make the assertion about the size of Facebook‚Äôs FTC settlement and all GDPR settlements as significant as they think, for a bunch of important reasons that start to be obvious once you break it down. The parent poster could be a FAANG CTO, and it wouldn‚Äôt change that it‚Äôs an apples and oranges comparison. (IMO it‚Äôs a good natural first principle not to trust anyone who has worked in analytics or ad tech who claims secret knowledge and is arguing from authority. Or, one could save a lot of time and simply distrust them regardless of such a claim) reply bemusedthrow75 13 hours agorootparentprevI don't see where I declared success. And I'm not even in the EU. reply AnthonyMouse 8 hours agorootparentprev> I think that after a certain size, these marketplaces should be regulated to insure due process between the parties. The solution is not for them to be big and regulated, it's for them not to be so big. The main thing that would help here is to inhibit vertical integration. For example, suppose people had a legal right to pricing information. Companies like Amazon and eBay would be encouraged to provide an API and have no right to stop anyone from scraping their site for anything it doesn't provide. Now anyone can make a product search engine that will show you results from any site. You're not stuck with Amazon's gawdawful search. And since anyone can do this, it's easy to enter the market and none of them will have dominance. Conversely, if you want to start a new retailer, or sell your own products directly from your own site, you just submit your site for indexing to the popular product search engines and customers appear. But none of the search engines can destroy you because there are dozens of them and the biggest one is only 15% of the market. We need more competition. The target of the rules should be to lower barriers to entry. reply 15457345234 14 hours agorootparentprevI've argued this before; these companies have taken on a utility role and need utility-type regulation, i.e. an obligation to provide service fairly and universally, an ombudsman, viable oversight, physical presence, a local call center to provide local employment and to give back to the community, etc. This situation where 100% of the taxi and food delivery profit from every small town in the world gets siphoned off back to a single office in California just isn't viable. Even from a within-US perspective it isn't viable. reply carlosjobim 13 hours agorootparentprevAs always expected in the HN comments: \"More government regulation\". Depending on Google for your business might not be ideal, but before these free online marketplaces existed, you would only be able to have a business and compete if you had the right political contacts or were born into the right family. Do you think a regulated online marketplace would let anybody set up shop like Google does? In order to be allowed a web domain you'd need 5 government certifications, a credit note for a million from the bank, membership in the local chamber of commerce, etc. The door would be locked and welded shut for everybody except those with the right contacts and financial backing. And those are not always the people who create the best businesses ‚Äì as anybody on this particular forum should know. > destroy thousands of prosperous businesses The marketplace owner created these thousands of prosperous businesses to begin with. Regulators did not. reply Sabinus 8 hours agorootparentLook we all know governments can do some crazy things and regulatory capture is awful, but you libertarian types really have some strange ideas about regulation. reply carlosjobim 8 hours agorootparentHalf of this forum would be out of a job if it wasn't for Google, but still they down vote my comment and demand government regulation. They have no clue what they're in for. While Google has it flaws ‚Äì big flaws ‚Äì at least for the most they give anybody a chance to compete in the results ranking. A government regulated search engine or web portal won't be anything like that. reply troupo 34 minutes agorootparent> Half of this forum would be out of a job if it wasn't for Google Hahaha wat. > still they down vote my comment and demand government regulation. They have no clue what they're in for. Oh, we do have a clue. We have close to two centuries [1] of data to draw from. Corporations and big companies will always go for the worst possible things and are only reigned in when the government steps in. Without fail. > at least for the most they give anybody a chance to compete in the results ranking. No, they don't. And they haven't done this for a very long time now. [1] Well, more. Government regulations are as old as governments, but let's take the last two centuries as something resembling more modern capitalism. reply BlueTemplar 3 hours agorootparentprevIt would be probably better to just ban the ¬´too big to fail¬ª ones. Regulations tend to only help them at the expense of smaller competitors. reply bemusedthrow75 13 hours agorootparentprevThis is a Poe, right? reply bemusedthrow75 14 hours agoparentprev> It's always struck me as very risky to have a business that is utterly dependent on the actions/policies of a separate business with whom you have no formal business relationship. Right, but the \"separate business\" here -- in a real-world analogy -- is akin to a commercialised offshoot of the department of transport. They may not make the roads, but they decide what goes on all the maps, they control most of the road signs, they benefit from the traffic monitoring data, and if you were to open up a shop selling only advice on where to shop, they determine whether your shop can be seen behind their signs. They profit from how they manage this, and the only way to get better management is to pay for it. Everyone pays for it, so the advantage dissipates until you pay more for your signs. There is little to no way to do business without these people, short of setting up a stall at the local covered market or farmer's market (Amazon, eBay, Facebook, Etsy) where you are beholden to another set of signage issues as well as the secondary knock-on effects of large-scale signage issues on the way to the market, over which you have even less influence. Beyond that: it's literal word of mouth. reply grey_earthling 13 hours agorootparent> Beyond that: it's literal word of mouth. This is the key. If you have enough enthusiastic, loyal, (rich and/or generous) devotees, then you can make a living from their donations (e.g. Liberapay) or subscriptions (e.g. Patreon). If you're doing something worthwhile or even just fun, you've probably got some. But if you don't ‚Äî and this going to sound harsh about a labour of love ‚Äî then evidently other people aren't (yet!) willing to pay you to focus solely on it. Maybe there's enough to cover some or all of the costs, or even make a surplus (but not a living), and you can carry on as a hobby/part-time/side-project. But for the thing to continue existing, someone (maybe you!) has to care about it enough to pay for it, and Google certainly doesn't. Google doesn't know anything about the unique service you provide; it only knows about the words on your website, and it can get those same words ten-a-penny from other websites. If Google's killing your site now, that means Google's been keeping it on life support since‚Ä¶ whatever your previous strategy was. They're selfish money-getters, they never promised you page views or ad revenue, and you're not useful to them any more. reply pixl97 9 hours agorootparentGoogle can still kill your site even if you're a word of mouth, pateron, liberapay funded site... That is by having scammers feed of the keywords of your product and selling shitty bullshit/scams siphoning the people that were told 'word of mouth' yet use browsers like chrome. reply BlueTemplar 3 hours agorootparentprevPatreon has a similar issue : they've banned some people for political reasons. reply duped 14 hours agoparentprevWhat if eBay were the only way to sell your goods on the internet? Because that's what the problem is with search - if Google doesn't weight your page high enough in results you're screwed. There's no other game in town. reply Scoundreller 13 hours agorootparenteBay already acts like it still is the only place to sell goods on the internet. But I'd say eBay was actually better back when it was the only mainstream way/place to sell many goods on the internet. eBay now sells promoted rankings. Funny when a vendor selling 1 product has it listed at several different prices, so you can save some money by finding the lowest priced one in their \"other listings\" that they don't promote. eBay sells Google ads on its pages. (sad seller noises). and eBay is one of the biggest ad buyers on Google. reply internet101010 8 hours agorootparentAnd that's why I utilized eBay's api to create a better, curated version of their site for personal use. On average my filters end up removing like 90% of the listings for various reasons. All I see is the stuff that is actually worth considering now. It's great. reply hakfoo 7 hours agorootparentprevI find eBay's optimizations have made it somewhat toxic. Once you get to a product page, any further navigation of the form \"similar/related products\" is sponsored listings only these days. They've narrowed a catalog that might have 1,000 relevant products into a pinball bouncing you between the same 12 sponsored listings. It's easy to figure \"this is all they've got\" and move on. The model probably works okay for rebadged Alibaba tat with no meaningful differentiation; the 200 sellers with the same widget can be forced to bid against each other for visibility. But for the classic line of \"eBay as the world's garage sale\", the last thing you want is to deliberately narrow your visible inventory. Customers are here for variety and the obscure, and restricting the market to \"that which we can get placement revenue from\" eventually drains them away. reply codexon 13 hours agoparentprevIt is very risky but making your own website and having it be easily found should be the way the internet is supposed to work. One shouldn't have to make a youtube channel, constantly tweet, and manage a facebook group when a single website should have sufficed. reply reddalo 13 hours agorootparentI miss that old Internet. Nowadays it's all social shit. reply johng 13 hours agoparentprevAlmost any site on the internet is going to depend on Google. It's too big and too important. They have a monopoly, I'm amazed it hasn't been broken up yet. reply circusfly 13 hours agoparentprev\"I wouldn't want to be in the situation where my revenue could dramatically go up and down purely because Some Company X making some kind of routine algorithm change. I wouldn't be able to sleep. \" If we compare to a small business on a highway and a company decides to move the highway so far away from them as to diminish their customer base effectively driving them out of business; this can't happen, since roads are governed as public resources. This is what the Internet needs. reply Groxx 13 hours agorootparentRoads do in fact change, and traffic patterns and access costs change with them. Lots of small towns are intimately familiar with this, booming or busting because of a new major road nearby. They change more slowly than internet traffic and are less globally impacting than a gigacorp's shuffling though, of course. reply akira2501 13 hours agoparentprevThese companies enjoy special legal protections that shield them from the liabilities of their actions which they say are absolutely required for them to exist. Perhaps those protections should now be rescinded and they should be held liable for their conduct. reply paulddraper 14 hours agoparentprevHow many businesses are utterly dependent on AWS? reply johng 13 hours agorootparentAWS has competition. I can run on 1000 other hosting providers and the users of the site won't be able to tell the difference. I can rank on every search engine except Google and it would still kill my business. They own 90% of all the traffic. Your analogy isn't valid. reply stevage 13 hours agorootparentprevAs customers - which is different. reply sneak 14 hours agoparentprevI agree with this, but then look at Uber: Without the cooperation and approval of Apple and Google via their respective developer programs and app stores, they wouldn't exist because you couldn't do notifications or location in webapps at the time. There are myriad examples like this of downright giant startups that would not exist if they refused to proceed just because Apple can veto them. Instagram is probably the largest example. Look what happened to Tumblr. reply pixl97 14 hours agorootparentDepends on the size of the business. Apple/Google can screw over small businesses all day long, but once they start getting bigger there is some assumed risk on A/G's part in future anti trust lawsuits if they screw over companies with enough wealth to hurt them in court. reply mbrumlow 14 hours agorootparentprevThese are inspite of. The moving fear is being vetoed only works for well funded startups. A small independent startup would have to consider this before betting the farm on a product that might get smashed by the feeling of the day Google and Apple app moderators have. reply withinboredom 14 hours agorootparentprevBut should those app stores even be the sole judge of whether or not those apps can exist? reply carlosjobim 13 hours agorootparentprevUber could sell their own devices to users and drivers. The app is _that_ useful for millions (billions?) of people, that people would buy it. People used to buy separate GPS devices, so it's not something out of the ordinary. reply notzane 13 hours agorootparentSpotify tried this, did not go well for them. Amazon and Meta learned the same many years ago reply carlosjobim 12 hours agorootparentMaybe Spotify didn't put enough effort into it? And besides, listening to music was nothing special, people had portable music players for decades before Spotify. Uber opened up a completely different way of transport. I don't doubt that they would have success with their own device. Amazon have been extremely successful with their dedicated book reading device, the Kindle. So thank you for that example. That really shows that Uber could have had success with their own device, better than I can argue for it. reply BlueTemplar 3 hours agorootparentHow is it ¬´a completely different way of transport¬ª when taxis existed for decades already ? reply lolinder 14 hours agoprevBetween the rise of fake \"30 best X\" articles that this discusses and the widely-documented problem of fake reviews on places like Amazon, I've increasingly found myself back to leaning on brand loyalty again. Picking based on brands I trust rarely gets me the \"best value\" item and certainly doesn't get me the absolute global maximum \"best overall\", but it's turned into the only reliable way to choose something in a finite amount of time that I will reliably not be disappointed with. There's obviously always the risk that just because a brand was good a year ago doesn't mean it's still good, but I've found that the rate of decline of most good brands is substantially lower than the SEO-spurred rate of decline of the quality of internet publications that purport to provide unbiased reviews. reply secretsatan 1 hour agoparentYes this, I've found product reviews looking distinctly dodgy for quite a few years now, while the article is about something I would see as a bit niche, electronics reviews have been in a hole for a while. I noticed some sites have been using amazon reviews for a long time. I still prefer for the most part, to buy some electronics from brick and mortar shops, at least then I can see the products in action, and I will seek out and travel to specialist shops for things like audio gear, they'll usually match online prices and let you try things out. reply riedel 14 hours agoparentprevBrand loyalty also goes for review sites itself. Sure I find myselfnsometimes getting frustrated if I look into a new product category (like best CD Ripping drive as of 2024 until I find the related forum post ). But I typically I rather first skim through the URL to see some sites I remember (notebookcheck, tomshardware, Chinagadgets.de, or whatever ) that I am loyal to until I get disappointed. This works because they are testing different products and I am kind of loyal. How loyal can I ever be to a site that only sells air purifier tests. How many times in my life do I need this test? I agree in this case it is even easier to be more loyal to brand because they probably sell more things than just air purifiers. reply overstay8930 14 hours agoparentprevYup, it's why I shop at Costco. They do a good job of making sure everything on the shelf is actually decent, so I don't have to google \" reddit\" in store. reply rchaud 8 hours agoparentprevThis is why I bought a Brother printer when I needed a color inkjet. Brother is known for no-nonsense laser printers, but not inkjet. I bought it anyway because I just trust the brand for keeping things simple feature wise. There are better inkjet options from Canon and Epson, but they both nickel and dime you on the ink, have DRMed ink cartridges and what not. I traded off better quality prints for peace of mind. reply Swizec 14 hours agoparentprev> I've increasingly found myself back to leaning on brand loyalty again You‚Äôre making me realize I too have been doing this for a while now. At least when satisficing instead of maximizing ‚Ä¶ and honestly I‚Äôm less and less interested in maximizing. These days when making a purchase I go to a friend group who knows their thing (podcaster friends when looking for a microphone, for example), ask what brand they use, then I go to that brand‚Äôs website and buy the highest-line product that I can afford. There‚Äôs little to no google searching involved and next to zero awareness of any ads. Ask a friend in the know and buy that. Done. If there are no friends in the know, I ask anyone who uses a thing that solves my problem ‚ÄúHey do you like your ? What do you like about it?‚Äù. If they say yes, I go buy that. Life is too short to spend in the quagmire of ecommerce and friends. reply a_wild_dandan 13 hours agorootparentAmazon always shows me the best household brands, classics like FINDYURT and ZUKESEYAKAMERICAUSA. They list classic product models like 'Kitchen Knife 8\" Chef Kitchen 9\" For Cutting For Vegetables for Fruits For Meats Pro Knife Shank With Accessories Blade Kitchen Knife.'But I can assure everyone with almost absolute certainty FWIW I read this level of certainty as \"I am privy to specific information pertaining to it and in fact I'm one of the people in charge of making sure it happens\" reply sharkweek 14 hours agorootparentTo be clear, I have never nor will I ever likely work at Google or any of its competitors. I have spent an ungodly amount of time building (and breaking) affiliate sites / information sites / et al as side projects. Haven't done it in a few years (having kids has sucked my energy for side projects pretty dry), but at one point had like ~15 sites in my portfolio that I used to experiment with. Death, taxes, and an algo update fucking with a site's traffic at Google's whim. reply codexon 13 hours agorootparentGoogle has always preferred top domains for a very long time, allowing them to abuse the rankings for years. One relevant example I can remember, OVH is the top result for servers in Japan yet they don't have any in Japan. https://www.google.com/search?q=dedicated+server+in+japan https://web.archive.org/web/20240219200935/https://www.googl... This has been going on for many years now. And they never get punished unless there's a huge uproar about it on ycombinator like with expertsexchange. reply dazc 14 hours agorootparentprevCertainty can also come from experience of previous actions when such exploits become commonplace. reply hx8 14 hours agoprevAnother bad aspect is that it's becoming harder to google for general information without being hit with production recommendations. For example, if you google an ambiguous term like \"Indoor light\" the entire front page is products. There is no information about how indoor light impacts sleep health, or comparing different technologies of light bulbs, or different styles of lighting a room, or showing how much energy we spend on indoor lighting. It's literally all products, on a topic which has a lot of nuisances to explore. Some search terms seem to trigger \"medical information\", \"scholarly journals\" or \"technical documentation\" subroutines and avoid products all together. reply kccqzy 12 hours agoparentThat's because your search query isn't specific enough. Google doesn't read your mind to figure out what about \"indoor light\" you seek. So it defaults to commerce. reply 01HNNWZ0MV43FF 13 hours agoparentprevEven Kagi is like that, since \"lighting\" is a product class. If I search \"Indoor lighting science\" on DDG almost all the product stuff is gone. I even get an NIH paper on lighting and health. reply anon84873628 8 hours agoparentprevIf type \"indoor light sleep health\" I get the results you would expect. Same for \"indoor lights technology comparison\" Do you really think it's unreasonable that \"indoor lights\" goes to e-commerce? What do you think most people who enter that term are looking for? I mean seriously, do you expect Google to read your mind somehow? Everyone complains that search results are deteriorating. What I think is really happening is that \"the internet\" isn't only targeted at nerds anymore. (And of course people are also strongly opposed to behavioral profiling which would be like reading your mind...) reply hx8 7 hours agorootparent> I mean seriously, do you expect Google to read your mind somehow? If there is any ambiguity Google defaults to commerce. I think the front page for such general queries should have room for science, history, art, culture. Google's default is to sell you something, and it is often the only thing it shows. reply hakfoo 7 hours agorootparentA Wikipedia style \"disambiguator\" might help. If Google silos queries into different top-level categories (i. e. research vs commerce) being able to clearly pick one of them, even if once you see the first results, solves the problem. Giving users the feeling of control and the ability to improve their skills seem to be completely beyond modern software companies though. \"It's all magic and algorithms beyond your understanding, mortal!\" reply rightbyte 14 hours agoprevThis article is really interesting. So, essentially each of the \"top publishers\" are some sort of SEO ring with different websites, linking each other I guess, or at least writing the same thing, to make Google's scraper believe it is high quality content? Or something similar. I don't understand the details really, but I have suspected something similar a long time. reply jacurtis 11 hours agoparentBasically yes. I used to write regularly for a large finance site. You've read articles from this site, they show up on HN regularly and its one of the top 100 sites in the world and bounces around in place among the top 3 finance sites. Anyway, point is when I wrote for them, we had a list of places we could link to and places we couldn't. A lot of them where other sister sites the parent company owned. If we needed a source we first had to try to find it there. If it wasn't available there then there were another list of sites we were allowed to link to, which was basically top 100 sites. Then there were sites we were NEVER allowed to link to. I remember one of those sites was Reddit. But there were many others. Then anything that fell in the middle was something we linked to if it was critical for the article, otherwise we wouldn't link at all. So yes, its basically an SEO circle jerk at the top. Which is why you see the same 100 sites in all search results. reply rightbyte 10 hours agorootparentDo you have any theory of why Reddit was banned? To prevent some \"astroturfing\" blackbox algorithm to flag the site? reply crazygringo 13 hours agoparentprevNo, there's no SEO ring or anything like that. The top publishers are just genuinely the sites that people click on and link to the most. There's no objective definition of \"high quality content\", there are just the links that people click or don't click when presented with search results. Google puts the links people click the most at the top. And people tend to click on results from sites they recognize, because the internet is filled with a ton of crap, and publications whose names you recognize are at least usually indicative of some kind of minimal level of quality. That's all that's going on. reply KTibow 13 hours agorootparentIt's worth noting it's not just \"people preferring well known sites\" it's \"Google preferring well known sites as part of EEAT\" reply jacurtis 11 hours agorootparentprevYou are explaining one component of SEO, which is click-through rate. Google will A/B test certain sites one spot higher or one spot lower to see if click-thru rate is positively affected. So if SiteA gets 80% of clicks while in spot #2, but SiteB gets 84% of clicks while in spot 2, then SiteB moves up to spot #2. The cycle continues as sites move up and down a few spots for fine tuning. However click-through rate is only one component of SEO. The biggest and most significant component of SEO is backlinks, which are ranked by quantity and quality of the link. So if google trusts SiteA a lot and SiteA links to SiteB, then google starts to trust SiteB. If other trusted sites also link to SiteB then the domain reputation grows for that site. Then there is the same ranking on a per-page basis as well. So if one page in particular is very well-linked to, then Google starts to link that page higher and higher for relevant results. This is the largest and most significant aspect of SEO. The click-through rate is a fine-tuning algorithm once you get to the top, but it alone isn't going to help. Google will never test SiteA with SiteZZZZZZZ on page 12. reply beavershaw 14 hours agoparentprevYeah here's a very detailed report on what's been going on written by a friend of mine. https://detailed.com/google-control/ reply crazygringo 14 hours agoprevThis has nothing to do with Google, but rather everything to do with brand-name journalism and what people click on. This article is criticizing Google for showing reviews and guides to consumer products from well-known publications that it argues are increasingly low-quality, instead of surfacing high-quality independent reviews from less popular sites. But this has nothing to do with Google. Google's search quality metrics are pretty simple -- Google is trying to list results in roughly the order of probability that people will click on them. Google is trying to get you to the information you're looking for the fastest. And the reality is that, if I'm looking for air purifiers, I am absolutely going to click on the links to well-known publications like Wirecutter, or Better Homes & Gardens, or Apartment Therapy -- or a forum like Reddit. I'm far less likely to click on some smaller site I've never heard of, because I trust it less. So Google is giving me what I want. And the idea that Google should somehow instead be analyzing the content of each product review site to try to determine whether the reviewers are actually independently testing the items or not, that it should be making some kind of determination of \"real\" quality separate from whether people click on it -- this seems both impossible and misguided. I don't want Google to be trying to pick which niche independent sites are high-quality or low-quality. I just want Google to avoid actual spam, and otherwise give me the results that lots of people link to and lots of people click on -- PageRank and all that -- which is, of course, going to be publications and sites with brand recognition. Google isn't killing small, independent sites. It's people -- users, consumers, people like me -- who generally aren't interested in small, independent sites because there's no reliable signal to determine which ones are good or trustworthy. If I'm looking for an air purifier, I don't have time to waste to look through 20 small, independent reviewers and try to figure out which ones are shills and which minority actually know what they're talking about. No -- I'm going to go to the major review sites, see which models keep popping up, check Reddit for some confirmation and Amazon for some sales rank figures, and make the purchase and get on with my day. Which sucks for small, independent review sites. But they're just in a tough business. And Google has nothing to do with that. reply ec109685 13 hours agoparentThe article points out numerous examples where a site like people.com is ranking for pet air purifiers, with zero evidence that they actually tested the products in question. This tweet thread goes into more detail https://x.com/SeanDoesLife/status/1717291171473727719?s=20 reply crazygringo 13 hours agorootparentYes but it's unreasonable to expect Google to figure out whether people.com is actually testing the products it reviews or not. All Google can figure out is whether people click on links to people.com when they search for air purifiers (they do), and whether the page in question is outright spam or has its content stolen from another site (it's not). The idea that Google should be trying to independently figure out some level of objective \"content quality\" doesn't make any sense to me. It's fine that it builds a knowledge base up out of objective facts to show in cards and whatnot, but I don't want Google trying to decide which review sites are more trustworthy -- I just want it to show me the review sites that other people are clicking on and linking to. For Google to insert \"editorial control\" over its search results would be an abuse of its power, to me. When I search Google, I want popular results to come up -- the \"democratically elected\" results, in effect, from PageRank and clickthrough rates. I don't want Google trying to make assessments of the accuracy of content when it comes to opinion, and review sites are nothing but opinion. reply ec109685 13 hours agorootparentA site that is excellent at SEO spam isn‚Äôt the same as a ‚Äúdemocratically elected‚Äù result. The list of sites for that term are utter crap, recommending the purifiers paying the top commission. Why would you want Google to perpetuate that ranking just because other users are getting duped to click on them? reply crazygringo 13 hours agorootparentWhat \"SEO spam\" you talking about? I'm searching for \"air purifier\" right now and my results are: 1) \"The Best Air Purifier - The New York Times\" (makes sense) 2-4) Links to \"air purifier\" category on Amazon, Home Depot, and Best Buy (makes sense) Then a \"discussions and forums\" section with a couple of links to Reddit (makes sense) Then a Google buying guide full of common Q&A about air purifiers (interesting), a list of shopping links to popular air purifier models (makes sense), and YouTube review videos (makes sense), all interspersed with some more top stores, brands, and review sites (Costco, Blueair, Levoit, Consumer Reports, Better Homes & Gardens -- all totally fine). All of this seems perfectly reasonable and, indeed, exactly what I'm looking for. I have zero complaints. I can't find any SEO spam whatsoever. Literally all of this in the first couple of pages seems entirely legit. reply ec109685 11 hours agorootparentThe example from the article was ‚Äúpet air purifier‚Äù, with fortune.com leading the ranking. reply reddalo 13 hours agorootparentprevOh my god, does Twitter now redirect to X for some users? I've seen more and more links that start with x.com, but my browser still redirects me back to twitter.com reply bemusedthrow75 13 hours agorootparentThe Stupid burns slowly, but it burns like white phosphorus, through everything. reply _a9 11 hours agorootparentprevUsing the 'share/copy link' button has been pointing to x.com since they did the whole x.com change but afaik going to x.com will redirect to twitter.com. No clue what they're on about using one domain but redirecting to the other. reply Scoundreller 13 hours agoparentprevIt was always hard to compete for generic keywords like \"air purifiers\". Wirecutter or whatever with its ton of backlinks would win unless you're doing really shady SEO. What's been lost is the niche side of independent sites. You used to be able to write a review about Air Purifier Model 5643563453 and you would rank on the first page for any searches for it if nobody else wrote a review about it. Now you just won't get that traction and will get generic corp results or at best, independent reviews on social media platforms. Google at least used to include a sprinkling of different types of results, a store, a wiki, a forum, a review site, a blog... Now you can do a search and get 10 results from the same .com for the brand name. reply rockskon 8 hours agoparentprevGoogle has such an extreme bias for popular websites that they now routinely ignore words used in your query if it means recommending a popular website or recommending the results of the closest-sounding search if it's a popular search result. It's extremely obnoxious. I have to do literal word searches in almost every Google search query I perform now. reply stevage 13 hours agoparentprevI think there will be tons of signals google could use to rank up a high quality independent site like this one. They just choose not to. reply granzymes 13 hours agorootparentIt sounds like the 2021 Products Review Update did help rank up high quality sites, but the media companies doing affiliate spam found a workaround with \"Best of X\" lists. reply crazygringo 13 hours agorootparentprevFirst of all, like what? What high-quality signals are there that can't be gamed? And second of all, what if they do, but users still don't click on it because they don't recognize the name of the site? Is Google supposed to be giving users results that they don't want to click on? What if users are so overwhelmed by the number of sites and figuring out whether or not they're trustworthy, that they just want to stick to publications they recognize? reply stevage 4 hours agorootparent> First of all, like what? What high-quality signals are there that can't be gamed? Probably the ones the author refer to would be a good start. >And second of all, what if they do, but users still don't click on it because they don't recognize the name of the site? Is Google supposed to be giving users results that they don't want to click on? It's an interesting question, but the premise of your question is even more interesting: that serving shitty web results is somehow what a search engine is meant to be doing, because that's what users \"want\". I'm very skeptical of it. reply codexon 12 hours agorootparentprevPeople clicking on known brands most of the time now is a behavior that was reinforced by google. I used to find good high quality results from domains other than the big brands through Google, now it is never the case. Now that smaller domains haven't been getting that search engine traffic, people wanting to make a sustainable business being a content creator have moved to other platforms for a long time now. reply evilmusic 13 hours agoparentprevI agree with a lot of what you‚Äôre saying regarding brand recognition and trust, but if that is how Google works, then they should stop publishing documentation + doing presentations + going on webinars + presenting in panels + discussing on Twitter how much they actually do assess the quality of the content and how much they do care about real product reviews. Also, putting the Wirecutter in the same bag as Apartment Therapy and Better Homes & Gardens is misguided‚Ä¶ looking through the examples on the article, it becomes clear that the majority of those lifestyle magazines are just recommending expensive products and popular devices on Amazon. I would just go to Wirecutter and Consumer Reports. reply Saline9515 13 hours agoparentprevYour way of browsing the web isn't what most users do. Most users click on the first link. Many of them don't know what reference websites are. A Spanish woman in her 20s has no idea what the wirecutter is. reply crazygringo 13 hours agorootparentFirst of all, users are more sophisticated than you think. And why you're bringing nationalities into this, I have no idea. And obviously Google isn't going to be surfacing English content like Wirecutter in Spain -- it will surface well-known publications in Spain. But secondly, even if you were right, it wouldn't matter. Users who click on the first link for everything don't change the relative clickthrough rates. The people who actively choose which links to click on would still be the ones influencing the ranking. Google is smart enough to control for clickthrough rates by their listing in results and knowing how far the user has scrolled. reply codexon 12 hours agorootparentI've had the opportunity to analyze click behavior for some pages, and there's a huge bias for people to click things at the top. Let's say for example 90% of users are unsophisticated and 10% are sophisticated. Even if the #2 link is preferred 100% of the time by sophisticated users, you're still going to see 90% of people clicking the first link. reply crazygringo 12 hours agorootparentBut in your example, Google would almost immediately learn that sophisticated users click on the current #2 link, and then make that the #1 link for everyone. It's really easy to do statistically. The bias for the top link gets removed, that's always the first step. Unsophisticated users are not negatively affecting the quality of results. reply codexon 11 hours agorootparentHow exactly do you know that people are clicking on the #2 link because they are \"sophisticated\" and not because they simply prefer #2 for personal reasons? reply crazygringo 10 hours agorootparentI don't even understand your question. They're the same, by your own definition. You said unsophisticated people will just click the top link, and I'm assuming that means some of them will click the #2 link as well just because it's there -- since people often open the top n links. But if they're making a choice based on the link description, rather than just because it's there, then by your definition they're \"sophisticated\". They're clicking it for personal reasons, because they personally evaluated the quality of the link description. reply codexon 4 hours agorootparentI'm talking about \"personal\" reasons like you work for the company behind link #2 and not because #2 is actually better than #1. reply MostlyStable 13 hours agoprevOne thing this made me think of is that it would be interesting to try and make a user curated list of these kinds of sites. Every once in a while, I'll come across a blog of either an individual or a very small organization that is, for whatever reason, extremely interested in a very particular product segment (for example: [0]), and get _way_ into the weeds on the product category. They are exactly the kinds of pages I would like to find when I'm researching a product and exactly the kinds of pages that searches either don't turn up at all or are are pretty far down the list. Having some kind of repository of these kinds of sites would be really useful. [0]https://gunsafereviewsguy.com/ reply Avamander 9 hours agoparentIt's a nice idea, but how do I find the lists that are actually legitimate? I've just resorted to RSS+bookmarking every small site with good content I see. The bookmarks get fed to YaCy, even though it kinda sucks. I just can't trust content aggregators and search engines to deliver good unique content. reply class3shock 14 hours agoprevThis isn't just product reviews. Try searching any question and more than likely multiple results on the first page will be \"articles\" with some title like \"So you want to know about x?\" and the same sort of, generic, possibly algorithm generated, useless content. I wish there was an extension where you could add to a communal blacklist of result url's to start trying to put a dent in the huge number of garbage results Google spits out. reply dankebitte 7 hours agoparentThere's uBlacklist [1], but there's not much community activity on its subscription list [2] or awesome-ublacklist [3]. [1] https://github.com/iorate/ublacklist [2] https://iorate.github.io/ublacklist/subscriptions [3] https://github.com/rjaus/awesome-ublacklist reply pmontra 14 hours agoprevWeb search is dead for that. When faced with a new product category I don't know anything about I go on YouTube and look for people explaining how to use those kind of products, then look for unboxing or reviews, finally I get a rough idea. Then I can search the web for the few products that seem to fit my needs and budget. The advantage of video is that it's (still?) easy to see if something is a genuine or a fabricated review. The disadvantage is that video takes a much longer time than text. reply frabjoused 14 hours agoprevI wonder if Google is internally acutely aware of how rapidly its reputation is tanking. I just haven't seen many public acknowledgements of flaws in its search engine, especially in relation to SEO. So I'm very curious if there is some ongoing code-red internally about this, or if it the problem just got lost in one of the hidden layers of corporate fog. reply ec109685 13 hours agoparentIt‚Äôs acknowledged obliquely at the end of this tweet: https://twitter.com/searchliaison/status/1716964371916800472 reply jacurtis 11 hours agorootparentThe reply post to that one [1] is a perfect example of what I see everyday on Google. You search for \"Best Men's Wallet\" and the top examples are from 1) Forbes, 2) BusinessInsider, 3) New York Times. These sites are huge content sites, getting lots of backlinks. Therefore, as mentioned in the tweet, they can basically create any content they want and Google will reward them. In this case, they are just grabbing affiliate links from wallet companies and ranking them based on top affiliate conversion/payout rates. Which is why you see all the sites have the same recommendations but you buy it and realize they are garbage. They are just cashing in some free internet coins by leveraging their SEO trust status. As a result, the rest of us looking for useful content in this search category can't find anything because of low-quality affiliate content by sites that honestly have no business writing about this stuff. [1] - https://twitter.com/SeanDoesLife/status/1716988691556798629 reply Workaccount2 11 hours agorootparentJust for funsies I'd like to see what would happen if google downranked sites with affiliate links. I know many small honest sites (and some big ones) do depend on affiliate links for their actual quality reviews, so probably would be unfair to implement it permanently, but man I would like to see what the results looked like without money involved. reply cableshaft 13 hours agorootparentprevBarely. With several follow-up tweets by them defending its bad behavior. reply overstay8930 13 hours agoprevThis is the part of the analogy where everyone finds out the Golden Goose is actually dead and Google just started painting regular eggs gold and hoped nobody would notice. reply 01HNNWZ0MV43FF 12 hours agoparentAnd the brief period where the Internet was a secret trick to get information that nobody else had, for free, is over reply kjkjadksj 12 hours agorootparentI for one welcome going back to libraries as a source of truth. Too bad no one else will realize that happened though and will continue reading the propaganda rags. reply JumpCrisscross 14 hours agoprev‚ÄúKagi Small Web offers a fresh approach by promoting recently published content from the ‚Äòsmall web.‚Äô We gather new content, published within the last week, from a handpicked list of blogs and surface it in multiple ways‚Ä¶‚Äù https://blog.kagi.com/small-web reply 15457345234 13 hours agoparentKagi needs to be free to use or have some sort of school partnership program if it's to embody the true spirit of the internet. The people most likely to benefit and have the time to enjoy small-web content are inquisitive children who don't have good sources of information at their disposal, i.e. the smart kids of dumber parents who don't have library cards. Kids who have infinite free time but zero chance of persuading their parents to pay for anything academic or 'nerdy'. There's a lot of them out there. reply jacurtis 11 hours agorootparentThe reason Kagi isn't like Google is because it is paid. Contrary to popular belief: servers aren't free to run or buy or maintain. Software developers also don't work for charity therefore they require paychecks. Both of these things require money. So if you have to pay the bills somehow then you either get users to pay for it (what Kagi currently does) or you get someone else to pay for it so it can be free to users (what Google currently does). So if Kagi makes it \"free\" then they need to start advertising, which then breaks the model because their customers are no longer the search users, but the advertisers. Now motivations and incentives shift and before you know it, you rebuilt Google with a different name and we are back where we started. The point is, that the fact that it is user-funded is exactly why its different. If you pick up the ad model then you will slowly evolve (devolve?) into what all the other search engines already are. reply JumpCrisscross 13 hours agorootparentprev> Kagi needs to be free to use or have some sort of school partnership program if it's to embody the true spirit of the internet That ‚Äútrue spirit of the internet‚Äù caused the ad-based cesspool we have today. Pretending for a free lunch doesn‚Äôt work. What you may be suggesting is school districts pay for Kagi, and in that I fully agree. But in terms of being free, no, free doesn‚Äôt work. reply 15457345234 13 hours agorootparent> But in terms of being free, no, free doesn‚Äôt work. Free worked for much longer than it didn't work, and it will work again. There just needs to be a drastic adjustment in the amount of greed considered socially acceptable, and I can already see that pendulum swinging back. reply Terretta 11 hours agorootparentIt didn't work because it was free, it worked because you had to pay something more like dollars per page you published to host content than like pennies per thousands of pages. For quite a long time, other than exceptions like geocities, you had to pay to publish your own content. People who had an axe to grind or a hobby to share were \"pamphleteering\" and it was great. reply arp242 12 hours agorootparentprev\"Free\" only worked when there was little to no economic interest in the web. So all we need to do is change our economic model, social norms, and human nature. Easy peasy. reply 01HNNWZ0MV43FF 12 hours agorootparentprevThose damned greedy servers needing electricity to run. reply cush 12 hours agoprevGoogle spent at least a decade incentivizing spam while they dominated up the search market, tearing any semblance of humanity and authenticity from the web. By the time Reddit realized what they had, LLMs had scraped them dry, then tragically and ironically, their plan to lock LLMs out of their API backfired and users deleted half of Reddit's content. There's so much value in the Small Web right now - microblogs, IndieWeb, etc., but it's just so hard to find these kinds of sites if you don't know what you're looking for reply Beijinger 13 hours agoprevWell. They do affiliate and bigger Gorillas do affiliate. But they claim to do a more honest review. Well. Air cleaner. I do my review here without any affiliate link: Most DIY filters outclass commercial available filters: https://dynomight.net/better-DIY-air-purifier.html https://energy.ucdavis.edu/wp-content/uploads/Case-Study_DIY... reply bluish29 13 hours agoprev> What do BuzzFeed, Rolling Stone, Forbes, Popular Science, and Better Homes & Gardens have in common? They are all in my ublacklist personal block list. That was before going full time with Kagi where I blocked most of them too. reply BLKNSLVR 2 hours agoprevGood quality product reviews are the antithesis of Google's core business. Google has little to no motivation to solve this in any kind of long-lasting, committed way. Actually, advertising is kind of the antithesis to most quality, factual, well-researched, unbiased information. reply muratsu 13 hours agoprevFrom a regulatory perspective, implementing the ability to remove certain websites from search results (similar to twitter mute/block) would solve the problem for everyone. Motivated communities can maintain their lists and share amongst themselves. reply bemusedthrow75 14 hours agoprevPresented like this it really is an absolutely damning indictment. reply userbinator 11 hours agoprevMy strategy is not to use search engines for product recommendations, but to go to the online shops directly, look for the 1-star and 2-star reviews, and read them carefully. Once narrowed down to a few products, I then research them specifically to find out the details. For the example of air purifiers, look on Ali if you don't want to DIY one with a box fan and HEPA filter. They're all going to be made by some Chinese OEM anyway, so you'll cut out some middlemen and reduce the price, and at least those listings often have far more technical information, and the reviews can have detailed pictures from buyers of the products' internals. reply lkdfjlkdfjlg 13 hours agoprevCan anyone tell where this screenshot comes from? https://housefresh.com/wp-content/uploads/2024/02/best-air-p... reply SushiHippie 13 hours agoparenthttps://ahrefs.com/serp-checker reply natch 3 hours agoprevGoogle is culpable here. But the actual people who work there seem to not care, or are gleefully unaware. But the publishers who sold out to the algorithm are also to blame. I don‚Äôt know how to break the vicious cycle here. reply edmundsauto 3 hours agoprevWhat I find interesting is the categories not clearly spammed but obviously in a high cost ecosystem at smallish scale. Rehab centers, front yard water features, etc. reply codeulike 13 hours agoprevThis is a brilliant breakdown of whats been going on with review lists. Its been obvious for the last five years or so that whenever I googled 'best laser jet printer' or whatever that I was getting fed a load of bullshit but this really steps through exactly how Google and the seo crew are racing us all to the bottom, and to the opposite of useful info. reply 15kingben 13 hours agoprevDo I have this right? 1. Google is overrun by low quality product reviews and comparisons 2. Google starts to weight highly sites with manual testing 3. All those sites shamelessly lie and say they have manual testing -> back to 1 Isn't this a problem that Pagerank is supposed to solve? Auditing content for quality is basically an intractable problem to scale reply kccqzy 12 hours agoprevThis is a genuinely difficult problem to solve. The article title is \"How Google is killing independent sites like ours\" but then (1) I have never heard of HouseFresh and I have no reason to trust them for their reviews; (2) even if the site was highly ranked on Google, I still wouldn't necessarily trust the brand because you know SEO and ranking manipulations exist and I cannot be assured that HouseFresh isn't just a site that hired some high powered SEO consultant. It all boils down to reputation: reputation of the manufacturer and reputation of the reviewer. But humans can't realistically remember the reputation of all the manufacturers and product reviewers online, so we naturally gravitate towards well-known brands and big media publish",
    "originSummary": [
      "Google's algorithm updates have favored big media publishers, disadvantaging independent sites like HouseFresh in search results.",
      "HouseFresh suffers from decreased traffic due to inaccurate product recommendations from major publications like Better Homes & Gardens and BuzzFeed.",
      "The article raises concerns about transparency in recommendations, bias towards big publishers, and compromised editorial integrity, advocating for applying high-quality review guidelines to all publishers for more accurate recommendations."
    ],
    "commentSummary": [
      "Google's search results favoring major media publishers spark concerns over low-quality recommendations and fake reviews proliferating online.",
      "Users voice discontent with conventional search engines, opting for alternatives such as Bing and community-curated platforms.",
      "Calls for industry reforms emphasizing authenticity and relevance over profit motives rise, addressing issues like SEO spam, biased search outcomes, and advocating for user-generated content on the \"Small Web.\""
    ],
    "points": 599,
    "commentCount": 287,
    "retryCount": 0,
    "time": 1708369759
  },
  {
    "id": 39432876,
    "title": "The Power of a Single Text File for 14 Years",
    "originLink": "https://jeffhuang.com/productivity_text_file/",
    "originBody": "Over 14 years of todos recorded in text My productivity app is a never-ending .txt file By Jeff Huang, updated on 2022-03-21 The biggest transition for me when I started college was learning to get organized. There was a point when I couldn't just remember everything in my head. And having to constantly keep track of things was distracting me from whatever task I was doing at the moment. So I tried various forms of todo lists, task trackers, and productivity apps. They were all discouraging because the things to do kept getting longer, and there were too many interrelated things like past meeting notes, calendar appointments, idea lists, and lab notebooks, which were all on different systems. I gave up and started just tracking in a single text file and have been using it as my main productivity system for 14 years now. It is so essential to my work now, and has surprisingly scaled with a growing set of responsibilities, that I wanted to share this system. It's been my secret weapon. Prerequisite: A calendar. The one outside tool I use is an online calendar, and I put everything on this calendar, even things that aren't actually for a fixed time like \"make a coffee table at the workshop\" or \"figure out how to recruit new PhD students\" ‚Äî I'll schedule them on a date when I want to think about it. That way all my future plans and schedule are together, and not a bunch of lists I have to keep track of. Making the Daily List: Every night before I go to bed, I take all the items on my calendar for the next day and append it to the end of the text file as a daily todo list, so I know exactly what I'm doing when I wake up. This list contains scheduled tasks (2pm meeting with Madonna, 4pm office hours), errands (sign a form, return a book), and work items (review a paper, prepare a presentation). It also lets me think about whether I've got the right amount of work for a day. Anything I don't want to do tomorrow, I'll shuffle back into my calendar on later dates. If the task is too big, I'll break it down into a piece for tomorrow, and the rest for another date. After years of doing this, I've gotten pretty good at estimating what I can finish in a day. Here's an example with names replaced so you can see what it looks like when I move a day's schedule from my calendar. 2021-11-31 11am meet with Head TAs - where are things at with inviting portfolio reviewers? 11:30am meet with student Enya (interested in research) review and release A/B Testing assignment grading 12pm HCI group meeting - vote for lab snacks send reminders for CHI external reviewers read Sketchy draft Zelda pick up eye tracker - have her sign for it update biosketch for Co-PI 3:15pm join call with Umbrella Corp and industry partnership staff 3:45pm advising meet with Oprah 4pm Rihanna talk (368 CIT) 5pm 1:1 with Beyonce #phdadvisee 6pm faculty interview dinner with Madonna As a Record: That daily todo list is where I also take notes, so it's a to do list that turns into a what done list. The best thing about these daily lists is I keep them all in a single text file separated by dates, so I have a record of everything I have ever done and when I did it. My current file was created 9 years ago when I started my current job. It serves as a research notebook, and as meeting minutes. I have 51,690 handwritten lines in one file now, documenting everything I have done as a professor, and nearly every person I have met with, along with notes about what we discussed or ideas I had. Here's what my list looks like at the end of the day, representing work accomplished. 2021-11-31 11am meet with Head TAs - where are things at with inviting portfolio reviewers? A: got 7/29 replies - need 3 TAs for Thursday lab - Redesign assignment handout will be done by Monday, ship Thursday 11:30am meet with student Enya (interested in research) - they're a little inexperienced, suggested applying next year review and release A/B Testing assignment grading 12pm HCI group meeting - automatically generate thumbnails from zoom behavior on web pages - #idea subliminal audio that leads you to dream about websites - Eminem presenting Nov 24 - vote for lab snacks. A: popcorn and seaweed thing got unofficial notification ARO YIP funding award #annual #cv read Sketchy paper draft - needs 1 more revision - send to Gandalf to look at? Zelda pick up eye tracker - have her sign for it update biosketch for Co-PI unexpected drop in from Coolio! #alumni - now a PM working on TravelAdvisor, thinking about applying to grad school 3:15pm join call with Umbrella Corp and industry partnership staff - they want to hire 20 data science + SWE interns (year 3), 4 alums there as SWE 3:45pm advising meet with Oprah - enjoyed CS 33 - interning at Facebook 4pm Rihanna talk (368 CIT) 5pm 1:1 with Beyonce #phdadvisee - stuck on random graph generating crash- monitor memory/swap/disk?- ask Mario to help? - got internship at MSR with Cher- start May 15 or 22 - will send me study design outline before next meeting - interviewing Spartacus as potential RA for next semester 6pm faculty interview dinner with Madonna (Gracie's) - ask about connection with computer vision - cool visual+audio unsupervised comparison, thoughtful about missing data, would work with ugrads (?), likes biking, teach compvis + graphics - vote #HIRE #note maybe visit Monsters University next spring, Bono does related work Shortcuts and Features: I use a consistent writing style so things are easily searchable, with a few shorthands. When I search for \"meet with\", it shows that I have had over 3,000 scheduled meetings. I have some tags like #idea for new ideas to revisit when I want project ideas, #annual for things to put on my next annual report, #nextui for things to add the next time I run my next UI course. A text file is incredibly flexible, and at any point, I can quickly glance to see what I've done that day and what's left. I usually keep an empty line between tasks completed and upcoming tasks. When a task is completed, I move the empty line. Any leftover tasks from the current day can go back into the calendar for when I may want to tackle it again, but that is rare because tasks were already sized into what I can do on that day. I can calculate aggregate statistics using the search box, or list all the lines containing a tag, and other operations using my text editor. I use Ultraedit because I'm familiar with it, but any text editor would have similar capabilities. Email: Email is obviously a part of my workflow. Everyone has all sorts of productivity advice about handling it, but I find a simple flagging system is sufficient ‚Äî flag Red if it's something I need to deal with, flag Orange if I need to deal with it eventually but requires some thinking or someone else to handle it, and flag Yellow for emails I send that I am waiting on a reply for, so I know to follow up later. I'll flag emails as they come in, whenever it's convenient. At the end of the day, I'll do a quick review of the Orange and Yellows to see if any need to be followed up or should become Red. Some peoples' workflows revolve around obsessively cleaning their Inbox. I don't really care about keeping my inbox empty because then I feel like I have new work to do whenever email comes in. So my daily routine looks like look at the daily todo list I wrote last night to find out what I'm doing today do scheduled things on that list during the day when I have free (unscheduled) time, do the floating tasks on my list and work on Red-flagged emails at the end of the day do a quick review of Orange/Yellow emails to see if they need any handling copy the next day's calendar items to the bottom of the text file This process has a few nice properties: It's easy to immediately see what to do when I wake up I don't need to remember in my head the things to do later (following up on emails, future tasks) It's easy to recall what happened in the past and see how much I can actually accomplish in a day There's no running \"todo\" list with items that keep pushed back day after day I use Remote Desktop so everything is accessible from every device My daily workload is completely under my control the night before; whenever I feel overwhelmed with my long-term commitments, I reduce it by aggressively unflagging emails, removing items from my calendar that I am no longer excited about doing, and reducing how much work I assign myself in the future. It does mean sometimes I miss some questions or don't pursue an interesting research question, but helps me maintain a manageable workload. So that's it. I would love to hear from you if you try my system, or have some ideas about it! Also in this series The Coronavirus pandemic has changed our sleep behavior Extracting data from tracking devices by going to the cloud Other articles I've written Behind the scenes: the struggle for each paper to get published This page is designed to last, a manifesto for preserving content on the web Illustrative notes for obsessing over publishing aesthetics CS Faculty Composition and Hiring Trends Bias in Computer Science Rankings Who Wins CS Best Paper Awards? Verified Computer Science Ph.D. Stipends This page is designed to last.",
    "commentLink": "https://news.ycombinator.com/item?id=39432876",
    "commentBody": "My productivity app is a never-ending .txt file (2022) (jeffhuang.com)429 points by yarapavan 15 hours agohidepastfavorite231 comments tasty_freeze 5 hours agoAt my first job out of college in 1985 I worked for a unknown, unsuccessful mainframe company. I started a TODO.TXT file when I arrived; when I left a year later I put my TODO.TXT (along with ADDRESS.TXT, DATES.TXT) on a 5.25\" floppy and took it with me to the next company, and the next, and the next. I didn't buy my own computer until 1997, and those files finally made it to a machine I own and control. Those files have been in continuous use for 39 years. TODO.TXT's icon is the upper left icon on my desktop and I view and edit it multiple times a day. I now have 46 .txt files for managing things, where each one pertains to some particular topic: food, music, travel, a website, etc. All are listed in my main TODO.TXT and since I use vim, I can just move the cursor on top of any of the files and type \"gf\" to jump to that other file. When I'm done, I hit Ctrl-O to jump back to the original file. It is hypertext without the html. reply lelanthran 3 hours agoparentI did pretty much the same thing for years, using Emacs org mode (same hyperlinked setup). Then I used vim using `CTRL-Wgf`, same as you. Then I added to my .bash_profile: alias todo='vim ~/.todo' Then I test drove a bunch of todo apps (yeah, Obsidian too). The problem I found with this, eventually, is that it's really not great at allowing me to perform my most common workflow: push-context -> start-new-thing -> finish-new-thing -> pop-context Or switch-context $OLD-THING -> switch-context $NEW-THING -> switch-previous-context Or even switch-previous-context -> switch-previous-context (repeat ad nauseum) Or switch-to-most-recent-context-in-admin -> switch-to-most-recent-context-in-other-project etc. Really, my tasks are not lists, they aren't even items to do. They're contexts which I want to switch to, switch between, discard once done (to maintain high SNR), branch off from, etc. It's context management. A todolist app isn't going to cut it, and in fact, none of them did. For the last 6 months or so I've been happily using a small tool I developed for myself, to suit my workflows (like the ones above). Even made a GUI for it. It's the only thing I've used since I started with TODO files in the 90s that gets better at managing my contexts over time. Everything else gets worse over time, because the mismatch between what I want (context management) and what they provide (TODO items management) can easily be negligible when first using it with few items entered, but as that list grows so does the magnitude of the mismatch in what I want and what they do. It's missing remote sync but I'll eventually add that in (so I can use it from the phone eventually). Easily the best 1 week I've ever spent on a project. reply seletskiy 1 hour agorootparentHey, I've realized the same thing (that my workflows are stack-based) awhile ago, but didn't get to the point of writing a tool yet. Dare to share? Also, which approach you use to efficiently store and re-store relevant context information? I often find that intricate but important details are lost during context switch. reply lelanthran 1 hour agorootparent> Hey, I've realized the same thing (that my workflows are stack-based) awhile ago, but didn't get to the point of writing a tool yet. Dare to share? Sure, but because I'm only using it for myself, you're going to have to compile it yourself if you want (for now, anyway). Originally designed to be CLI app, I find I only use the GUI these days (also in the repo). Start here in case this is not for you: https://github.com/lelanthran/frame/blob/master/docs/FrameIn... > Also, which approach you use to efficiently store and re-store relevant context information? I often find that intricate but important details are lost during context switch. Everything is stored in a hierarchical DB, AKA the filesystem :-) Each 'frame' is a directory with a specific set of files, and the directory name serves as the name of the frame. At any given time, a metainfo file in the root points to the currently active frame. reply hawski 5 hours agoparentprevYou made your private wiki. I used to use Zim and it was splendid. Then I went down a few rabbit holes and explored a more GUI-less approach, but in my case nothing ever approached the level of Zim. It is more on me, than on tools. Now, when I'm doing a job that finally doesn't cause me a burn out I am slowly going back to Zim and I like it. reply Propelloni 1 hour agorootparentFull agreement. I use Zim Desktop Wiki for years now and still use it daily, but it has become harder. I also write on mobile and the experience is lacking. I don't mind syncing with Syncthing, but there is no good editor. Markor is able to open Zim files (which are just text files with markup) but it is just not cutting it. Maybe I need to wait until the itch becomes too large to ignore and start my own little side project :/ reply gercius 3 hours agoparentprevYou would (probably) like obsidian reply ravishi 13 hours agoprevI use a similar system, but I duplicate and rename the file at the start of each day. Then I remove stuff that got done the previous day. Or stuff that is old and not relevant anymore. The system has evolved over the years. The greatest thing about it is how flexible it is. When faced with new requirements (new projects, job change, etc) I can just start taking notes in a different way and see if it sticks. I also commit it to git every 30 minutes using a cron script. Its awesome. reply JordiGarcL 13 hours agoparentDoing the exact same thing using Obsidian and the obsidian-git plugin, which allows for automatic git push at a given interval. Works very well and it's very convenient to use. reply kelsolaar 41 minutes agorootparentSame setup here and pretty happy about it, only minor issue is the `Update time on edit` plugin creating some conflicts if I have my vault opened on two machines. reply drewzero1 13 hours agoparentprevI don't usually take the time to write out a daily to-do list, but I do keep one for long-term projects that I tend to lose track of. Each line is a task and at the beginning of the line is the date it was initiated/requested. It's semi-structured but doesn't take any longer than typing out a note. At the beginning of the month I duplicate the file and rename it for the new month, then I clean out just like you do. I've tried apps, I've tried tracking systems, and this seems to work best for me for now. I can keep it open all the time in a tab of my text editor which I would have open anyway and it backs up with the rest of my files. reply AlecSchueler 12 hours agoparentprevWhat are the benefits of keeping it in git? Are the commit messages useful? Not trying to knock the idea, I just can't imagine branching or reverting or being curious about the history of a to-do list. Maybe it could be cool for statistics over time. But I'm really curious to hear how you're using it! reply m463 7 hours agorootparentI find that files under revision control have one awesome property: they can be ruthlessly minimal. The intermediate note-to-self stuff, the reference material, the stuff that was cancelled, all can be deleted. But you have the peace of mind that you can get it back. This might be especially important if it is a text file you run your life with. My neighbor needs the name of the plumber we used. What was the amount of the bill? When did I order xyz last? What was the size of the wiper blades of my car? reply steve1977 3 hours agorootparent> My neighbor needs the name of the plumber we used. What was the amount of the bill? When did I order xyz last? What was the size of the wiper blades of my car? But how do you look that information up in git? reply dewey 2 hours agorootparentYou can just search over all the diffs, that‚Äôs how I sometimes do it in Fork.app reply xanathar 11 hours agorootparentprevI have a similar setup as far as git is concerned - I use it to push a backup of my Joplin notes (note that I don't sync multiple Joplins, just backup the current one). Pros: pushes only the differences, keeps history, can be rolled back, works offline (push fails but commit works), offers a time log of changes. Commit messages are not needed really - the timestamp is enough and rolling back is only for exceptional events (e.g. accidentally deleting important stuff, etc.). reply medstrom 11 hours agorootparentIt can also save you when you try a different sync solution, and you see in git status that it messed up the sync. reply ozim 2 hours agorootparentprevSynchronize between computers is easy and I can also use git client on my phone. Hosting private repo on gitlab solves hosting. reply jimbokun 13 hours agoparentprevI've landed on a similar process, but one file per sprint instead of one file per day. I take the last sprint's file, save a copy for the new sprint, summarize what got done to report in sprint meeting, then add the tasks for the new sprint to present at the meeting as well. Still have JIRA tickets to track tasks in a more formal way. But the text file is far more flexible and easier to quickly edit and view everything at a glance, as well as including things that don't fit cleanly in a ticket. reply mushufasa 8 hours agoparentprevI used to do this but that made it harder to search on the occasion I'm looking for a prior note. I do everything in vim and it's more keystrokes to search across separate files. I've been using one file for 2-3 years now and it's only 1mb. reply quesera 8 hours agorootparentI do the same in vi/vim/neovim, in a custom filetype with syntax highlighting for TODO items, status markers, tags, code blocks, etc: \"my/work/xxx/201810.notes\" 34502L, 1946600B written 5.5 years on this particular file, but I've been using the same system for at least twice as long as that. The datestamp is the most recent time I thought it might be useful to rotate into a new file. It was not! I find it much more convenient to have everything in a single topic-based file, although I do separate personal from work, and employers from each other. reply adharmad 6 hours agorootparentCare to share your file format with some dummy entries? reply danielrk 5 hours agoprevI used to use a .txt file like this for over 10 years that I edited with Vim and could also edit on my phone using Dropbox. And then I used the Mac Notes app and realized the search, sync, autosave experience, and other properties made Notes way more useful for me. Similarly, for over 10 years I used to compile my CV with LaTeX using a fancy template I designed, until I realized the time it took to edit it was just not worth it. So I pasted it into a Google Docs file with Arial font and just text bullet points and no rich text features. My life became way easier and my CV looks way less annoying. (This may be irrelevant to the original post but) I found that a lot of the optimizations I thought were good for me were stemming from an unnecessary attachment to a minimalist technical aesthetic. reply lelanthran 3 hours agoparent> Similarly, for over 10 years I used to compile my CV with LaTeX using a fancy template I designed, until I realized the time it took to edit it was just not worth it. I went the opposite way - trying to typeset[1] in word processors just became too hard as changed CV to emphasise some things over others. My CV[2] in LaTeX was extremely quick to adjust for someone else who wanted an identically formatted CV. [1] I like the ability to do proper boxes all over the page for things that require box-models, like advertisments, which is what your CV actually is! [2] In case you are interested in what the output looks like: https://www.lelanthran.com/downloads/resume-1page.pdf reply overvale 5 hours agoparentprevThe same exact thing happened to me and I came to the same exact realization. I moved all my dot files to an archive and started using built-in software. It's been great. reply theonething 4 hours agoparentprev> used the Mac Notes app But then it's not Vim. That was a little tongue in cheek, but seriously, I need Vim when I write/edit text. Currently have a todo.txt that's on iCloud. reply danielrk 4 hours agorootparentHaha. I still use Vim for coding and for the rare occasions where I need to edit text that needs to be in a plain txt file. reply BasilPH 14 hours agoprevI'm doing something similar with Obsidian daily notes[^1]. I also have a weekly note that I use to plan the next week. Similar to how the author talks about scheduling their next day the evening before, I've started planning the big tasks for next on Friday afternoon, as this gives me momentum on Monday morning. Related: I've found the 3/3/3 technique from Oliver Burkeman[^2] and the concept of open and closed lists to be a great complement for this type of organization. It hits the sweet-spot of flexibility and consistency for me. [^1]: https://help.obsidian.md/Plugins/Daily+notes [^2]: https://ckarchive.com/b/e5uph7hx43mn reply zeta0134 14 hours agoparentHappy Obsidian user here. I love that the \"vault\" concept it uses is literally just a folder of markdown files, meaning I'm still in full control of my data. I don't use their proprietary sync service, I just drop it into a regular folder and let syncthing take care of cloning it to every device I own and a few extras for backup. Obsidian itself has got to be the nicest markdown editor I have ever used, hands down. It gets so many of the little details absolutely right, down to tiny things like a quick shortcut to turn a list item into a checkbox (Ctrl+L) and then into a checked box (Ctrl+L again), without needing to even think about the underlying syntax. But you totally can, if you need that control. It's great. reply vunderba 11 hours agorootparentVaults are great. I compartmentalize all notes surrounding each consulting job as a self-contained folder/vault - that way I only have to search relevant information but still have access to it at a later time if I want to open that vault again. reply 331c8c71 13 hours agoparentprevI plan exclusively on paper despite using Obsidian quite extensively for taking notes. I also do weekly and daily planning. Initially I tried to plan on Obsidian as well but it didn't work for me. Writing on paper is slow and not only it calms me down but also directly incentivizes me to state my tasks and goals concisely. Similarly, the limited space on a planning page helps me to be realistic in terms of things I set to accomplish. reply realfeel78 12 hours agorootparentPaper often wins for a lot of things. reply fuzztester 11 hours agorootparentTaking hard-copy printouts of code to study it for bugs, design or code review is one area some people I know use it for. Edit: I guess even for non-code text files, though I haven't used it for that purpose myself, yet. Bet many authors do. reply sgc 8 hours agorootparentYou can go through a text to be published 5 times on a computer, print it out and for any text of decent length, I guarantee you will find a ton of stuff you missed. I have never tried using an e-ink device for that to see if it has the same effect, but I would be curious of the feedback on that if anybody here has done so. reply fuzztester 37 minutes agorootparentTrue dis. reply sssilver 1 hour agoparentprevThose who are familiar with both Obsidian and Bear ‚Äî what are the principal differences for this particular use case? reply andygeorge 14 hours agoparentprev+1 for Obsidian, it's invaluable for my day-to-day AND long-term stuff reply blackhaj7 13 hours agoparentprevI use Obsidian but it is unbearably slow upon when opening the app for me, to the point where I want to move away. It‚Äôs also dare-I-say-it too customizable for me. I just want it to look nice and do standard notes stuff without having to spend hours tinkering. The only thing keeping me is that it is just markdown. I don‚Äôt like the idea of being locked in with the proprietary formats of other apps reply Al-Khwarizmi 13 hours agorootparentCurious, I have starting using Obsidian recently and one of the things that I love about it is that it's lightning fast on my systems, including startup time. Much snappier than other note-taking programs I've used, and than 95% of the programs altogether (only the likes of Notepad are faster). Maybe it's because I don't have many notes yet and it becomes a behemoth if the vault gets too big? reply machomaster 12 hours agorootparentUsually the slowness of Obsidian is caused by plugins. Try to have 50+ plugins and you will feel the slowness even in a small vault. reply realfeel78 11 hours agorootparentWhat hardware/OS are you using? I have a shitload of plugins but it's lightning fast for me on Mac. reply machomaster 8 hours agorootparentIntel Core i7-4790K, 32GB, Linux. I currently have 147 community plugins installed. Is your shitload bigger than my shitload? :-) I don't have all of them enabled though. Only about 2/3. :-) It's not only the number of enabled plugins that matters. Some graphical plugins eat almost no resources. But then there are other plugins that are constantly rescanning files and O is not necessarily \"n\", but worse than that. You can start with setting up the following plugins to their full potential and see how it goes. :-) Breadcrumbs Dataview Dynamic Table of Contents Filename Heading Sync Juggl Link Favicons Linter Omnisearch Spaced Repetition Supercharged links reply ezst 4 hours agorootparentprevMay I suggest giving Trilium notes a try? It's like opensource obsidian plus typed notes plus self hostable sync plus a web frontend for places where you can't install it. reply realfeel78 11 hours agorootparentprevSlow on what hardware/OS? It's instantaneous for me on Mac, but can be painfully clunky on iPhone. reply martin82 6 hours agoprevPlanning the next day on the evening before has to be the single most important productivity hack in the world. reply wernsey 38 minutes agoprevI didn't see anyone mention Markdeep [0] yet. I started with a notes.txt file for the system I maintain. I found myself gradually adopting Markdown syntax because I need bulleted lists and headings to separate different sections. I also needed hyperlinks to documentation or StackOverflow answers. So one day I just added the Markdeep tags to the bottom of the file and renamed it to notes.md.html I still keep it open in a text editor for day to day use, but it looks really nice when you open it in a browser. [0]: https://casual-effects.com/markdeep/ reply ramses0 14 hours agoprevI've had pretty decent luck with `todo.txt` style tracking, but also tend to run into issues with tasks or notes \"going stale\" so came up with this system. `today` basically opens `~/Desktop/$YYYY_MM_DD-todo.txt`, but it'll start you off with a copy of the most recent (previous) file. This lets me have \"durable\" files (I can grep for pretty much anything and get a date-specific hit for it, similar to doing a `git log -S`), and also lets me declare \"task-bankruptcy\" without any worry (I can always \"rewind\" to any particular point in time). The addition of `report` (aka: `diff $YESTERDAY $TODAY`) is helpful to see what I've added/removed. Yeah, there's better ways to do things, but the degenerate simplicity of `open ~/Desktop/todo.txt` is fantastic. Having the equivalent of `open ~/Desktop/$TODAY.txt` (with no ceremony) has been very valuable to me! $ cat ~/bin/today #!/bin/bash TODO_HOME=\"$HOME/Desktop\" TODAY=\"$( date \"+%Y-%m-%d\" )\" TODAY_FILE=\"$TODO_HOME/todo-$TODAY-todo.txt\" PREVIOUS_FILE=\"$( ~/bin/previous )\" if [[ ! -f \"$TODAY_FILE\" ]]; then cp \"$PREVIOUS_FILE\" \"$TODAY_FILE\" fi report \"$TODAY_FILE\" printf \"Press Enter to Continue, Ctrl-C to exit.\" && read -r PROMPT open \"$TODAY_FILE\" echo \"$TODAY\" $ cat ~/bin/previous #!/bin/bash TODO_HOME=\"$HOME/Desktop/\" TODAYS_DATE=\"$( date \"+%Y-%m-%d\" )\" MOST_RECENT=\"$( ls \"$TODO_HOME\"/todo-*-todo.txtsed 's/^.*todo-//g'sed 's/-todo.txt//g' ; echo \"$TODAYS_DATE\"sort )\" PREVIOUS=\"$( echo \"$MOST_RECENT\"awk -- \"BEGIN { YET=0 } /^$TODAYS_DATE/ { YET=1 } { if ( !YET ) PREV=\\$0 } END { print( PREV ) }\" )\" PREVIOUS_FILE=\"$( echo \"$TODO_HOME/todo-$PREVIOUS-todo.txt\" )\" echo \"$( realpath \"$PREVIOUS_FILE\" )\" $ cat ~/bin/report #!/bin/bash TODO_HOME=\"$HOME/Desktop\" TODAY_FILE=\"$TODO_HOME/todo-$( date \"+%Y-%m-%d\" )-todo.txt\" PREVIOUS_FILE=\"$( ~/bin/previous )\" echo \"${PREVIOUS_FILE}...${TODAY_FILE}\" diff -U0 \"$PREVIOUS_FILE\" \"$TODAY_FILE\"grep -v ^@@ reply nickjj 13 hours agoparentI've been doing something similar for 20+ years at: https://github.com/nickjj/notes - Running `notes` will open this month's notes for YYYY_MM.txt in your default $EDITOR - Running `notes hello world` will append `hello world` to YYYY_MM.txt - Running `$stdoutnotes` will append another program's output to YYYY_MM.txt (useful for piping your clipboard) I find this offers the least amount of resistance for quickly adding notes. Every method of input is 2 seconds away on the terminal and grep makes things searchable enough where I can still pull things out from files 5-10 years ago without issues. I tried YYYY_MM_DD.txt for a while but I found it to be too fragmented. Oftentimes I want to look at a few day's worth of notes at a glance. reply emadda 11 hours agorootparentYou might want to try iso week numbers. Every week starts on Monday and is always 7 days. Gives you quite a granular time reference but not too fine like days. reply FredPret 13 hours agoparentprevThis is brilliant - thanks for the great idea reply twodave 13 hours agoprevI tend to format my plaintext notes as markdown to add a small amount of organization. I also have some light folder/file organization to break things into categories/topics/days, depending on what it is. That gives me a nice clean slate to work from each day, but lets me pick up where I left off on more long-term stuff. Finally, I push it all to a private Github repo so I can get to my notes easily from wherever I am. Overall this system works for me for several reasons. First, I hate pretty much every note taking app out there. Second, I like having control over my files. Most importantly, though, I don't actually need to write notes all that often, and this way of doing things is convenient. When my brain is so crowded I need to overflow some thoughts or tasks for the day/week somewhere, this system is there. When I'm managing it all in my head just fine, I don't have to worry about keeping notes up to date, and I can count on my own system not to send me a push notification bugging me about it. reply charlie0 11 hours agoparentSounds like Obsidian would make a great candidate for you on a note taking app. reply cyanydeez 12 hours agoparentprevI'm guessing you're younger than 40. At some point, either your jobs novelty will wear off or your memory will just degrade. what happened to me is the lack of novelty in day to day and long term plans created a lot of unreliable data. having done a task multiple times, recalling the latest event details became a mixed bag of questionable facts. I still have the same capacity but now because I had such a great capacity it's redundancy causes issues. just take note. reply zogrodea 7 hours agorootparentThis reminded of the memorable Sherlock Holmes quote. Hopefully it is good advice for those of us who are a bit younger. \"His ignorance was as remarkable as his knowledge. Of contemporary literature, philosophy and politics he appeared to know next to nothing. Upon my quoting Thomas Carlyle, he inquired in the naivest way who he might be and what he had done. My surprise reached a climax, however, when I found incidentally that he was ignorant of the Copernican Theory and of the composition of the Solar System. That any civilized human being in this nineteenth century should not be aware that the earth travelled round the sun appeared to be to me such an extraordinary fact that I could hardly realize it. ‚ÄúYou appear to be astonished,‚Äù he said, smiling at my expression of surprise. ‚ÄúNow that I do know it I shall do my best to forget it.‚Äù ‚ÄúTo forget it!‚Äù ‚ÄúYou see,‚Äù he explained, ‚ÄúI consider that a man‚Äôs brain originally is like a little empty attic, and you have to stock it with such furniture as you choose. A fool takes in all the lumber of every sort that he comes across, so that the knowledge which might be useful to him gets crowded out, or at best is jumbled up with a lot of other things so that he has a difficulty in laying his hands upon it. Now the skillful workman is very careful indeed as to what he takes into his brain-attic. He will have nothing but the tools which may help him in doing his work, but of these he has a large assortment, and all in the most perfect order. It is a mistake to think that that little room has elastic walls and can distend to any extent. Depend upon it there comes a time when for every addition of knowledge you forget something that you knew before. It is of the highest importance, therefore, not to have useless facts elbowing out the useful ones.‚Äù ‚ÄúBut the Solar System!‚Äù I protested. ‚ÄúWhat the deuce is it to me?‚Äù he interrupted impatiently; ‚Äúyou say that we go round the sun. If we went round the moon it would not make a pennyworth of difference to me or to my work.‚Äù reply sph 1 hour agorootparent'Tis a lovely quote... though I believe our memory is less like an attic and more like the eponymous house in Mark Z. Danielewski \"House of Leaves\". It is not a fixed size, but it grows and shrinks, and you might find it still houses long lost or useless memories, while somehow being too full to store the name of the person you just introduced yourself to. reply kbos87 12 hours agorootparentprevThis hits hard. As I've gotten older I've accumulated a lot more complexity in my life. Finances that I need to take seriously, properties, family stuff, a decade+ of depth in my career... it takes a lot to keep track of everything, and to make it legible when you come back to it a year or 5 years later. reply mortify 5 hours agorootparentMy guiding principle for good note-taking is whether they can be understood by someone who has no background on what I'm writing about because that person will be me 3-5 years from now. reply keybored 11 hours agorootparentprevYou make notes, organize them a little, make sure to back them up/synch. them but you don‚Äôt make notes for everything? How bizarre. Now let me berate you for being (apparently) young. reply twodave 6 hours agorootparentprevI... what the hell man? I'll turn 40 within the year. The only thing about my brain that's degraded is I sometimes find myself in the pantry with no idea why I'm there. I attribute that more to having 4 kids between the age of 7 and 14 who think I'm their personal assistant half the time. I don't see that getting better or worse in the next 10 months. But whatever, thanks for kicking off a thread of bad assumptions about my mental capacity, age and (this one's not your fault but sometimes you don't know how big the snowball will be when it reaches the bottom of the hill) overall character. reply doctor_eval 12 hours agorootparentprevAgree with your take. I am currently enduring a home builder who seems to think he doesn‚Äôt need to take notes. He has made so many errors and has had to do so much rework that he‚Äôs run out of time to fix legitimate problems. His belief in his ability to retain information way exceeds his ability. Understanding and accepting my own constraints and limitations helped me to become far better at what I do. People who don‚Äôt take notes make me very anxious. (This includes waiters). reply psychlops 11 hours agorootparentEven if your builder took notes, he would still have errors and omissions. In my experience with builders (and many other areas), it is my job to make sure they do their job. Notes or no notes. Same with waiters. reply doctor_eval 11 hours agorootparentNot sure how you ensure waiters do their jobs. Do you follow them to the kitchen?! reply psychlops 10 hours agorootparentRemind them when they inevitably forget. Unless they remember, then they get tipped more. I hadn't thought of following them to the kitchen....maybe next time. ;) reply doctor_eval 6 hours agorootparentI get that you‚Äôre joking but the underlying point is that there‚Äôs only so much you can control without doing the job yourself. With the builder - yep, told him we don‚Äôt need the stud wall. Met him every morning to discuss issues and progress. I start taking notes and sending them post meeting. Did not talk about stud wall because it‚Äôs no longer being done. 2 weeks later, stud wall appears. Sigh. Ask him to remove stud wall. He tries makes me feel like it‚Äôs my fault, but fails. reply gumby 14 hours agoprev> Every night before I go to bed, I take all the items on my calendar for the next day and append it to the end of the text file as a daily todo list, so I know exactly what I'm doing when I wake up. T This is a key win. Most of the rest he describes is support (also cruicial). But setting up your day the night before is amazingly powerful. Many of the things I plan for the day I actually schedule into my calendar (12:30-13:00 read and respond to those three unanswered messages from Jane). reply blowski 14 hours agoparentThis is the challenge of the modern manager, especially in remote jobs. You turn on the computer with a plan, and then 345 Slack messages and 10 Zoom meetings later, you consider working on it. As an EM, I really miss that state of flow and productivity. I‚Äôm whinging because I see other managers that have nailed this so much better than me and I don‚Äôt know what I‚Äôm doing wrong. reply briHass 8 hours agorootparentHeavily guard your morning time, and don't even open your email/chat programs until closer to lunch. At least for me (maybe it's the caffeine rush), but that morning block is the most valuable/productive time of my day and going down a rabbit hole early can kill the rest of my day's accomplishments. In my previous role, I worked with a primarily West Coast team (being East Cost, myself), and it was wonderful having that first 3 hours of uninterrupted time to do deep work before the 'other' stuff crept in. The downside, of course, was then needing to be on-point in meetings where everyone else was caffeinated/ready-to-go and my brain was losing steam. reply blowski 2 hours agorootparentI used to work like this with email, to the point that I‚Äôd intentionally not sync emails for a few hours. But in my company, everything is done through ‚ÄúSlackOps‚Äù. Incidents (including alerts) line management duties, deployments, getting access to environments. Plus lots of information I need is in Slack. Maybe some combo of Zapier, extracting into local text, and strict observance of non-slack time and I could make it work. reply gumby 13 hours agorootparentprevI address it with asynchrony: I look at my mail when I get up and normally at three other fixed times. Sometimes there's something complicated going on via mail and I have to be more responsive. Today I'm trying to debug something with a prospective partner who is in Japan (normally in NYC) so I check for messages from them between tasks. But otherwise it's systolic. I also run a lot of automation over my mail most of which causes me not to see as much. For slack, we have a culture that it's either transient (doesn't matter what someone wrote yesterday) or, depending on channel, archival (\"here's the documents from partner P\") which means you search for it but don't otherwise follow in real time. We're relatively hardcore about channels so that you can ignore ones that aren't germane to you. So I skim them in the same times I check mail. Zoom meetings...I have the luxury of mostly only attending meetings with agenda and objectives published ahead of time. We try to do as much as possible asynchronously though we have one outside partner who doesn't do any homework and tries to use meetings to get work done rather than just use them for things that can't be handled asynchronously. And also: certain topics are only handled on certain days, e.g. patent (bletch) related stuff I only work on tuesdays and fridays. Otherwise it will just sit in my inbox or wherever. reply ambicapter 14 hours agorootparentprevCan you get started on the things you need to do before attending to those Slack messages? reply aeturnum 10 hours agoprevPersonally, I feel that Todoist[1] has revolutionized how well I can track things. You can use it exactly like a text file, where each line / task is tracked individually. I'll often add tasks for my current day (that will fail over to tomorrow if I don't get to them). However it's real strength (compared to a text file) is in its features for repeating periodic tasks. Other products I've used struggle with irregular timing structures, but todoist simply understood (for example) \"the first monday of every month\" for street sweeping. I still keep daily notes at times and track what I'm doing in other spots, but getting used to using Todoist to help me do periodic tasks at the proper rate has been one of the only productivity tools that felt like they \"really worked.\" I pay for the app because not doing so would feel ungrateful, but I don't really use any of their premium features. The base app is more than good enough in my view. [1] https://todoist.com/features reply bomewish 10 hours agoparent+1 for Todoist here. Here is how I use it. Each project I do (dev stuff, research stuff) gets a ‚Äòproject‚Äô in todoist, and I use the ‚Äòboards‚Äô view. Then, each project is going to have a bunch of mini-projects (e.g. end-to-end data project needs to build web scraper, set up database, set up backups, set up frontend) ‚Äî then each column on the board is one of these sub-projects and all the tasks go underneath them. It is really great software and I‚Äôve used it for many years and pay for it now. reply Gbox4 14 hours agoprevI've been using what is essentially a single sticky note (Raycast floating notes feature) for a year now and it works great. I put todos, meeting notes, ideas, and everything else in there with zero organization. When I want to remember stuff I read it. When I finish stuff I delete it. Has worked for me better than Notion, Obsidian, Reminders, Tick Tick, etc. I've found that for productivity tools, there is an inverse correlation between time it takes to setup and how effective it is. reply safehuss 1 hour agoparent+1 for Raycast Floating Notes. Looking forward to their version 2.0 which will support tabbing and markdown. Should make it even better. reply al_borland 12 hours agoparentprev>I finish stuff I delete it. I run into the issue where I‚Äôm told to start new things all the time, then things don‚Äôt get finished, because of other new things that ‚Äúneed‚Äù to start. And no one ever seems to care than nothing actually gets done‚Ä¶ but someday they might. So the list of what I need to look through keeps growing with nothing to keep it in check. reply realfeel78 13 hours agoparentprevYou get it. reply abulman 14 hours agoprevI'm also using Obsidian daily notes, with all un-actioned items shown on a page with a dataview API[^1]: ```dataview TASK FROM \"VaultName/Journal\" WHERE !completed ``` Since at least 2012 I've also been using a text file format from http://todotxt.org/ and more recently I wrote a program that takes a crontab-like list[^2] to pre-generate entries on a daily, by-day-name (every Sunday for example), and I also pull in a list of holidays from gov.uk, so they are also populated. [^1]: https://github.com/blacksmithgu/obsidian-dataview [^2]: https://github.com/alister/alister-tools/blob/main/.todo.cro... reply dopu 13 hours agoprevI've arrived at something similar after going through a lot of different solutions: Evernote, Quiver notes, Apple Notes, Logseq, Tana: now I just keep everything in one big Journal.md file in Obsidian. I added a datestamp shortcut that inserts the date as a title in \"2024 February 19 (Mon)\" format, and get to writing. I use subheadings sometimes if I'm writing a lot on a particular day and it gets messy, but most days it's just a hodge podge of everything, and that's fine. It works. A big issue with computer notetaking software, I've realized, is that I was spending too much time trying to figure out where to put things: what note should this be connected to, which folder should this be in, etc. Dumping everything into a single document, under today's date, gets rid of that. The other issue this solved was that I never looked back at what I'd written previously: opening a bunch of files was too tedious to ever do unless I was explicitly looking for something. With this, I can just scroll down and see what I was doing last week, immediately. reply dankco 13 hours agoprevI love the idea of using plain text files for note taking and task tracking. As others have commented on specific tools and workflows that make this easy for them to stick with, I thought I'd add mine. I use textnote [0], which is a tool I built for exactly this workflow but is hopefully flexible enough to accommodate many of the similar processes mentioned here. It simply opens a plain text file in your terminal and provides lightweight tooling for tracking by date and rolling up previous notes into archives if desired. Thanks for opening another great discussion of plain text note taking as a productivity tool! [0] https://github.com/dkaslovsky/textnote reply nonethewiser 13 hours agoprevThis matches my experience. Its all about ingestion speed. Ingestion of the thoughts in my head, that is. Most note taking systems require you to structure it. That‚Äôs basically the whole point. I find that this just gets in the way. I just need a sort of working-memory dump. I have a note directory with a root level markdown file which I use for general stuff which i dont use much. I also have folders for each task # with a similar sort of markdown file. And sometimes (rarely) other useful assets. reply riston 11 hours agoprevI am using a similar structure a single text file usually split by each day, which also gives a good start to writing my standup notes. I tried to avoid bringing in some strange formatting rules etc, quite free form. With a single file, it's effortless to go back and find out maybe why something was done in that way or why it wasn't done at all. Looks kind of like a work diary to me, I have seen people trying to do a similar thing by bringing too much structure/org modes etc and making it so complicated that they forgot at first why they are doing it. KISS reply d--b 13 hours agoprevI don't know how to articulate it, but I could never do anything like this. The ability to organize one's life like this is so foreign to me, it's almost like he's describing what it's like to be an octopus. I think it may be my emotional state that I can't manage. There is absolutely no way, I could decide what I'd be doing the next day every night. The thing is my state of mind would prevent me from doing half the tasks in the list. So shit would just pile up in that text file, making me every day more nervous about things. For what it's worth, I think I am more \"normal\" than the person who wrote that piece. So that's a consolation... reply sph 51 minutes agoparentThe issue about productivity tools is that they target two categories of people: - those that like to obsess over productivity tools instead of doing work - those that share the same brain structure as the author's If you are neither of those, have ADHD, OCD or any other brain variety, most advice on the matter is worthless at best. The only way out is find how your brain operates, and what works for you. There is no silver bullet. reply Al-Khwarizmi 13 hours agoparentprevIf it's a consolation, I feel the same as you. I clicked because what I have is a gigantic TODO .txt file that grows and grows and grows. I typically only look at the bottom (newest) part, typically at what fits on screen. The rest is full of things I should have done at some point and never actually did. reply rubslopes 13 hours agorootparentIf I could make a suggestion, try dividing your list into two: a to-do list and a \"someday, maybe\" list. That's a concept from GTD that helps a lot with peace of mind. reply al_borland 11 hours agorootparentprevI‚Äôve been in this place more times than I care to count. I find taking some time to go through it to ask if these things still need to be done can help a lot. In many cases, what was once thought to be important is no longer important or even needed/wanted; delete these things. In other cases it‚Äôs more of a nice-to-have, not something that is really needed. For these, if it‚Äôs for you or someone you like, a someday/maybe list (as another comment suggested) is good, otherwise drop it. Once the list is shorter and current, I find it easier to get things done. When the list gets too long that I can‚Äôt bring myself to read it anymore, this is generally what I do. I‚Äôve also found it helpful to have a kind of ‚Äúbacklog‚Äù list, and then something just for what I‚Äôm going to do today. That today list needs to be short. 3 things is the max for me; some days it‚Äôs just one. If I happen to finish it all, I can look at the backlog to add something. Being realistic about what can be done in a day is really important. Getting all that other stuff out of my view helps me to stop thinking about all the stuff I‚Äôm not doing, as it‚Äôs not helpful to dwell on it. reply supportengineer 13 hours agorootparentprevI enter my to-do items into Google Calendar. If I can't finish them today I move them to tomorrow or another future date. Once they are done they stop moving. reply s_m_t 13 hours agoparentprevThe trick is that you don't actually have to do all the tasks you write down. It is still nice to have a record of what you've planned so if you ever decide to jump back on any task you have a history of what you have done and any context associated with it on hand. A lot of my todos are something like \"I found this article interesting but I don't have the current skills to really understand everything in it\" or \"I want to add this feature to X but I think I will wait until the new version comes out because it will be easier then\" or \"I want to remember this when I finally decide to do Y\". reply al_borland 11 hours agorootparentThis is what my read-later list is like. I always keep adding to it, but I don‚Äôt really have any part of my life carved out to read any of it. It‚Äôs full of good intentions to learn about things or start new hobbies. I migrated it a while ago and was really disappointed to find a lot of dead links. It makes me wonder what I missed out on. reply stormdennis 12 hours agoparentprevMy sister is far far richer and more successful than I am. She once told me that every night the last thing she'd do was to make a mental list of things to do tomorrow. That was it. Nothing written down. reply karolist 12 hours agorootparentI feel there's more to the story. One thing is being organized, another is being able to execute consistently, and being organized is not a precursor to that. I am extremely smart, and productive, sometimes. Most of the times my mind jumps to random areas of interest, life happens, fight with wife/GF, parent illness, alcohol binges and I'm back to a baseline with almost no output. As if I'm sabotaging myself. Consistency is key, how you achieve it is second. reply lelanthran 2 hours agorootparent> fight with wife/GF Life hack - you'll have fewer fights if you have Wife XOR Girlfriend, and not Wife OR Girlfriend :-) On topic, I'm pretty much the same - my own brain sabotages me via nerd-sniping more often than any enemy ever could. I don't really think there's anything to be done about it, TBH. reply jjjjj55555 13 hours agoparentprevDoes that mean that you don't get stuff done? Or does it mean that you just decide what to do moment by moment? If it's the latter, then why does having it written out add any more stess? For me, NOT having stuff planned out is what's stressful and the difference in productivity is noticeable when I have some sort of to-do list/schedule vs. when I just wing it. reply et-al 12 hours agoparentprevI think the author is trying highlight that one doesn't necessarily fancy tools. The productivity space has so many options that it's easy for one to get overwhelmed with settling on the \"best\" option. My partner relies on a Leuchtturm weekly planner; my father bought into the whole Stephen Covey system; a coworker has a stack of Post-its, and I use a hodgepodge of Google Calendar (far off single events), Apple Notes (weekly tasks for work) + Reminders (medium-term todos). The whole point is not to rely on one's (aging) memory to keep track of stuff. As long as tasks aren't falling between the cracks, keep doing what you're doing. reply materielle 12 hours agorootparentI agree with the parent comment, but I would phrase it as: This article feels so foreign to me because I‚Äôm not trying to be productive. I don‚Äôt have a productively system, because I‚Äôm not. I complete my works tasks. Those usually have lists. After work and on the weekends, I spend time with friends and family, and do hobbies as I enjoy them. Beyond that, I don‚Äôt try to remember things. I let stuff slip through the cracks (ok, I have a planner for birthdays). I don‚Äôt try to get things done. For what it‚Äôs worth, if you looked at my life on the outside, you‚Äôd probably think I was ‚Äúproductive‚Äù. I can speak multiple languages, I make music, I play sports, and I have various programming projects going. But I don‚Äôt do any of those things because they are productive. Every day after work, I spend an hour or two doing whatever I enjoy in a completely non-systemic manner. And I find that over a multi-year time span you actually can accomplish a great deal with this ‚Äúnon-system‚Äù reply lycopodiopsida 45 minutes agorootparentYou don't use productivity systems for planning your leisure time and hanging out, you use them to free more time and to let the things go your way at the same time. I can compare myself to my wife, who is not able to plan anything properly, and it is fascinating. She lets thing slip through the cracks, often small things which have no big consequences, but once in a while they snowball. A recent anecdote: a cabriolet she ordered gets cancelled, because she left the order unfinished for 3 months by not checking the order status and the model is not produced anymore. All easily avoidable with a proper organization. Her life mostly runs smooth only because I (and my productivity system) keep track of all the administrative stuff, otherwise things would bite her every other day. reply endofreach 10 hours agorootparentprevNo disrespect, but having / performing hobbies doesn't fall under \"being productive\" for me. You're just spending time randomly however you see fit (and seem to be happy with the outcome). Which is a nice thing, but probably the exact opposite of what is discussed here. Sure, if you wander around, you can see nice places. But planning a trip, because you want to see specific sights, is something different. Both are nice ways, depending on what you want from life. reply troupe 12 hours agoparentprevI find it helpful to give myself permission decide NOT to do something. The to-do list is a bunch of things that I thought were important when I wrote them down. If the next day I only do half of them, but feel those were the important ones, then I have success. If I decide that half of them aren't worth doing, the doing those would be failure. The goal of the list isn't to beat you up, but just a tool to make sure that at the end of the day you did the things that were important to you and YOU get to decide what that means. reply al_borland 11 hours agorootparentTony Robbins has a todo/project planning system called RPM. The training for it is hours and hours long, and the maintenance of it is also crazy. I wouldn‚Äôt recommend anyone use it verbatim, it‚Äôs just too much. That being said‚Ä¶ It did bring up the concept you mention. It basically had a person set their goal, then write down everything they could do to get there. From there you pick the ones that will get you the most bang for the buck. And when you hit your goal, you‚Äôre done. If that means it only took 5 tasks out of 47 possible, great. Goal achieved, trash the rest of the tasks and move on. reply 6c696e7578 13 hours agoparentprevI've a notes.txt file that follows me around in most jobs I do. It's more a journal than a planner. Sometimes I put TODO in, sometimes what to do when a change needs to be implemented so I don't forget. That works quite well as I can return to that later, or see what I was doing some months ago. It's in vim, which works well too for me as I'm already familiar with how to edit. One of the popular getting things done methods was to keep your stuff in one place, at least this article keeps inline with that idea. If it works, keep doing it. reply emadda 11 hours agoparentprevI think it may be the kind of work that you do too. It seems his work is well suited to his system. He can manage his own tasks, and the actual complexity of the work is stored in other systems and documents (like research papers). reply NoPicklez 5 hours agoparentprevAnyone, I mean anyone can write a simple to do list. Surely when you go to bed at night or wake up in the morning you have in your mind what it is you want or need to do that day. reply vonjuice 13 hours agoparentprevime it's a journey of knowing yourself. If you adopt this person's workflow it won't work for you, but if you try something similar, start small and gradually add more organization, you might end up with something that works reply web3-is-a-scam 13 hours agoprevMy productivity app is just todo.txt in one drive using specificaly notepad.exe. You put .LOG at the top of the new file with a return. Save and close the file. Every time you reopen the file, the timestamp is append to the file. Add your notes, save, exit notepad. Open it again when you need to update, rinse and repeat. Nothing I‚Äôve ever tried has been more effective than just keeping this endless file. reply royjacobs 13 hours agoparentOh wow, I didn't realize notepad had that feature. Awesome! reply fuzztester 11 hours agorootparentMetapad is a nice replacement for Notepad, with a few extra useful features, but don't know if it has the feature you refer to. Likely not, because work in it stopped a while ago. But it is still available. I had used it for some years. Still may in future. https://liquidninja.com/metapad/ reply abhinavk 13 hours agorootparentprevI cannot confirm right now but Notepad was rewritten for Win11. It might have lost that feature. reply Kokouane 13 hours agorootparentJust tested. Both the F5 and .LOG technique work on Windows 11 notepad.exe reply gl-prod 13 hours agorootparentprevIt still working, I just tested on Windows 11 and Notepad. reply machomaster 12 hours agorootparentprevYou can always use Autokey/autohotkey to get this functionality in any software you may want. reply allanrbo 13 hours agoparentprevYou can also insert a timestamp in notepad.exe simply by pressing F5. (At least it used to be like this - haven't tried on newest versions of Windows). reply js2 14 hours agoprevEverything old is new again: John Carmack's .plan¬π: https://garbagecollected.org/2017/10/24/the-carmack-plan/ Archive: https://github.com/ESWAT/john-carmack-plan-archive ¬π https://www.rfc-editor.org/rfc/rfc742 reply plg 12 hours agoprevI have been using Apple Notes which is great because I can read/write in the moment on a Mac, on iPad, on iPhone, and everything is (almost always) synced and stored in the cloud. I have been doing one note per day. On the other hand, PTF (Pity The Fool) who tries to export these notes. I know some people have written exporters but of course, whenever Apple decides to change format, PTF. I would like to change to a more portable format, e.g. markdown, but I am looking for a solution that syncs nicely across devices. Maybe iaWriter. Maybe just sublime text or even emacs and just put everything in a dropbox folder. reply mortify 5 hours agoparentIf you don't need collaborative notes, Memos is a great markdown supported option. reply al_borland 11 hours agoparentprevI‚Äôve gone back and forth between Apple Notes and more portable solutions many times now. It has been awful, but I decided to stick with Apple Notes. If/when it goes away, I will need to suffer one more time. If I keep trying to app hop looking for the perfect tool, I‚Äôll suffer every 3-6 months for the rest of my life and never find peace. I like that Apple Notes gives me the option to write, easily add images, make tables, etc. While 98% of my notes are just text, and some of these things can be done in markdown, it is higher friction in markdown. So I‚Äôm choosing the lower daily friction and extra features, knowing that I‚Äôll probably experience one high friction migration day in the future, but that day could be 10 years from now. The biggest issue moving notes out of Apple Notes was the extra new lines and spaces all over the place. I have to assume if the app is going to shutdown that some nice developer will make a little tool to take care of this. If not, meh‚Ä¶ I‚Äôll use it as an opportunity to clean up some clutter. reply lloeki 1 hour agorootparent> some nice developer will make a little tool to take care of this There are already a bunch: - (discusses the internals) https://devlog.notespub.com/2022/08/site-generator-for-apple... - https://github.com/dunhamsteve/notesutils - https://github.com/ydkhatri/mac_apt - https://github.com/threeplanetssoftware/apple_cloud_notes_pa... reply overvale 11 hours agoparentprevText files in iCloud (or whatever whatever) works great. There‚Äôs a bunch of good iOS text editors. I like Runestone. reply twothreeone 13 hours agoprevI've been using chat apps \"send to self\" for this exact same workflow.. at work I just use Slack as it supports threads and basic formatting (e.g. render code blocks separately and clickable links). So every day has a few threads on different things I'm working on and I can just add notes on them as I go throughout the day. For my own projects I use a messenger app, which is not as nice because most messengers do not support threads. I was actually considering switching to an external text file for versioning purposes.. and being able to render code blocks would be nice, org mode looks like overkill though. reply Kaibeezy 5 hours agoparentI like Slack-to-self for the reasons you list and also because I think of things to do whilst out walking the dog, sitting on the bog, or should be sleeping like a log (like now). reply conqrr 9 hours agoprevDid the exact same thing, sublime shortcut to add a datetime stamp (Python command) whenever I needed it. Moved to Obsidian recently and the daily note takes care of that. The folder system helps a bit with organizing recurring themes that the daily log journal didn't. Edit: Reading the comments. Wow Uncanny so many people doing something similar reply intrasight 14 hours agoprevI've been doing this long enough now (decades) that some of my .txt files (I have one per client/project) are in the size range of 20mb. reply ta1243 13 hours agoparentThat's about 2.5-3 million words, 5 times the length of Lord of the rings. reply intrasight 13 hours agorootparentLots of code and data ends up in my .txt files. Also I just checked actual stats. Only three client/project files are over 10MB. Most are in the 2-4MB range. reply kjkjadksj 13 hours agorootparentprevI wonder what astronomical figure they‚Äôve billed the client just to spend that much time on the notetaking part of this project reply intrasight 13 hours agorootparentWhat astronomical figure does a law firm bill for all their words. reply mightybyte 13 hours agoprevI also use plain text files for a lot of my personal organization. My system isn't quite like what OP describes, but it has some things in common. Some of my files are also structured by date as a never-ending journal. This isn't for a todo list, it's for a journal of things I encounter that I'd like to be able to find again and that I don't want to accumulate as clutter elsewhere...i.e. in browser tabs, etc. Sometimes it's a web link, or maybe something I learned somewhere, something someone told me, etc. I include notes whatever words / strings I think I might use if I want to find this particular thing later. I use org mode and make each date be a top-level bullet so I can nicely leverage powerful text search tools like ripgrep, regular expressions, etc. I don't find it useful to force everything into a single file. Instead, I'll organize these text files somewhere inside a directory structure that I can recursively grep. Unlike the OP I do use mutable TODO lists to track high level lists of things that I want to continue to spend mental energy on, but I do like the chronological list of done things and I might think about adding something like that or maybe augmenting the chronological notes file I already have. I do depart from the world of plain text for keeping track of larger amounts of information such as good papers I encounter, complete blog posts that I might want to refer back to, etc. For this I use the fantastic DEVONthink tool. It's got a large array of powerful features including automatic OCR and indexing of images and an excellent search feature, but the one that I use the most is its ability to make a \"web archive\" from a link. This downloads all of a web page's resources and stores them in the database locally, making it really easy to refer back to things that I've seen before regardless of whether I have internet access or not, whether the website is still around, etc. reply kredd 14 hours agoprevI actually sent this to my friend as he always thought I'm crazy for doing this. Mine one is a bit simpler though, just a big .txt file with TODO and DONE sections. Some of TODOs just have dates next to them if they're urgent, otherwise it's just it's just a simple list. That being said, I do use my calendar-equivalent app on my phone for very time-sensitive stuff, just in case. reply c-smile 4 hours agoprevOriginal EverNote (v 2.0, first public version) was built on that idea - endless tape of notes : https://notes.sciter.com/wp-content/uploads/2017/09/evernote... Plain text is OK, or Markdown as poor man WYSIWYG for that matter, but at least images should be there too. reply anonacct37 13 hours agoprevGoing on 5+ years using a single giant org file. It's the only system I've ever been able to stick with for more than a couple days. I think of it as my labbook. reply AndyPa32 12 hours agoparentAlmost the same here. But I have two files, one as archive for completed stuff. reply wim 13 hours agoprevIt's something I also tried to do for a while, just by using VSCode and a bunch of text files. I really like the lightweight-ness of just being able to edit as if it's text, but wanted to have for tasks what VSCode has for code: command palettes, \"syntax highlighting\", jump to \"references\" (like dates) and an editor which understands structure (like an outliner, but without all the awkward text selection issues). Anyway all of that led us to try and build a dedicated \"IDE\", but for tasks/notes and multiplayer support [1]. Hopefully it's going to be useful for others working from their todo.txt/thoughts.txt! [1] https://thymer.com reply evnc 12 hours agoprevReminds me of Heynote, posted to HN recently[0]. In general I think this approach of \"super easy capture into an append-only log\" is great, especially if it can be paired with features to enable editing/re-discovery/search/synthesizing old ideas together, which exist in a separate view/mode from the \"just get something down as fast as possible\" mode. Working on something like this, but just in nights/weekends free time with other obligations, so it's been slow going. [0] https://news.ycombinator.com/item?id=38733968 reply jameschensmith 14 hours agoprev> So my daily routine looks like > [...] > 5. copy the next day's calendar items to the bottom of the text file Interesting. For a file with 51,690 lines at the time this post was created, I'm curious why the file is not ordered with the most recent day at the top of the file. reply MathMonkeyMan 13 hours agoparentAll that text is heavy to push out of the way when you insert a new line break. Also, the feeling of accomplishment as the job's entire history whizzes by when you press ctrl+end each morning. reply nickthegreek 14 hours agoprevPrevious: My productivity app for the past 12 years has been a single .txt file (2020) (December 23, 2021 ‚Äî 523 points, 202 comments) https://news.ycombinator.com/item?id=29661167 reply danjc 4 hours agoprevThis illustrates well that you don't need another productivity app - you just need a basic tool and discipline. This also generalizes to other domains. reply pantulis 1 hour agoparentAgree, the rationale of the FA and the HN comments goes to show that productivity is not about the tool, it's about how you figure out a system that works for you. reply hk__2 12 hours agoprevI mostly do this, but with a physical notebook. I wouldn‚Äôt be able to work with a .txt file because it‚Äôs too limiting: you can‚Äôt draw anything, you can‚Äôt easily make arrows between stuff, you don‚Äôt have the nice mental reward when you strike some item off your list. This is for the day-to-day organization. For the rest, I dump all my knowledge in a wiki (MediaWiki), and I use iOS/macOS‚Äô Reminders app to remember things to do far in the future (like \"cancel XYZ subscription\" in 6 months) or at very specific times. reply testcase_delta 12 hours agoparentI have an iPad and use notability for this. I have a 350 page note that's filled with to-dos, doodles, screenshots of quotes, etc. I love being able to just scroll up endlessly to see what I was thinking about or working on, all in chronological order. reply pchm 13 hours agoprevI have a TODO.txt and it‚Äôs the only productivity system that I‚Äôve ever been able to stick with. Just a list of stuff I need to do, what‚Äôs done gets moved down or deleted. Maybe there‚Äôs value in having an archive (a DONE.txt?) but I‚Äôve found that after a while most notes/items lose the context and often it‚Äôs hard to decipher what they were about. One thing I haven‚Äôt figured out yet: I‚Äôd love to be able to keep this file open at all time, have it pop up with a hotkey. Currently it‚Äôs just a TextMate window that I often close by accident. reply Full_Clark 9 hours agoprevI use the notes app on my phone and I'm also split between two calendar apps and three email accounts, none of which integrate well with each other. It's a real dog's breakfast of a system and while I forget few things, I procrastinate many of them because the act of even trying to track what's done vs outstanding is very taxing. ahh well, c'est la vie reply wouldbecouldbe 12 hours agoprevTo be honest, that's why apple notes really is great. I have the same, but apple notes manages to save my mess on all my devices without ever overwriting my own changes. reply koch 9 hours agoprevLooks like markwhen[0]. When making it, which initially started out as a strictly timeline-making tool, I realized it is essentially a log or journal language - write a date, any date, and add some stuff to it. Good for notes, blogging, a calendar, etc etc. [0] https://markwhen.com reply pwillia7 12 hours agoprevI ended up at the same point after a lot of trying and failing but wanted a _few_ extra features than notepad offers. It's also important to me to be able to take notes in a browser. I do use logseq/obsidian in my better moments, but having another faster system is so helpful for a number of reasons. I have been building my own text bookmarklet[0] that I use for this. [0]: https://github.com/pwillia7/Text_Bookmarklet reply bobbylarrybobby 12 hours agoparentI'm curious what the reasons for another faster system are? Obsidian is pretty fast for me; I can't imagine it being much faster. reply crtified 12 hours agoprevMy todo.txt is more of a digital whiteboard, a temporary summary. Once the day or task is done, the text is wiped. (That's not to say I don't document things - only that I don't use my todo.txt as long-term record-keeping) The concept (along with sentiments such as https://news.ycombinator.com/item?id=39434558, where the mundane inefficiency of having to access the todo.txt window every time is the annoyance) makes me think that a wall-mounted screen dedicated to displaying the list may be an improvement for some people. Alternatively a multi-monitor setup where one (perhaps smaller) screen is permanently dedicated to the list. After all, back in the old days when people did literally do all this with physical black/whiteboards and similar, you didn't have to \"pull up the whiteboard\" every time you wanted to look at it - you just turned your head a little, or shifted your gaze. In that particular sense, having to open or pull up a file every time is a regression, an added inefficiency. reply akira2501 13 hours agoprevOld man checking in. I use the TOPS Steno Pad and PaperMate Gel Ink pens. Light weight, damage resistant, and no power required. Still works a treat. There's something about writing information down on paper that makes it store in my memory differently. I read volumes of digital text on my monitor every day, but I write very little into the steno book, so almost everything I write gets stored very deeply in my memory and is very easy to recall even when I don't have the original. reply MathMonkeyMan 13 hours agoparentI sketch all of my ideas on yellow legal pads for the same reason. It's my playground. Mostly diagrams and code snippets. Playing around with the names of things. For my \"TODO/notes,\" though, I still use a plain text file in gedit. No timestamps, no nothing. When I want to remember how to build Envoy with bazelisk, I search for \"bazelisk\" and see the most recent, oldest, and all intermediate attempts with my notes (e.g. \"runs out of memory, but if you're not building the tests, you can get away with 6 cores\"). I've gotten into the habit of \"tagging\" potentially useful information with words that I might search for later. Need to save an error message for possible later reference? notes.txt. Meeting notes? notes.txt. TODO? notes.txt. Rough draft of slack message? notes.txt. You get the idea. reply Ecoste 14 hours agoprevNow do an article on how to get the discipline to keep this going. reply dudinax 13 hours agoparentAs an undisciplined person who nevertheless does something similar, elimination of the fear of forgetting something is enough motivation. reply cpeterso 7 hours agorootparentIt‚Äôs your ‚Äúbackup brain‚Äù. reply zwieback 14 hours agoprevTakeaway: if you're already highly organized and disciplined a simple tool is all you need reply jimbokun 13 hours agoparentI see the causation in the other direction: a simple tool is more likely to help you get organized and stay disciplined, due to the low activation energy to get started and keep using it. Otherwise, there is a great temptation to futz around with your organization tools instead of making plans and getting things done. reply wyre 13 hours agoparentprevI'm not highly organized or disciplined and simple tools still work better, but a single txt file is too simple. I've had good luck using Things 3 as an easy and flexible to-do list for tasks I will forget and I've been using Obsidian as a tool for everything else. My takeaway is that the effectiveness of organizational tools scales with discipline and the simplicity of the tool removes organizational friction. reply whartung 13 hours agoparentprevI don‚Äôt see any tool working if you‚Äôre not disciplined. Organization is just a facet of that. reply jcul 12 hours agoprevI've tried various note taking / organising strategies in the past and nothing compares to pen and paper for me. I used to go through a lot of notepads as \"scratch\" work and would also lose things that I would like to look back on. So for the past few months I've been using a rocketbook. I have a special format for weekly todo / done tasks, a kind of daily log format, pages for meeting notes, and then scratch pages for rough notes etc. I usually upload the weekly / daily / meeting stuff or research stuff I may want to keep. Rocketbook ocrs the page and uses anything with ##s as a title so I can find stuff quickly. I have set up different Dropbox folders for different categories of notes. It's been working quite well for me. Some things like documention, or draft documentation etc. I do store in markdown text files, and sync between my devices with syncthing. On my phone I used termux and vim for editing them, which works surprisingly well. reply dandy23 13 hours agoprevI used to track every project in its own text file. Every task and a description was in this file. It was great, but got a bit messy. So now I use EasyOrg [1] where I also track each project's todos in its own text file, but now with time scheduling, search by time, links to other tasks in the file etc. [1] https://easyorgmode.com reply tarr11 14 hours agoprevOP uses their calendar as a supplement for their the todo file. There is a lot of functionality implied in that decision: - calendars have mobile apps which enable quick and precise entry - calendars understand time spans - calendars have many options to display events - calendars have cloud syncing - calendars are backed by a queryable data store Not saying using a text file is good or bad, but I think a more accurate title would be ‚Äúmy productivity app is a never ending txt file and a calendar app‚Äù reply mortify 5 hours agoparentI love using my calendar as a data store. The \"notes\" section of the calendar is often full of info. E.g. for travel it includes my itinerary, confirmation codes, links to my boarding pass, etc. One stop shopping for an event. However it's limiting for detailed lists of things to do (e.g. great for travel, lousy for packing list), and it's terrible for time-independent data: notes from talking to my heating contractor, screenshots of setting for my media player, etc. reply joemi 11 hours agoparentprevIndeed. The calendar app really is doing the heavy lifting in their case, not the txt file. I can kind of understand the author not really registering just how much the calendar app does for their organization, since calendar apps are so ubiquitous. reply Herodotus38 8 hours agoprevI started doing this for work after reading this here 2 years ago. It‚Äôs been helpful, but I haven‚Äôt made it a consistent habit to check the night before. I don‚Äôt access the todo list from home (partially due to barriers inherent in my work‚Äôs IT dept and also of my own making). I also keep it pretty limited to my work and don‚Äôt let my other hobbies/projects/family onto it too much. It‚Äôs been a good record and reference for things! reply denvaar 12 hours agoprevI am drawn to the idea of keeping todo lists, but it seems like whenever I start to do it, I begin to feel stressed out or overwhelmed. Not so much by the contents of the list, but by maintaining a list in and of itself. Kind of like an obsessive type of problem. Does anyone else feel this way? reply lycopodiopsida 39 minutes agoparentYou have to find your way of doing things. I mostly stick to GTD, and I've used Apple Reminders, org-mode, and Omnifocus (currently back to Omnifocus) over the years. My workflow does not change so much between these applications, I have to adapt it to technical limitations of the application somehow, but these are the same projects, due dates, tasks, inbox, reviews. Now what stresses me out out and overwhelms me is not having a system in place - now I have to keep all the tasks in mind and to think of them all the time. reply Cthulhu_ 12 hours agoparentprevThat's not unique, and also, it's often the trap of any productivity tool; doing things in the tool becomes a productivity goal and gets you the feeling of productivity in itself, instead of the tasks it's supposed to help you organise. reply Gigachad 12 hours agoparentprevI only do them when there is a big gap between finding out about the task and doing it. Mostly I do them on the end of the day friday so when I get back on Monday and have completely forgot what I was working on, I can see a few checklist items for what I was in the middle of. Creating a 20 point check list is just pointless. Unless maybe it's a list of things you need to verify before pushing something forward and you absolutely can not forget any of them. reply rabbitofdeath 14 hours agoprevI also use a simple text file, but for sake of context switching - I have one file for one topic - account x has a file, account y has a file, topic z has a file and everything related to it goes in. This is all now curated in my Obsidian vault that is synced via the fantastic Git plugin. reply mark336 11 hours agoprevI use my own messageboard: https://willashani.com/gigabots/threads Feel free to post! And I use Apple Notes. I like the messageboard because you can see the relationships in a tree-based structure. I store techy things in the messageboard above. I store non-techy things in Notes or when I am in a hurry and its not high priority. reply fma 9 hours agoprevThis is basically this auto focus system I was introduced to. I use Google Sheets so I can have a few columns. I can print it out if needed, and easily accessible on my phone and desktop. http://markforster.squarespace.com/autofocus-system reply bloopernova 14 hours agoprevMine is: Orgzly Reloaded syncing to a webdav share, which is mounted at ~/Org so it can be opened by Emacs. For shared shopping lists, my wife and I use the OurGroceries Android app and website. It's simple and just works. reply ivalm 14 hours agoprevThis is how I run all my one-on-ones, just an append only list where every meeting just gets appended on top of another with a date. If there are kaban tasks/external documents/etc that are relevant they still get linked into this page. It very amazing to see what we're working on now, a week ago, a month ago, etc. And as a collaborative free-form document it gives both the manager and their report the ability to craft a story of what's happening (and check in on progress in a way that dashboards fail to represent correctly!). reply hu3 13 hours agoprevI use https://joplinapp.org because it allows for pasting images and files. Supports markdown. Has easy sync and also mobile and desktop apps. Free and open source. reply vunderba 11 hours agoparentJoplin is good but I absolutely hate the way that they structure your notes. If you have all of your notes in a folder ~/my_notes ~/my_notes/work ~/my_notes/music etc Joplin takes them and stores the notes internally as a SQLite table with UUID named markdown files. It makes it very difficult to use bash tools, finding them, other IDEs, etc to work with your files after Joplin has ingested them. Compare this to apps like Obsidian and Logseq (also open source) which don't mess with your markdown file organization. reply iaw 13 hours agoprevVSCode and Markdown go a long way for me because it allows for index linking and some other nifty tricks. The main challenge is making sure the md files are organized. reply machomaster 12 hours agoparentIf one uses .txt files, then one might as well use .md. And in the way, one might as well use something like Obsidian. So information is still textual, but there are a great lot of additional niceties one can now have. reply disillusioned 12 hours agoprevThere's a virtue in being able to take this and plug it into a large-enough-context-windowed ChatGPT to be able to search/converse with. Makes me think that the real play is to use ChatGPT for, say, an ongoing todo list/dialog/personal notes system for that purpose. Or wire up a custom GPT to reference notes stored elsewhere. Either way, the idea that you can interrogate, intelligently, a list of your own ramblings, is pretty damn cool. reply jcoletti 14 hours agoprevInteresting. I've been using Things (similar to Apple Reminders) for 10+ years, which I thought was really minimal, but a .txt is about as barebones as you can get. Makes me want to give it a whirl. Curious about the use of Remote Desktop with a mobile device. Being an iPhone user, I'd prefer putting it in iCloud Drive or something more easily accessible natively. reply AceJohnny2 14 hours agoprevObligatory reference to Emacs Org-Mode [1]. Author's approach is basically Org-Mode with fewer helpers. Org-mode's power is that, at core, it's just a text file, with gradual augmentation. Then again, Org-Mode is a tool you must install, accessible through a limited list of clients (Emacs originally, but also VSCode), and the power of OP's approach is that it requires no external tools. [1] https://orgmode.org reply Zambyte 13 hours agoparentI found the hierarchy imposed by Org was more friction for me than it was worth. Adding Org Roam into the mix and making many bite sized files in a directory, and hyperlinking them together has proven to be incredibly useful to me. Notes fall out of my brain and are instantly discoverable. I often find useful notes that I completely forgot that I wrote. reply Lyngbakr 12 hours agoparentprevA nice thing about Org-Mode is that you can keep an active todo list in one file for daily tasks and then at the end of the day send all done items to an archive file (C-c C-x C-a). That way, you still have all your tasks in a searchable format if you ever need to go back to them, but the active file ‚Äî which you open each day ‚Äî is small and snappy. reply erik_seaberg 12 hours agorootparentThere's also org-journal, where a global keybind (I use the recommended C-c C-j) adds an empty timestamped item to a dated file. reply ErikAugust 13 hours agoprevI made a Node CLI that captures everything in a flat JSON file: https://github.com/ErikAugust/todo/blob/main/applications/cl... reply ivanjermakov 14 hours agoprevI ended up using plain text files because it's the most efficient editing experience with my editor. Although I have multiple files for different things: work reminders, abstract ideas, bookmarks, etc. For time sensitive events I use stock calendar app on my phone because it's the only thing I need notifications for (except email). reply komodus 9 hours agoprevI do the same grouped by week with some personal markup like: FEB 18 - todo x done ! important ? optional FEB 11 - todo x done ! important ? optional reply Kaibeezy 5 hours agoprevJarring that the author doesn‚Äôt use 24:00 time notation. 3:45pm meet with Oprah 4pm Rihanna talk 15:45 meet with Oprah 16:00 Rihanna talk Monospace typeface keeps it tidy and the timestamps pop in a visual scan. Zero AM/PM ambiguity. reply porridgeraisin 3 hours agoprevI use heynote.com just like this reply eschneider 14 hours agoprevYeah, I use a similar text file journaling system to this and have for years. Let's me know what I need to work on every day, let's me know exactly where I left of debugging, etc, makes status reports a snap, and makes figuring out what I did all year at review time simple. Would recommend. reply nnx 8 hours agoprevAnyone using Freeform (the recently added iOS/macOS app) similarly? It feels to me like it can be a powerful yet simple productivity helper. reply overvale 4 hours agoparentI like it! Kind of an infinite canvas app. I had a board with boxes for upcoming meetings, notes from calls, useful links, screenshots of things I was thinking about, etc. A really good system actually. reply nunez 8 hours agoprevsame, except I use MacOS Stickies. Way faster and much more persistent than Trello (where they end up, eventually) reply overvale 5 hours agoparentStickies is under appreciated. I saw someone in a coffee shop once outlining an essay or something in stickies. They had each point they wanted to make on a sticky, with some details, and they were moving them around the screen to re-order their ideas. Maybe not the most durable idea, but you could screenshot your screen if you wanted some kind of version control. reply inferense 13 hours agoprevthere's a much better way providing simplicity with full data ownership and real tasks out of the box in daily documents https://acreom.com reply usernamed7 5 hours agoparentSorry but the fact that you would reply to a thread about a TXT file with the gibberish \"providing simplicity with full data ownership and real tasks out of the box\" shows you don't get your own product, and that you are unable to make a compelling case to use it over a simple TXT file. reply akho 10 hours agoprevThis looks like a bullet journal (with the transfers to next day & from calendar), but without the journal. I, too, find the approach very natural. reply swah 11 hours agoprevNothing really works for me long run - my todo files get a bunch of random notes with the actual tasks, and I don't want to go back to them.. reply walteweiss 14 hours agoprevMake it .org file and you‚Äôre in a different league instantly. reply kingkongjaffa 13 hours agoparentI've maintained a single log.org file for 5 years now and it's been great! reply walteweiss 1 hour agorootparentI have multiple files, by the way. And as a recent org user I‚Äôm not quite sure what is better. It feels like org designed with huge files in mind, although for me it works with multiple files as well. reply bachmeier 13 hours agoprevI tried doing it this way. While it sounds nice, and apparently works for some people, my biggest problem was stuff getting buried hundreds of lines into the file. I couldn't trust it to remind me of anything beyond the few items at the top of the file. Another problem was having everything as a big blob of text mixed together, unless you take on some overhead when entering things (put them in the right place) or during the day (moving things around). If this appeals to you, I'd recommend using a big html file with Javascript to query the things you want to see, using class names as tags. A good text editor will have snippet support, and you can just dump any new items at the top of the file as they come in. If you want to get fancy, you can write in markdown and convert to html on the fly. reply cpeterso 7 hours agoparentI split my todo list into three files: today, this week, and later (backlog). Every Sunday, I move dated task reminders (and copy some recurring weekly reminders) from my later file to my ‚Äúthis week‚Äù file. Every day I move reminders from my ‚Äúthis week‚Äù file to my today file. Ideally my today file would be empty at the end of every day, but in reality I always have tasks that roll over to next day‚Ä¶ or week. This system helps me focus on my most immediate work and still have peace of mind that my system won‚Äôt lose the reminders in my later file. I store my todo files in Dropbox so I can access them from multiple devices (including my phone) and get automated backup for free. reply namegulf 7 hours agoprevSame here but it's a spreadsheet Previously standalone (libre) now google sheet reply hanezz 13 hours agoprevSimple txt file is the best indeed. For easily creating a new file (from a template) everyday in VSCode, can recommend the vscode-journal extension. reply jckahn 14 hours agoprevThis really is the way. Just one long append-only dump of all pertinent information. It‚Äôs the perfect complement to what‚Äôs in my head because I know where everything is! reply swah 10 hours agoprevI use all the systems mixed together, a bit of text files, a bit of Obsidian, apple notes, notion.. reply xandrius 13 hours agoprevJoplin + Dropbox + Markdown = free form, full control over data, checkboxes (if needed), mobile/desktop support - top reply rocky1138 14 hours agoprevI do this as well, except I tend to stick to markdown for formatting. This file is routinely committed to git. reply due-rr 14 hours agoprevI love the idea. Do you think he uses Remote Desktop from his phone? Or does he only use a desktop or a laptop. reply yard2010 13 hours agoprev> 4pm Rihanna talk (368 CIT) 5pm 1:1 with Beyonce #phdadvisee Sounds like a nice day. reply LeicaLatte 9 hours agoprevGood choice for non visual users maybe reply cptaj 14 hours agoprevI use notepad++ with 2 columns of files and around 40 tabs open. reply mrshu 13 hours agoprevProbably needs a \"(2022)\" in the title. reply chrsw 13 hours agoprevI thought I was the only one that did this reply system2 5 hours agoprevMy obsidian already got out of hand. I switched to asana for business todo but it also got out of hand. I still create txt files occasionally. I think it is inevitable. reply j45 13 hours agoprevLists in text files work great. So unbelievably great. I go back to them often, and some things can outgrow them completely when you want to: - Reduce the work between my list and collaborating with others (a shared list) - First class convenient experience on all my devices is useful - Notes synced automatically can go a long way. - When projects grow, and there's more details to manage, along with updates, than not. Has anyone used the text file appoach that can be compatible with LogSeq/Obsidian? I'm not sure why, but this time using mainly LogSeq has clicked. I do run it inside of an Obsidian Vault just in case... but haven't used Obsidian much. I really like the feel of per line item like a text file that logseq provides. There are tradeoffs too though. I think I might be enjoying it because it's plaintext, with a little bit more metadata, when/if I want it. The hashtags I invent that only I know are almost an unfair advantage for pulling up all the meetings with a person, or a topic, etc. reply snarfy 14 hours agoprevI noticed your format is pretty close to markdown, which is itself just ascii. Might as well go all the way. reply superkuh 14 hours agoprevSame. Ever since I had my proprietary rich text note taking application/database corrupt and become inaccessible in the early 2000s I've used a single notes.txt file with filepaths for noting images and other rich media. It is super simple to search within; everything is in one place. And it'll never become corrupted or inaccessible. reply ukuina 14 hours agoprev[2022] reply yieldcrv 5 hours agoprevsame, I use TextEdit specifically because its seemingly the only gui text editor left on MacOS that isnt online sometimes I use notes, for the syncing, but then I regret it reply goodburb 11 hours agoprev[duplicate] reply rnewme 10 hours agoparentWhy not just git? reply dbacar 13 hours agoprevOverkill. Just use Obsidian and never look back. reply athorax 13 hours agoparentHow is a single text file overkill vs. using a whole application to structure your notes and format with markdown? reply joemi 11 hours agorootparentIt's not a single text file, though. They're using a single text file AND a calendar app. (That said, I still think it's simpler than using Obsidian.) reply Glench 11 hours agoprevAbout a month ago I made a chrome extension that adds a \"sometime this week\" todo list at the bottom of google calendar (a feature I copied from Hey calendar). Any items that don't get done roll over to the next week and I can go back to previous weeks to see what I got done. Super helpful to help plan out my week that way and integrated directly into my calendar. reply seoulbigchris 11 hours agoprevHow did the meeting with Madonna go? reply dudul 12 hours agoprev> My daily workload is completely under my control the night before This being the key for such a system to work. Preparing a todo list the day before would be mostly pointless for people who have to deal with interrupts and such. reply renewiltord 12 hours agoprevSame. I saw a very productive friend just build a list like this: - Thing to do - Subtask before that can be done - Another level deeper - Another task And he just deleted things from the list when done. I adopted it and quite like it. I've tried keeping it in git, using some tool, etc. but in the end the Notes app on Mac with the same format helped because I dislike the strike-through stuff. It just occupies cognitive space. Just deleting feels better. Notes.app is nice as well because if I have it on a hot corner I can access it easily. reply swah 10 hours agoparentWhat about attachments? :) Comments on new stuff on that topic? Etc. I need more, at least something like Todoist/Ticktick... reply renewiltord 5 hours agorootparentUnderstood. I do better without those things because I'm more likely to over-record than miss out on information but I can see how things land differently for someone with the inverse abilities. reply devmor 14 hours agoprevI just use a weekly planner. I have a nice one without pre-printed dates made by Moleskein. I find the act of writing things down with a pen helps me remember them better as well. reply TOGoS 14 hours agoprevI use a format[1] that's _slightly_ more structured, in that files are divided into explicit entries with headers to indicate whatever metadata I want, and I also use this same format for storing other information (metadata about my music[2], workshop projects, orders, whatever). Other than that, same, bro. [1] https://github.com/TOGoS/TEF [2] https://www.nuke24.net/music/music.txt reply yorman2251 11 hours agoprev [‚Äì] Flffmff reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jeff Huang has utilized a single text file for 14 years to manage daily tasks, appointments, notes, and ideas, stressing the necessity of using a calendar alongside it.",
      "His productivity system includes creating a daily task list before sleep, adding tags for quick retrieval, and handling emails effectively, aiding in workload management and organization.",
      "This method not only allows him to stay organized but also serves as a comprehensive log of his activities over time."
    ],
    "commentSummary": [
      "Users share their diverse experiences and methods of organizing tasks and notes using tools like text files, Obsidian, Vim, git, and various productivity apps.",
      "Discussions revolve around the effectiveness, adaptability, and productivity tips within these systems, emphasizing the search for the most suitable approach based on individual requirements.",
      "Different strategies for memory retention, note-taking, and productivity are explored, highlighting the importance of finding a personalized system for optimal results."
    ],
    "points": 429,
    "commentCount": 231,
    "retryCount": 0,
    "time": 1708366452
  },
  {
    "id": 39428409,
    "title": "How to Enhance Firefox UI: Improving User Experience",
    "originLink": "https://github.com/black7375/Firefox-UI-Fix/wiki/%5BArticle%5D-1.-How-to-make-a-better-default-Firefox-UI",
    "originBody": "black7375 / Firefox-UI-Fix Public Notifications Fork 187 Star 4.9k Code Issues 83 Pull requests Discussions Actions Projects 1 Wiki Security Insights [Article] 1. How to make a better default Firefox UI Jump to bottom Edit New page MS_Y edited this page ¬∑ 4 revisions Disclaimer: This article covers Firefox v89, and there are some improvements today. After the release of Firefox v89 in 2021, there was a tremendous amount of controversy(reddit, HN) over the UI, and this repository quickly gained popularity for addressing most of the issues. Waterfox and Floorp browsers also set my theme as the default. Between writing that last post and hesitating to write it, time has passed, it's 2023, and Mozilla has decided to end support for Windows 7 and Windows 8. Firefox v115 is the last version to support legacy Windows and will be available until September 2024. Of course, mozilla has cleaned up a lot of legacy code, and this project also required me to do a lot of work and remove features related legacy Windows. So it seems like a good time to write this post. Since this article also covers UI on legacy windows, I don't think it should put it off any longer than that. What was better than the Proton UI in version 89? Let's dive right in. Principles For me, there are 3 big and 16 small criteria for UI/UX design. I don't have the data or the environment to do A/B testing, so I have no choice but to predict user behavior, which is very important. I had to think deductively, using various theories. Intuitiveness: Be easy to understand and use. Simplicity: Remove unnecessary things or make it simple to reduce cognitive burdens. Visibility: Key features are visible for ease of operation. Accustomably: Reduce the amount of learning to adapt quickly. Consistency: Reduce the exception for easier remembrance. Predictability: Reducing anxiety by making you think it's controllable. Affordance: Provides clues how to manipulate. Efficiency: The goal should be achieved quickly and accurately. Clearly: The distinction shuold be well-recognized. Proximal: Close enough to move quickly. Bulky: Large size is easy to recognize or move. Snappy: Smooth and no janky for responsive Flexibility: Accepting requirements and minimizing mistakes Accessibility: Accommodate a wide range regardless of age, disability, etc. Functional: Accommodate a wide range of features to meet needs. Contextual: Exist naturally in each situation. Harmonious: Exist naturally in each elements. Pleasure: Pursue satisfaction for fun or creatively. Compatibility: consider lower version APIs, higher version roadmaps, upstream or OS, RTL, etc. Problems I'd write down what the problem was that made me start this project and made people uncomfortable. Firefox v88 Photon UI Firefox v89 Proton UI When I tried the developer edition of version 89, I was in for a big shock. This is because, despite the neat first impression, it was just too uncomfortable. There were three main issues. Tab: Tabs that look like buttons Padding: It's too big Icons: It's hard to read on its own Why Tab(Accustomably, Affordance) Accustomably Often referred to as Jakob‚Äôs Law, when they encounter a new app, they expect it to behave the same way as the traditional UX they already know. Certainly, Firefox's new tab UI seems like a difficult adjustment. But saying that you shouldn't change your UI simply because you're not used to it can stifle progress and lead to decisions that only follow trends. You can be a fast follower, but you have to be careful because it stifles innovation. Affordance Okay, so it makes sense to provide another rationale. I can use Gestalt psychology to describe the problem. Law of Proximity: Off the toolbar, so feel it's a different group Law of Similarity: Different color from the toolbar, so feel it's a different group Case Studies: Apple Safari Apple has announced at WWDC that macOS 12 Monterey is getting an all new Safari browser. However, after many complaints from users, Apple came up with a compromise in Beta3 tabs that look like buttons. macOS Monterey Beta 3: Apple Redesigns Safari Tab Interface Following Complaints macOS Monterey beta 3 brings redesigned Safari tab interface to address complaints But it still failed, and Apple eventually declared a complete rollback to classic tabs. Apple Releases Safari 15.1 for macOS Big Sur and macOS Catalina Apple reverts to more traditional Safari tab design in macOS Monterey RC following controversy Apple isn't necessarily right, but the \"button-like tab\" can be criticized, given that it's not easy to roll back to the beginning many times. Why is too much space bad? (Visibility, Proximal, Bulky, Contextual) Visibility The first is intuitively understandable. The web browser is a web content viewer and should not interfere with showing content. But Proton UI is losing out due to a lot of padding. This is fatal on devices with small screens, such as 13-inch laptops. Proximal & Bulky There is a logical way to explain this. It's Fitts's Law. The Fitts's Law predicts that the time required to rapidly move to a target area is a function of the ratio between the distance to the target and the width of the target. The formula and explanation for Fitts's Law is as follows: $$\\begin{eqnarray} MT &=& a + b \\cdot ID \\\\ &=& a + b \\cdot \\log_{2}(\\frac{2D}{W}) \\end{eqnarray}$$ $MT$(Movement Time): Average time to complete the movement $a$, $b$: Constants that depend on the choice of input device $ID$: Index of difficulty $D$: Distance from the starting point to the center of the target $W$: Width of the target measured along the axis of motion $a$ and $b$ are constants, so they don't matter. In other words, $ID$ is what matters, and the $\\log_{2}$ can be omitted, as can the constants. The ratio of $\\frac{2D}{W}$ that is left over is important here. $2:1$, so reducing the distance is the first step Maximize the width, but not the distance Fill in the extra space, to prevent padding from getting too large Now let's compare which UI is easier to click. As you might expect, Photon(v88) is easier to press. The $W$ in the Amazon icon is larger The $W$ in the menu is a little smaller, but the $D$ is much smaller. Especially for the Amazon icon, the loss makes the clickable area look smaller than it actually is. It's a good idea to fill in the padding area to make the icon look bigger. Contextual In this situation, Mozilla deprecated the compact UI. But there was something even worse.. Try to spot the difference in the following screenshot. That's a little hard, isn't it? The answer is that the tab bar and toolbar are slightly different heights. In v89, these were Compact UI, Normal UI, and Touch UI, respectively. Indeed!!! The difference between each UI was almost meaningless. Case Studies: Google Chrome It's also interesting to compare this to the UI spacing in the most popular browser, Chrome. Do icons help? (Clearly, Accessibility) Clearly & Accessibility Content design considerations for the new Firefox and wondered if it would be a good idea to remove the icon. I did a quick search and was able to find Nielsen Norman Group's study of icons. The results of the study confirmed my suspicions. Text-only was the worst result. Lower bars are better Lower bars are better Higher bars are better Case Studies: 2023 Chrome Design Refresh Chrome's new UI, rolled out this year[reddit thread], includes icons. Although I don't like the shape of the icon. Redesign Tabs There's more to creating tabs than just how they look. Here we'll take a look at some of the Lepton Theme tab design choices. Tab states Simplicity, Consistency, Predictability, Affordance, Clearly, Functional, Harmonious, Compatibility What is the states of the tabs you know? Selected Multiple Selected Hover (Mouse up) Text overflow Tab overflow (Tab scroll mode) Reaching tab resizing Sound & Muted Autoplay Block PIP(Picture In Pictuure) Pinned Container No favicon Changed title Loading Unloaded Crashed Shared Theme UI Density ...ETC All of this needs to be presented consistently within 36px to 240px. This seems like a complicated problem. Let's take a look at some common mistakes in the Proton UI. Which tab is the selected tab? The correct answer is to the left. The first(Photon) and third(Lepton) are easy to recognize because they are connected to the toolbar. To reduce visual overload, Lepton shortens the container indicator's in the background tab to make them shorter. But secondly(Proton), it's hard to tell which tab is selected between the highlighted background color and the bar at the top. The top bar is the container tab indicator, which feels similar to the selected tabs in predecessor UI, Photon. But what about when it's Selected, Multiple selected, Hovered, or General? Photon: Connected with a toolbar to recognize Selected Easily distinguish between Multiple selected and Hovered with the color of the top context line Multiple selected's background color is distinct from Selected, and Hovered's background color is closer to General. Proton: Selected and Multiple selected look almost exactly the same Lepton: Connected with a toolbar to recognize Selected They all have different background colors to distinguish them By comparing the results of setting many states arbitrarily, you can see how simple, consistent, and affordance Lepton is. Muted Muted + Hover Muted + PIP Muted + Hover + PIP Photon: Muted is indistinguishable from Muted + Hover. When the PIP indicator is present, Muted's position changes significantly. Proton English: Muted, display only as text PIP is only known when you are in hover. Proton Korean: Surprisingly, different display languages result in different UIs Muted to hide the favicon No indication is obtained when PIP Lepton: All the information is known, and the positions of Muted and PIP are constant When hover, it gives a clear signal that it is controllable Let's look at more cases. Proton was nearly impossible to use in Korean. Muted + Loading Muted + No favicon + PIP Muted + Pinned Muted + Pinned + Hover + Titlechanged Photon: Useless icon when Muted + No favicon + PIP Muted + Pinned + Hover has a different effect than Muted + Hover There is a lot of information overload in Muted + Pinned + Hover + Titlechanged Proton English: Useless icon when Muted + No favicon + PIP Muted + Pinned + Hover is indistinguishable. Proton Korean: Loading indicator to hide the Muted icon No indication is obtained when PIP Lepton: Always represent all information If No favicon, replace it by increasing the size of Muted. The position is always the same, even when Pinned If the locations overlap, like Container and Titlechanged, combine them to display. Utilizing space Lepton also has a smart and efficient behavior based on the width of the tab. Like Proton, it shows all the close buttons when there is enough width. If the tab reaches the end and runs out of width, similarly hide the close button on the background tab. The size of the close button has been reduced to reduce accidental clicks. You can still close it by hovering over the background tab. Adjust the padding and gradient so that the tab titles are also effectively visible. Lepton sees the New text on the last. By using an optical illusion to save space. Lepton is a wiki with the words clearly visible. (Adjust only when width is insufficient) Lepton shows more of web contents Photon and Lepton have tab separators Distributions There are certain criteria for what constitutes a good UI for each person, but it's also a matter of \"taste\" to some extent. The following are all good examples: Lepton with common tabs to match Proton Photon tabs with context lines at right angles Original proton tabs So I've picked three cases to match the concept of distributions. Lepton, Photon, Proton like tabs My theme in particular is known for its support for Photon tab UI. Add icons and adjust padding Panel and context menus Icon and adjusting the padding eliminated the previous complaints. There was an issue with unbalanced strokes, but our great contributor @zapSNH fixed it and now we have a top-notch set of icons that are consistent and recognizable across browsers. Edge Chrome LeptonEdge Menu whose icons are blank for no particular reason They are consistent, but they look too much the same throughout and are difficult to scan The metaphor of some icons is confusing, like New InPrivate window. (It took me a long time to realize it was a person) Chrome The icon style is not modern The icon style is angular compared to the rounded UI The icon looks too small for the size of the text The icon size is disproportionate Lepton The icon style is modern There is a consistent set of appropriately sized icons Filled icons increase the likelihood of scanning (Only for important icons) Metaphors distinguish the appearance of icons The same goes for context menus. Edge Chrome LeptonEdge: I don't know why there is no icon for Add all tabs to favorites. Chrome: The icon doesn't exist at all. Lepton: The icons are laid out in a way that makes them clearly distinguishable. In the context menu for web pages, only Firefox has a group of shortcuts at the top. It achieves some quickly accessible effects like the pie menu. Shortcut group vs Pie menu More cases The following image is a set of icons before they were updated, so the quality is poor, but it should serve as an example. The global menu and various panels have icons, and you can see them change as folders are opened. Menus and panels will definitely shrink in size. The start of the text is aligned. The global menu also have icons. Add icons to the menu of each bookmark library folder and responds when it is opened. Add icons for different case layouts. Theme System default theme Lepton provides a UI that adapts to different OS and their settings. Windows 7: Follow the aero window design closely Windows 8.1: Just like Windows7, I've made sure that the title board blends in nicely. Windows 10 Light: I followed Windows 10's Legacy Edge colors to make it look the best for Windows 10. Windows 10 Dark: Same as above Windows 10 Dark + Accent titlebar: Windows 10 has a titlebar accent color setting. If it is enabled, you should follow it. MacOS Light: On a Mac, it should follow the color of the Mac. MacOS Dark: Same as above If you're using Windows 11, you can use the userChrome.compatibility.os.win11 option to use Windows 11 colors similar to Edge. It works well on Linux, of course, and fixes color issues and non-native titlebar button issues. Linux color issue Linux titlebar button Speaking of which, there are times when you need to make space for a titlebar button in the toolbar, like in Tabs on bottom or One Liner. Each OS has different values and locations. Win7,8: 105px, right Win10: 138px, right Mac: 72px, left Fullscreen uses native UI and should be treated as 0px Linux: 84px (May vary by system theme) Can be positioned on both left and right May or may not have three buttons Improvement proton theme I've shored up the shortcomings of the Proton theme and filled in the details. Fixed accessibility issues caused by the lack of color contrast in the Proton theme itself. Improved color contrast and outline shadow for bundled themes I've applied the proton theme to the parts that don't have it. It was a process of creating more uniformity. Proton theme apply I also applied dark mode to the parts that weren't applied. More darkmode Finally, I've made sure that the colors apply well if you're using a custom theme rather than the default theme. It would be great if you could infuse enough of your personality into the products you use. Colorful components In tables and lists, I've added a different color for even rows to make them easier to read. Even row background color for table and list Finally, audio player has curved borders to follow the Proton design, and the video player has made the content more visible. I've also modified the icon to make it look better. Media player(Audio, Video) Interactions There are interactions to help you understand what's going on, as well as interactions to entertain users. Helpful interactions Changes in mouse pointer when hovered over, or changes in the elements, provide appropriate hints that something is being done. If the action is different in nature and appears smaller than other actions, such as a zoom command in a panel, it helps to change the mouse pointer. It's intuitive when the mouse is over a field and it has a border to indicate that it can be pressed. The text of active items has accent color. Make the deleted items grayscale icons as if they were disabled, and stroke the titles. However, if you hover, you will see the title without strokes. Animations Animations make the UI feel smoother and more natural. But sometimes it also makes the behavior feel \"slow\". Therefore, I designed it like this: Easing curve of photon Follow the easing curve of photon, accelerating rapidly at the beginning and decelerating smoothly at the end Follow the above principle and shorten the duration of the starting animation to make it feel like it's reacting \"fast\". Follow the above principle and lengthen the duration of the exit animation to make it feel like a \"soft\". Larger objects have a longer duration than small The duration inside the nested object should also be long The limitations of the GIF format make it look choppy, but it's actually smoother Background Color Arrow rocate Container Tab Sound & Pinned Tab Sidebar The same applies for optional features that are not default. Autohide Back Buttonr Autohide Page Action Autohide Tab Bar Illustrations Photon's friendly illustrations reduced the embarrassment of the error. So I restored it. Of course, it's great for a welcome Welcom back DNS error Session restore Search result Conclusion This theme takes into account customs, psychology, and research findings and inherits the best of the existing themes. It also supports OS, dark mode, custom themes and interactions to suit your situation. Of course, there are some things we can't do due to the limitations of CSS, like page action button behavior. Now, you should have a better idea of the goal of my theme: \"ü¶ä I respect proton UI and aim to improve it.\" Discuss on Hacker News. Add a custom footer Pages 11 Home [Article] 0. Firefox UI UX history [Article] 1. How to make a better default Firefox UI Principles Problems Why Tab(Accustomably, Affordance) Accustomably Affordance Case Studies: Apple Safari Why is too much space bad? (Visibility, Proximal, Bulky, Contextual) Visibility Proximal & Bulky Contextual Case Studies: Google Chrome Do icons help? (Clearly, Accessibility) Clearly & Accessibility Case Studies: 2023 Chrome Design Refresh Redesign Tabs Tab states Utilizing space Distributions Add icons and adjust padding Panel and context menus More cases Theme System default theme Improvement proton theme Interactions Helpful interactions Animations Illustrations Conclusion Compatibility Issues Solution Installation Guide Options Screenshots Show Off Your Config Tips Tutorial Uninstallation Guide Add a custom sidebar Clone this wiki locally",
    "commentLink": "https://news.ycombinator.com/item?id=39428409",
    "commentBody": "How to make a better default Firefox UI (github.com/black7375)364 points by black7375 22 hours agohidepastfavorite233 comments tomxor 21 hours agoThe main issue I've had with their new UI is the massive size of everything, it's fair enough for fat finger phone displays but is annoyingly wasteful for any regular sized desktop. This is partly due to the increased padding as per the article, but also because they removed the \"compact\" UI density option. However you can bring this back under about:config browser.compactmode.show Then go to menu > more tools > customize toolbar ... \"density\" at the bottom. The option is labelled \"not supported\", but it's been like that for years. reply logicprog 20 hours agoparentIt's strange, I much prefer a substantial amount of padding to my interfaces. Having a good amount of padding lowers the visual noise/clutter and gives everything room to breathe, which may not give a specific practical advantage, but makes me feel less anxious looking at it. reply medstrom 17 hours agorootparentIt's strange, the absence of padding gives me room to breathe, because if I can quickly scan a menu with my eye then I feel I have good overview and control over affairs. With more padding, you cannot do the same scanning motion with the eye, you have to read each item as a single atom unto itself, and suddenly the menu has become a jungle of megaliths where it's easy to get lost. reply nathansherburn 9 hours agorootparentThere is research out there that shows use of white space can improve things like reading speed and comprehension. An example for text paragraphs: https://www.semanticscholar.org/paper/Reading-Online-Text%3A... I think it really depends on individuals though. If you can memorise a dense screen of buttons you'll be able to work faster, avoid scrolling etc. But it'll make the UI harder to use for people who don't use it regularly. Ultimately, every UI has to strike a balance. If you do it right you'll piss off both sides equally. reply Log_out_ 3 hours agorootparentThe ever smaller seeing slots in the \"knights jousting helmet of ui\" give me anxiety because I ride to battle and work with that things low info density. reply Shrezzing 19 hours agorootparentprevPadding's great on my desktop with two 24\" monitors. On my one-screen 13\" laptop it's less welcome. reply pxc 9 hours agorootparentPadding still gets in my way on a 38\" ultrawide with two 24\" monitors next to it, because my poor (uncorrectable) vision requires substantial UI scaling. If you can use tiny fonts for everything else I'm sure the padding is less painful but it's super annoying if you have to scale things up and you can actually get completely lost in it if you have to use much fullscreen magnification. This would be less frustrating if I could easily scale up UI fonts without also scaling up the whole UI proportionally, along with the padding. In terms of apps in my life with annoying padding or wasteful use of screen real estate, though, I have to say Firefox doesn't even remotely make the list. reply logicprog 19 hours agorootparentprevOkay fair haha reply jwells89 18 hours agorootparentprevWhitespace can be a good thing as you note, but thoughtful allocation distribution is critical, particularly on desktop operating systems. Firefox default isn‚Äôt the worst here but it‚Äôs also far from the best. reply paulddraper 12 hours agorootparentprev> which may not give a specific practical advantage It, in fact, gives a specific practical disadvantage. reply logicprog 12 hours agorootparentI mean, I don't really care to optimize \"number of buttons per screen\" (and text can be as dense as it wants, although I usually set my font size to 16pt or above) but to each their own. reply wtallis 8 hours agorootparentThe relevant cost of too much padding isn't fewer buttons on the toolbar. It's that too little of the screen is actually showing me the web page I want to see. It's usually most important to quantify in the vertical axis; today's bloated touch-oriented UIs are horrific for 16:9 wide screens. Add up the taskbar, window title and tab bars, URL toolbar, the 72pt dickbar menu at the top of the web page with single-line labels, and the cookie banner at the bottom of the web page, and you're lucky to have half of the shortest dimension of your screen devoted to real content until you start excising the bad UI elements. It's like being back in the 1990s and seeing the old horrors of people who said yes to every adware toolbar that asked to install itself, except we're now wasting far more vertical space for far less functionality. reply gloryjulio 17 hours agorootparentprevI uses a 40inch 4k monitor. The padding looks horrible. Chrome is not better in this regard though reply pdntspa 12 hours agorootparentprevThat feeling of anxiety when looking over software should be taken as a cue to get better at it, not a feeling to be processed as such. Because it goes away as mastery goes up. Pretty soon all that whitespace becomes anxiety-inducing in and of itself reply logicprog 12 hours agorootparentI think you're making condescending assumptions in order to explain away my different preferences, and I don't appreciate that. The feeling I'm experiencing is not anxiety at not knowing how to use the software or read the information presented, I am very comfortably a power user of basically every piece of software I use regularly. It's simply the fact that high levels of visual noise are more difficult to process than when information is clearly separated out and grouped and given enough visual space to be processed independently. Dense interfaces are just less visually restful. This is why I actually tend to prefer pieces of software with little interface at all, just keybindings, like my config of emacs. And I see no reason why being a power user would inherently make white space anxiety inducing, since there is no sensible psychological mechanism for the two to be connected in that direction, unlike the sensible psychological and vision processing connection between dense cluttered interfaces and a feeling of visual clutter. Furthermore, my feelings in this matter extend far beyond user interfaces: not only do I prefer clean user interfaces with generous use of negative space, I prefer that in my books, and the walls of my house, and the organization of my room. If my wall was covered in posters and sticky notes, instead of a nice clean beige with one or two posters, that would make me feel anxious as well, and it isn't because I don't know how to read a post-it. reply oblio 11 hours agorootparentprevI have a few decades worth of experience with software and cluttered UIs just suck. reply kwanbix 1 hour agoparentprevYeah, this is a problem for me also. FF UI is so big. Also, why a huge white space between the reload and url bar???? It is a horrible waste of space. reply darkwater 17 hours agoparentprevSo we have people in the camp \"don't waste space with padding, please\" and then, each time a KDE discussion appears \"how the hell can they cram so many information and text with no padding, it's unreadable\" camp. Damned if you do, and damned if you don't. reply medstrom 17 hours agorootparentThat's why much like the dark/light mode switch has become standard, \"compact\"/\"touch mode\" should too. reply Fnoord 14 hours agorootparentGoing full circle Hildon / Maemo. reply SamuelAdams 15 hours agorootparentprevWell, yes and no. Different people want a different user experience. So this strikes me as a need for a new user configurable option. Make a new user option, so it is easy for users to compact information if they want to. It sucks having to support multiple states (more things to test and verify) but it seems like there is an audience for both ideas. reply jlarocco 11 hours agorootparentprevThe mistake is to think there's a single \"right\" way to do it. Either make the UI flexible enough to accomadate everybody's personal preferences, or accept that some people won't like it and will choose something else. reply Klonoar 15 hours agorootparentprevKDE is fine with little to no padding. Their problem is often that they have inconsistent padding/spacing, which just throws everything off as death by a thousand cuts. reply jasonjayr 21 hours agoparentprevIs there any insight as to why do they mark it as \"not supported\" ? reply marionauta 20 hours agorootparentIt's corporate talk for \"if it breaks, don't complain\" reply arboles 20 hours agorootparentIt's also a sneaky strategy to deal with features you've decided to remove, because users are that fucking stupid. 1. Instead of just removing the feature, hide the feature and call it unsupported so the users who remember the feature can't complain yet. 2. Then finally remove the feature in the next update, with justification that it was an unsupported option and used by few people, so users can't complain. Frog boiled. With each update the company seems to be acting rationally on \"metrics\" and principles, but the decision was set internally before that. reply afavour 19 hours agorootparentIt can be metrics driven the whole way. - Compact mode is rarely used and a pain to maintain - If we hide the feature, what's the user reaction? - Minimal user reaction to hiding, we're safe to remove reply ndriscoll 18 hours agorootparentOf course, people who modify their settings in the first place are more likely to disable telemetry, particularly if they're choosing a non-default, low market-share application that specifically bills itself as privacy friendly. reply afavour 9 hours agorootparentIf you disable telemetry that‚Äôs being sent to a company you trust and a product you care about then that‚Äôs on you, frankly. reply bitvoid 19 hours agorootparentprevIf I recall correctly, that was their justification for no longer supporting it: too few people used it. Except it was tucked away in a small dropdown at the bottom of the customize toolbar screen, which requires right-clicking the toolbar to get to. If it was in the actual settings somewhere or, better yet, given as an option during the first launch flow, I imagine more people would've used it. I didn't even know about it until after it became unsupported. reply micromacrofoot 19 hours agorootparentprevnever remove features: ‚Äúproduct is too bloated‚Äù remove features: ‚Äúproduct is tricking us‚Äù reply jwells89 18 hours agorootparentUsually when people call a feature bloat it‚Äôs because its presence, resource consumption, etc is too great relative to its value and utility to users or it‚Äôs not particularly relevant. I‚Äôd hesitate to call something like optional compact UI metrics ‚Äúbloat‚Äù. To me the term is better applied to e.g. features associated with only tangentially related services or something running in the background sucking up CPU cycles for little user benefit‚Ä¶ basically the modern Microsoft playbook. reply stevage 13 hours agorootparentFor Firefox the classic example is Pocket. reply asadotzler 10 hours agorootparentprevAll features require maintenance as code around them and through them changes with time. Feature bloat is very often code bloat and maintaining code costs money, especially 20 million lines of it. When a module owner sees an opportunity to improve their module by removing low-use features that are built on code that's a challenge to maintain, that's a good thing for the long term health of the code base and the features it provides and the app they make up. reply arboles 12 hours agorootparentprevCode bloat is not that obscure of a thing. I think a decent portion of people realize that a program with features upon features is stretched too thin to meet users' needs in high quality, or in a timely fashion, especially if they paid $0 for it. Don't get me wrong. When Firefox removes a feature, often it's not out the concern of bloat to be able to serve existing users better, but to shift resources for the next revamp that will make the browser ever more \"modern\", to claw for a new userbase. reply arboles 19 hours agorootparentprevBrowser companies, famous for prioritizing avoiding bloat. reply 3abiton 21 hours agoparentprevThat was my issue with ir, I will try the flag today! reply sikhnerd 16 hours agoparentprevThanks for posting this, really makes a big difference! reply cassepipe 15 hours agorootparentI have been using that for some time but I just tried Lepton now. Installing Lepton is as easy as one-liner and imho well worth it. reply gsich 11 hours agoparentprev\"not supported\" - that was out of spite for people who criticized the \"modern\" design choice. reply black7375 22 hours agoprevI'm the author of Lepton, a popular theme for Firefox. You can see how I made various decisions from my perspective and how I improved on some of Mozilla's less-than-stellar decisions. I think that might explain how we improved it and made it popular. This article is part of a series. - https://github.com/black7375/Firefox-UI-Fix/wiki/%5BArticle%... reply tetris11 22 hours agoparentHow I find Lepton? In the addons store there's nothing listed under themes for that name. Or is Lepton a previous generation UI for Firefox that no longer exists except on old releases? Sorry if these are stupid questions, I did read the article but I may have missed some things. reply black7375 22 hours agorootparentSince Mozilla does not allow it as an add-on, you must download and install it yourself. Yes. I know it's really uncomfortable. Nevertheless, the fact that it was this popular is also proof that the existing UI was inconvenient. https://github.com/black7375/Firefox-UI-Fix/wiki/Installatio... reply iruoy 21 hours agorootparentIt takes 2 minutes and the instructions are very clear. Should not be a problem. This is the first time I've heard of Lepton. Now I've installed it I don't think I will go back. Thanks! I don't care about the tabs being buttons though. Mainly the huge amount of space the new design uses. And the lack of icons. reply user_7832 16 hours agorootparentprevDo you know how lepton would interact with the various firefox css themes (like those on the r/firefoxcss subreddit)? Some of them look really good but I'd imagine there might be conflicts and resulting instabilities. Btw thanks for posting this, I remember using lepton a long time back, I need to switch from edge soon haha reply jraph 22 hours agoparentprevI would not usually customize too much my UIs and just try to get used to stuff instead. It's just less friction, and it's nice to be able to install something and be used to the defaults. Your Firefox UI customization are so good and easy to setup that this is an exception. Thanks for making them. And they feel maintained, which is an important point. I didn't know you wrote extensively on this, it looks interesting and it looks like it is well documented, I'll be sure to read this. Thanks again! reply AlienRobot 20 hours agoparentprevThat's very interesting, specially that telemetry is being used to justify removing interface items. In my opinion the address bar is so incredibly large you could put 10 buttons in there and you would still have space, so I can't imagine a reason to bother removing things besides wanting to remove everything until there is nothing you can remove left. You seem to be knowledgeable about UI/UX. May I ask you a question? I have a theory that monochrome icons are worse than colored icons. Do you know if there are studies about this or if there's any consensus? Thanks in advance. reply black7375 20 hours agorootparentIn the middle of the article, there is a brief discussion about icons and colors. I also think that well-coordinated color icons are good for readability and usability. https://github.com/black7375/Firefox-UI-Fix/wiki/%5BArticle%... However, it is difficult to apply it universally to support a variety of colors. If it is similar to the background color, it is difficult to distinguish and there may be contrast issues depending on the light/dark theme. I think it's just the ease of development of a solid color icon that matches the color of the text. reply AlienRobot 19 hours agorootparentOh, I missed that. It's satisfying to see what I assumed to be true to be laid out so concretely as data points. I just wish it was something more 3D and \"skeuomorphic\" instead of just making a flat gray arrow into a flat green arrow. For example, if Chrome used a yellow star instead of a white star outline, I bet a lot of people would say it looks ugly as hell and sticks out like a sore thumb, but I'd prefer it. I can barely tell these monochrome icons apart. reply wolverine876 15 hours agorootparentprev> the address bar is so incredibly large you could put 10 buttons in there and you would still have space, so I can't imagine a reason to bother removing things Space isn't the only issue. Fewer options generally (very generally) yields better design - it's easier to find things, less distraction, cleaner, etc. reply 12345hn6789 16 hours agoparentprevthank you for your work. The installation process was very easy. I would recommend though, to simply outline the steps and move the advanced section to the bottom completely. ``` 1. Run script in your OS cmd line. 2. Navigate to `about:support` and click clear startup cache. ``` reply bloopernova 19 hours agoprevBetter browser UX, in my strongly-held opinion, starts with vertical tabs. With horizontal tabs, you can have maybe 6 to 8 tabs open before things tabs get difficult to manage or track. With vertical, nested tabs; links that open in a new tab are automatically made a child tab. From that you can infer structure and context more easily than horizontal tabs. Then you add colours to indicate different sites and now you see tab groups more easily. On top of that you can bookmark tab trees, thus saving progress of your research, documentation, etc etc. My CSS file and a couple of screenshots are here: https://gist.github.com/aclarknexient/88673880d373864eee1927... (I need to add a screenshot with nested and coloured tabs, will add that once I submit this comment) reply jwells89 18 hours agoparentFan of vertical tabs here, it‚Äôs a major boon for how I browse, which tends to involve a number of long-running tasks. Tabs work better than this than bookmarks, because cleaning out bookmarks sucks with how undeveloped all browser bookmark managers somehow still are in 2024. I‚Äôm not too partial to nested tabs, but I think ‚Äúpanes‚Äù (Firefox extension Sidebery nomenclature) or ‚Äúspaces‚Äù (what Arc calls them) where you can swap what group of tabs (including pinned tabs) is represented by the tab sidebar with a click is powerful, particularly combined with association of a pane/space with a browser profile. So for example, a single browser window can switch between being dedicated to general browsing, shopping, online university courses, or software development, and if I want to split a pane/space into a new window temporarily, this is possible too. reply a_subsystem 12 hours agorootparentI've been using Arc and I've become a fan of folders + vertical tabs also. But I also want bookmarks with tags. I don't want to keep 'pinouts for xyz motherboard' open somewhere all the time. I want to tag it with 'dell', 'motherboard', and 'pinout' and then search when I need it instead of having to remember what 'Space' I put it in. I prefer bookmarks to web search when possible because often enough I spend stupid amounts of time searching for a little tidbit and may want to come back to it later. Rather than cleaning out bookmarks, I would keep a few main bookmarks and folders on the toolbar, and file away everything else under one few big folders with tags. I use Firefox for tagging and it seems to be a fantastic way to keep track of thousands of sites with small cognitive load. Am I missing some workflow that's 'better'? reply jwells89 11 hours agorootparentYeah, not having bookmarks is probably Arc‚Äôs biggest weakness. I find bookmarks useful too, but only add them when I know I‚Äôm going to keep them around for a while. Tags might work well but the friction involved is likely too high for me to consistently use them. It‚Äôs easier to just keep anything remotely short term in tab form. reply dmix 19 hours agoparentprevPlus looking at the screenshots of the \"controversial\" FF89 one of the biggest changes seems to be redesigning the tabs w/ more padding. I didn't even notice that change because I have the top tab bar hidden and use Tree Style Tab that has a design which blends nicely with FF. I don't like FF in particular but the tab tree is 100% enough of a UX gain vs all the small details chrome/safari does slightly better that I don't think twice (besides dev panel, I use chrome for frontend work). reply stevage 13 hours agorootparentI use FF primarily, but sometimes Chrome for debugging, on macOS. I literally cannot tell them apart. reply 015a 19 hours agoparentprevTo each their own, but: I strongly believe the opposite. Of course having more than 6 or 8 tabs open in a horizontal tab bar makes tabs difficult to manage and track. I've used Arc for the past 6 months. Having 6 or 8 tabs open in a vertical tab bar is also difficult to manage or track. I end up just spam-closing all of them and starting fresh pretty much every 4 hours anyway. Here's the two arguments I've heard that I resonate more with, one in either direction. (1) Count the number of pixels dedicated to the tab bar space when it is horizontal versus vertical. I've never seen a functional vertical tab bar that used the same or fewer pixels than a horizontal one. (2) But: Monitors almost always have more horizontal pixels than vertical pixels. So, actually, a vertical tab bar better-leverages the aspect ratio your monitor is built at. This feels true at 16:9 and greater aspect ratios; it feels untrue, to me, at the 16:10 aspect ratio; and unfortunately, this is an extremely common aspect ratio as its ~the aspect ratio Macbooks are made at. reply pxc 8 hours agorootparent> (2) But: Monitors almost always have more horizontal pixels than vertical pixels. So, actually, a vertical tab bar better-leverages the aspect ratio your monitor is built at. This feels true at 16:9 and greater aspect ratios This is true, and as a vertical tab bar user, it's important to me. However, when every app follows this logic and decides it gets to have a sidebar (or two!), all of a sudden I'm looking at a lot of apps that are barely usable unless they're maximized. I often find myself thinking 'hey, fuck you, $APP! those pixels are for my vertical tab bar!!' reply J_Shelby_J 17 hours agorootparentprevWhen chrome first started getting popular many Firefox users were on vertical tabs, and it was not lost on me that chrome was made for small screen laptop users. A smart design decision even if it feels gimped for vertical tab users. reply jwells89 17 hours agorootparent> chrome was made for small screen laptop users Makes sense if one considers how many 13‚Äù Macbooks and original MacBook Airs were likely being toted around the Google campus in the late 2000s, with Macs being the predominant development platform there at the time. Further down the road in 2011, Chromebooks appeared, most of which are on the smaller side which also acts as incentive to keep Chrome usable on smaller screens. Desktop Safari and MS Edge are also decent in this regard, likely a result of their parent companies being makers of popular small-screen computers. reply DonHopkins 16 hours agorootparentprevIt bewilders me that any rational UI designer would be so arrogant as to make the unilateral unchangeable decision for all their users that they should only have tabs on one side, be it the top, bottom, left or right of the window. Why restrict users to using tabs on only one side and one side only? What's so special about that side, and bad about the other sides? What if the user is left handed, or has a tall monitor, or a wide monitor, or lots of windows, or only a few? While you're at it, why not just remove all the arrow keys from the keyboard except one? Then users can argue over whether the left-arrow key is better than the up-arrow key, and users who don't like having only an up-arrow key can buy a keyboard with only a left-arrow key. But all keyboards have all four arrow keys, so there are no arguments about which arrow is better: you just use whichever arrow you want, whenever you want. Most people prefer to use all four arrows at different times for different purposes, and put their tabs along all four edges, too! reply jsrcout 12 hours agorootparentI, too, remember when GUIs (not UXs!) respected the user's desires, workflow and general agency. That's not entirely gone yet, but it's going. Heck, Office 365 updated lately and now I have this ongoing little contest of displaying the tiniest number of coworker Teams comments in the hugest amount of white space. The results beggar belief. Edit: Teams (it's a Chrome browser window, but that's not really relevant to my rant) is taking up like two square feet on a 4K monitor, and 3-6 messages plus links to a couple attachments are all it will show me - along with enough white space for another 300 channel names (but only channel names, nothing else), headings the size of movie trailers, just oceans of space and garbage I don't need and can't use. In an ideal world, there'd be some way to adjust the layout a bit, and in fact when we used Slack before Teams, that was pretty easy: you move the divider between channel names and messages to the left, thus giving less space to channel names, more space to message content. Done. But that's too much power to allow a user to have, in these modern, enlightened times. So while Teams has such a divider, there's no provision to adjust it. Want to see more than a handful of messages at the same time? Gonna need a bigger monitor. Probably 55\" would be a good size. Of course, that's one example out of a billion. It's not that software won't cater to my particular workflow... it's is that software no longer allows me to make reasonable adjustments to support a workflow that works for me, and in fact removes still-remaining means to do so on a regular basis. Really it's the whole \"you're holding it wrong\" mindset that I get so tired of. We all labor under our own constraints and just a little leeway on basic customization goes a very long way to making software more usable. Just let me view more than six messages at once, ok? Please? reply throwaway-56453 10 hours agorootparentSoftware maintenance is not the primary cost of a configuration option. The biggest cost is the customer support load [1]. This is especially a problem if the customer manages to get the application into a \"busted\" state. Your complaint, where the software is less convenient than it could be because the sidebar is too big, will never result in a Priority 1 Customer Support Ticket. Adding the ability to resize a sidebar can actually cause that to happen. Someone accidentally resizing the sidebar to become unusably small or large and calling support because they don't know what they did or how to fix it will describe it as an emergency. This is not a lie, since the thing they need to get at to complete their work is impossible to get to without resizing the sidebar, and they don't know how it happened or how to do that. They resized the sidebar while trying to drag and drop something else, and since they weren't using it at the time they went some twenty minutes without even realizing anything was wrong, and now they've forgotten what they did. Hopefully, it escalates up the support chain until someone finally manages to figure out what happened, or it escalates high enough and the customer is willing to give remote desktop access to a support tech [2]. If you're not lucky, they just give up and switch to a program that, hopefully, doesn't break on them. Source: I used to work the support line for customer-facing software. I don't any more, but I still work directly with customer success agents, so I still regularly see this exact kind of problem. [1]: Or, if you won't take it from me, take it from Joel Spolsky: https://www.joelonsoftware.com/2000/04/12/choices/ [2]: Some companies just don't do remote desktop control support. This is so that they can directly tell their customers \"we never ask for control of your computer, so if someone claiming to be us asks for it, they're a scammer.\" Remote desktop access is a dangerous way to do tech support because, if you accidentally wind up talking to a scammer instead of a real support tech, they can wreak havoc with that level of access. OTOH, the company I work for sells software that works with barcode scanners and printers, and I really don't feel keen on trying to talk people through setting them up so they work properly... reply skydhash 8 hours agorootparent> Someone accidentally resizing the sidebar to become unusably small or large and calling support because they don't know what they did or how to fix it will describe it as an emergency. And that's why you have an in-house support departments whose job is to solve these issues. It's like buying a tractor, then relying on the company several towns away to provide repairs. Customizability is good. Because when I settled in a workflow, I don't want to see things I don't want occupying space or distracting me. It's like not using part of the desk because you're supposed to have piles of books in this place. I'm not advocating to have trillion of options a la VIM. But anything except the core purpose of the software should be customizable in some way, including the options to hide it. reply throwaway-56453 6 hours agorootparent> And that's why you have an in-house support departments whose job is to solve these issues. There's a market for software that doesn't require that. Small companies don't have in-house support departments. They contract with an IT provider. Big companies do, but theirs isn't materially different than the contracted one (a typical \"big company\" is really just three small companies standing on each others' shoulders wearing a trenchcoat). Their purchasing decisions are going to be based on what they think will reduce the amount of support calls they have to deal with. > It's like buying a tractor, then relying on the company several towns away to provide repairs. John Deere software locks their parts. They haven't gone out of business, so a lot of farms must be doing just that. Edit: This is not an argument against RTR. Anyone who's willing to break a warranty seal and tinker with the inside of the computer or tractor they own should be allowed to do so. Being able to customize is good. Being able to accidentally customize is bad. reply DonHopkins 11 hours agorootparentprevI was less than satisfied about they layout of the buttons in xcalc, so I built a version of the window manager that let me specify the root window containing the windows to manage, and gave it the window id of the xcalc window, so it put window frames around each calculator button so I could move them around, resize them, even close them into icons (bigger than the button) if I didn't want to use them. Not what the xcalc or window manager designers had in mind, probably. reply michaelmrose 12 hours agorootparentprevIf you squish vertical tabs so as to have as few characters shown as horizontal tabs you'll have plenty of horizontal space for an apples to apples comparison. reply kjkjadksj 12 hours agoparentprevVertical tabs start to suck on a laptop. For one my auto hide sidebar hack for firefox broke and I am too lazy to fix it, so now I have basically a whole inch by several inches of dead gray space because I don‚Äôt have a full column of 100 tabs ever open, maybe 5 or 10. This wouldn‚Äôt be so annoying except many modern websites use browser viewer size instead of the user agent to make the call you are a mobile or desktop user. So I basically have to use fullscreen browser windows because considering I already lose an inch to the tree style tabs, I really don‚Äôt have much width to lose before the site assumes I am an iPad and gives me a ton of hamburger menues that also suck to navigate because of how they tend to wrap text on constrained width browser viewers. Most of the time I just have to disable the sidebar entirely which of course adds a few clicks everytime I change tabs now. reply bloopernova 17 hours agoparentprevReplying to my own comment: HN faux pas, or good information organization? :) In the Tree Style Tab options page, there's an Advanced section that has a live-reloading user style sheet section. Very cool for testing out font choices without restarting the browser. I've changed mine to use Apple's really nice SF Pro font, condensed. Somehow the Iosevka Mono that I use everywhere didn't look quite right on the tab titles. The CSS: :root.sidebar tab-item.unread .label-content { font-style: italic !important; } :root.sidebar tab-item { font-family: \"SF Pro\" !important; font-stretch: condensed !important; font-weight: 300 !important; } :root.sidebar tab-item.active .label-content { font-weight: 500 !important; } Same link as my previous comment now has a screenshot of the result of that CSS: https://gist.github.com/aclarknexient/88673880d373864eee1927... reply Dwedit 15 hours agoparentprevVertical tabs would be unusable on a window that's resized to be half the width of a 16:9 display. reply WorldMaker 14 hours agorootparentI do it all the time and I don't think it is \"unusable\". About the only \"issue\" with it is often that it shrinks the viewport just enough to trigger \"mobile\" breakpoints in CSS, but for some websites (YouTube, especially) that can be a feature as much as a \"bug\". reply michaelmrose 12 hours agorootparentprevThis is a good point especially on laptops. It might be nice to display side tabs until a breakpoint and thereafter autohide reply FeepingCreature 19 hours agoparentprev> Better browser UX, in my strongly-held opinion, starts with vertical tabs. With horizontal tabs, you can have maybe 6 to 8 tabs open before things tabs get difficult to manage or track. TabMixPlus! Dynamic width vertically scrollable fully customizable multirow tab bar. I have 50 tabs open, 22 on screen, and perfect overview. It still technically runs on current Firefox, but you have to engage in some very vigorous modding. See the README https://github.com/onemen/TabMixPlus but be aware that this will completely disable extension signature validation. (I blame Mozilla.) reply SSLy 18 hours agorootparentI think Sideberry is nowadays more comfortable than TMP. reply DonHopkins 16 hours agoparentprevVertical tabs are better in some situations and for some users, horizontal tabs are better in other situations and for other users. So all users should be able to choose to place tabs along any side of any window, and change which side and what position any tab is at any time. Not just tabs for emacs frames or web browser windows, but for ALL windows including top level and internal application windows. And you should also be able to mix tabs from different apps in the same frame, of course. Why not? I implemented tabbed window with pie menus for UniPress Emacs in 1988, and still miss them! Later in 1990 I developed several other versions of tabbed windows with pie menus for NeWS that let you manage any NeWS and X11 windows, and drag the tabs around to any edge. https://news.ycombinator.com/item?id=38338008 DonHopkins 3 months agorootparentUniPress Emacs for NeWS, with tabbed windows and pie menus: 1988. https://en.wikipedia.org/wiki/Tab_(interface)#/media/File:Hy... https://www.youtube.com/watch?v=hhmU2B79EDU https://news.ycombinator.com/item?id=38337808 DonHopkins 3 months agoparentcontextfavoriteon: Vertical Tabs in Visual Studio Code This is why you should be able to choose which side and position any tab is positioned along any window at any time, and change them at any time by dragging them to where you want. Then you can assign meanings to each side, depending on your workflow, for example (this should be under user control, not set in stone, of course): Tabs on the top for important stuff. Tabs on the bottom for administrative stuff. Tabs on left for things you haven't read yet. Tabs on right for things you've already read. Then drag the tab from the left to the right after you read something (like moving it from your \"in box\" to your \"out box\"), or pin its tab on the top or bottom of it's important and you want to keep it around and easy to find. And if you really want, you should be able to hide the tab to save space. And not only tabs for apps like browser and IDEs, but also the desktop window manager should support tabs on top level windows in a consistent manner, so you can drag tabbed windows in and out of other window frames, as well as arranging them in hierarchical outlines along the edges. All this is super obvious, and saves a lot of time and effort, so it bewilders me why tabs like I described and implemented in the 1980's aren't universally supported on all desktops and applications by now. It's not because they're patented. Adobe tried, and sued Macromedia over it, but that patent (illegitimate in my view, since it ignored the prior art, and was extremely obvious and not patentable) has long since expired. https://www.metafilter.com/2805/Adobe-sues-Macromedia-over-i... https://news.ycombinator.com/item?id=38337876 DonHopkins 3 months agorootparentAlso, not everything is a file. Tabs should apply to all edges of all windows, including top level windows, not just one edge of only windows with files in them. And you should be able to drag any window out to top level and it still has its tab attached, then move it around to any position along any edge, or hide it, and of course snap windows together along their tabbed edges, either tiling or overlapping. How do you control all of that? That's where the pie menus on the tabs come in, of course. Thanks to the tabs, you can even pop up pie menus on windows that are completely covered up, and perform commands on them even though they're not visible, like bringing them to the top (stroke up) or down (stroke down), or closing them (diagonal stroke for confirmation submenu, then stroke up to confirm), or whatever (paste into terminal emulator, evaluate code in editor, etc). https://news.ycombinator.com/item?id=38347429 DonHopkins 3 months agoprevAnd as long as you can have tabs on any side of a window, how about multiple tabs on the same window? Like child tabs as well as label tabs, that are links to other windows. Another cool use of vertical tabs is for the tabs on the left to select between windows, and the tabs on the right select between children of the current window (not sub-windows, but related windows or sub-directories). And you can use the tabs along the top as breadcrumbs to navigate back up the tree. Some IDEs kind of do that with a directory browser on the left and a function browser on the right, but with outlines and scrolling lists instead of actual tabs. You could navigate the tab tree by clicking or gesturing left or right with a pie menu on a tab, sliding the right column of tabs over to the left to descend into the tree. Like a Finder window that shows directories as tabs on the right instead of icons inside. You could also have top and bottom edge tabs for different kinds of children (i.e. xml attributes vs elements, object methods vs properties, different views or editors, etc). The original NeXT file browser had breadcrumbs along the top (but not tabs): https://www.youtube.com/watch?v=rrTag7nSHlw&t=701s https://news.ycombinator.com/item?id=38341279 donatj 3 months agoprevVScode started with vertical tabs only back in the day. It was a very interesting design choice. They switched to horizontal tabs from pressure. DonHopkins 3 months agoparentI just can't get my head around the mentality of making that decision for all of the users, hard coding it, and forcing it on them, not allowing you to choose for every window, or change your mind at any time, and simply drag any tab to any edge you want, whenever you want. What makes user interface designers so arrogant and sure of themselves and lazy that they think one particular side is the only side, and the best for everyone, no matter what your screen size, resolution, aspect ratio, layout, number of tabs, icon or label size, workflow, direction of text flow, handedness, visual acuity, physical dexterity, task, and preference? And then when you inevitably run out of space for tabs along the one edge, instead of simply allowing you to put more tabs along the other edges, you either add more horizontal rows along the top, so you get this abomination [1], or you have tiny little hard to use scrolling arrows at each edge so you can't see all the tabs at once, so you get that abomination [2]: Is it ever okay to have multiple rows of tabs? [1] https://ux.stackexchange.com/questions/15558/is-it-ever-okay... Awesome Scrolling For Wide Tab-Interface Applications - ScrollTabs: [2] https://www.jqueryscript.net/layout/Awesome-Scrolling-For-Wi... It's like only putting only one arrow key on the keyboard. https://news.ycombinator.com/item?id=38337425 DonHopkins 3 months agoparentcontextfavoriteon: Vertical Tabs in Visual Studio Code I've been implementing and using vertical tabs since around 1988, with I released a commercial product with tabbed windows, the NeWS version of UniPress Emacs, and used it to develop a hypermedia authoring environment for HyperTIES at the UMD Human Computer Interaction Lab. https://en.wikipedia.org/wiki/Tab_(interface)#/media/File:Hy... Vertically tabbed windows combine synergistically well with pie menus, and are great for window management, especially when you have many windows. They are purposefully NOT patented, since the idea is so fucking obvious, but it's disappointing they took so many decades to catch on finally. Still there aren't any decent desktop window managers I know of that implement tabs the right way. (tvtwm is not the right way!) The later NeWS Toolkit versions from the early 1990's let you drag the tabs around to any side of the window you like: left, right, top or bottom, to any position along any edge. The user should be able to decide which edge and where the tabs are attached to for each window, it should not be hard wired like the tabs in VSCode and web browsers typically are. Being able to choose which edge the tab is on and where the tab is gives users better more flexible ways to organize and manipulate their windows. https://en.wikipedia.org/wiki/Tab_(interface) HCIL Demo - HyperTIES Authoring with UniPress Emacs on NeWS, tabbed windows, pie menus: https://www.youtube.com/watch?v=hhmU2B79EDU I had a video of the NeWS tabbed windows, demonstrating dragging the tabs to different window edges, but youtube took it down because it contained copyrighted music (Herbie Hancock's Rockit). Oh, here's the original video you can download from my server: https://donhopkins.com/home/movies/TabWindowDemo.mov Here are some different version from 1988-1991 for different versions of NeWS: https://donhopkins.com/home/archive/NeWS/tabwin.ps https://donhopkins.com/home/archive/NeWS/tab-1.ps https://donhopkins.com/home/archive/NeWS/tabframe-1.ps https://donhopkins.com/home/archive/NeWS/tab-3.0.2.ps Here's another NeWS program that uses vertical (by default, but any edge if you want) tabs on windows around PostScript objects that you can push and pop on the stack with \"direct stack manipulation\": The Shape of PSIBER Space: PostScript Interactive Bug Eradication Routines ‚Äî October 1989 https://donhopkins.medium.com/the-shape-of-psiber-space-octo... PSIBER Space Deck Demo: https://www.youtube.com/watch?v=iuC_DDgQmsM reply black7375 14 hours agorootparent> And not only tabs for apps like browser and IDEs, but also the desktop window manager should support tabs on top level windows in a consistent manner I miss the tabbed window feature in KDE 4. This is the feature I'm most disappointed to see missing from version 5. - https://bugs.kde.org/show_bug.cgi?id=343690 - https://community.kde.org/Plasma/5.4_Errata I think this is a really essential feature when displaying tiled windows. reply Yaina 21 hours agoprevI don't know, this post wants to convince readers that there are UX rules from which the theme author created an objectively better Firefox theme, yet most of the changes strike me as personal preferences. It's obviously well made and maintained, but personally I don't think it's visually very appealing and looks in parts more cluttered. So I think people have different preferences, Firefox went with one design but they also enable support to make these changes, and that's all nice. But I find the post to a bit silly, in that the author wants to prove that their preferences are empirically right. reply black7375 20 hours agoparentThe UI is definitely a matter of taste, which is why I created the distribution in three different shapes. However, it was confusing that when muting, there was no indication that it was loading or there was no tab separator. reply Yaina 19 hours agorootparentyeah, all the things that happen in the Korean version seem like bugs that should be filed IMO! reply ihateolives 22 hours agoprevEver since Firefox borked their tabs I've given up on it as my default browser. Every now and then I open it up for testing and when I still see buttons instead of tabs I make a mental note of trying again in half a year or so. Light theme is especially insulting with white buttons on light-light gray. It even doesn't repsect Windows' system theme settings, because in Windows you can have apps keep light theme but apply colors to taskbar, Start menu and title bars. Edge gets it, Chrome doesn't, but at least there's enough contrast, Firefox completely misses the mark. reply FirmwareBurner 21 hours agoparentI use Firefox as my alternate browser. And whenever I fire it up it needs first to get in my way and interrupt me to tell me about the new changes and features it has implemented since the last time I opened it like Pocket, VPN, etc. God, stop it, just let me start browsing what I came here for, stop imitating Microsoft and their dark patterns of shoving Office 365 and Gamepass in your face between updates. Go and advertise your features to people who don't yet have Firefox installed, but I'm already your \"customer\", so stop bugging me. This is why I'm mainly on Chrome. It may be inferior and spying on me but it never gets in my way. reply pprotas 21 hours agorootparentChrome automatically opens up a tab with an ad for their new features after every automatic update. Isn‚Äôt that kind of the same thing? reply dathinab 18 hours agorootparentyes hand the amount of times Firefox has done so in the last few years is also just a handful (like less then 1 time per year in average) And some of this notifications where really reasonable to have like the containers. (Except the color thingy, that was some nonsense.) The only exception is if you somehow end up in a situation where the browser (profile) is for whatever reason frequently fully reset, in which case you might have seen the same notification multiple times. reply yoavm 21 hours agorootparentprevI feel exactly this way, but about Chrome. I suspect it's just bias - I only open Chrome once a week or so so this screen is very annoying. Firefox is always open so I don't even recall seeing anything like that. reply hobs 21 hours agorootparentprevDamn that new tab must pop up for me like... once a month! What a distraction! I guess giving all my data to Google is probably worth it. reply FirmwareBurner 21 hours agorootparentJoke all you want but Firefox's market share agrees with me as most users value convenience above all else. At least Google knows not to be intrusive and not fuck with the UI so often. Chrome looks almost exactly like it did over 10 years ago. Why can't Firefox imitate that quality of life feature? It's not that hard to not change shit at the surface. They've been bleeding market share for years including faithful long time users who enjoyed the 'old, boring' Firefox but don't agree with the current direction of imitating Chrome at every step. Old Firefox = Windows 7 New Firefox = Windows 11 reply viraptor 21 hours agorootparentGoogle mainly knows how to display ads. The raise of Chrome wasn't so much about features at the time as about people being bombarded with \"install chrome\" ads. On Google results, in Gmail, in Adsense, everywhere. There was an absurd amount of money sacrificed (not really spent since the ads were internal) to gain the users. I'm not sure any other aspect can be reliably compared in that scenario. reply paldepind2 19 hours agorootparentDo you have any data to back that narrative? I know Mozilla is pushing that explanation, but from how I remember it Chrome gained market share because it was technologically superior to Firefox when it was released. It had a much more modern UI, much faster JS implementation, sandboxed tabs, etc. All features that it took Firefox years to copy. Some would say that Firefox has never caught up. I find the ads narrative pretty hard to believe. Back in the days, Firefox could compete with IE, which was the default in Windows, by being technically superior. It seems very likely that Firefox's users, who had gone out of their way to install Firefox, would also be very willing to go out of their way to install a new better browser even in the absence of ads. reply viraptor 12 hours agorootparentEhh... UI was comparable. Chrome was a bit faster. For the ads, see some snapshots of what that campaign looked like at the time https://searchengineland.com/googles-jaw-dropping-sponsored-... It was a massive push of basically spammy links all over the internet. To be clear, I'm sure some people switched for the features. But given the scale of the sponsored push to every internet user, we can't really say features were the reason for most people. There's no way to run the experiment the other way. reply paldepind2 2 hours agorootparentThanks for the link. That certainly shows that Google was sponsoring an aggressive and fairly ugly ad campaign. > UI was comparable. They were not at all comparable. Here is an image of what Firefox looked like when Chrome was released [1]. Here is an image of what Chrome looked like at release [2]. Barring some design tweaks Chrome looks roughly like any modern browser whereas Firefox looks ancient by modern standards. It has the app menu, no integration with window decoration, a separate search box, tabs below the address bar, etc. Lots of things that Firefox would copy over the coming years. There's a reason Chrome was named after its Chrome‚Äìthe UI was a huge selling point. 1: https://user-images.githubusercontent.com/25581533/141687681... 2: https://blogoscoped.com/files/google-chrome-browsing/search-... reply dathinab 18 hours agorootparentprevit also really helped that in difference to today wen Chrome was new it's performance gain over FF was often quite significant (same for security) while that isn't really true anymore in any relevant way it's still stuck that way in many peoples heads And sure part of the perf issues where quite often not well behaving FF extensions, toolbars etc. also often installed by unrelated programs preexisting installed on the same computer and that FF needed some major refactoring/rewriting just because it was quite a bit older (which are done by now but had inevitable but sad effects like XUL extensions being gone). reply prmoustache 20 hours agorootparentprev> Chrome looks almost exactly like it did over 10 years ago. I suggest you look for screenshot of Google Chrome in 2008. Market share is what it is because google is pre installed in the majority of the mobile market, that firefox had bad performance rep in some areas, and that google is a synonym of internet in the mouth of the majority of people nowadays the same way explorer was a few decades ago. Most people don't care about those UI changes. reply Izkata 15 hours agorootparent2008: https://techcrunch.com/2008/09/01/first-public-screen-captur... 2016: https://www.pcmag.com/reviews/google-chrome 2024: https://flathub.org/apps/com.google.Chrome They're basically the same, nothing like what Firefox has been doing. Buttons went flat, bookmarks moved to the right, and two dropdowns got combined into one menu. Tabs haven't changed at all. reply 1317 8 hours agorootparentyour 2024 screenshot is outdated; here's a more recent one: https://www.google.com/chrome/static/images/heroes/browser_n... reply dathinab 18 hours agorootparentprev> market share agrees that's not how it works the marked share is there for reasons which have little to do with that, at least when it comes to \"non technical people\" (i.e. not HN crowd) 1. what matters the most is what is pre-installed (like iOs Safari, Android Chrome, etc.) 2. then what matters a lot is mind share, Chrome still has in many peoples minds the image of \"the good alternative\", \"fast\", \"reliable\", \"modern UX\". While many people still think about FF as slow and clunky even through a lot of this opinions came from well over 10 years ago 3. What also a huge amount is if you can use it for all task you do. Due to apps like Slack, MS outright refusing to fully support Firefox or for example Notion having had egregious FF only bugs a lot of \"normal\" users have over time moved away and just never come back. The most sad thing is if you look at the technical details it's seldomly FF fault. E.g. basically every time I looked into it when some media player (and I think it was also the case for Notion as far as I remember) didn't work it was because the sites not being standard compliant with CORS. Another (older) example is FF missing media codecs due to licensing issues which Apple/MS fixed by having an OS and Google by having a ton of money. Or polyfills for bleeding edge sometimes not yet even standardized Chrome features being slow. Stuff like that is in my experience kinda true for close to any (systematic) issue of a sites not working correctly on FF. Lastly when it comes to non technical people the overlap of people which would stop using FF because of stuff like that and the ones which anyway wouldn't use FF because they use some fancy chromium derivative like brave is quite high. The reason they are bleeding market share is not because they updated to a new UI. It's because it's today hardly possible to run a browser which isn't either chromium or webkit based. reply medstrom 21 hours agorootparentprevBut your premise that one is more intrusive, isn't even true. Like others said by now, Chrome does the same thing if you don't open it often. reply hobs 21 hours agorootparentprevI would agree that most people would be glad to give up their privacy for minor convenience. if I was a developer of Firefox I might care that its marketshare is falling, but truthfully nobody needs to be married to a web browser, or take its rise and fall too seriously. Still not going to use a Google browser though, that's just a self own. reply FirmwareBurner 21 hours agorootparentThe rise and fall reflects user popularity. And Firefox is less popular than ever, with both the new generation who grew up only knowing Safari and Chrome, and also with the old generation who they pissed off by trying to copy the shitty parts of Chrome instead of staying true to what made them popular in the first place such as speed, light weight, and getting out of your way by not charging the UI every 6 weeks. reply yoavm 21 hours agorootparentSure, maybe everyone left Firefox because of the \"what's new\" tab and the UI changes. But maybe it's because Safari is default on some devices, and Chrome has the world biggest marketing company behind it. reply FirmwareBurner 20 hours agorootparentOf course Google won the browser war mainly because they're World Heavyweight Advertising Moneybags World Champion, but Firefox also did itself no favors by annoying it's old and faithful userbase with often, unnecessary and unpopular UI changes a while ago. reply mikae1 21 hours agoparentprevInstall Sidebery[1] and hide the native tabs[2]. This is the way. :) [1] https://addons.mozilla.org/en-US/firefox/addon/sidebery/ [2] https://superuser.com/a/1424494 reply medstrom 21 hours agorootparentI have neither! No visible tabs at all. Just cycle tabs with Ctrl+Tab hotkey. This is clean. Once in a month I want some kind of overview, but extensions like Tab List can give me a popup menu. The tabs needn't be visible constantly! reply DonHopkins 15 hours agorootparentAnd the tabs should have pie menus, too. reply medstrom 12 hours agorootparentAre you serious? Details? This sounds new and interesting. reply DonHopkins 11 hours agorootparentSee my other posts, and the links! https://news.ycombinator.com/item?id=39432170 reply thecosmicfrog 22 hours agoprevIs anyone else not at all bothered by the tabs \"being buttons\"? I feel like I'm the only one sometimes. Are they really that jarring for first-time users? reply werdnapk 21 hours agoparentI'm a daily FF user and I wasn't even aware this was considered an \"issue\". I haven't put a single thought into this until seeing these comments... and I'll go back to not thinking about it again as I find the tab bar completely usable as-is. Thanks for the thoughts though. reply badsectoracula 21 hours agoparentprevTabs being buttons doesn't really bother me and if anything this is such a common alternative to tab appearance that even Windows 95's tab control has a mode to make tabs look like buttons (AFAIK it was used in the original task bar). It was also used for, e.g. switching channels/windows on mIRC since the 90s too. However personally i do not like how these particular \"tab buttons\" look like and if nothing else (they remind me of those long pills that often feel hard to swallow :-P), i am used to them looking like tabs and see no reason for that change (fortunately Firefox allows you to customize its look and i have a userChrome.css that makes it look more to my liking). reply PaulKeeble 21 hours agoparentprevIt doesn't impact my use of Firefox. They are just wasting pixels putting a gap where one doesn't belong. But I think at this point I am used to insane UI decisions and just roll with whatever organisations give me, few seem interested in any form of consistency or easy discovery. reply ihateolives 21 hours agoparentprevIt bothers me because it's different enough from everything else with tabs that I have (not only browsers, file managers, editors etc) and I just can't jibe with it. Why throw all semantics out of the window? It looks like a button but doesn't behave like one. reply pndy 18 hours agorootparent> Why throw all semantics out of the window? Almost everything nowadays is designed with mobile-first in mind approach - whether it's a smartphone or tablet, or desktop software. That throw all semantics out of the window - look what happen to e.g. Gnome over the years. What's worse I'd say, is that the lack of clear differentiation between types of interface elements made easier to hide options within GUI under various dark patterns (active element vs static information etc.) - whenever its required to do so. As for Firefox GUI changes: Mozilla ask their users for feedback many times and the feedback was given - often strongly criticizing the upcoming changes but they ignored it and introduced changes anyway. I did submitted mine when they were about to rollout Australis but I didn't bother myself to say anything when Proton was about to be introduced because I knew that the corporate facet of Mozilla doesn't care and they'll do whatever they like. reply kjkjadksj 11 hours agorootparentprevA tab is literally a button with slightly different pixel shading. Its exactly the same thing as its always been. reply alimbada 20 hours agoparentprevI never noticed it and even after it's been pointed out it's not an issue. Seems very nitpicky by those for whom it is an issue. reply jwells89 17 hours agoparentprevI wouldn‚Äôt say they bother me per se, but they don‚Äôt exactly feel right either. They‚Äôre incongruent without good reason. reply flurdy 20 hours agoparentprevI don't give a monkey if it looks like a button or not. But I have a problem with a lot of the Firefox themes making it very difficult to quickly see which tab is the active one. I generally look for themes where this is obvious. reply zx8080 21 hours agoparentprevThere's so much things going south with each update I just don't care anymore. The overall state of UX is very sad for Firefox and also Thunderbird (I had to stop using it after their menu bar fiasco and other \"improvements\"). reply encom 17 hours agorootparentFirefox v89 was the last version I used. I just couldn't be bothered with them constantly changing and removing features for no reason. I switched to Vivaldi, which offer basic functionality like vertical tabs, and a fully customisable UI out of the box. It's far from perfect, notably being closed source which was hard for me to swallow, but it annoys me far less than Firefox ever did. I have it set up how I like it, and that setup has stayed static in the three years I've used it now. Firefox frankly feels user hostile in comparison. Thunderbird was ruined with version 115, so I switched to Kmail. I miss calendar integration in my email client though. reply madeofpalk 19 hours agoparentprevI can understand why someone would make a different design decision, and I would probably agree with their rationale to prefer 'connected' tabs. But no, I don't have a problem with Firefox's tab style. It's immedaitely learnable. I've never once second guessed which was the active tab or what those things up there are. reply zx8080 21 hours agoparentprevWhy wasting space for a button lower border if it's actually a tab's title and not a button? reply mavamaarten 19 hours agoparentprevI had exactly the same thought. I even installed the theme in question to see if I would like tabs better and honestly I prefer how Firefox does it stock. reply tiltowait 17 hours agoparentprevI see constant complaints about how \"garbage\" the Firefox UI is, and I just don't see it. It's ... fine? I mean, it's basically Chrome's UI with a slightly different tab bar, yet here we are with a very long post about alleged fixes. reply square_usual 16 hours agoparentprevEvery time someone says \"button tabs are objectively wrong\" I'm reminded of the fact that most normal, non technical users I know prefer Safari's button tabs. That's a feature you have to seek out and turn on, btw. reply tvshtr 21 hours agoparentprevIt doesn't freaking matter on hi-dpi screen. Also I've just hidden them (tabbar) and I'm using compact vertical tabs. reply Archelaos 20 hours agoparentprevIt is ugly, but I can live with it. reply Schlaefer 20 hours agoparentprevI moved on to sideberry and hide the normal tab bar completely. But if you open two identical tabs in the default layout I have no clue which one is active anymore. You can't understand that UI, you have learn it. It's infuriating. reply SiempreViernes 19 hours agorootparentIt doesn't render the one you clicked on in a different colour? That might be a issue with the theme you use, my firefox shows the active tab in a clearly different colour from the inactive one. reply mvdtnz 21 hours agoparentprevI don't care about it at all. None of this UI criticism seems very important to me. I'd rather the Firefox team spent time making the browser less buggy and get feature parity for obvious missing features. reply pxoe 20 hours agoprevthe real UI/UX nightmare of firefox is the legacy cruft. it's like every single basic browser function opens in it's own kind of a different view. settings - browser tab. history - a sidebar. actually that's just when you hit ctrl+h, manage history? ctrl+shift+h? that's a separate window. bookmarks, downloads - also that window. (that Library window that seems to be dragged up all the way from firefox 3/4.) downloads have their own little popup, but ctrl+j and manage downloads open that window. bookmarks - ctrl+shift+o opens that window, but ctrl+b opens a sidebar. profiles? just kidding, there isn't really a user-facing user-usable profile function, but there is a what looks like a legacy interface hidden at about:profiles. passwords? browser tab, with it's own look that doesn't really correspond to anything. there's one bit of cohesion - settings and addons are both browser tabs and even have links to each other. everything else, an absolute mish mash of browser tabs, windows, sidebars, popouts, different uis everywhere. and it's been a mess like this for a while, and seems like it will be like that for a while still, because there is just no singular vision and no real effort to straighten that out. they added a firefox view thing recently - and it just seems to be a yet another thing that piles on to those different uis with a yet another different ui. (it has history there, so there's like three different ways to view history, which is at least one too much.) one could guess that maaaybe they are trying to fix it with that, and maybe port functionality from Library window to View tab, but at this point in time, it's just adding to the mess. to a degree this is an exaggeration, cause well, other browsers have recently taken a liking to sidebars and have their popups and menu things as well. but the real sticking out thing is (legacy? it looks legacy) Library window that pops up here and there, and how that contrasts with some functions (settings, addons, passwords) that open in a browser tab. it ends up making opening browser functions somewhat unpredictable as to what kind of thing it'd open, definitely so at first and with just a persisting feeling of 'everything opens up differently just because'. you get used to it, but it's still a mess. reply jwells89 17 hours agoparentI think it‚Äôs fine for some things to have a popover and window, but it needs to be implemented intelligently. Like in Safari, downloads are a popover by default, but the popover can be ‚Äútorn off‚Äù to become a window. Use cases for both are served, yet consistency is maintained. Firefox could use a good dose of this thinking. reply kjkjadksj 11 hours agorootparentThats exactly how firefox works out of the box, no? On my firefox at least theres a popover with a few recent downloads and a button to show all downloads. reply jwells89 10 hours agorootparentNot quite, in Firefox the downloads popover and downloads window are totally separate from each other with different UIs that can both be open simultaneously. In Safari the window and popup are one in the same, with the user being able to choose how to present it. reply j1elo 19 hours agoprevEveryone is writing their pet peeves, so here are mine: * Having a large tree of bookmark folders, navigating it to add a new bookmark is horrible in the small pop-up that is the \"Add Bookmark\" UI. * The Bookmarks sidebar allows to search by name but not to find where they are. A bookmark search add-on (Bookmark search plus 2) solves this, but it shouldn't be needed. EDIT: I've been told about right-click -> Show in Folder. This is great! Not the best UI, though (the mentioned add-on is still much more intuitive) * Cannot have multiple sidebars. So you cannot have Tree Style Tabs opened (for vertical tab handling) and the bookmarks folders & search at the same time. Bonkers. Actually, that's all. Mostly it's about handling of bookmarks! Not sure if the rest of the UI is just fine or that I got used to it and I'm now blind to its quirks, but I feel pretty comfortable with Firefox. I never felt a strong need to complain about style redesigns, like some other people do. reply jwells89 17 hours agoparentBookmarks have received woefully little attention in all browsers for reasons unknown to me. If one pulls up a browser from 20 years ago, bookmark management is basically identical or even slightly better in some circumstances. I guess making bookmarks better isn‚Äôt sexy so nobody‚Äôs bothered. reply TillE 9 hours agorootparentOver the years, there have been dozens of serious attempts at reinventing bookmarks (from third-party services and plugins), and none of them have caught on. My conclusion is that they're just not a concept that works for people; they got squeezed out by web search on one side and complex note-taking applications on the other. reply jwells89 9 hours agorootparentI think there‚Äôs room for better bookmarks, but they have to be a part of the browser proper‚Ä¶ third party apps and even browser extensions can never be integrated to the required extent, but none of the big browser makers have iterated in this space at all. reply eitland 4 hours agorootparent> but they have to be a part of the browser proper‚Ä¶ Raindrop is more or less perfect for me. Yes, for a lot of things I just start typing:orbut that works without bookmarking. For longer time storage however, I just click the raindrop icon and fill inn tags and it is done. reply yorwba 19 hours agoparentprevDo you perhaps have muscle memory for the Ctrl+Shift+B shortcut that used to open a large side panel but now just shows a narrow horizontal bar? I do and I'm certainly annoyed by this redesign, but I discovered the \"manage bookmarks\" shortcut Ctrl+Shift+O that opens a larger pop-up window with your bookmarks, which so far seems almost as comfortable as the old side panel. It also lets you search for bookmarks and right-clicking to select \"show in folder\" in the context menu shows you where in the hierarchy it is. (Though all my bookmarks are in \"other bookmarks\", so I don't expect to be using this much.) reply j1elo 18 hours agorootparentNo, I don't have muscle memory, and in fact you have introduced me to the Ctrl+Shift+B shortcut to show or hide the Bookmarks Toolbar! :-D I won't use it though, because I have it visible and it's not something one typically changes a lot. With \"side panels\" I mean what strictly speaking Firefox calls \"Sidebar\": menu View -> Sidebar -> choose ONE among Bookmarks, History, Synced Tabs, Bookmark search plus 2, or Tree Style Tab. Why the hell I cannot have e.g. Tree Style Tab AND a Bookmarks sidebars on the left, at the same time? Seems silly to me. Ages ago I worked with Qt and made desktop applications that could have detachable panels (QDockWidget), or their native equivalents such as palettes on Windows, that could be placed anywhere on a main window; but now that we're living in the future it seems we went backwards on what our UIs are able to do. reply lopkeny12ko 20 hours agoprevMy greatest \"unnecessary Firefox UI change gripe\" is the removal of browser.urlbar.clickSelectsAll 4 years ago. And as you might expect, Mozilla does not care. If you read the bug report, this literally cannot be explained by anything except user hostility. https://bugzilla.mozilla.org/show_bug.cgi?id=1621570 Literally no other text field in any UI behaves like this. I cannot fathom why Mozilla chose to both ship this \"feature\" AND remove the option to opt out of it. Some users prefer it. And that's fine! But don't take away my god damn option and force it down my throat. reply _notreallyme_ 20 hours agoparentActually their argument was that all other major browsers behaved like that. You can check with chrome, and indeed it behaves like firefox. For the user hostility, there argument was that people who dislike the new behavior do not have telemetry enabled, and thus they do not deserve to have the features they want. It's quite ironic considering firefox main advantage is their privacy oriented model... reply Yaina 17 hours agorootparentI don't find it ironic at all. The purpose of telemetry is to be able to obtain information about the user population at large. It's anonymous and the data only flows one way (i.e. you don't see personalized ads based on telemetry data), but of course some data about your browsing behavior is being sent somewhere, yes. It's a trade-off: You sent some anonymous usage data but in turn that contributes to decisions made about the product. If you opt-out of sending this data, obviously, it does not contribute to the pool of data from which decisions are being made. Now, that a small group of people with very specific opinions and preferences is the same that disproportionally also opt out of sending telemetry... I don't see how that is Mozilla's problem. You can't have your cake and eat it too, as the saying goes. reply bscphil 13 hours agorootparent> Now, that a small group of people with very specific opinions and preferences is the same that disproportionally also opt out of sending telemetry... I don't see how that is Mozilla's problem. I disagree. If you create a piece of software and develop a userbase that disproportionately opts out of telemetry relative to your software's alternatives, congratulations, you won. You got the power users, the developers, the people who care enough to submit quality bug reports, they're all on your side. Game over. You don't need telemetry to understand what features these users need because they will tell you - loudly and forcefully - in bug reports filed if you break something. Assuming we're talking about open source software, and we are, they may also be the people sending you patches and improvements for these features. Telemetry is what you need if you're making a mass market product that meets the needs of 80% of users. It isn't necessary, and in fact may not be useful, if you're developing software designed around the needs of the people contributing to the software. Some software tries to do both. But the way you do that isn't by looking exclusively at telemetry and then pretending that what you see there describes the behavior of all user categories, at least when it comports with the plans of your UX team. It's by listening to the people who are most passionate about the software. reply asadotzler 10 hours agorootparentThe users who opt out of telemetry are a very tiny minority barely worth considering in terms of numbers. Further, they are explicitly saying \"I don't value my vote on feature usage as much as I value turning off this data report which contains zero personal information and is used for no other purposes than changing the product. Again, don't want a voice in how the product evolves, cool, just don't complain about not having a voice down the road when something you don't like happens. It's like people who don't like their local politician who spent the year before bragging about how voting was stupid and just a way for the government to track you so you weren't going to do it. reply bigDinosaur 11 hours agorootparentprevAn idea can be good regardless of telemetry, telemetry is descriptive and not prescriptive. Telemetry is inherently reductive in that sense. You're making a leap of logic that's genuinely unfounded - an idea can be good or bad and this is wholly independent of telemetry unless your only concern is maximising or minimising use of some kind of feature. I would never dismiss an idea because someone has telemetry disabled and it seems like a genuinely disturbing idea to even hold the position that a user with telemetry disabled is lacking value. reply asadotzler 10 hours agorootparentA user with telemetry disabled isn't lacking value, it's throwing away its vote. Plain and simple, if you want a vote that tells the software maker how you think the product is best used, that's telemetry. If you don't care about that vote, throw it away by turning off telemetry. Now, telemetry is only one (small) input into whether a feature warrants maintenance in a codebase of over 20 million lines, but it is one input that you have involvement in. As I said, up to you if you want to use your vote or throw it away but it's silly to complain about not having a voice after tossing yours in the dumpster. reply lopkeny12ko 20 hours agorootparentprev> Actually their argument was that all other major browsers behaved like that Yes, I understand, and that's true. But no other native text field behaves like this; only other browsers. In fact, one of the formerly big selling points of Firefox over Chrome for me, at the time, was that in Firefox, interacting with the URL bar didn't select all (read: it behaved like all other GTK text fields). \"Making Firefox behave more like Chrome\" is an anti-feature when most of your users aren't using Chrome precisely because of asinine behaviors like this. reply cannam 18 hours agoparentprev> My greatest \"unnecessary Firefox UI change gripe\" is the removal of browser.urlbar.clickSelectsAll 4 years ago Totally agree. Four years on, and it still trips me up daily. Ironically, the usual failure mode for me is actually the one this change was supposed to help with - I want to select the whole URL, so I instinctively double-click it. This has the effect of selecting everything on the first click, then reducing the selection to a single word on the second. I am momentarily perplexed, then I recover and start clicking again, but now it takes three more clicks to get the whole URL selected. It's surprising how annoying this is! The explanation given in the tracker seems to amount to \"at some point in the future, we might do something else that justifies this\". Four years later and I'm not seeing it? reply magicalhippo 18 hours agorootparentI vastly prefer the current way. It makes it very easy to manipulate parts of the URL. If I want to replace the URL I just open a new tab instead, and if I want to copy it I use ctrl+d which focuses URL and selects all. reply keyneus 19 hours agoparentprevI switched to Vivaldi as a result of the removal of this feature from Firefox, because Vivaldi still allows you to choose this behavior. Are there other Unix browsers you're aware of that allow you to disable click-to-select? It'd be nice to at least have some options, although I'm generally happy with Vivaldi. reply mrob 19 hours agoparentprevAnd it's still broken, because the \"Paste and Go\" feature doesn't work. Common sense suggests \"Paste and Go\" would be equivalent to using \"Paste\" (which correctly inserts text from the clipboard at the cursor position) followed by \"Go to the address in the Location Bar.\" But if you unselect the automatically selected URL, position the cursor within it, then use \"paste and go\", Firefox ignores the previous URL and simply tries to go to the text in the clipboard. This could potentially be a security risk by tricking people into visiting URLs they didn't intend to. If they don't want to fix this, it should be renamed to \"Clear, Paste, and Go\", because that's what it actually does. reply dathinab 18 hours agorootparentFor non-technical people it would be a huge security risk of not behaving that way, similar Clear, Past and Go would just be unnecessary confusing. Technical people have the tendency to use keyboard shortcuts. reply mrob 17 hours agorootparentIf there is one command called \"Paste\", and another command beginning with and commonly abbreviated to \"Go\", \"Paste and Go\" should be equivalent to using both in sequence. If it's impossible to make it act in the expected manner, and impossible to label it correctly, the only remaining option is to remove the feature. reply dathinab 13 hours agorootparentyou are missing the point most non-technical people treat urls mostly as blobs (which doesn't mean they don't understand it's consisted of parts, but that's irrelevant) so the URL field is mostly operating one urls as a while that's why if you click on it in difference to normal text it will always select the whole url, because most times most people will either copy that url or fully replace it similar \"Past and Go\" also operates as the url as a whole, not text segments. So it pasts the new url to where the old url was and \"goes\" to the new website additionally if you don't just replace an url but edit it a \"do this edit and directly go without giving me a chance to double check it\" functionality doesn't really have any reason to exist as its way too niche and people who do that likely anyway use keyboard shortkuts instead of the context menu sure there probably could be a better name e.g. \"Replace Tab and Go\". Or they could not show it if you don't have all text selected. But \"Past and Go\" isn't a description of functionality anymore but has become something like a slogan or special term. So neither renaming it nor changing behavior is really acceptable from a UX POV. reply ikt 20 hours agoparentprev> this literally cannot be explained by anything except user hostility Really? It literally says why it was changed: it was a special behavior only implemented for Linux, it was not consistent with Firefox on other OSes, and with other browsers on Linux itself. The prefs were causing broken edge cases complicate to handle, taking into account all the possible pref combinations (for example under certain combinations it was not possible to select a word), and having to execute more tests for them. Not removing the prefs would have not saved many resources, since we still need to maintain them. reply lopkeny12ko 20 hours agorootparent> it was a special behavior only implemented for Linux, it was not consistent with Firefox on other OSes, and with other browsers on Linux itself. So GTK text fields behave a certain way on the entire platform (Linux). Other browsers choose to implement a behavior that is totally inconsistent with the rest of the platform. As far as I am concerned, Firefox was the only browser that implemented this correctly. Do you truly personally believe the right move here was to match the beahvior of other browsers, who themselves are incorrect by not respecting platform conventions? > The prefs were causing broken edge cases complicate to handle Don't fix something that isn't broken. > Not removing the prefs would have not saved many resources, since we still need to maintain them I can hardly see how \"having more code means it makes it harder for me to maintain\" is a legitimate argument. This argument makes no sense. Delete the entire URL bar then. The URL bar requires lots of code and is hard to write unit tests for. (/s) 1. Mozilla engineers are literally paid to maintain the browser, 2. not wanting to update unit tests to deal with a pref is pure laziness, no excuse. reply FeepingCreature 19 hours agorootparentI agree. It seems there are two conflicting views here. \"Firefox is a kind of browser, which happens to be running on a desktop.\" \"Firefox is a kind of desktop app, which happens to be rendering websites.\" In the first, Firefox should act like other browsers because \"browsers\" are the relevant reference group. In the second, Firefox should act like other apps on the platform because the platform is the relevant reference group. Personally, I think the second view is simply correct. How often do you switch between browsers? For all but a few power users, switching browsers is vanishingly rare compared to switching desktop apps. This suggests that at least for browser chrome, desktop consistency is much more important than browser consistency. reply dathinab 18 hours agoparentprev> as you might expect, Mozilla does not care internal options are internal options, no browser cares much about them outside of e.g. some huge company support contracts if you have to go to `about:config` for anything but dev or MDA related things then you can't expect things to continue working with any update and every option is code which needs to be maintained if I should guess they rewrote the code which used the option and did the faster/cheaper thing of not re-implementing a feature they officially anyway don't support reply OJFord 21 hours agoprevWhenever I see someone using (or a screenshot of) horizontal tabs I'm just shocked that people put up with it, or perhaps even more so that it's still the default in all major (and minor?) browsers. I recommend Sidebery, but more than that I just recommend something to get your tabs listed vertically in a sidebar, so they don't squash each other up as you add more until you can't tell what's what at a glance, the width is fixed, and you can always read the title. Which probably means implicitly 'I recommend FF', since I imagine Safari/Chrome/Edge don't let you make that kind of modification? FF actually doesn't even need an add-on for it, I used to do it just with a userChrome tweak I copied from somewhere, but Sidebery has a nice 'panels' (like tabs of tabs to switch between that the sidebar shows) feature, and you can bind them to the built-in 'container tabs' (which I always think should surely be called 'tab containers'?) so that work stuff opens in your work panel, for example. reply beAbU 21 hours agoparentWhenever I see someone using (or a screenshot of) vertical tabs I'm just shocked that people put up with so many open tabs at once. I have maybe max 20 tabs open at any given time. When I'm done with $TASK I close all the tabs (save 2-3 that remain open and pinned, mail, calendar, etc). The tab bar is like my stack. I only fill it up for the task at hand, then clear the stack and move on when done. If there are things I need to come back to, I use bookmarks. Vertical tabs and the browser's bookmarks manager are just too similar for me to want to use tabs over bookmarks. Especially considering a crash can wipe all open tabs. I am glad the browser gives me this option as a setting. And a browser that forces vertical tabs on me will probably lose me as a user for life. Also: > since I imagine Safari/Chrome/Edge don't let you make that kind of modification? MS Edge supports vertical tabs out of the box. Brave also, but I can't comment on the rest in your list. reply J_Shelby_J 17 hours agorootparentI feel the same way. Keep limited tabs open most of the time. But the advantage to vertical and nested tabs is that when I need to open a dozen links (like grabbing interesting links from a page without stopping mid flow) I can do it without having to ruin my environment. The tabs are there ready to be consumed: nicely nested below the source tab. Doing that with a horizontal tab bar would destroy my productivity until I closed all of those tabs. It allows you to switch tasks. As far as wasted space‚Ä¶ I hardly ever use a laptop screen. Text content is ideal at 70ch? That‚Äôs like a 1/3rd of a 27‚Äù monitor. reply asadotzler 10 hours agorootparentprevLook at Mozilla telemetry and you'll see most people have a few tabs open, and overwhelmingly most people's tabs all fit in the horizontal bar with no scrolling. People with more tabs than that are a low single digit percentage. reply DonHopkins 16 hours agorootparentprevYou wouldn't be having this argument if you could put your tabs on any edge you wanted to, at any time, even using multiple edges of you run out of space along one edge. It's an entirely artificial argument that doesn't address the real problem: incompetent UI designers preemptively and inflexibly making that decision for all users in all situation, and forcing everyone to use one edge or the other instead of any or all, therefore squandering 3/4 of the usable perimeter. reply foolswisdom 21 hours agoparentprevI know edge at least supports vertical tabs? I don't like them though (I don't have enough horizontal screen space, plus it doesn't free up the top bar space). If I start to have many open tabs I start cutting them down anyway. reply OJFord 21 hours agorootparentFair enough not having the horizontal space for it. It's usually still the opposite that annoys me (small centred content not making use of space) but of course this depends on zoom level etc. too. I'm not sure what you mean about not freeing the top bar though? The topmost thing I have is the back/forward/refresh/URL/extensions/menu bar. (file/edit/view/etc. hidden unless I press Alt, but that's not where tabs would be I don't think.) reply foolswisdom 13 hours agorootparentThe space where tabs go (when horizontal) isn't removed in vertical tabs mode. So using vertical tabs doesn't save any extra on vertical space, only uses additional horizontal space. reply OJFord 13 hours agorootparentWell what I'm saying is I don't have it, so either Sidebery handles it or I put something in userChrome, but either way it's something I have not had to think about in all the years I've been using vertical tabs, such that I was not aware of the issue. edit: the latter (or Sidebery does do it but my previous solution didn't and this is now redundant, I suppose), I have: #TabsToolbar { visibility: collapse; } #sidebar-header { display: none; } (~/.config/mozilla/firefox/profile/chrome/userChrome.css) reply asadotzler 10 hours agoparentprevMost users, like 90% or more have tabs that all fit visibly in the horizontal space. Look it up. Mozilla's telemetry is public. 5 or 6 tabs fit just fine and in a sidebar are hugely wasteful of pixels. You think everyone browses just like you so you can't imagine why the browser is shaped for people not like you but your browsing habits are in the ultra-minority. Take a look at the telemetry before making silly posts like this. reply eitland 3 hours agorootparentMost users also don't actually need modern processors. Most drivers don't need to pull a trailer. Most people don't need a wheelchair ramp. Still arguing for stopping processor development, removing the possibility to use towing hitches and removing wheelchair ramps isn't something I hear people argue for. It is almost like in other areas of life we accept that different people have different needs and wishes, but in software everything needs to be pixel perfect the way the designers envisioned it. Why? reply justsomehnguy 34 minutes agorootparentprev> sidebar are hugely wasteful of pixels You know what is also a waste of pixels? Websites pretending they are mobile view only on my 4K desktop monitor in landscape. Blogs, new Reddit, Twitter, Mastodon, news sites, whatever else. The vertical tabs wouldn't hurt that shit at all. And if the vertical tabs would be a first class citizen in a browser it would greatly help so if needed I could have switch to it. reply bloopernova 19 hours agoparentprevMy userChrome tweaks for vertical tabs: https://gist.github.com/aclarknexient/88673880d373864eee1927... Includes screenshots! :) reply mort96 21 hours agoparentprevI find that having a huge sidebar shifts the content off-center in a really uncomfortable way. reply OJFord 16 hours agorootparentAren't you currently on a site where most of the content is more off-centre, and having a left sidebar would actually put it more in front of you? I can sort of sympathise, I'm fairly 'OCD' (not actually, but as people say) about similar things, but it's never bothered me I don't think. Maybe try with it a bit smaller? I don't think mine's 'huge' - maybe a bit wider than single horizontally displayed tab would be (just as my choice for the length of title it allows me to read), a couple of inches on a 24\"-diagonal monitor. reply Izkata 15 hours agorootparentprevThere's userChrome modifications that make Firefox's sidebar collapsible, expands on mouseover and covers the page instead of pushing the page aside. I use that with TreeStyleTabs. Mine when collapsed is only ~3 favicons in width. reply bscphil 13 hours agoparentprev> I recommend Sidebery I've seen several people mention this addon. Can you (or someone using it) give a reason or two to prefer it over the standard alternatives (e.g. Tree Style Tab)? reply OJFord 13 hours agorootparentI already mentioned 'panels' and container integration. Idk, I don't know why Tree Style Tab is any more 'standard alternative' (or certainly not that any others are, afaict TST & Sidebery are the most popular two) - they're both extremely popular (Mozilla gives 196k users of TST & 75k of Sidebery; nothing else close) and both 'Recommended' by Mozilla. Use whatever you want, I did say more than that I just recommend using something to get vertically listed tabs. The tree-ing is a much less big deal to me tbh, though Sidebery does do that too fwiw. reply c-hendricks 13 hours agoparentprevSafari has a side bar where tabs are in fact vertical. Tab groups will also let you, um, group tabs, and they're presented as a tree in the sidebar. reply mvdtnz 21 hours agoparentprevEdge has vertical tabs out of the box. And yeah we get it, because vertical tabs people and tree tabs people are the vegans of browser discussions. Please, we get that you like vertical tabs. No need to keep going on about it. reply medstrom 21 hours agorootparentPlease, don't perpetuate the myth that vegans \"keep going on about things\" unless you've seen someone do it in real life. reply OJFord 16 hours agorootparent'unless you've seen someone' is not a good standard to use here, because of course some people are like that, that's where the stereotype comes from - but the vegans who don't 'keep going on about it' are inherently not as visible: it's not often you'll know someone is a vegan and not talking about it, essentially never for strangers. So if you really care about reasoning with people to challenge the stereotype, you need to know % vegans (according to some survey or whatever) in a population relevant to a person so that you can ask them to consider not if they've ever seen one vegan talk about it, but if they've seen a number X close to that % of the total people they've seen talk about it (and if not that they've encountered that-X% extra vegans without realising it). reply a_gnostic 19 hours agorootparentprevMost vegans I know use Edge. reply mvdtnz 20 hours agorootparentprevI have. reply J_Shelby_J 17 hours agorootparentprevI use tree style tabs btw reply nyx_land 20 hours agoprevThe worst thing about the Proton UI that I never hear anyone talk about is that it is objectively a massive downgrade if you're not using a modern 1080p (or higher res) monitor. My laptop is an old X220 Thinkpad, so that change actually affected my ability to work efficiently by pointlessly wasting a ton of screen space, and it's part of the same trend in software engineering of user-hostile decisions that impact people who either can't afford or don't want to use newer hardware for whatever reason. Except in this case it's not even something like making everything a bloated Electron or React app, it was just changing the UI to something that looks way worse for no reason other than to create the superficial impression that Mozilla still cares about Firefox. I run a customized userchrome because Proton sucks so much for me to use. reply alex3305 21 hours agoprevBrave browser recently also introduced buttons. I was a tad annoyed with it, but accepted it. Until my wife recently saw the new look while I was working and asked how I could even put up with this. She argued that tabs should look like tabs and not buttons. Fortunately for Brave the rollback is quite easy with a flag in `brave://flags` where you can disable the `brave-horizontal-tabs-update` feature. reply kilroy123 21 hours agoparentYes, I don't love it either. However, they FINALLY have truly pinned tabs that don't go out of sight, like Firefox. So I'm happy with the change. reply madeofpalk 19 hours agoparentprevThe downside of using hidden flags like this is that most of the time you're just delaying the inevitable, as opposed to just biting the bullet now. Up to you to decide which is the least worst. reply Astraco 21 hours agoprevThe same happens with Thunderbird. The new UI is a mess I can't find anything now. I fear when the rename K9 mail for Android to Thunderbird they break the UI too. reply thrdbndndn 21 hours agoparentIt's (almost) every major software. I mainly use Chrome, and in their newest M121 release they made not one, but three major UI changes and I hate every single one of them. For the curious, they are (together with my rant): 1. the new \"simplified\" bookmark save flow which is more complicated than the old one; 2. loss of the ability to disable system notification (i.e. to use Chrome's built-in one, which I prefer); 3. loss of the ability to disable \"copy to highlight\" context menu option via a command line argument, which I never use and it just messes up my muscle memory for right click -> copy.* * Seriously, why is it so tough for software in the CURRENT YEAR to just offer fully customizable context menus? How hard is that? Funnily enough, this used to be a staple feature in nearly all the popular freeware back in the 2000s and 2010s. It feels like the whole UI/UX scene has taken a nosedive lately. reply KTibow 19 hours agorootparentIf I had to guess why they might not let you customize it - the people who make the software don't know or don't care about the other way you want it - it adds a bit more complexity / sometimes code debt to let that thing be customized - the design might be a specific way that they don't want to change reply DonHopkins 15 hours agorootparentprevSimon Schneegan's \"Kandu\" cross platform pie menus, as well as his older \"Fly-Pie\" and \"Gnome-Pie\" projects, let you create and edit your own pie menus with a WYSIWYG drag-and-drop direct manipulation interface. Kando: The Cross-Platform Pie Menu (github.com/kando-menu) https://news.ycombinator.com/item?id=39206966 https://github.com/kando-menu/kando A first glimpse at Kando's Menu Editor! https://www.youtube.com/watch?v=rLJ1-z9i3cI Development Update for Kando's Menu Editor! https://www.youtube.com/watch?v=FIF6k9OxQ80 Item labels in Fly-Pie! https://www.youtube.com/watch?v=Yyl5nMPI1f0 Fly-Pie 10: A new Clipboard Menu, proper touch support & much more! https://www.youtube.com/watch?v=BGXtckqhEIk Fly-Pie 7: GNOME Shell 40+ and a new WYSIWYG Menu Editor! https://www.youtube.com/watch?v=sRT3O9-H5Xs Gnome-Pie https://vimeo.com/30618179 Gnome-Pie 0.4 (12 years ago) https://vimeo.com/35385121 https://schneegans.github.io/gnome-pie reply encom 17 hours agorootparentprev>fully customizable context menus? How hard is that? It's not hard at all, Vivaldi does it. I consider Vivaldi the least worst browser. reply atomicfiredoll 15 hours agoparentprevI recently got fed up with Gmail and downloaded Thunderbird, which I haven't used in a looong time. I was wildly disappointed by the modern incarnation. The lack of checkboxes feels mind boggling... I still feel like I must be not grasping something. Sometimes one of my hands isn't on my mouse or keyboard, but the interface felt like it constantly demanded I use both together (e.g. shift+click) to get things done. I ended up cleaning my inbox with the Gmail web interface. Despite my gripes, at the end of the day Gmail was just better and more efficient at it. Maybe it's nostalgia, but it legitimately feels like a 15 year old version of Outlook would run circles around Thunderbird's UI. This is painting with a broad brush, but coming from a background that included design, I've honestly come to resent modern UX designers. There are great ones, but there are also ones who are more interested in the design than the user and who ignore or bend the user data to support their (sometimes wild) opinions. reply amelius 21 hours agoparentprevRule number #1 of interface design: don't change the interface! reply jFriedensreich 21 hours agoprevInstead of losing the basics of usability I whish firefox would embrace new directions that have been shown by arc browser or at least allow the level of customiziation it used to. I made a prototype but the step to fork just to modify the ui seems ridiculous. https://www.youtube.com/watch?v=IxE8CD4dylQ reply qwertox 20 hours agoprevWhere can I vent about that \"upgraded\"-page which wants me to click through many dialog boxes of stuff I don't want to know about? A bigger problem with it is that it unfocuses from the last used/selected tab to this newly added tab so that it's hard to find where I left off, which ca",
    "originSummary": [
      "A Firefox fork is being developed to enhance the user interface, focusing on addressing the shortcomings of the Proton UI in Firefox v89.",
      "Clear icons, proper spacing, and tab layout are emphasized as crucial elements in UI design, with Lepton being highlighted as the most user-friendly option among different designs.",
      "Guidelines on customizing Legacy Edge browser and enhancing the Proton theme are provided to offer users a tailored and efficient experience while respecting the original Proton UI."
    ],
    "commentSummary": [
      "Github discussion on enhancing the default Firefox UI debates excessive padding in the new design, affecting user preferences based on screen size and vision impairment.",
      "Users are divided on the balance between padding and information density, touching on the removal of low-use features for code maintenance and the advantages of the Lepton theme.",
      "Mentioned frustrations with browser UI changes and the necessity for more customization options in software interfaces underscore the significance of accommodating diverse user needs in design."
    ],
    "points": 364,
    "commentCount": 233,
    "retryCount": 0,
    "time": 1708340559
  },
  {
    "id": 39430095,
    "title": "AstraZeneca Breakthrough in Lung Cancer Treatment",
    "originLink": "https://www.ft.com/content/b845e8ab-9cbc-482c-aa22-0b5c020be099",
    "originBody": "Accessibility helpSkip to navigationSkip to contentSkip to footer Sign In Subscribe Open side navigation menuOpen search bar SubscribeSign In Search the FT SearchClose search bar Home World Sections World Home Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa Most Read US warns China against dumping goods on global markets Crew abandons ship in most damaging strike yet by Yemen‚Äôs Houthis Geologists signal start of hydrogen energy ‚Äògold rush‚Äô The extremists driving Netanyahu‚Äôs approach to war with Hamas Alexei Navalny‚Äôs team accuses Kremlin of hiding activist‚Äôs body US Sections US Home US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 Most Read Companies Sections Companies Home Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport Most Read Gazprom grapples with collapse in sales to Europe Morgan Stanley accused of duping ECB with fake Frankfurt job title Capital One agrees to buy Discover Financial for $35bn Bad property debt exceeds reserves at largest US banks AstraZeneca unveils successes in treatment of lung cancer Tech Markets Sections Markets Home Alphaville Markets Data Cryptofinance Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Most Read Gazprom grapples with collapse in sales to Europe News updates from February 19: Navalny‚Äôs widow calls on Russians to ‚Äòshare her fury‚Äô; Houthis claim attack on British ship Capital One agrees to buy Discover Financial for $35bn Pension fund demand drives resurgence of UK corporate bond market Live news: Hungary to ratify Sweden‚Äôs Nato membership Climate Opinion Sections Opinion Home Columnists The FT View The Big Read Lex Obituaries Letters Most Read The strange loyalty of Putin‚Äôs global fan club Europe must ask: what if Biden wins in November? JD Vance: Europe must stand on its own two feet on defence Sinking skyscrapers, new beaches: Chicago faces the climate crisis Ten reasons why a mass-market sale of NatWest stock is now a bad idea Work & Careers Sections Work & Careers Home Business School Rankings Business Education Entrepreneurship Recruitment Business Books Business Travel Working It Most Read Work from home if you want but don‚Äôt expect a pay rise ‚ÄòThe one thing I‚Äôve always had is deal flow‚Äô: RedBird‚Äôs Gerry Cardinale Good news. We‚Äôve got the contract for the worst idea in the world How Kharkiv‚Äôs tech start-ups became the ultimate test of business resilience Business books: what to read this month Life & Arts Sections Life & Arts Home Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Most Read Michael Sheen, James Graham and Adam Curtis collaborate and miss with dystopian drama The Way ‚Äî review The perfect Austrian ski resort ‚Äî and why it‚Äôs fighting for survival What‚Äôs the secret to the perfect restaurant soundtrack? From the multiverse to a steampunk London ‚Äî the best new sci-fi books ‚ÄòOppenheimer‚Äô blitzes 2024 Bafta Film Awards with seven wins HTSI MenuSearch Home World US Companies Tech Markets Climate Opinion Work & Careers Life & Arts HTSI Financial Times SubscribeSign In Search the FT SearchClose search bar AstraZeneca unveils successes in treatment of lung cancer Subscribe to unlock this article Limited time offer Save up to 40% on Standard Digital Essential digital access to quality FT journalism on any device. All discounts based on monthly full price over contract term. Cancel subscription renewal anytime. SAVE 40% ON YEAR 1 $468 $279 for 1 year $23.25 monthly equivalent SAVE 25% ON 6 MONTHS $234 $175 for 6 months $29.17 monthly equivalent SAVE 10% MONTHLY $39 $35 per month Up to 12 months Subscribe now What's included Global news & analysis Expert opinion Special features FirstFT newsletter Videos & Podcasts Android & iOS app FT Edit app 10 gift articles per month Explore more offers. Trial $1 for 4 weeks Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial. Select What's included Premium Digital $75 per month Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%. Select What's included Over 65% off Print $49 for your first 6 months Then $199 for 12 months. Insight and expertise in your hands with the iconic FT print edition, delivered Monday to Saturday. Select What's included Terms & Conditions apply Explore our full range of subscriptions. Digital Explore digital > Print Explore Print > Print + digital Explore Print + Digital > Teams or organisations Find out more > Check whether you already have access via your university or organisation. Why the FT? See why over a million readers pay to read the Financial Times. Find out why Useful links Support View Site TipsHelp CentreContact UsAbout UsAccessibilitymyFT TourCareers Legal & Privacy Terms & ConditionsPrivacy PolicyCookie PolicyManage CookiesCopyrightSlavery Statement & Policies Services Share News Tips SecurelyIndividual SubscriptionsProfessional SubscriptionsRepublishingExecutive Job SearchAdvertise with the FTFollow the FT on XFT ChannelsFT Schools Tools PortfolioFT AppFT Digital EditionFT EditAlerts HubBusiness School RankingsEnterprise ToolsNews feedNewslettersCurrency Converter Community & Events FT CommunityFT Live EventsFT ForumsFT Board NetworkBoard Director Programme More from the FT Group Markets data delayed by at least 15 minutes. ¬© THE FINANCIAL TIMES LTD 2024. FT and ‚ÄòFinancial Times‚Äô are trademarks of The Financial Times Ltd. The Financial Times and its journalism are subject to a self-regulation regime under the FT Editorial Code of Practice. Close side navigation menuFinancial Times International Edition Subscribe for full access Search the FT Search Switch to UK Edition Top sections Home WorldShow more World Israel-Hamas war Global Economy UK US China Africa Asia Pacific Emerging Markets Europe War in Ukraine Americas Middle East & North Africa USShow more US US Economy Investing in America US Companies US Politics & Policy US Presidential Election 2024 CompaniesShow more Companies Energy Financials Health Industrials Media Professional Services Retail & Consumer Tech Sector Telecoms Transport TechShow more Tech Artificial intelligence Semiconductors Cyber Security Social Media MarketsShow more Markets Alphaville Markets Data Cryptofinance Capital Markets Commodities Currencies Equities Wealth Management Moral Money ETF Hub Fund Management Trading Climate OpinionShow more Opinion Columnists The FT View The Big Read Lex Obituaries Letters Work & CareersShow more Work & Careers Business School Rankings Business Education Entrepreneurship Recruitment Business Books Business Travel Working It Life & ArtsShow more Life & Arts Arts Books Food & Drink FT Magazine House & Home Style Travel FT Globetrotter Personal FinanceShow more Personal Finance Property & Mortgages Investments Pensions Tax Banking & Savings Advice & Comment Next Act HTSI Special Reports FT recommends Lex Alphaville Lunch with the FT FT Globetrotter #techAsia Moral Money Visual and data journalism Newsletters Video Podcasts News feed FT Live Events FT Forums Board Director Programme myFT Portfolio FT Digital Edition Crossword Our Apps Help Centre Subscribe Sign In",
    "commentLink": "https://news.ycombinator.com/item?id=39430095",
    "commentBody": "AstraZeneca unveils successes in treatment of lung cancer (ft.com)345 points by cebert 19 hours agohidepastfavorite127 comments cebert 19 hours agohttps://archive.ph/eNqky formercoder 18 hours agoprevMy aunt was diagnosed with stage 4 lung cancer and is only alive because of this drug. It‚Äôs a marvel of modern medicine. reply e40 16 hours agoparentMy wife was diagnosed in Nov '23. Stage 4. She had a mutation that allowed Tagrisso/osimertinib. There are many mutations, but this one is prevalent in Asian women who have never smoked. There was a trial published just after her diagnosis that showed progression free survival at 2 years of 57% (chemo+osimertinib) vs 41% (osimertinib). https://www.nejm.org/doi/full/10.1056/NEJMoa2306434 She's 6 days into the chemo. There are 4 rounds of pemetrexed+carboplatin and then maintenance carboplatin (forever). Every 21 days. So far, it's been rough, but hopefully her body will adjust and once she's just on carboplatin hopefully her quality of life will be better. She had few side effects from the osimertinib, but the main one was mouth sores. They're gone now, thankfully. Another thing people need to know about this: this diagnosis often comes with regular thoracentesis (removal of fluid from the chest). In this case, it's in the lining of her lung, not lung itself. Advice to anyone in this situation: make all future appointments for thoracentesis, because you don't want to go into the ER/ED to get it done. And, we've stopped allowing residents to practice on her (once it was quite painful). EDIT: Forgot to add: carboplatin crosses the blood-brain barrier, as do the cancer cells. The end state for this cancer, even with the above treatments, is usually tumors in the brain. reply Balgair 13 hours agorootparentHey, best of luck to you both. I've been there with close family members and cancer. It's no fun at all. Easily the hardest thing I've ever had to do in my life was just to care and watch. Make sure to grab family and friends to help out. The want to help out, most of the time, at least. I know it's hard to ask, but make sure to do it anyways. And yeah, standard generic advice to take care of yourself too. I hated that advice, as there just was never any time to do so. One thing that helped was to schedule that 'me' time in and make others aware of it. If you put it on the schedule, you'll have a better chance of taking it. Two tips with MDs: 1) Carry and use a notebook in all MD interactions. Just a simple journal is all. Time, medications given, dosages, MD administering, etc. It's a good back up to have, sure, but you'll likely never use it. The real power is when MDs see you writing things down. They take you more seriously and they take themselves more seriously. I think they think that you'll sue now and have some proof. But who knows. 2) If you're in for a long term treatment (1+ nights) put a big bowl of candy outside the door. Nurses etc. will stop by more often to check up on things and generally seem to like you more. Consider putting cigarettes in there too, depending on if your nurses smoke. Then they will really like you and go above and beyond. Again, best of luck to you both. FUCK cancer. reply e40 11 hours agorootparentThanks. Appreciate the tips. reply sizzle 12 hours agorootparentprevCan you refuse allowing residents to practice on you or your loved ones for better quality of care? reply s1artibartfast 11 hours agorootparentSure, you can generally refuse any medical care in the US. With respect to residents, It can be as simple as asking and picking the right doctor for your procedure. Their credentials are generally public. reply e40 11 hours agorootparentprevYes, you can, and the doctors we talked to said it would not negatively impact them. reply LambdaComplex 14 hours agorootparentprev> this one is prevalent in Asian women who have never smoked. Does that mean the rate is lower for Asian women who do smoke? reply hgomersall 14 hours agorootparentMy understanding, though not by any means an expert, is that lung cancer that non smokers tend to get is different to that which smokers tend to get. reply e40 11 hours agorootparentThis is my understanding. reply s1artibartfast 14 hours agorootparentprevI think they just mean that the incidence is much higher in that group than would otherwise be expected. Not all lung cancers are related to smoking, but I'm not aware of any where smoking decreases incidence or is protective. reply evaneykelen 13 hours agorootparentprevI wish her and you all the strength in the world. reply riversflow 15 hours agorootparentprevnext [5 more] [flagged] dj_mc_merlin 15 hours agorootparentPractice some empathy. reply basseed 13 hours agorootparentwhat did he say?? reply e40 11 hours agorootparentThat we were being selfish for not allowing residents to practice. reply StevePerkins 15 hours agorootparentprevJesus. reply declan_roberts 17 hours agoparentprevIt feels so good to read a comment like this. Thank you for sharing and I‚Äôm really happy for you and your family (and modern medicine that makes it all possible). reply agumonkey 16 hours agoparentprevIt's a great sight to see people outliving research protocols. Best wishes. reply hereme888 14 hours agoparentprevWould you be willing to share a few more details for me to understand? It would help me talk more concretely with my colleagues. So she had stage 4 lung cancer. \"Is alive only because of this drug\": - Was her life expectancy obviously prolonged? If so, by how much has she exceeded it so far? - Has the cancer progressed more slowly, halted, or regressed? Thanks. reply formercoder 14 hours agorootparentI‚Äôm not completely read in to all the details but she told me prior to this drug her diagnoses was a ‚Äúdeath sentence.‚Äù The lung cancer had metastasized throughout her bones. The indication was sudden lower body paralysis caused by a spinal tumor. She had spinal fusion surgery as well. I understand because it‚Äôs stage 4 she will never be in ‚Äúremission‚Äù but it has regressed significantly. reply hereme888 11 hours agorootparentThanks for sharing. At such an advanced stage and aggressive metastasis, for it to have regressed at all... wow. She must have been in so much pain. reply Spooky23 13 hours agorootparentprevMy wife had stage 4 melanoma. Prior to the newish immunotherapies about 5 years ago, it was a death sentence ‚Äî 6-9 months life expectancy from diagnosis. Now, it‚Äôs 60-70% 5 year survival rate. Unfortunately my wife wasn‚Äôt one of them. In general, these types of cancers spew mets and spread quickly. Many are resistant to chemo, go to the brain (chemo cannot help there) and only respond to high focused radiation like SRS or proton beam. Immunotherapies essentially suppress checkpoints that cancer cells use to avoid immune response and/or cause your body to target specific checkpoints. I can‚Äôt read FT.com, but I believe it‚Äôs talking about a targeted therapy that allows your body to control the cancer. There‚Äôs alot of research happening around things like immunotherapy combined with custom versions of Moderna and other vaccines that will significantly improve treatment and save people going through what my wife went through. It‚Äôs a good time. reply hereme888 11 hours agorootparentSorry to hear about your wife. Yes, once those melanomas go deep enough in the skin, they spread everywhere like a plague. I know there's even researchers creating AI-assisted treatment regimes that match specific mutations, other aspects of people's DNA, and all the immunotherapy drugs, in order to mix and match the best possible solution. Ongoing research and not yet widely available. I look forward to learning more about the newer developments. reply cschneid 8 hours agorootparentprevMy dad was in a similar situation ~10 years ago. Stage 4 lung cancer, although a different underlying mutation (ALK). Prognosis was less than a year. But he got on some new-at-the-time inhibitor drugs (Crizotinib iirc) which eliminated (\"no evidence\") the cancer. What is left does slowly mutates around the drugs, and he goes in for regular monitoring and has had some radiation therapy to knock down specific growth spots, but he's still here with us. I can't speak to the one in the article, but the drugs he's on aren't chemo, but they interrupt one of the pathways the cancer is using to infinitely grow. They have side effects, but not bad ones comparatively. reply hereme888 5 hours agorootparentAmazing. Thanks for sharing. Oncology is such a complex field. I imagine discoveries in this field will only advance exponentially due to increasing investments and AI tech. reply jfengel 17 hours agoparentprevThat is indeed remarkable. Stage 4 lung cancer usually makes you seek out hospice care. reply kthartic 17 hours agoparentprevWow, that's incredible. So wholesome reading a comment like this on HN, makes me more hopeful of the future. Thanks for sharing reply latchkey 17 hours agoprevMy mom died from lung cancer. It was going to happen, life long smoker. Dying from LC is a terrible way to go. Your lungs fill with fluid and you slowly drown in your own fluid. The pain meds make you hallucinate terrible thoughts. I wouldn't wish it on anyone. For the benefit of humanity, I hope this treatment works. reply Scubabear68 17 hours agoparentSame with my dad in the 80s. His descent was horrible to witness, in my case at the age of 21. I‚Äôll never know the truth, but I think his doctor took pity on him, dad died very shortly after a push of morphine. reply latchkey 17 hours agorootparentThanks for sharing and sorry to hear that. My mom was only 56 (2004) and I got a call from the hospice very early in the morning. At her request, I had tried to take care of her myself for the couple weeks after her diagnosis, but once the hallucinations started, due to her increasing the pain meds, she started screaming one night and the neighbors called the police thinking I was abusing her. Thankfully, they called a case worker and I was able to get her into hospice pretty shortly after that and then it was just a couple days more at that point. It was pretty traumatic. One of those life events you never forget. RIP mom. reply Scubabear68 16 hours agorootparentMy family tried home care in the terminal stage, but it was too much for my mom and sister. My mom also had some serious mental illnesses, compounding the difficulty we had as a family. The trauma was so great I did not really hit a normal state of emotional and mental stability until I was about 30. I was the only one there as he took his last breath, and it still brings tears when I think about it. reply arisAlexis 11 hours agorootparentHave you gotten therapy for that trauma? reply latchkey 3 hours agorootparentI have. One thing one of the therapists told me was that it is ok to be angry at her for the abuse I took from her growing up, due to her mental illnesses. That worked wonders. Once you get past anger, it is pretty much all downhill from there. reply Xenoamorphous 13 hours agorootparentprevI'll never understand how in this day and age we still allow people to die suffering this way. How come we are ok with euthanasia for pets but not people? reply nicoburns 12 hours agorootparent> How come we are ok with euthanasia for pets but not people? I think because: - It mostly wasn't an issue we had to deal with until recently because we didn't have the medical techniques to preserve people's lives so long anyway. - There are some concerns around people being forced into it that require any laws enabling it to be carefully designed. - Religion providing it's usual function of inertia around societal customs slowing the change. Pretty much everyone I know under the age of 60 (and many older) support euthanasia for people who have dementia or some other unpleasant condition that affects their quality of life (self-determined, not forced on people), so I'm quietly confident that it will be happen soon. I'll be pretty mad if it's not available for me if/when I get to the point that I want it! reply Mo3 12 hours agorootparentprev\"We\" don't. Here in NL euthanasia is freely available if needed. reply samus 12 hours agorootparentprevIt is allowed in some places. The problem is it's a terrible slippery slope which has been used to get rid of people whose lives were considered to not be worth living. reply lr4444lr 12 hours agorootparentprevBecause pets are property. There is no concept of consent. Human consent to die vs. pathological suicidal ideation is a very tricky thing. reply gosub100 11 hours agorootparentprevIt doesn't need to be legal to do it. People commit suicide all the time. The government doesn't need to be involved in each and every section of our lives, from cradle to grave. reply kevinmchugh 14 hours agoparentprevWatching my grandfather die with emphysema at age 12 was the only anti-smoking message I ever needed reply latchkey 14 hours agorootparentWeirdly, my father who has smoked (and drank) daily his entire life (now 78), is still alive and kicking. Camel unfiltered and a lot of vodka. I never smoked and I was never a big drinker. But, I stopped drinking entirely a couple years ago cause it was just too painful to look at him with his current health issues, and think to myself that I'm going to end up like he is. No thanks, I don't need that poison. reply ianlevesque 12 hours agorootparentAbout 20% of smokers escape that way. Enough for everyone to know one, but the odds are still not in your favor. reply kkio 17 hours agoprevMy father has been taking Tagrisso since he was diagnosed with Stage IV lung cancer in January 2022. It‚Äôs been able to give him a relatively normal quality of life I don‚Äôt think my family would‚Äôve seen otherwise if he‚Äôd gone through chemo and radiation. Just chiming in that I‚Äôm super thankful to that these newer treatments are so effective. That said it‚Äôs not cheap and he still has to work to keep his insurance‚Ä¶ otherwise it‚Äôd be ~10k/mo. reply e40 16 hours agoparentSo glad to hear your father has made it 2 years. I have hopes for my wife (see my recent comment). reply wing-_-nuts 11 hours agoparentprev>he still has to work to keep his insurance‚Ä¶ otherwise it‚Äôd be ~10k/mo. I mean, it'd be covered under the ACA or presumably medicare as well? reply phone8675309 14 hours agoparentprevI'm glad that Tagrisso has helped your father. Do we know how much public funding for Tagrisso that AstraZeneca mugged from the respective citizens of the countries where they operate and how much they're going to sell it back to us for? reply nradov 13 hours agorootparentIf you're honestly interested in the history of Osimertinib (Tagrisso) then this article has a good overview. It specifies the studies involved so that you can dig into funding sources. Generally for drug development, public funding only covers basic science and maybe early-stage trials. The majority of the expense is in phase 3 clinical trials which pharmaceutical companies pay for. https://doi.org/10.1093/annonc/mdw129 reply olliecornelia 13 hours agorootparentprevnext [2 more] [flagged] phone8675309 12 hours agorootparentWhat is your problem here? The parent comment was discussing the cost of the medicine, and I was responding to that. reply siva7 17 hours agoprevIs there actually some kind of list which shows in a table what kind of cancers are currently treatable by which experimental medicine? reply nradov 14 hours agoparentBy definition no one knows for certain which types of cancer are treatable by experimental drugs. That's why they're still experimental and not yet approved. But this site has a good list of resources for finding clinical trials that might help. https://www.cancer.net/research-and-advocacy/clinical-trials... reply baq 18 hours agoprevA friend died of lung cancer. Went from mild cough to funeral in about 5 weeks. 40 yo. 2 pre-teen kids. Started to look after my own health a bit more. This is a very welcome development. reply lowercased 14 hours agoparentPSA: If you have dependents, or think you might, get some life insurance in place now while you're relatively young. I've had too many friends/colleagues pass away unexpectedly without any (or not remotely enough) life insurance, leaving a spouse and young children behind, inevitably turning to gofundme and similar just to get by. reply alexey-salmin 14 hours agorootparentI actually tried that but the ones I've found had explicit clauses against cancer and other diseases in fine print. Apparently they only help if you get hit by a car and alike. Probably need to make another pass. (I'm in Europe) reply lowercased 13 hours agorootparentYeah, the comment may have been more US-oriented (where I'm living). I do know a couple colleagues who can't get any term life insurance because of some pre-existing medical conditions. That's not good for them, obviously, but for most others, it makes sense to get something early. Years ago, at 35, I got an $800k 20 year term for about $55/month. Close to 20 years later, we have more savings/wealth, so my family would not be in hardship or having to go begging if I died tomorrow. A 15 year $500k policy now costs me $80/month. I recently got that, but may let it lapse in a few years if we don't need that any more. FWIW, my policies don't have any specific denials for certain causes of death (particular disease, etc) but I did have to provide medical records (and I'm not in perfect health). reply evantbyrne 13 hours agorootparentprevAlso to hijack this thread: if you or a loved one is/was a long-term smoker, take yourself/pressure them to have routine cancer testing. Right now, while our liquid biopsy has limited availability, for most people that means getting CT scans. Lung cancer is very fast moving and survivability is directly related to how early it is detected. The overwhelming majority of people who are eligible for routine CT scans for lung cancer don't ever get them! reply CoastalCoder 10 hours agorootparent>The overwhelming majority of people who are eligible for routine CT scans for lung cancer don't ever get them! Surely there are tradeoffs here, because CT scans use ionizing radiation. Have CT scans gotten much more efficient (regarding radiation dose) in recent years? reply briHass 8 hours agorootparentYes. The one specifically designed for lung cancer screening is a 'low-dose' CT scan, that is usually around 1/5th of a standard CT scan. This is similar to the low-dose CT scan used for identifying heart disease, a coronary calcium scan. In fact, there some research showing that a single scan can do both at the same time (https://www.nature.com/articles/s41467-021-23235-4), quite well, especially coupled with ML analysis of the scans. For both of these diseases, it seems like the small amount of radiation (2-3x normal yearly background) is well worth the radiation risk done twice a decade for low risk (>40 y/o) and perhaps up to yearly for high-risk former/current heavy smokers (>50 y/o), which is the current recommendation. Even out-of-pocket, these scans tend to be very inexpensive (for medical diagnostic tests), at around $150-200. reply evantbyrne 8 hours agorootparentprevA person with 20 years of smoking history that waits for symptoms before getting tested might as well sign their own death certificate. CT scans aren't something that a person with a low risk profile would have routinely for a wide variety of reasons, which is why bringing an affordable liquid biopsy to market is so important. A lot of work has been done to make CT scans theoretically valuable to a wider audience, but that is not going to translate to widespread early cancer detection in-practice as evidenced by the >90% of eligible patients that never get tested. reply justinclift 7 hours agorootparentprev> get some life insurance Be super careful though, as many insurance companies (not only life insurance) just seem like scams. With you only that finding at the worse possible time, after the event. The alternative being \"try to plan for not having to rely on them\", which itself may also not be possible. reply wing-_-nuts 11 hours agoparentprevDamn it man, here I am reading this at 41 with a mild cough. Thanks for that! reply superfunny 16 hours agoparentprevI'm really sorry to hear that - cancer is terrible. reply graphe 15 hours agoparentprevAny more details? No history? reply baq 15 hours agorootparentSmoker, was always coughing a bit. His grandfather also died of lung cancer, but he didn‚Äôt make the connection until it was too late. reply instagraham 17 hours agoprev> Tagrisso earned $5.8bn in sales in 2023 ‚Äî 13 per cent of AstraZeneca‚Äôs total oncology sales ‚Äî making it the company‚Äôs highest earner. The drug seems to have been approved in 2015-16. If that's the case, why is this announcement of efficacy happening now? A long-term outcome after a short-term tentative approval? reply loeg 16 hours agoparentIt was approved for certain types of cancers and has slowly been approved for more and more types of cancer, basically. > Dec 21, 2020 Approval Tagrisso Approved in the US for the Adjuvant Treatment of Patients with Early-Stage EGFR-Mutated Non-Small Cell Lung Cancer > Apr 18, 2018 Approval FDA Approves Tagrisso (osimertinib) as First-Line Treatment for EGFR-Mutated Non-Small Cell Lung Cancer > Jun 6, 2017 AstraZeneca Presents Tagrisso (osimertinib) Data in Patients with EGFR T790M-Mutation Positive Lung Cancer and Central Nervous System Metastases > Mar 31, 2017 Approval Tagrisso (osimertinib) Receives FDA Full Approval > Nov 13, 2015 Approval FDA Approves Tagrisso (osimertinib) for EGFR T790M Mutation-Positive Non-Small Cell Lung Cancer https://www.drugs.com/history/tagrisso.html reply yieldcrv 16 hours agoparentprevUS approval follows the path of one treatment for one ailment. so if something was useful for 100 ailments, it needs 100 different approvals, and a company willing to attempt doing that what more likely happens is one treatment is denied for the one ailment the company thought they were tackling, and that one treatment is never pursued again so there's a lot of waste in this particular approach to protecting the public. I'm not familiar with other approaches. reply s1artibartfast 15 hours agorootparentOne other facet to consider is that it is already legal to for doctors to the medicine for any of those 100 approvals. those trials serve three purposes, 1) to generate high quality data to support doctor choice, 2) provide defense for medical malpractice suits against doctors, and 3) support insurance reimbursement. Doctors already have discretion to prescribe Tagrisso for the common flu. However, a doctor that does so will struggle to get insurance to cover it, and will have a harder defense if the patient sues. reply boppo1 15 hours agorootparentprevSeems like there's a lot of opportunity with those unused treatments, if you can get the right arrangement of capital. If the company gives up on pursuit, perhaps the'd sell it for less than the R&D cost. Someone could go scoop up 'losers' and find a better way to match them to ailments. reply s1artibartfast 15 hours agorootparentThis happens. Drug companies sell off IP and technology quite often. The bottle neck is you need someone with a better use hypothesis, and a billion dollars to test that. Drug Trials and development are obscenely expensive. reply ackbar03 15 hours agorootparentprevWhy doesn't the company pursue it for other purposes? Sounds extremely narrow, as per the comment above. There may be related stuff where it works as a treatment depending on the endpoint? reply s1artibartfast 15 hours agorootparentBecause it is extremely expensive and complex to pursue each each use. At some point of diminishing returns, it makes sense to let the medical community lead the process of finding and testing uses. reply yieldcrv 13 hours agorootparentprevbecause there is always lower hanging fruit at that point. Even if this was the most altruistic, non-profit funded goal to solve an obscure ailment, once that inevitably fails - and statistically it is inevitably - all capital has been expended, your team's reputation has been spent. You go to something that has more consensus instead of your alchemy concoction nobody's ever head of and wasn't good for the one thing you made it for. reply adamredwoods 16 hours agoprevAccording to Wikipedia, it depends on a specific cancer cell mutation, T790M, which could also be a forced mutation from using other EGFR tyrosine kinase inhibitors. This alludes how tricky, and how fast, cancer cells mutate and adapt. >> In people treated with osimertinib, resistance usually develops within approximately 10 months. https://en.wikipedia.org/wiki/Osimertinib reply zoratu 15 hours agoparentIn my MIL's case it was L858R. T790M also present, effectiveness of drug dramatically increased with presence of both. Osimertinib shrunk the tumor from baseball to golfball, and then her left most lung lobe was surgically removed. She was stage 2 at intake and without metastastis. The drug was not approved for that case in 2021 and she was admitted to an ongoing case study for it. reply rootbear 10 hours agoparentprevMy sister had the ALK variant of non-small cell lung cancer. The special treatment she got worked until it didn‚Äôt, about 16 months. One oncologist said the cancer had likely mutated and the ALK specific therapy was no long effective. reply pauln99 18 hours agoprev‚ÄúStatistically significant and highly clinically meaningful‚Äù perhaps, but not so much that they can say what that actually means. My understanding is that of all cancer drugs approved by the FDA between 2000-2016 (around 90 drugs), the median life extension was just over 2 months... reply s1artibartfast 17 hours agoparentYes, but this isnt as negative as it sounds. That 2 months average is often driven by more people alive at the end of a 2 year trial. Also, 16 years of 2 month extensions ends up being quite meaningful. reply pauln99 14 hours agorootparentTo be clear, those aren't 16 years of accumulating improvements. The median life extension - across all 90-odd drugs - was two months. reply s1artibartfast 14 hours agorootparentI understand that specifically is for stage 4 non small cell lung cancer, but I don't think it's appropriate to cherry pick a single example. There are other cancers and stages we're much more progress has been made. reply ndr 17 hours agoparentprevSad stats truth: statistical significance does not say much about effect size, and it's misinterpreted by ~half articles. See https://www.nature.com/articles/d41586-019-00857-9 for more reply losvedir 17 hours agorootparentThat's why the \"highly clinically meaningful\" part of the quote is there. reply jncfhnb 18 hours agoparentprevThe distribution skews right on these things. Median probably is not the right way to think about it reply pauln99 17 hours agorootparentMaybe I've misunderstood, but for the average person that's even worse news, right? Two months was from clinical trials where there's no doubt that the \"thumb is on the scale\" in a host of ways, creating an exaggerated estimate of efficacy that would be realised in the real world. The reality is closer to no life extension, with the remaining lifespan is spent sicker than you'd have been without the drugs. reply icegreentea2 16 hours agorootparentImagine a toy scenario where a cancer will kill 80% of people with it at exactly 3 years, and all 20% remaining will uh live full lives (so say another 20 years). If you created a drug that shifted the ratio to 60/40, and measured right at the 5 year mark. In a population of 100 untreated people, you'd get like 4080 total months lived (80x36 + 20x60). In a population of 100 treated people, you'd get like 4560 months lived (60x36+40x60). That's only generating an extra 4.8 months of life per person treated. However, you've doubled the number of people alive at the end of trial. Now obviously this is a toy example that's probably a bit exaggerated, but this type of behavior is exactly why median/average life extension is an inadequate measure alone. reply s1artibartfast 16 hours agorootparentprev>The reality is closer to no life extension, with the remaining lifespan is spent sicker than you'd have been without the drugs. I dont think that is true at all. The data is pretty clear that lifespan is significantly increasing. >Over time, the mean overall survival has improved from two months in 1973 to five months in 2015. Regarding long-term survival, a clear rise in 2yS is noted, increasing from 2.6% in 1973 to 12.9% in 2013 (latest year of which 2yS data can be calculated; Figure 1 and Tables 1 and 2), occurring mostly after the mid-1990s. A more modest increase is seen in the 5yS, from 0.7% in 1973 to 3.2% in 2010. >https://www.researchgate.net/publication/348455938_Long-Term... reply pauln99 14 hours agorootparentI'm sorry, but two months to five months after 50 years and hundreds of billions of dollars. That was hardly the promise of the \"war on cancer\"! reply s1artibartfast 14 hours agorootparentTo be fair, there are other cancers that are essentially cured. Lung cancer is a particularly nasty one for a number of reasons. It's hard to fight with the laws of physics or biology, so I'm curious who promised you something else. In general though, I tend to agree that in the US we spend far too much on what essentially boils down to performative end of Life Care. If you ask someone if they would rather spend $200k for 2 months or leave it to their family, I think most people would pick the second option. When someone else is paying, they choose the first option. reply pauln99 13 hours agorootparent\"This year marks the 50th anniversary of the ‚ÄúWar on Cancer‚Äù declared by Richard Nixon, a former President of the United States of America. By signing into law the National Cancer Act on December 23, 1971, Nixon hoped this action to be the landmark legislation taken by his administration. Nixon apparently had confidence that cancer would be conquered in 5 years.\" You're right, there are a small handful of unusual cancers that we have cures for, and that's great. And some small progress has been made overall. But a lot of money has been spent, and it has been half a century or more, and we don't have much to show for our efforts. reply JumpCrisscross 12 hours agorootparent> we don't have much to show for our efforts This is mind-bogglingly wrong for most cancers. Several death sentences have been turned into treatable diseases. reply s1artibartfast 12 hours agorootparentprevNixon hyped up some legislation 53 years ago, few believed the hype, and the hype was clear to the rest very shortly thereafter. You are right that progress is slow and expensive in a lot of areas. It seems you are frustrated or angry about it. What do you want people to do? Are we not spending enough on research? Are we spending too much? reply jncfhnb 14 hours agorootparentprevYou are, again, failing to note that it‚Äôs a right skewed distribution. The mean is not what you care about. reply s1artibartfast 13 hours agorootparentTo be fair, the Improvement for the tail is pretty Grim too. We have 'only' seen 5 year survival improve from 0.7% in 1973 to 3.2% in 2010. I think the skewed distribution is less relevant than the sample group. There are limits to what we can expect from medicine. We don't have improved survival after decapitation, but that doesn't mean surgery has been at a standstill. It would be interesting to look up survival rates for earlier stages. I expect the difference would be more substantial. reply pauln99 13 hours agorootparentprevIn fairness, I was responding to the first sentence in the quote from the article. It seems relevant, as a patient to ask, on average, how much longer will I live if I take this drug? reply s1artibartfast 13 hours agorootparentTo which, an average is a pretty poor way to convey non-normal data. If you run a year long trial where you flip a coin and immediately kill or save people based on the results, your average survival will be 6 months reply arrosenberg 16 hours agoparentprevAnecdotal, but Keytruda was approved in 2019 when my dad was diagnosed. He got 4 extra years (3.8 of which were pretty high quality) that I can assure you he would not have gotten without it. The progress on lung cancers specifically has advanced quite a bit in the last 10 years. reply e40 16 hours agoparentprevIn this case, the extension is often far longer. See comment by @kkio. Our oncologist has people that have been on it for years, as well. reply jseliger 15 hours agoparentprevMy understanding is that of all cancer drugs approved by the FDA between 2000-2016 (around 90 drugs), the median life extension was just over 2 months... An exciting, underrated possibility: therapies in conjunction with each other, even if the therapies are initially studied in isolation. Single-agent chemo doesn't work for many cancer types, but multi-agent chemo often does. The same may be true of the small molecules, antibody drug conjugates (ADCs), and monoclonal or bispecific antibodies being tested now. I have squamous cell carcinoma initially of the tongue, and now of the head, neck, and lungs (https://jakeseliger.com/2023/07/22/i-am-dying-of-squamous-ce...), and I'm on a clinical trial monotherapy at UCSD called MCLA-158 / petosemtamab. But, if I live long enough, which isn't super likely but isn't impossible, there's a decent shot that the FDA will finally approve Moderna's mRNA-4157 personalized vaccine for melanoma‚Äîhopefully in 2025. If I can get mRNA-4157 off label and combine it with pembrolizumab (Keytruda) and petosemtamab (assuming the latter is approved, too), that combo may be much more potent than any of the three in isolation, and I've already failed pembro as a monotherapy and pembro + carboplatin + paclitaxel. And if that combo doesn't work, or has some unexpected negative effect (including death), well, I'm going to die anyway, and the risk seems worthwhile. As another commenter observed, the median can be skewed by the number of people who don't respond; among those who respond to a given drug, some don't respond, but some live surprisingly long. reply AtlasBarfed 17 hours agoparentprevI think that is a perverse incentive of Medicare so it gets covered in treatment. The may work better, but the drugs only run the studies to that threshold. My mom died of lung cancer like 10 years ago and the immunotherapy drugs like keytuda all has those disclaimers. Her pd1 wasn't a good fit, but some like Jimmy Carter responded really well reply dghughes 17 hours agoprevI wish there had been something for IPF which is progressive scarring of lung tissue. My Dad died of it after a long battle. The doctor even said it's worse than lung cancer at least for that there are treatments. reply icegreentea2 16 hours agoprevI -think- this is the study referenced https://ascopubs.org/doi/full/10.1200/JCO.22.02186 reply genman 19 hours agoprevIt is about Osimertinib, sold under the brand name Tagrisso. It works (in ‚Äústatistically significant and highly clinically meaningful‚Äù manner) against certain types of lung cancer when administered on early stage. It was provisionally approved after success in phase 1 trial by FDA in 2015 and by EMA in 2016. Side effects: Very common (greater than 10% of clinical trial subjects) adverse effects include diarrhea, stomatitis, rashes, dry or itchy skin, infections where finger or toenails abut skin, low platelet counts, low leukocyte counts, and low neutrophil counts. Common (between 1% and 10% of clinical trial subjects) adverse effects include interstitial lung disease. reply hef19898 18 hours agoparentSide effects do not realy sound bad so, as someone who went through a light, comparatively, chemo I saw a lot worse with other patients. Plus, the alternative is lung cancer in late stages. And you sure as hell don't want that! It is really good to see progress in cancer treatment so! reply GenerWork 18 hours agoparentprevThat doesn't sound too bad if it can cause lung cancer to regress. reply e40 16 hours agoparentprevThe side effect you didn't mention: mouth sores. It's the only one that my wife had (in the first month). So far, no other problems, though the chemo started last week (see my comment about it, if curious). reply quantumwoke 12 hours agorootparentThat would probably be from the chemo your wife is on rather than the osimertinib. Chemo tends to cause mouth ulcers along with hair loss etc. reply e40 11 hours agorootparentNo, the chemo started a month after the osimertinib and the sores appeared in that first month. The oncologist confirmed it was a side effect of it. reply hanniabu 17 hours agoparentprev> It works (in ‚Äústatistically significant and highly clinically meaningful‚Äù manner) against certain types of lung cancer when administered on early stage. What is considered early stage? Pre symptoms? And what type of success rate are we talking here? reply vharuck 3 hours agorootparentCancer staging is complex, as one would expect for a wide-ranging category of diseases that follow different courses and cause harm different ways. Stage depends on tumor size, distance of spread, what organs it spread to (lymph nodes are especially worrisome), and how much the tumor cells differ from normal tissue. There is a large manual used by oncologists to record this information, and another large manual for cancer registrars to summarize it. https://www.cancer.org/cancer/diagnosis-staging/staging.html There are simplified categories of early/late stages, usually used in public health statistics. \"Early stage\" here is in situ (hasn't broken out of the tissue where it first appeared) or localized (hasn't left the primary site/organ). \"Late stage\" is regional (metastasized to adjacent organs) or distant (metastasized to non-adjacent organs, usually through the bloodstream or lymphatic system). >What is considered early stage? Pre symptoms? Not necessarily. This is generally true for things like kidney cancer, which is really deadly. People don't usually get checked until they have symptoms, which is usually after it's metastasized. But benign tumors can cause symptoms by physically pressing on organs. Benign brain tumors, which can kill, are an extreme example of this. reply 1letterunixname 18 hours agoprevHey onco-nosology nerds: Is there a cell-typed unambiguous classification system, research, and clinical treatment protocol nexus attempting to capture and disseminate information on all cancer types in a more precise and organized effort? reply thallada 16 hours agoparentNCI has the current treatment guidelines for each cancer and is updated when new treatment options become available: https://www.nccn.org/guidelines/category_1 Unfortunately, these are very clinician-focused and are very hard to understand without a medical background. They also don't usually cover experimental treatments that are still in trials, but in some cases can be extremely helpful. I work on an app at Outcomes4Me (https://outcomes4me.com/) that is trying to convert the NCI guidelines into an algorithm so that we can tell patients in simpler terms what their treatment options are based on their specific diagnosis. We also try to find clinical trials recruiting for their specific diagnosis. We've done this for breast and lung cancer so far. reply eber 18 hours agoparentprevI don't know if this is what you mean, but Foundation Medicine has a biobank of genomic data (HIPAA) with cancer patients' genomic profiles.[1] It's pan-tumor and pan-cancer. EDIT: if you're looking for more of a \"what is the current best known treatment plan for xyz\" then this might not be what you're looking for. The standard of care is very much changing very often, I don't know if anyone has a solution for disseminating that. foundation medicine also has a patient-focused side where you can get your own test and get pointed to the treatment for your cancer genomic profile [2]. The societies are probably trying the most to get all of that data available/up-to-date (for example: the American Cancer Society) [1] https://www.foundationmedicine.com/service/genomic-data-solu... [2] https://www.foundationmedicine.com/patient/start-with-step-o... reply badcppdev 18 hours agoparentprevMore precise and organised than what? reply someonehere 18 hours agoprevDidn‚Äôt Cuba have a vaccine for lung cancer? I seem to recall seeing one in a documentary (maybe a Michael Moore film?) reply overstay8930 17 hours agoparentStill in trials in the US/EU, but it's not useful for anyone over 60 because the immune system is too weak. reply sva_ 17 hours agoparentprevFor those curious, https://en.wikipedia.org/wiki/CimaVax-EGF reply alexnewman 18 hours agoparentprevMy father died of lung cancer. I also ran a failed medical tourism company which sent people to get this treatment in Cuba. More modern versions of the Cuban treatment are now few approved in the USA as immunotherapy. Didn‚Äôt save my dad reply badcppdev 18 hours agorootparentSorry about your Dad but did it help other people? How many did you send and how many had positive outcomes in terms of prolonged life above the expected outcome? reply sudosysgen 8 hours agorootparentYou might be interested in the Phase IV trials in Cuba, which were published here : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5346887/ There was also a published study by Roswell Park, on a few hundred patients many of which flew out to Cuba to take it, but it's only Phase 1, so no efficacy, only safety and antibody titers : https://www.roswellpark.org/newsroom/201903-safety-analysis-... reply herculity275 18 hours agorootparentprevSo sorry to hear about your dad! > I also ran a failed medical tourism company which sent people to get this treatment in Cuba Why did the company fail? How effective was the treatment? reply sudosysgen 8 hours agoparentprevYes, for non-small cell lung carcinomas. Its not a panacea, but seems to be effective. After observational studies of patients who flew to Cuba to take it confirmed efficacy, phase 2 clinical trials are underway in the US : NCT02955290 It's unfortunate it took this long. If it wasn't for the sanctions, it would certainly already have either finished (or failed, though unlikely) trials. reply londons_explore 15 hours agoprevI predict that ~30 years from now, someone will take all these drugs that have shown some success in treating some cancer, mix them all together at a low dose, and find the combination works better than any of the components. reply bglazer 13 hours agoparentThis is already done. There are many labs currently looking for \"synergistic\" combinations of treatments. reply jamaicahest 4 hours agoprev [‚Äì] Why are the top comments in this thread all personal anecdotes? Is this reddit now? reply ranguna 1 hour agoparenthttps://news.ycombinator.com/newsguidelines.html > Please don't post comments saying that HN is turning into Reddit. It's a semi-noob illusion, as old as the hills. reply wavemode 4 hours agoparentprev [‚Äì] Since when is it wrong for people who have personal experience with a topic to share personal anecdotes? And what does Reddit have to do with it? reply jamaicahest 2 hours agorootparent [‚Äì] I was under the impression that hacker news was not a place for sharing personal anecdotes, but rather a place for factual debate. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Financial Times provides in-depth global news coverage, including the Israel-Hamas conflict, global economy, and US politics.",
      "Readers can access sections on companies, tech markets, climate, opinion, work and careers, and life and arts on the website.",
      "Subscribing offers full access to expert analysis and quality journalism across different devices."
    ],
    "commentSummary": [
      "The discussion is focused on the effectiveness of cancer treatments, such as AstraZeneca's Tagrisso, for lung cancer patients with specific mutations.",
      "Topics include personal experiences, challenges in early detection, insurance coverage's impact on treatment, and debates on drug efficacy.",
      "Mention of new immunotherapies, targeted therapies, and the potential for combination therapies to enhance outcomes for cancer patients is highlighted."
    ],
    "points": 345,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1708353078
  },
  {
    "id": 39435320,
    "title": "Jeff Dean: Exciting Trends in Machine Learning",
    "originLink": "https://www.youtube.com/watch?v=oSCRZkSQ1CE",
    "originBody": "I'm going to talk to you about exciting Trends in machine learning it's going to be a very broad talk it's not going to go into detail in any particular area but I think it it's important to understand what is happening in this field and what is exciting and also you know what are the opportunities and also what are the things we should be sort of aware of as we build out this technology for everyone um and I'm presenting the work of many many people at Google so some of this is work I've been involved in as a co-author some of it is not just cool work I think you should learn about so uh with that I'm going to now give you another glimpse of the slido number uh 2207 20101 um and uh this is how you ask questions for this talk um so with that let's let's start with some observations um so in recent years I think machine learning has really changed our expectations of what we think of computers as being able to do if you think back 10 or 15 years ago you know speech recognition kind of worked but it wasn't you know really seamless it you know made lots of Errors computers didn't really understand images and from the pixel level what was in that image um language was kind of uh there were a bunch of work in natural language processing but it wasn't really a deep understanding of language Concepts and multilingual uh data but I think we've moved from that state to one where you actually expect computers to be able to sort of see and perceive the world around us in a much better way uh than they were able to 10 years ago and that opens up all kinds of amazing opportunities in you know pretty much every field of human endeavor because all of a sudden you know think about when animals evolved eyes you know we're sort of at that stage in Computing uh we now have computers that can see and sense and that's a completely different uh ball game uh and the other observation is increasing scale larger scale use of comput compute resources specialized computers I'll talk about you know uh larger and more interesting and richer data sets larger scale machine learning models all scaling all of those things tends to deliver better results and that's been true for the last 10 to 15 years and every time we scale things up things get better all of a sudden new capabilities emerge or the accuracy of some problem reaches a threshold where before it was kind of unusable and now all of a sudden it becomes usable and that enables new kinds of things um and also the kinds of computations we want to run because of this new machine learning learning based paradigms is pretty different than traditional handwritten twisty C++ code that you know a lot of basic CPUs were designed to write to to run effectively and so we want different kinds of Hardware in order to run these computations more efficiently and we can actually in some sense focus on a narrower set of things we want computers to do and do them extremely well and extremely efficiently and then be able to have you know that increasing scale uh actually uh be even more possible okay so uh I mentioned some of these things but there's been a decade of amazing progress in what computers can do so thinking about uh this uh you have you know going from the raw pixels of an image to you know maybe a categorical label of one of 10,000 or a thousand different categories you know computers didn't used to be able to do that a decade ago and now can uh audio uh waveforms to you know what was being said in that utterance of you know 5 Seconds of of audio uh that's speech recognition and we've made tremendous progress on that um translation hello how are you to bonju you know being able to translate from one human language to another is an incredibly useful uh capability for computers to help us with um and we've even been able to go from you know something like this is a nification photo of a cheetah on top of a a Jeep uh going from something like that to a description of that um not just a categorical label like leopard but you know a little short sentence that describes what is going on in that scene so that's pretty amazing like we made tremendous progress in this what's more amazing though is we've been able to reverse a lot of these arrows in the last uh few years um so going from a categorical label like leopard to you know the computer will generate you 50 or 100 different images of leopards or you know whole how cold is it outside to audio waveforms that's just text to speech that's been around for a while but it has improved a lot um the translation reversal is not that surprising but is getting better and better and then even going from a short description of an image you want and getting you know an image or sometimes now even a short video clip of what you want or a short you know an audio clip when you describe a sound in language um so these are capabilities that are now starting to emerge and I think are pretty exciting about um you know what we can build with computers now as opposed to a decade ago um so let's look at the level of improvement we've had in the last decade so Stanford uh developed a benchmark called imet which many of you have heard of uh which is basically going from you know you get some training data of this form like a bunch of color images and labels one of and labels uh and then you can train your system on about a million images like that and then you're given a bunch of images you've never seen before and you have to predict you know what is the the actual label for these new images that you've never seen before and a lot of machine learning work is how do you generalize from observations you've made on data to new settings new images you've never seen before and um so in 2011 the first year that contest was run uh the winning entry got uh 50.9% accuracy and then uh the next time the contest was run uh in a very famous landmark paper uh affectionately known as Alex net um by Alex keski ilas leter and Jeffrey Hinton they made a giant leap forward uh in accuracy you know 133% or so Improvement in accuracy which was just remarkable and they were the only one of about 28 entrance that year that used in Earl Network um but this was a major Improvement and the next year nearly all the entrance used in Earl Network simply because this was such a revolutionary Improvement and clearly a a really major approach to just learn from the actual raw data rather than trying to hand engineer features that are indicative of a leopard that's a really you know hard thing to do what features would you hand design to to decide this is a leopard as opposed to a giraffe or a car um but learning from data actually makes that possible um so I think that's that's a pretty big Improvement but it's also easy to ignore the Improvement that's happened since then like we've gone from 63% to now 91% accuracy on this task and so that that is actually you know pretty amazing we know human accuracy on this task is actually uh a bit below that level because it's actually pretty hard there's a thousand categories 40 different breeds of dogs people don't actually know you know if they're staring at a photo which breed of dog is that so uh that's pretty amazing uh this was you know about a 10e span and it's revolutionized computer vision if you look at speech recognition so this is a popular open source U uh Benchmark for measuring speech recognition accuracy here it's measuring word error rate you know the percentage of words that are wrong uh and you want lower numbers obviously so we've gone from 13.25% down to 2 and a half% and this is a much shorter span this is only like five years so you know basically kind of one word in you know six or seven is wrong to one word in 40 or so is wrong and that that makes a huge difference in usability of these systems all of a sudden you can rely on it you can start to dictate your emails and it mostly gets things right uh pretty great um and so I mentioned that scaling things up uh actually improves the quality of these models and so we actually want Hardware that enables us to scale up more efficiently how can we you know for the same amount of dollars of compute Hardware or the same amount of energy get something that gives us an even higher quality model because it's more efficient and so it's really thinking transforming how we design computers and they're uh you know machine learning optimized Hardware is much more efficient and there's major improvements that have happened generation to generation and this enables these larger scale models with lower economic and energy costs so there's two really nice properties of neural networks the kind of machine learning models that everyone is is using these days um the first is reduced Precision is okay like if you carry out the computations in the machine learning model to one or two decimal digits of precision instead of six you know that's fine you know a lot of times some of the optimization algorithms for these things actually introduce explicit noise in order to make the model learn better and so you can think of reduce precision as just a way of you know in some sense adding a bit of noise to the learning process and it actually sometimes works better and the other property is all the computations all the algorithms you're hearing so much uh noise about in some sense are really just different transpositions of ways of assembling different linear algebra operations so things like Matrix multiplies Vector operations of various kinds so if you can make and and that's really what those algorithms are is repeated applications of lots of different linear algebra uh Primitives and so if you can make a computer that's really good at reduced Precision linear algebra that's what you want for learning uh these really high quality models at sort of reduced uh computational cost or energy cost and so we've been doing this for a while at Google we saw that there was a real need for uh in our systems to build a system uh the initial version of what's called a tensor Processing Unit or TPU is really this uh architecture designed for low Precision linear algebra and the first one we built was for inference when you already have a trained machine learning model but now you want to apply it in a produ product setting you now need to like apply all this compute in order to recognize what's in that image or to have someone utter or something in a audio in their microphone and then be able to recognize what they're saying and so we built uh the first generation which was really just a single card system uh that had one of these accelerators on it TPU V1 um and that uh actually was a you know about a 30 to 80x improvement over using a CPU at the time in terms of both uh Energy Efficiency and and computational performance um later generations of the TPU we then focused on uh larger scale systems composed of you know multiple chips that were designed for both training and inference and so this is the TPU V2 board with four of these chips uh the TPU V3 board is sort of a a close cousin of this one but we added uh water so there there's actually uh water going to the surface of the chips to help with cooling uh the tp4 board uh we added cool colors uh which is nice and those three later generations were designed to be assembled into larger systems that we call pods and so the pods uh increased in scale over the generation so the first one um they have very very simple but high bandwidth networks uh in the Pod so basically each chip in this uh first generation was connected to its Four Neighbors in a 2d mesh so you have a 16 X6 grid of chips in some sense in these in these RS and every chip is connected to its neighbor with basically a wire and so that means you don't have to do any routing in the network um and so you can have very high-speed bandwidth very lowc cost connections because you're only trying to go six inches to the next chip or something like that um the Next Generation you know what extended this to 1024 chips in eight racks and the Next Generation actually used uh 64 racks of 64 chips each um it's actually multiple of these data center rows and gives you 1.1 exop flops of lower Precision floating Point computation um with 496 chips and then the mo most recent uh generation uh which we dis uh disclosed publicly last year at the end of last year is the V5 series it has comes in two variants one is for sort of uh inference where you have a pod of 256 chips and then the V5 P has a lot more memory per chip uh much more bandwidth between chips uh and much more memory much more bandwidth between chips and much more memory bandwidth um and has you know uh close to half a peda flop per chip of uh 16bit floating Point performance and and double that for inate performance and so one of these pods is also bigger so it's uh close to 9,000 chips for xof flops of compute okay so let's talk about language now we talked about image recognition and speech recognition advances uh but language is actually one of the areas that I think people are seeing the most change in what computers can do um so I've actually been excited about language models for a while even before NE networks so I I partnered with some people in our Google translate team to they basically had a really capable system that was um very high quality translations but it was designed for a research contest where you only had to translate like 50 sentences in two weeks or something and then you submit your entry um and so it would do like a disc seek for you know 200,000 engrs that it needed to look up for every sentence it would translate um and so I said oh well that if you have really high quality translations it'd be good to actually bring these into to real practice and so we built a system that would serve um an engram model basically kept statistics of how often every five-word sequence occurred in two trillion tokens and that gives you about 300 billion unique uh five Gams and then we just would store that in memory on a bunch of machines we'd look them up in parallel for the 100,000 things you need to translate a sentence and uh we came up with a an Innovative new algorithm called stupid back off that kind of ignored the right mathematical thing to do uh and did something much simpler so that when you looked up a 5 G and there was no data there you would just look up the 4 G that was its prefix and use that if it was there and if it wasn't there you'd look up the three G and so on uh and that actually worked reasonably well compared to the fancier fizer nay smoothing which is what you really want to do uh but is actually kind of computationally hard um so I mean one lesson from this is simple techniques over large amounts of data are very effective this has been lesson throughout my my career uh that you can actually do very simple things and the data speaks when you do that um then uh my colleague tamas melov was interested in distributed representation so instead of representing a word as a sort of a discret thing you want to represent it as a very high dimensional Vector so we're going to represent different words with different say 100 dimensional vectors and through a training process we're going to try to move words that appear in similar contexts nearer each other and we're going to try to push apart words that appear in in different contexts um and if you sort of train over a very L large amount of data with a relatively simple training objective that says okay if these things uh are appear in similar context push them closer and if they're different push them apart and you do that over trillions of tokens then you end up with uh really nice properties where in this 100 dimensional space right a 100 dimensional space is a hard thing to wrap your head around um but in that high-dimensional space things that are very similar end up near each other so if you have mountain and Hill and Cliff they will all tend to be kind of near each other in that high dimensional space uh so points in space are interesting but perhaps more interesting directions also are meaningful in this High dimensional space because there's a lot of different directions you can go in 100 dimensions and it turns out that for example if you look at where King is in this space and you want to get to Queen you go in a certain direction so you can compute that by subtracting the vector uh King from Queen and that's the direction you go um so it turns out that King minus Queen that's the direction is roughly the the same as the direction you would go to get from from uh man to woman and so directions are meaningful and different directions mean different things so going from the present tense of a verb to the past tense is a different direction regardless of what the verb is um and so that says that there's a lot of power in these distributed representations they're they're encoding a lot of different kinds of information in the 100 dimensional Vector that represents the word then my colleagues uh Ilia Oriel and quac um developed a model called sequence to sequence learning and so basically this used a neural network where you would have an input sequence uh let's take the case of translation so you put in an English sentence one word at a time and the system kind of builds a representation from its own current state plus the new word that it's now seeing and updates that state to have now a in the same way you have the the distributed representations for an individual word you now have a distributed representation for the sentence you've seen so far and you can update it with a recurrent neural network called a long shortterm memory and then when you hit an end of sentence marker You Now train the model to spit out the correct the the translation of that sentence so you have a bunch of training data which is like an English sentence and a French sentence that mean the same thing and you train the model when it sees this English sentence it should spit out this French sentence and you just repeat that process over large amounts of paired training data and sure enough um you can use a neural encoder over this input sequence to initialize the state which kind of gets you into the I've now absorbed the input sentence and I now want to decode a word at a time the correct translated sentence and we're going to use that to initialize the state of the neural decoder you scale this up and it works you get major improvements in Translation accuracy um Oriel and qua then published a workshop paper showing that instead of translation you could use context for a multi- turn conversation so basically the sequence of interactions you've had with one person or you know one party and then a computer model responding and then the other person or the person then utters another response and there's multiple turns that is your context previous previous multi- turn interactions and then you can train it to generate a good reply in that in the context of you know the multiple turns of things that happened before and it's the same model basically it's a sequence to sequence model but now the sequence is initialized with the context of all the the conversational turns that have happened and it's possible then to have effective multi- Turner interactions using a neural language model which is pretty neat um then a collection of other Google researchers plus an intern uh came up with a model called the Transformer so remember I said in this model this is a recurrent model so you have some State you take the next token and you do some processing to update the the the new state to have absorbed this token and then you go on with that new state to absorb another token and update the state again so that's a very sequential process right because in order to absorb the the third word you need to have done the processing for the second word in order to have done that for the second word you need to have done the processing for the first word that's not so great like in computers we like to do things in parallel not in sequence if we can if we can get away with it and so what this model did was say we're going to just process a bunch of data in parallel all the words in this input and then we're going to attend to different pieces of it rather than trying to have just a single state that we update sequentially going through the words um and what that said was don't try to force that state into a single distributive representation just save all the representations of all the you know tokens or words that you've seen and then attend to them like pay attention into the parts that make sense to focus on when you're doing this translating this part of the sentence or trans Translating that part of the sentence and you get higher accuracy with 10 to 100x less computer so remember I said all that stuff about computer hardware improving and specialized Hardware you know that's giving us you know large significant improvements over time but we're also seeing algorithmic improvements like this uh also multiplying together with those improvements and so you're seeing now the ability to train through algorithmic advances plus machine learning Hardware much larger scale models and you know much more capable models because of that um and so then a group of people uh decided to train uh scale up and train on conversational style data using a Transformer model instead of a recurrent model and that gave you know quite good results and in particular a way of evaluating this so that it's uh both sensible in what it responds and also specific you don't want your chat bot to be overly vague like yeah it's nice you wanted to actually say something sensible in response to to what you you interacted with it because that makes it more engaging and useful okay so I talked about some of these but there's been a progression of neural language models and also a pression of work in neural chat pods uh you know a neural conversational model uh Mina uh chat GPT from open AI uh Bard which we released about a year ago uh at Google uh and then a progression of neural language model so the sequence to sequence work I talked about gpg2 from open a which was you know some of these have parameter counts which you can think of as a rough sense of the scale of the model so one and a half billion parameters in 2019 the T work from Google from some colleagues of mine uh 11 billion parameters you know very very capable there oh the Transformer work I should mention that underlies a lot of these like the T here and the T here that's all for Transformer so people have now really seen the advance in the Transformer model and architecture that 10 to 100x Improvement in computation and really move to using that as the the basis of these these large language models uh gpt3 uh gopher from uh some deep mind colleagues Palm from Google research uh chinchilla from deepmind Palm 2 from Google research and then gp4 from open Ai and then Gemini which is the the project I co-lead with my colleague Oriol vinals uh we have a large collection of people in lots of different uh research offices working on building capable uh multimodal models so one of the things we wanted to do was move from not just a language-based model that understands text but one that can deal with all the different modality simultaneously so you can feed it you know text plus an image or you know Audio Plus some text and ask it to do something and it will be able to sort of uh fluently and coherently deal with whatever kinds of modalities you want to do give it so our goal when we started this project about a year ago was train the world's best multimodal models and use them all across Google um and so there's a Blog about Gemini there's a you know a website you can go to and there's a tech report um by the Gemini team of which I'm a proud member so Gemini was really multimodal from the beginning so one of the things we did I mentioned we didn't want to just deal with text we wanted to deal with with images and video and audio and we turned that into a sequence of tokens that we then train a Transformer based model on uh and then we have a couple of different decoding paths one we train to generate tokens uh and that are textual and then the other we initialize a decoder with the state uh that the Transformer has learned and then we can sort of go from that state to a full uh you know set of pixels for an image um and we support interleaving these sequences of text it's not like you give it an IM text input and an image input you can sort of inter leave them for video you might put in you know a video frame and some text describing that and then another video frame in some text or the Clos captions of the audio that's being said in the text uh and then have the Transformer kind of use the C the fact that it's been exposed to all these modalities during training to now build common representations across all the different modalities you want to give it um we have a few different sizes so the V1 generation of Gemini comes in three different sizes so Ultra is kind of the largest scale uh and most capable model we have Pro is like a good size for running in a data center uh and we use that in a lot of different product contexts uh so like our our Bard product which has now been renamed Gemini uh confusingly um uh is uh running on uh the pro model or the ultra model we just announced last week uh and then the Nano model you actually want a lot of these machine learning models to be able to run on device so on a small phone or a laptop and the Nano model is very efficient for for doing that and and fits quite reasonably you can quantize these things to make them even smaller uh and so on um so one of the things about our training infrastructure is we we wanted to be able to have a very scalable fabric that can deal with you know you specify a very high level uh description of the computation you want and then have a a a system that then maps that computation onto the available Hardware you have and so I mentioned we have these pods and so for example you might describe your computation as I have these two parts that I care about I don't care where you put them and not let the underlying Pathways uh Software System that we've built uh decide where to put them so it might decide to put this part on one pod and this part on another pod and then it knows where the chips are located and what the topology and bandwidth is between them so when this chip needs to communicate with that one it'll use this link the very highspeed uh Network I mentioned and when you need to have you know this part of the model communicate over here then it will go up to the data center Network which is you know much less bandwidth um to send data from one place to another but it's kind of seamlessly happens and the Machine learning researcher developer doesn't have to worry about it uh from that perspective other than just understanding there are different performance characteristics um so one of the things about training Large Scale Models is as you scale up you know failures will happen you'll a machine will die one of the TPU chips will you know overheat and and start to malfunction in some way uh and so minimizing failures is really important uh some of those failures you want to minimize can be Almost Human self-inflicted things so for example we had a a sweeping way of upgrading kernels on our machines which is a perfectly fine approach if those machines are kind of independent computations but if they're all part of the same you know thousand machine computation you actually would prefer to take the machines down upgrade all thousand kernels simultaneously and then bring them back up rather than having rolling failures throughout so we kind of optimize some of our repair and and upgrade processes uh but you also once you've done that you then want to minimize the time to recover uh because the faster you you uh can recover the the sooner you can actually be making uh useful forward progress and so we have a a metric we call goodput which is the percentage of time that model training is actually making useful forward progress uh as opposed to recovering from a checkpoint or waiting for some other part of the system to be started and one of the things we we use is rapid recovery from other copies of the model state from memory in those other machines rather than going to a Distributive file system to recover from a checkpoint and that you know makes the recovery time you know a matter of a few five to 10 seconds rather than several minutes uh in terms of training data you know we want this model to be multimodal so we want to train it on a large collection of web documents you know various kinds of books uh various kinds of code in lots of different programming languages plus images audio and video data um we have heuristics for filtering those data sets uh some of them are kind of handwritten heris some are modelbased classifiers of like do we think this is a high quality document in various ways um the final mixtures uh of the training data are determined through ablations on smaller models so we'll run smaller scale models with different mixes should we use 32% code or 27% code and then evaluate the performance on a wide range of metrics to to better understand that uh we've done some things like increased the weight of domain relevant data towards the end of training so we want to enrich it with say more multilingual data towards the end of training uh in order to make the multilingual capabilities improve I do think data quality is a really interesting and important research area um and I think it's we've seen that you know having really high quality data makes a huge difference in the performance of the model on tasks you care about and so that means you know in some sense that's as important or even more important in some cases than the actual say model architecture you're using um and and so you know I think it's a pretty important area for future research you know having the ability to learn curriculums automatically seems important uh identifying high quality examples and and low quality examples and so on and then there's been a bunch of advances in not only training these models but also how do you elicit the best qualities of the model how do you actually ask questions in a way that causes the model to be able to answer questions in a more effective way um so for example asking models to show their work improves the accuracy of the model and also the interpretability uh and so some of my colleagues came up with a technique called Chain of Thought prompting uh so if you remember back to third grade math class your your teacher would always encourage you to show your work right and the reason they want to do that is both to see your thought process and getting to the answer but also to kind of encourage you to think about you know what are the next steps and how am I going to break this complicated problem down into a smaller set of of steps and so if you ask a model you know give it you usually you give it an example of a kind of question you wanted to answer and then the actual answer for that question and then you ask it a new question and then you ask it to answer that question and so here's an example of a question and and then the way the model was taught to respond is just to figure the answer out and give it um and then there's a more there's a different question and the model output actually says the answer is 50 which is wrong but if you instead ask the model and show demonstrate to it you know how do you show your work um and you say okay well Sean started with five toys if you got two toys each then that is four more toys 5 plus four is nine so the answer is nine that's that's the work your third grade math teacher would be really proud of um and more importantly if you do that the model actually you know elicits these sort of more incremental steps to get to the answer and it gets it right because it's now had longer to think about the the steps to basically more time to think about through the steps of of uh getting to the right answer uh and so that's actually and it's a pretty dramatic effect right these two these two lines are the same underlying models at different scales and what you see these are two different M sort of mathematical oriented benchmarks uh the one on the right is sort of eighth grade math problems and this one is you know a bunch of arithmetic problems what you see is the quality of responses is pretty bad when you just give it standard prompting but at some point the model scale becomes large enough that all of a sudden when you ask it with Chain of Thought prompting your accuracy shoots up quite a lot so that says there's a really interesting science of how do you ask these models questions in a way that actually you know both makes them more interpretable and also more likely to give you the right answer okay so let's talk about multimodal reasoning in a Gemini model and I think an example is a nice one is a good way to understand what this model can do so here's the prompt you know here's a solution to a physics Problem by a student and then there's just a uh a picture of the problem and the students written out answer in kind of handwriting and then the rest of the prompt says try to reason about the question step by step that's again kind of the Chain of Thought prompting style thing did the student get the correct answer if it's wrong please explain what's wrong and solve the problem and make sure to use latch for Math and round off the final answer two decimal places and so that's the input this kind of like Hokey image of handwriting and a ski going down a slope and all this kind of stuff conservation of energy blah blah blah and then this is the output of the model so the student get didn't get the correct answer student made a mistake in the calculation of potential energy at the start of the slope um potential energy at the start is given by MGH student used the length of the slope I guess that's the hypotenuse instead of the height in the calculation correct solution is you know it means the to blah blah blah therefore we can write this is this is actually in latch but we've rendered it for your reading convenience uh and substituting the values there it is it worked the problem out to two decimal places so think about what this means all of a sudden we can give models like kind of multimodal input you know a complex you know picture of a whiteboard and a problem and ask it to do something and it can do it you know it's not always going to do it right but but it can and uh this can be an amazing educational tool so think about you know a student trying to work things out on their own and they're taking pictures of their solution and and you know the system is kind of helping helping them figure out what they did wrong um we know that individualized tutoring has outcomes that are two standard deviations higher when you have a one-on-one human tutor for Education than when you have a much uh broader scale classroom setting so could we get close to that in terms of having individualized uh tutoring I think I think that that possibilities within our Collective grasp um okay so evaluation you know that was kind of a uh qualitative example of Gemini's capabilities but it's also good to look at how it Compares on a bunch of different uh characteristics um evaluation you know really helps us identify the model strengths and weaknesses helps understand you know is training going well so we're constantly evaluating these metrics as we're training the model um helps us make decisions about what to change is our math performance you know lower than we would hope and so maybe we should enrich the training mixture with more math oriented data but what will that do to multilingual performance um there's a lot of complicated trade-offs some of which you make at the beginning of training some of which you're kind of monitoring online and trying to make uh principled or seed of the pan decisions I guess um and helps you compare the capabilities to other models and systems um and so the high highest level summary is you know we looked at 32 academic benchmarks uh and the Gemini Ultra model exceeded the state-of-the-art performance in 30 of the 32 um and so if we look and delve into some of these in in depth uh there's a bunch of text oriented or general reasoning or math oriented benchmarks um and if you compare Gemini Ultra with gp4 which is generally the the prior stateof the-art in most of these problems uh what you see is uh the ones in blue is the state-of-the-art and so we we state of the art on seven of the eight uh the 90% on mlu is interesting because this is a very broad uh set of questions in 57 different subjects you know chemistry math uh international law philosophy um and the group that put together the The Benchmark um measured human expert level performance at 89.6% I think or maybe 89.8 and so this actually exceeds human expert level performance in these 57 categories uh which is which is quite nice we're happy with that uh and then there's a bunch of coding related ones down here and math oriented ones here uh yeah I mentioned the % so if you look at image understanding benchmarks you know these are now getting into the multimodal aspect of this you know we uh got state-of-the-art results on eight of eight benchmarks ranging from uh one of the nice things uh was this Benchmark came out a week before we published our paper and and we'd never seen it before so we our Val team quickly added this Benchmark to our Val set and discovered that we exceeded the state-of-the-art results uh and by a you know reasonable margin uh which is nice it's it's always nice when you have a a benchmark you've never seen before and you do well on it uh because you're always worried about like leakage of of test training data into the test set and so on um if you look at video understanding you know again the multimodal capabilities of of this model really really shine pretty well state-of-the-art on six of six of six uh benchmarks including you know the important English cooking video captioning Benchmark uh and video question answering and so on um and if you look at audio you know the word error rates of of this on a bunch of diff four different uh uh public uh uh speech recognition benchmarks as well as a speech translation Benchmark state-ofthe-art m505 and multilingual capabilities are are quite good we're state-of-the-art on four of the five so yeah first I hope you appreciate our vow team because this is a tremendous amount of work to evaluate these models and really understand the capabilities in this level of detail and uh that's uh pretty pretty awesome um and it does give us a nice firm idea that the Gemini model is pretty capable uh and we also have you know measurements of pro and ano in the paper Okay so these large Transformer models can actually generate uh surprisingly coherent conversations uh which is kind of the the evolution of that sort of neural conversational model and then the Transformer based versions of those um so if you look at B.O I guess I have to update my slides so uh this will be gemini.com um so I was actually preparing a talk before we were using Gemini models uh a few months ago in Bard uh Gemini uh and I said you know reverse the letters of hot chips and tensor processing units for me so you know just to show what these models could do and it says sure the reverse strings are you know they they are great but then it went on to say I can also do this for you in Python here's the code uh you know Define reverse string as a function here's the string print reverse string of that reverse string of that uh use code with caution I always recommend that um and then it also goes on to explain it code first defines a function called reverse string takes a string as input reverse string function works by looping through the string code then prints reversing and it's always going to be helpful is there anything else I can help you with um so this is pretty amazing right like the fact that someone asked a question and it did what it was asked but then it also said by the way here's this thing called programming and some python code uh and here's what it here's how you would do this uh WR writing you know code to do this I I think that's pretty cool and then again a real Educational Opportunity um is there anything else you can help me with sure tell me more about tpus so you know the model has a fair amount of World Knowledge it knows that tpus are you know basically I I've told you a lot of this they're specialized Hardware processors developed by Google to accelerate machine learning they can help improve the efficiency and performance uh here are some of the benefits faster training inference I hope again I hope this helps um and and you know I think one of the kind of fun things about these chat Bots they can have different kinds of personalities right The Bard is sort of your helpful friend and will help you answer lots of questions uh so we actually put Gemini Pro in Bard uh in Gemini last month and there's a uh public site called lmy that can evaluate different chat agents because there's now a lot of different chat bots in the world and the way they do that is they get users to write their own prompt they pick two random chat Bots that they have configured in their system and then they send the query to both of them the prompt to both of them show and then show the output anonymized so you just say which is better the left or the right um and then from that you can compute what's called an ELO score so ELO was a I believe a Hungarian uh mathematician who was trying to develop ways to rank uh chess players and so when you have a tournament um basically you get more ELO points when when you defeat uh one opponent and so you can in a tournament setting uh compute an ELO score and roughly an ELO score being Higher by 100 means that the stronger competitor will win 64% of the time and 400 points is meant to be a 10 to1 advantage and so the the the pro L uh model actually achieves the second highest uh ELO score in this thing there's probably like 30 different models I'm showing the top of the leaderboard uh which is pretty nice uh so this is like a free service uh and these ones are are not and so I tweeted about this and then this person who i' never seen said it's actually it's really pretty good check out this example and you know have we have a way for people to share their interactions with bar that they liked and they actually want to make public and so he he decided to do that so if you click on that he was asking about please estimate the number of companies per million residents in each of the following countries the US England South Korea Taiwan and Singapore present your numbers in table form and so Gemini said absolutely here's a table showing the number blah blah blah there it is the number of companies for million just as you asked as you can see England has the highest number and it sort of interprets the results for you too England has the highest number followed by the US South Korea towan then Singapore it's important to note these are just estimates the actual number May Vary depending on source and definition I hope this helps let me know if you have any other questions um and so he went on to say you know we're you find the numbers for the companies oh I apologize for not providing the sources um it's important to be transparent where information comes from so the company numbers I use came from a combination of different sources so for the US they used the dun and Brad street database England I used the office of national statistics blah blah blah blah blah please note these sources have somewhat different definitions so it actually understands the subtleties right like the there's different definitions of what constitutes a company and it sort of knew where to go off and and get all this uh different kind of sources of data it didn't have that memorized but it was able to make use of it um pretty neat okay and another Trend I think is important is that uh um further refinement of these General models can make amazing domain specific models so some of my colleagues took some of our earlier work on the palm model and then the Palm 2 model which was a general purpose model kind of like uh trained on General text and decided to enrich it and further train it on medical data so medical kind of questions and medical articles and what they found was the med Palm model the first one actually exceeded the medical passing Mark for the medical boards and that when you then six months later they trained it on the med the Palm 2 model to do med Palm 2 they actually got expert level performance on the medical boards uh for this particular task now this is not a full general purpose uh setting it's like a bunch of medical questions but it does show the capabilities of having a really capable General model and then training it in a domain specific way for specific uh problems okay uh I'm going to go quickly through generative models uh producing images and video you've probably seen this as a trend in the world so we have couple of different research projects uh party and imagine uh and you know one of the kind of cool things I mentioned you can give prompts that describe what you want in visual imagery and then have models that can generate these these images that are kind of constrained by the encoding representation of processing the sentence and then conditioned on that it will generate pixels for an image so a steam train passes through a grand Library oil painting in the style of rembrand there you go uh a giant cobra snake made from X where X might be corn pancakes Sushi or Salad which is your favorite I I kind of like the the Ferocious lettuce looking snake but the corn one is pretty nice too um you know a photo of a living room with a white couch in a fireplace an abstract painting is on the wall and bright light comes through the windows so if you happen to need a picture like that for for a presentation or something something like I did you can uh do that and it can be pretty detailed descriptions you know a high contrast photo of a panda riding a horse Panda's wearing a wizard hat and reading a book the horse is standing on a street against a gray concrete wall colorful flowers and the word peace you know blah blah blah dlsr DSLR photograph daytime lighting and there you got you know there are many plausible interpretations of that but at least you got one example of what you what you asked for um and this is now integrated into Bard so the uh K through 12 uh government uh School agency in Illinois uh was really excited about being able to create images of their mascot hyperlink the Hedgehog um so there's hyperlink surfing riding this AI wave uh and this person was very excited about you know the prompt was a human buying coffee at Costa Coffee in London Coast to coffee is a very popular coffee chain um and one of the things that these models have often struggled with is the Fidelity of text uh actually you know putting the text you asked for making it look like a real font and so on and here you see it does does a pretty good job um I won't talk through a lot of the details but essentially you know you put in a prompt that gives you a representation of what that sentence in a distributed Vector based setting is and then conditioned on that the model is trained to generate first a small scale image and then take uh another model that is designed to improve the res increase the resolution of an image uh conditioned on both that lower scale lower resolution image plus the text embedding and then we apply that one more time with the larger image and the condition on the text edting to produce the full scale24 x024 image um and you can really see the effects of scale so if we train four different models uh with 350 million to 20 billion parameters um and then given them the same prompt you know a portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign in the chest that says Welcome Friends um what you see is you know it kind of have got the kangaroo aspect at the smallest scale but not and the orange Hy I guess but not much else there is a sign but it's you know again not it struggled with text let's say uh as you scale up a bit more the kangaroo got a little better it now knows a bit more that Sydney Opera house looks something like that but it's kind of a little chunky and doesn't have a lot of detail uh it's closer to Welcome Friends uh but it might be veg me he I'm not sure but then as you scale up you now get a pretty nice image of the Sydney Opera House and your kangaroo and the orange hoodie with the right text so you see scale is is an important aspect of this and this is why you're seeing all these advances in you know the last decade is essentially scale and better training methods and algorithms really contribute to higher quality results this graph just effectively says the same thing but I think the kangaroo says it better um I think it's also important to realize that there's a lot of machine learning uh kind of invisibly helping people in various ways uh and particularly on phones so you know a lot of camera features in modern smartphones have gotten significantly better over the years through combinations of computational Photography methods and machine learning methods uh together you know so portrait mode where you make the background all blurry so you look all fancy in the foreground um is a nice nice technique for some of these uh portrait style photos uh night sight where you're trying to take an image in very low light conditions you can essentially take lots of readings from the sensor and integrate those in in software to create you know much higher uh lighting conditions than the actual conditions under which you did that that also helped you take better astrophotography and portrait blur and color pop or nice features sometimes when you want them um Magic Eraser so if you actually understand images and you point at like one of the telephone poles and says make and say make these go away then the system can do that maybe your waterfall photo had these you know uh other tourists in front of it and you didn't want them there you can you can erase them uh there they go um and there's a lot of features on the phone and many of them are sort of about how do you transform one modality into another uh you know so sometimes uh you want to be able to say screen a call uh so maybe you don't want to actually answer your phone but have a a computer generated voice answer the phone for you ask what the person's calling about and then give you a transcript of what they said uh and then you can decide you know do I want to accept this phone call or not um you know hold for me can kind of listen on the phone for you so you don't have to hold uh hold on Bank of America when you're calling their customer support live caption can show can take any video playing on your phone and listen to the audio and then give you transcripts uh captions of what's being there maybe you're trying to watch a video in a lecture hall like this and you don't want the audio to disturb people um so there's a lot of cool cool features of this and a lot of these are running on people's phones without them necessarily realizing it or thinking about what technology is under them there um and this has amazing uh advances for for uh sort of people in limited literacy settings you know you can point your camera at something and it can read you what you're pointing it at or maybe you don't speak that language and you're trying to understand it it can it read it and translate it for you um I think I'm gonna go quickly here and maybe skip some of this section uh I will just skip over some of this but there's pretty awesome advances in uh yeah so I I'll start here you know I think Material Science is a pretty interesting area where you know basically machine learning is starting to influence lots and lots of aspects of of science um both through kind of automated explorations of interesting parts of a scientific hypothesis space or through you know creation of very rapid simulators that are learned rather than than sort of traditional high high large large scale kind of HPC style computation you know in some areas you've been able to learn a simulator that is uh sort of the functional equivalent of a hand-coded simulator but is now 100,000 times faster and so that means that all of a sudden you can you know search a space of 10 million possible chemicals or materials and identify ones that are interesting and promising and have certain properties uh that you would would normally have to apply a lot more compute for uh and so some of my Deep Mind colleagues uh were actually looking at uh interesting ways of searching the space of possible materials for those with interesting uh uh uh properties so they have a structural pipeline that can you know represent a PO potential material as a graphical neural network and then a compositional pipeline that can sort of mutate sort of known structures into ones that are sort of interesting and adjacent and then uh use an existing database of materials to then be able to Output energy models and a bunch of stable uh interesting possible compounds uh and so this automated discovery of 2.2 million new Crystal structures uh leads to a bunch of interesting you know possible candidates for actual uh synthesis in the in the lab to see what properties they actually have and I think there's huge potential for using machine learning in all aspects of healthcare um really uh we've been doing a fair amount of work in the space of Medical Imaging and Diagnostics uh for quite a while uh and those problems range from ones where you have 2D images to some some where you have sort of 3D volumes from uh MRIs or other kinds of 3D CT scans um and then some where you have just a single view to ones where you have multiple views and and large images very high resolution things for pathology for example um and so there's been a a fair body of work I'm going to talk briefly about two of them though uh so one of the areas we've been working in the longest in this space uh is the area of diabetic retinopathy and so diabetic retinopathy is a is a degenerative eye disease that can um you know if you catch it in time it's very treatable but if you don't uh you can suffer full or partial vision loss and really people who are at risk which is sort of anyone with diabetes or pre-diabetes should be screened every year but in a lot of parts of the world there just aren't enough opthalmologists to do this screen those who have been trained in sort of interpreting these retinol images um and so this is something where machine learning can actually help a lot because you can actually train a model based on you know uh trained opthalmologists annotating images to say yes that's a one that's a three a two that's a five um and if you train a model on board certified optomologist you can actually train a model that is as as effective as board certified Opthomologist if you then go on to get that same training data annotated by retinal Specialists who have a lot more uh expertise and and experience in the these cases you can actually train a model that is uh on par with retinal Specialists it's kind of the gold standard of of care in this space and there are very few of those in the world but you can all of a sudden make the screening quality be that of a retinal specialist uh using a GPU on a laptop um and so we've actually partnered with uh organizations in India a network of Indian eye hospitals and and the government of Thailand as well as France and Germany um and we're sort of uh doing lots and lots of screening every year uh and then dermist so Dermatology is a condition where it's interesting because you actually don't need special equipment to sort of us gather data that is useful for interpreting you know do you have a Dermatological condition or not uh and so we've got a system now deployed where um you can take a photo of something as you see in the video and it will give you a sense of you know what this might be what are other similar looking images in sort of Dermatological databases uh and it can help give you a sense of is this something very serious or is this something s fairly benign um okay uh and then finally I think deeper and broader understanding of the machine learning methods as we sort of deploy them in more places in the world is really really important and you know as we've gone from doing basic research in in machine learning to then using it in a lot of places in all of our products we started to think about a you know a set of principles by which we want to you know think about the implications of using machine learning what considerations should we have for you know various ways in which we might apply it um and we in 2018 published a set of principles that we came up with uh really these were designed to help educate our own uh internal teams about machine learning and things you should be thinking about as you're thinking about applying it to problems you care about uh and so for example you know avoid creating or reinforcing unfair bias often when you train these models they're trained on uh World data from The Real World and that's often the world as not the world as we'd like it to be but the world as it is and so it's really important when you're deploying machine learning models that you don't sort of train on data that is biased in unfair ways and then accelerate that because now you can automate and make these decisions more rapidly um so there's a bunch of techniques you can apply uh on a sort of algorithmic basis to remove some kinds of bias um and what we strive to do is sort of apply the best known current techniques but then also do research on advancing the state-of-the-art in these areas of bias or for example um uh accountable to people we think you know making models interpretable is an important aspect of that um you know being sensitive to privacy when that makes sense in the setting your deploying it uh and you know be socially beneficial uh and so I I'll point out a lot of these are sort of active areas of research uh so we you know we've published about 200 different papers in uh in the last five years or so six years related to fairness or bias privacy or safety and you can see those there okay in conclusion you know I think it's pretty exciting times for computing I think there's a change underway from and coded software systems to ones that are learned and that can interact with the world in various interesting ways and interact with people in interesting ways um the modalities that computers can now sort of ingest and understand and sort of produce are growing and are you know I think going to make using computers much more seamless and natural you know a lot of times we sort of restrict ourselves to typing on a keyboard or something like that but I think we now have the ability to talk to a Computing system in a very natural way it will understand what we say it'll be able to produce a natural sounding voice in response or a nice image if that's what we asked for and so I think that's pretty exciting so there's tremendous opportunity for sure uh but there's also a lot of responsibility how do we sort of take this work forward make sure that it's socially beneficial uh and really uh kind of do good things in the world with it and so with that thank you very much [Music] um I will I will put up one more plug for the slid or number There It Is Well thank you very much for your talk thank you very very much for your talk please don't send more questions to slid though okay it's a very nice idea but we are overwhelmed at this point so what we are going to do is we um I'm going to give you some some questions some there were some Trends in the questions that appeared in slid though so I'm going to ask you some of these questions and then for those of you who made it to this Auditorium we'll give a we'll ask we'll take one or or two questions from the audience uh so one of the questions um let me start with a question that you probably expect okay more data is it going to to make your model better twice more data are we going to see twice as h as good a performance yeah I mean it's a it's a good question and it's a it's not a simple answer I think I mean I think we've seen that more high quality data absolutely makes the model perform better when you have the capacity to sort of train on that larger amount of data so it's important to think about the model's capacity you know sometimes you need to increase the scale of the model as well when you have more training data we've seen uh more data actually hurt so if you actually get a lot of lowquality data you can actually for example decrease the model's ability to effectively do mathematics problems or things like that so it's it's a Nuance thing but in general more high quality data and more capacity for the model will make the model better yeah so a next question in this uh um in the slide that emerged is okay so what is the future of llms now that the vast majority of high quality training data has been exhausted how would you react to that I would disagree with that assertion a bit uh you know I think we've not really begun to train on say video that much I mean we've done small amounts of video but there's a huge amount of video data in the world I think actually understanding the world through Visual and audio data will be different than sort of training on a lot of language you're going to want to do both but I don't think we've really exhausted the training data in the world yeah I I tend to agree with you I think we still have a lot to go multimodel models you emphasize that in your talk um do they achieve better performance on all domains than targeted models for each domain separately or you can paraphrase this question and answer a version of that question I mean I think I think in some cases they do so the question is as you add more modalities does that improve the performance on other modalities and hope so and and generally we do see some aspects of that um but I I you know I think if you collect a if you have a narrow problem and you collect a very targeted data set that is designed to tackle that just that problem that will often you know give you good performance on a problem Oh the problem but if you have a complicated problem or it's hard to collect very specialized data what you want is a model that has a huge amount of knowledge of lots of different things in the world you know from language and from images and audio and then to be able to apply that model to the problem you care about and then if you have a little bit of data for a problem you care about then you're going to want to start with that base model and then fine-tune it or do in context learning or something like that to make the performance uh quite good maybe I could follow with another question which is kind of related today the cost of training large models prevents small startups from making impact what kind of projects would individuals with less resource work on would you like to comment on that yeah absolutely I mean I think um there's a really large set of problems in the machine learning domain I I'm going to address it more from a you know what interesting research can one do MH in the in the broad area uh where Maybe you don't have access to large Data Centers of of compute and so on and I think there's just an really wide open set of things uh so I mentioned the quality of data eval automatic evaluation of data quality or online curriculum learning or optimization methods or a lot of these kinds of things can actually be demonstrated on you know one GPU or you know a handful of gpus under your desk and actually make pretty significant and Innovative advances you know the or Transformer work was done on 8 gpus I think so uh that or the sequence to sequence model for sure was 8 gpus and so I think there's advances to be had from clever ideas good evaluation of them and even demonstration of them at small scale okay another set of questions that we got is is llms everything is Transformers everything what else is there should we be working on other kinds of uh models is is the emphasis on llm llms stiffling other other work in machine learning yeah I mean it is a worry right like are we crowding out other innovative ideas that maybe uh are you know not as fully developed and so they don't look as good as some of the things that have been you know much more fully explored and were sort of uh you know um in the kind of now gentle exploration of the space around what works well when maybe something over here would work really well you know I think a lot of the time uh showing even at a small scale that some other idea is a really interesting Direction can be done with some modest amount of experimental evidence um and I think that's an important area to go I would say mo you know I I tend not to use llm because I think we're moving to a multimodal world okay uh and I think multimodal is going to be more than just kind of the human modalities you think about like uh Visual and audio and language but other modalities that are important in the world and you know like time series of interesting you know heart rate sensor data for healthare applications yeah there's probably 50 to 100 modalities of data you'd want to be able to deal with I see I just saw the the the clock and we've really run over time so I would like to end here by thanking Jeff Dean for his talk thank [Applause] you",
    "commentLink": "https://news.ycombinator.com/item?id=39435320",
    "commentBody": "Jeff Dean: Trends in Machine Learning [video] (youtube.com)305 points by belter 12 hours agohidepastfavorite104 comments paxys 9 hours agoParts of it were good, but it mostly felt like he was reading through a slideshow created by the Google marketing team. reply babl-yc 9 hours agoparentAgreed -- given the talk is titled \"Exciting trends in Machine Learning\" it feels pretty incomplete to gloss over ChatGPT blowing up. OpenAI bet big on (1) emergent behaviors as you scale language models and (2) RLHF / fine-tuning to follow instructions. Both those topics were lightly covered but very much from the \"Google did this\" perspective (word2vec, step-by-step reasoning). reply elevatedastalt 8 hours agorootparentThe research breakthroughs all happened at Google, so it's not surprising. reply janalsncm 7 hours agorootparentRLHF was invented by OpenAI. DPO was invented at Stanford. reply seanbethard 5 hours agorootparentExactly. Google. reply candiodari 3 hours agorootparentAnd all the people that actually did the inventing promptly left Google. (yes I get there's 2 exceptions) reply lern_too_spel 8 hours agorootparentprevThe original paper on emergent behaviors in LLMs is from Google: https://arxiv.org/abs/2206.07682 The difference between OpenAI and Google isn't due to different research directions between the two firms. It is a difference in ability to execute. reply babl-yc 8 hours agorootparentGoogle certainly had influential papers on language models having emergent behaviors, but this isn't the one that inspired scaling up GPT. It was published August 2022 and GPTs at OpenAI were getting scaled up long before this. reply seanbethard 5 hours agorootparentprevFrom the GPT-2 paper: ``Also thanks to all the Googlers who helped us with training infrastructure.'' reply pama 6 hours agorootparentprevThis paper was interesting at the time but the main sentence in it was wrong: ‚ÄúThus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models.‚Äù Such behaviors can totally be predicted if one looks at the evolution of the likelihoods for these behaviors by scaling and extrapolating from the small models. reply canjobear 4 hours agorootparentMaybe you can predict emergent abilities post hoc, but I recall no one predicting beforehand that, for example, a pure language model could do translation simply be giving a prompt that said ‚Äútranslate to French‚Äù reply benlivengood 5 hours agorootparentprevI don't think many-shot prompting or chain-of-thought were predictable from model size. They just showed up at a particular model+data size. reply throwaway4good 1 hour agoparentprevI watched some of this after it was promoted to me on YouTube. It is very much: Look at this other cool blackbox that Google has made - I think it has a helpful personality. And sure: Applied AI / ML is a part of computer science but I had hoped it would be more of a walk-through of what has happened in terms of advances in theory, algorithms, architecture, training methodology, and perhaps some sort of explanation of why does it work giving this general purpose model a bunch of medical data? (Is it merely an elaborate fuzzy search, how does it extrapolate, is there actually reasoning going on and how does emerge from a neural net etc...) reply godelski 4 hours agoparentprevWhat I hate about the ML community is that papers have become ad pieces now. I'm all for them releasing technical papers, but do we have to fuck up the review system for it. We don't need to railroad our research community and doing so is pretty risky. I don't buy the scale is all you need side. But if you do, doesn't matter if I'm wrong, we'll just get there when Sam gets his $7T. But if I'm right, we need new ideas if we're going to keep progressing without missing a step. reply seanbethard 3 hours agorootparentDomain adaptation across verticals is the only driver of innovation. Check out the NeSy computation engine. Its ``semantic parsing'' is domain parsing and its symbols are numeric. Scale if you want. It works for images. That's all that matters, right? Mapping language onto the latent space of images gets you crude semantic attributes sometimes. If you have to push for multimodal out of the gate maybe start with articulatory perception. These LVMs aren't going to cut it. ML research isn't meant to further your understanding of anything. You can't separate it from corporate interest and land grabbing. It's the same thing re-hashed every year by the same people. NLP is pretty much a subfield of IR at this point. I love how fast my comment was blacklisted to the bottom of this thread. lol reply rvnx 10 hours agoprevAt 1:00:18 it's claimed, \"more than just publications\" and that products are live for the public to use. And they mention something that sounds cool. It's called DermAssist, it's to find similar images related to skin-diseases. However, when checking the website, you are supposed to join a waitlist, but then it 404s. Is the tool already shutdowned or it was never released ? reply nsoldiac 6 hours agoparentHi, my partner worked in technology strategy for a large healthcare system where she saw many derm AI-based applications evaluated over the last decade. (Dis)Incentives aside, they all followed a similar\\ story arc overpromising in their research findings and underdelivering in actual care delivery. They've been around for longer than you'd imagine. I hope they reach their potential, but suggest approaching them with healthy skepticism. reply dr_kiszonka 8 hours agoparentprevIf you are in the US, \"DermAssist is not available in the United States\"(https://health.google/consumers/dermassist/) reply realprimoh 9 hours agoparentprevIt's not shutdowned I'm pretty sure. Looks like a bug. Hopefully it will be fixed soon. reply rvnx 9 hours agorootparentLet's hope so, because it seems really useful. In the meantime, I found an open-source equivalent https://modelderm.com/en.html but if we could see the certified + tested tool it'd be nice :) reply karmasimida 4 hours agoprevNot to shade on anyone, but even Jeff Dean isn't good at predicting the future. The pathways model/architecture aren't exactly where things are moving towards, LLM is. https://blog.google/technology/ai/introducing-pathways-next-... reply knowriju 4 hours agoparentCare to share some topics where the things are moving towards? I understand diffusion, GaNs and Mamba is in vogue these days, but those are different logical architecture. I am unsure where the next level ML physical architecture research is moving towards. reply karmasimida 2 hours agorootparentI think at this rate, everything is moving towards Transformer based models(text/audio/image/video), as Sora has shown, there isn't really anything Transformer can't do, it can generate both real life quality photo and video. Its ability to fit ANY given distribution is beyond compare, the most powerful neural network we have ever designed, nothing else is even close. GANs are on the contrary, not hot any more in industry, diffusion models have achieved high fidelity in image generation, hard to see how GAN can make a comeback. It is faster, but it image generation in terms of quality is done, the wow factor is no more. This might be a hot take, but I think architectural changes is going to die down in industry, Transformer is the new MOS transistor. As billions of dollars pumping into making it runs faster AND cheaper, other alternative architecture is going to have a hard time compete. reply npalli 8 hours agoprevSeeing a lot of people talk about auto-summary tools which reminds me of a joke. ‚ÄúI took a speed-reading course and read War and Peace in twenty minutes. It involves Russia.‚Äù ‚Äï Woody Allen reply Mistletoe 6 hours agoparentThis is such a great comment about so many things right now. Thank you. reply denfromufa 6 hours agoprevThe highlight of this event was running with Jeff at Rice University before his talk: https://x.com/JeffDean/status/1756319820482592838?s=20 reply opisthenar84 5 hours agoprevI'm surprised he didn't mention much about vector search reply H8crilA 6 minutes agoparentIt is a finished thing, no? So many systems/products have it built in for quite a few years now. reply idkdotcom 8 hours agoprevIt's a nice high level overview about the state of the art in machine learning. If you have watched his other talks, Jeff Dean generally does a very good job explaining things from a high level point of view. Here is a controversial view: I think that the current neural networks driven approach coupled with massively distributed computing has plateaued. For the machine learning field to move forward, it will need a different, less data/compute hungry paradigm. reply nomilk 10 hours agoprevI see from comments I'm far from the only one using AI to summarise videos before deciding whether to watch them. Reminds of the meme \"why spend 10 minutes doing something when you can spend a week automating it\". i.e. \"why spend an hour watching a talk when you can spend 5 hours summarising it with AI and debating the summary's accuracy\". This sounds silly but potential gains from learning AI summarisation tooling/flows are large, hence why it warrants discussion. Learning how to summarise effectively might save hours per week and improve decisions about which sources deserve our limited time/attention. reply bmitc 9 hours agoparentI feel like I'm missing some boat, but I'm not sure what boat it is. These \"AI\" systems seem very superficial to me, and they give me the same feeling as VR does. When I see VR be some terrible approximation of reality, it just makes me feel like I'm wasting my time in it, when I could go experience the real thing. Same with AI \"augmentation\" tooling. Why don't I just read a book instead of getting some unpredictable (or predictably unpredictable) synposis? It's not like there's too much specific information there. These tools are just exploding the amount of unspecific information. Who has ever said: \"hey, I have too much information for building this system or learning this topic\"? Basically no one. It's just going to move everything to the middle of the Bell curve, leaving the wings to die in obscurity. reply saalweachter 8 hours agorootparentI was thinking the other day: Star Trek computers make a lot of sense if they are working with our current level of AI. You can talk to it, it can give you back answers that are mostly correct to many questions, but you don't really trust it. You have real people pilot the ship, aim and fire weapons, and anything else important. reply cpeterso 7 hours agorootparentAnd nobody in Star Trek thinks the ship computer is sentient. On the other hand, the holodeck sometimes malfunctions and some holodeck character (like Moriarty) becomes sentient running on just a subset of the ship computer. That suggests sentience (in the Star Trek universe) is a property of the software architecture, not hardware. reply vinay_ys 6 hours agorootparentFirstly, they had unlimited energy and replicators - which means they could make whatever hardware they wanted. And they also had bio-neural circuits. And photonic chips. So, hardware was already way ahead of software. All this goes to show that in real world, the actual science (and fiction) around material sciences was already quite advanced compared to software. reply nomilk 9 hours agorootparentprevIf you know a book‚Äôs worth reading, going ahead and reading it works well. But for a lot of books/talks there‚Äôs competition for time - eg my bookshelf has 20 half read books (this is after triaging out the ones that aren‚Äôt worthy of my time) - any tooling that can help better determine where to invest tens or hundreds or hours of my time is a win. Regarding accuracy, I think we‚Äôre at a tipping point where ease of use and accuracy is starting to make it worth the effort. For example Bard seems to know about youtube videos (just a couple of months ago you‚Äôd have to download it -> audio to text -> feed into a LLM). So the combination of greater accuracy and much easier to use make it worth considering. reply bmitc 8 hours agorootparent> If you know a book‚Äôs worth reading, going ahead and reading it works well. But for a lot of books/talks there‚Äôs competition for time - eg my bookshelf has 20 half read books (this is after triaging out the ones that aren‚Äôt worthy of my time) - any tooling that can help better determine where to invest tens or hundreds or hours of my time is a win. Is it that hard to determine that a book is worth reading where worth is measured from your perspective? It's usually pretty easy, at least for technical books. Fiction books are another story, but that's life. Having some unknown stochastic system giving me a decision based upon some unknown statistical data is not something I'm particularly interested in. I'm interested in my stochastic system and decision making. Trying to automate life away is a fool's errand. reply nomilk 7 hours agorootparent> Is it that hard to determine that a book is worth reading I'm a huge believer in doing plenty of research about what to read. The simple rationale: it takes a tiny amount of time to learn about a book relative to the time it takes to read it. Even when I get a sense a book is bad, I still tend to spend at least a couple of hours before making the tough call not to bother reading further (I handled one literally 5 minutes ago that wasted a good few hours of my life). I'm not saying AI summaries solve this problem entirely, but they're just one additional avenue for consultation that might only take a minute or two and potentially save hours. It might improve my hit rate from - I dunno - 70% to 80%. Same idea for videos/articles/other media. reply Ludleth19 7 hours agorootparentI get where you're coming from and definitely vet books in similar ways depending on the subject, but I also feel like this process is pretty limited in ways too and appeals to some sort of objective third party that just doesn't exist. If you really want to know or have an opinion on a work/theory/book at the end of the day you have to engage with it yourself on some level. In graduate school for example, it was pretty painfully obvious that most people didn't actually read a book and come to their own conclusions, but rather read summaries from people they already agreed with and worked backwards from there, especially on more theoretical matters. I feel like on the long term this just leads to a person superficially knowing a lot about a wide variety of topics, but never truly going deep and gaining real understanding on any of them- it's less \"knowing\" and more the feeling of knowing. Again, not saying this in an accusatory way because I totally do engage in this behavior too, I think everyone does to some degree, but I just feel the older I get, the less valuable this sort of information is. It's great for broad context and certain situations I suppose, but in a lot of areas I consider myself an expert, I would probably strongly disagree with summaries given on subjects and they also tend to miss finer details or qualifying points that are addressed with proper context. reply majormajor 7 hours agorootparentprevI think the more you outsource \"what is worth my time\" the less you're actually getting an answer about what's worth YOUR time. The more you rule out the possibility of surprise up front, the less well-informed your assumption about worth can possibly be. There are FAR too many dimensions like word choice, sentence style, allusion, etc, that resist effective summarization. reply ceruleanseas 9 hours agorootparentprevLLM accuracy is so bad, especially in summarization, that I now have to fact check google search results because they‚Äôve been repeatedly wrong about things like the hours restaurants are open. reply tracerbulletx 9 hours agorootparentThere's a huge difference between summarizing a stable document that was part of the training data or the prompt, and knowing ephemeral facts like restaurant hours. reply nvader 7 hours agorootparentTechnically true statement. If you're offering it to imply that the GP bears responsibility for knowing what document was in the training data and what's not, I have to quibble with you. Knowing it's shortcomings should be the responsibility of the search app that is currently designed to give screen real estate to the wrong summary of the ephemeral fact. Or, users will start to lose trust. reply seanbethard 5 hours agorootparentprevIt's because they don't understand language. You may have been mislead by their ability to generate language. reply JKCalhoun 7 hours agorootparentprevI had a conversation with a friend where he suggested that he had had a broad range of experiences just from gaming. I think the context was a conversation about how experiences in life can expand you ‚Äî something like that. The whole premise bothered me though. I can remember a bike ride where I was experiencing the onset of heat stroke and had to make quick decisions to perhaps save my life. I remembered decades ago lost in Michigan's upper peninsula with the wife, on apparently some logging road and the truck getting into deeper and deeper snow as we proceeded until I made the decision to instead turn around and go back the way we came lest we become stranded in the middle of nowhere. I remember having to use my wits, make difficult decisions while hitchhiking from Anchorage, Alaska to the lower 48 when I was in my early twenties.... The actual world, the chance of actual death, strangers, serendipity ... no amount of VR or AI really compares. reply smoldesu 6 hours agorootparentYou're not wrong, but I also think the problem predates video games. Films, novels and even religious texts all are scrutinized for changing people's perspective on life. Fiction has a longstanding hold on society, but it inherently coexists with the \"harsh reality\" of survival and resource competition. Introducing video games into the equation is like re-hashing the centuries old Alice in Wonderland debate. Playing video games all day isn't an enriching or well-rounded use of time, but neither is throwing yourself into danger and risk all the time. The real world is a game of carefully-considered strategy, where our response to hypothetical situations informs our performance during real ones. Careful reflection on fiction can be a philosophically powerful tool, for good or bad. reply hn_throwaway_99 6 hours agorootparentprev> Why don't I just read a book instead of getting some unpredictable (or predictably unpredictable) synposis? It's not like there's too much specific information there. I'm trying to understand this comment, because I couldn't disagree more. It is the absolute explosion of available data sources that has me wanting to be much more judicious with where I spend my time reading/watching in the first place. Your comment was interesting to me because I feel like I agree with one of its main sentiments: that AI generated content all kinda \"sounds the same\" and gives a superficial-feeling analysis. But that is why I think AI is a fantastic tool for summarizing existing information sources so I can see if I want to spend anymore time digging in to begin with. reply makerdiety 6 hours agorootparentRelying on glorified matrices (that's what machine learning is) for world data curation is just begging to handicap yourself into a cyborg dependent on a mainframe computer's implementation. An implementation and design that is rarely scrutinized for safety and alignment features. Why not just make your brain smarter, instead of trying to cram foreign silicon components into your skull? reply kendalf89 5 hours agorootparentWhy not both? reply makerdiety 4 hours agorootparentBecause maximizing both biological vectors of self-improvement and computing based avenues of skill acquisition is limited by the fact that it's a multi-objective optimization problem when you combine them together during maximization. Optimizing one de-optimizes the other. They, biology and computers, conflict with each other in fact. So, at best, you have to reach for a Pareto frontier. And, it turns out, technology can't be trusted, as there is always some sort of black box associated with its employment. Formally, there is always a comprehension involved when it comes to the development and integration of technology into human life. You can't really trust this stubborn built-in feature of technological and economic success if you don't pierce through its secrets (knowledge is the power to counteract cryptographic objects). After all, it could be a malicious trojan horse that \"basic common sense\" insists on us all using for \"bettering\" our daily lives. A very unfriendly artificial intelligence is trying to sneak through civilization for its own desires. And you're letting it just pass on by, as a result of your compliance with the dominant narrative and philosophy of capitalist economics. reply mrbonner 8 hours agorootparentprevI just read ‚ÄúRobust Python‚Äù book. My overall reaction is that book could have been written with half the length and still be valuable for me. I can't stop thinking if I could ask LLM to summarize each chapter for me, I still could \"read\" the whole book in the manner the author outlinea but save a tons of time. reply tmaly 6 hours agorootparentprev100 percent on things moving to the center of the curve. For now, that‚Äôs not a bad thing if you need to know what the average information is. As time goes by it might not be a good thing. reply 63 10 hours agoparentprev> and debating the summary's accuracy Perhaps consider simply reading the description for an accurate summary. From the description: > Abstract: In this talk I‚Äôll highlight several exciting trends in the field of AI and machine learning. Through a combination of improved algorithms and major efficiency improvements in ML-specialized hardware, we are now able to build much more capable, general purpose machine learning systems than ever before. As one example of this, I‚Äôll give an overview of the Gemini family of multimodal models and their capabilities. These new models and approaches have dramatic implications for applying ML to many problems in the world, and I‚Äôll highlight some of these applications in science, engineering, and health. This talk will present work done by many people at Google. reply nomilk 10 hours agorootparentIn this case the video description contains a useful Abstract. AI summaries can offer additional value though, going into more/less detail (as desired), and allowing you ask follow up questions to drill into anything potentially of interest. reply swyx 9 hours agorootparentprevsure its an accurate summary, but is it at a granularity or specificity that you want? LLM summaries lets you move around the latent space of summaries and you probably dont agree with the one chosen for youtube descriprtions. reply munificent 5 hours agoparentprev1. Author spends a week producing a video when writing an article would have taken a day. 2. Viewer spends hours summarizing the video to an article so they don't have to watch it. P R O G R E S S reply sammyatman 8 hours agoparentprevThis is exactly why I built https://www.askyoutube.ai. It helps you figure out if a video has the answer you want before you spend time watching it. It does this by aggregating information from multiple videos in one-go. I don't think it completely replaces watching videos in some cases but it definitely helps you skip the fluff and even guides you to the right point in the video. reply esafak 4 hours agorootparentDo you transcribe the videos or use the captions, because GPT4 can already do the latter? reply sammyatman 2 hours agorootparentIt can be either depending on the mode, I don't think GPT4 can already do the latter though. reply ren_engineer 9 hours agoparentprevI've A/B tested this with webinars and the tools I've tried tend to miss some really valuable/interesting stuff even when I give it the full transcript. Same goes for when I try to use ChatGPT or other tools for full interactive analysis, even when I basically hand it what I'm looking for as if I hadn't watched the video it will leave out the critical information reply kirill5pol 3 hours agoparentprevI made a tool that might be interesting for people here! https://www.platoedu.org/videos/oSCRZkSQ1CE/watch It's not really giving summaries but gives topic/section timestamps and highlights what was discussed (for example: The Transformer Model (21:06 - 24:48) - Introduction of the Transformer model as a more efficient alternative to recurrent models for language processing) The main focus is actually creating Anki-like spaced repetition questions/flashcards for videos and lectures you watch to retain knowledge, but I found the section information quite helpful for finding which parts of the video contain the info relating to topics/concepts reply tomrod 9 hours agoparentprevWhat is your workflow for this, if you don't mind me asking? reply jerpint 8 hours agorootparentIf you‚Äôre interested I did a YouTube video and short blog post about it https://www.jerpint.io/blog/yougptube/ https://www.youtube.com/watch?v=WtMrp2hp94E reply kendalf89 5 hours agorootparentThis is pretty cool. Would it be possible to just stream the audio directly into Whisper, maybe using something like vlc, at x2 play speed to get the summary faster? reply nomilk 8 hours agorootparentprevMy approach wasn't fancy, just asked bard (aka gemini). I was drawn to bard/gemini for this since the source video is on youtube, so figured google would better support its related service (although that was an arbitrary hunch) https://imgur.com/a/psb64IP reply sammyatman 8 hours agorootparentprevTry out www.askyoutube.ai! reply castles 6 hours agoparentprev> Learning how to summarise effectively might save hours per week and improve decisions about which sources deserve our limited time/attention. If you like summaries, you'll probably love de-summaries (WIP): https://socontextual.com/ reply theGnuMe 1 hour agoparentprevWhat ai tool do you use to summarize? reply name_nick_sex_m 8 hours agoparentprevWhat tool do you use to summarize video? reply nomilk 8 hours agorootparent(since it's a youtube video) I used bard/gemini: https://imgur.com/a/psb64IP I have no idea if it's the best (or even a good) tool. Other commenters suggest some other tools (for both text summaries and condensed video summaries - a sort of 'highlights reel'): https://news.ycombinator.com/item?id=39435930 https://news.ycombinator.com/item?id=39435964 reply kirill5pol 2 hours agorootparentprev(Little self-plug) I made a tool that‚Äôs pretty relevant https://www.platoedu.org/videos/oSCRZkSQ1CE/watch It's not really giving summaries but gives topic/section timestamps and highlights what was discussed. (Main focus is actually making mini-courses off of YouTube videos but I found the section summaries really useful for figuring out which parts to watch) reply zyklonix 7 hours agoprevSummary: https://www.videogist.co/videos/jeff-dean-google-exciting-tr... reply nliang86 7 hours agoparentThanks for the post - I put together the tool above. I tried to strike a balance between being concise but also capturing all the important details. For that reason, the tool is hit or miss on longer (> 45 min) videos - the summary on this video is good but I've seen it omit important details on other long videos. The tool also captures relevant screenshots for each section. Hopefully it's helpful. You can summarize additional videos by submitting a youtube URL in the nav bar or the home page. Also, feedback welcome! reply crudalex 7 hours agorootparentAre you using LLM to summarize? If yes can you share the prompts used? reply swyx 10 hours agoprevnext [4 more] [flagged] swyx 10 hours agoparent(to give credit, this started from https://summarize.tech/ and then i hand edited it down and deleted generic commentary while i read the summary. submitted here for ppl who also prefer to read rather than watch a 1hr video) actual human opinion: i think nothing strictly new was disclosed here, but its always nice to have in 1 hour a high level overview of \"how jeff dean looks at the world\" which aligns you accordingly with at least the public facing understanding of what google wants us to understand about their work. Gemini 1.5 has made a huge splash by being genuinely better than GPT4 in some respects (see the recent reddit HVM post) but doesnt seem to have been covered in any detail here. was also not aware of Bar, which seems to be \"Google LMSys\". reply nomilk 10 hours agorootparentCan you link to the reddit post (I googled it and got something completely different) EDIT: think I found it? https://www.reddit.com/r/singularity/comments/1atjz9v/ive_pu... reply swyx 10 hours agorootparentyes thats it reply ldjkfkdsjnv 10 hours agoprevnext [9 more] [flagged] t8sr 10 hours agoparentThis is one of the most accomplished people in software development talking to a class of CS students. It‚Äôs not entertainment. Have we reached a point where lecturers need to post ‚Äúhot takes‚Äù on Twitter as a prerequisite to keeping people‚Äôs attention? reply hiddencost 10 hours agorootparentAlso one of the people who built the modern AI world. From the data centers to the bulk processing software to the team structures to tensorflow, to some of the most cost effective chips in the field, to many of the early blockbuster results in the field. They certainly fumbled by not investing in massive LLM scaling early enough, but Jeff Dean has been planning this day since his graduate work on neural networks in the 90s. reply ein0p 10 hours agorootparentprevNonetheless, I can empathize with GP, I wish the talk focused on the future more, and on history and marketing of Google a lot less. Yes, we get it, Google used to lead in this space, still does in some narrow niches, but recounting the glory days is not how you win mindshare. Show me the demos, make me excited about your vision. reply willsmith72 9 hours agorootparenti would've been impressed by this 2 years ago. i think it's got to the point where real, valuable ai is in the hands of the everyday consumer, so we start judging the models for ourselves. having seen google continually get crushed over the past year, a bunch of benchmarks just fail to impress. in particular in this case, they're comparing their latest model to gpt4, which hasn't changed that much in almost a year. reply ein0p 8 hours agorootparentNot only that, in some cases they‚Äôre comparing apples to oranges as well, undermining their credibility further. Eg chain-of-thought vs non-CoT results. I don‚Äôt even know why they‚Äôre doing that, seems like their results would be impressive enough even without this. reply alienchow 9 hours agoparentprevYou have absolutely no idea who Jeff Dean is, do you? reply esafak 4 hours agorootparentJeff heyday, in terms of media attention, was twenty years ago; you shouldn't blame people for not recognizing him. reply Mistletoe 5 hours agorootparentprevIs he Jimmy's brother? reply gandalfgeek 10 hours agoprevnext [5 more] [flagged] carbocation 10 hours agoparentFrom clicking through the video and watching this summary, I didn't feel like this summary was very good. (No malice intended, just feedback.) reply gandalfgeek 8 hours agorootparentAppreciate the feedback. This was a weekend project. I wanted to explore summarization without paraphrasing, so that the output was a cut up version of the input video. Agreed that it terms of conceptual clarity often a textual summary that is synthesized comes out ahead. reply wanderingmind 10 hours agorootparentprevCan you provide specific details of why it was not good and what critical details did it miss? reply yeknoda 10 hours agorootparentprevDidn't watch it. Also hate it. reply adamnemecek 10 hours agoprevnext [14 more] All machine learning is convolution. reply Legend2440 10 hours agoparentOnly certain types of neural networks use convolutions. It's not universal. reply CamperBob2 9 hours agorootparentWhat are some examples of types that don't? reply dontwearitout 9 hours agorootparentThe transformer module is currently dominating ML, and is widely used in text, vision, audio, and video models. It was introduced in 2017 and shows no real signs of being displaced. It has no convolutions. https://en.wikipedia.org/wiki/Transformer_(deep_learning_arc... http://jalammar.github.io/illustrated-transformer/ reply CamperBob2 6 hours agorootparentIf they use dot products on at least one layer with fully-connected inputs, which they do, along with everything else derived from the basic MLP model, then they're technically performing convolution. Of course, the convolution concept breaks down when nonlinear activation functions are introduced, so I'm not sure the equivalence is really all that profound. reply adamnemecek 9 hours agorootparentprev‚Ä¶ or does it https://grlearning.github.io/papers/11.pdf reply hackerlight 9 hours agorootparentprevI think anything that doesn't have an explicit convolution layer? Transformer, MLP, RNN don't automatically have a convolution layer, although for many tasks you can add it in if you want. reply Legend2440 9 hours agorootparentprevTransformers, MLPs. reply adamnemecek 9 hours agorootparentprevNo, ALL machine learning architectures are convolutions https://grlearning.github.io/papers/11.pdf reply Legend2440 8 hours agorootparentReading the paper, they're saying convolutions are powerful enough to express any possible architecture. But that's just computational universality - this does not mean they are convolutions. reply atq2119 4 hours agorootparentEvery matrix multiply can be viewed as a 1x1 convolution, just like every convolution can be viewed as an (implicit, for larger than 1x1 kernels) matrix multiply. I'm not sure this is particularly enlightening, but it's probably one small step of understanding that is required to truly \"get\" the underlying math. Edit: Should have said that every matrix multiply against weights is a convolution. The matrix multiply in the standard attention block (query times keys) really doesn't make sense to be seen as a convolution. reply dartos 10 hours agoparentprevAll computers do is add. reply tambourine_man 8 hours agorootparentAnd conditional jumps reply c0pium 7 hours agoparentprevRandom forest would like a word. reply seanbethard 5 hours agoprev [‚Äì] There's a typo in the title of the talk but don't worry I fixed it: Trends in Computer Vision ``In recent years, ML has completely changed our view of what is possible with computers'' In recent years ML has completely changed our view of what's possible with computer vision. ``Increasing scale delivers better results'' This is true for computer vision. ``The kinds of computations we want to run and the hardware on which we run them is changing dramatically'' Optimizations on operations for computer vision isn't exactly dramatic change. Who is We? Trends in Machine Learning, 2010: semantic search for advertising. Trends in Machine Learning, 2024: semantic search for advertising, short-form video content. https://blog.seanbethard.net/five-epistemes/ reply seanbethard 3 hours agoparent [‚Äì] lol. Three computer vision researchers dislike this comment. Do any of you want to respond to it? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Exciting trends in machine learning are discussed, focusing on advancements in neural networks, TPUs, and Transformer models that enhance image and speech recognition tasks.",
      "Noteworthy models such as GPT-3 and BERT for natural language processing, alongside the Gemini project for multimodal tasks, are highlighted in the discussion.",
      "Emphasis is placed on the significance of data quality, ethical deployment, and responsible use of machine learning technologies, stressing the potential for further advancements in Large Language Models (LLMs) and exploration of new ideas in machine learning research."
    ],
    "commentSummary": [
      "The discussion highlights the evolving landscape of AI, including Google's achievements, debates on breakthrough origins, and the effectiveness of AI and ML in healthcare.",
      "Topics cover the limitations of current neural networks, personal research significance, conflicts between biological and technological self-improvement, and tools for video summarization.",
      "Emphasis is placed on the value of AI summaries for time-saving and decision-making enhancement, as well as debates on machine learning architectures and computational universality."
    ],
    "points": 306,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1708379808
  },
  {
    "id": 39428609,
    "title": "Integrating PRQL Functionality in PostgreSQL",
    "originLink": "https://github.com/kaspermarstal/plprql",
    "originBody": "This extension let&#x27;s you write PRQL functions in PostgreSQL.When I first saw PRQL on Hacker News a few months ago, I was immediately captivated by the idea, yet equally disappointed that there was no integration for PostgreSQL. Having previous experience with writing PostgreSQL extensions in C, I thought this was a great opportunity to try out the pgrx framework and decided to integrate PRQL with PostgreSQL myself.The maintainers of both PRQL and pgrx were very nice to work with. Thanks guys.",
    "commentLink": "https://news.ycombinator.com/item?id=39428609",
    "commentBody": "PRQL in PostgreSQL (github.com/kaspermarstal)254 points by kaspermarstal 22 hours agohidepastfavorite136 comments This extension let's you write PRQL functions in PostgreSQL. When I first saw PRQL on Hacker News a few months ago, I was immediately captivated by the idea, yet equally disappointed that there was no integration for PostgreSQL. Having previous experience with writing PostgreSQL extensions in C, I thought this was a great opportunity to try out the pgrx framework and decided to integrate PRQL with PostgreSQL myself. The maintainers of both PRQL and pgrx were very nice to work with. Thanks guys. fforflo 19 hours agoNice work. A few months back, I experimented with having a DSL like PRQL in Postgres, but back then, I found the language a bit cumbersome; however, it was great as an idea. IMHO, the best \"data transformation\" language is jq and awk is second. PRQL and EdgeQL (EdgeDB) are the most interesting ones to watch how they evolve, though. I've also written a PG extension to make jq available in Postgres [0] I believe Postgres, in general, will flourish as a host for DSL languages [1]. 0: https://github.com/Florents-Tselai/pgJQ 1: https://tselai.com/pgjq-dsl-database.html reply canadiantim 18 hours agoparentWould love to see EdgeQL become adopted beyond EdgeDB. I don't like the vendor lock-in with EdgeDB, but I think they're doing great work reply fforflo 17 hours agorootparentYes, they are. Beyond the core database offering, I'd also like to underline the quality of their software engineering work. There are not many Python-powered databases out there; their codebase has some real gems from the setup.py to their core compiler and Postgres-based storage layer. reply ttfkam 12 hours agorootparentprev\"A jaw-dropping amount of effort has been spent attempting to bridge the gap between the relational paradigm of SQL and the object-oriented nature of modern programming languages. EdgeDB sidesteps this problem by modeling data in an object-relational way.\" All the best to the team. I however truly hope this isn't the direction the industry moves toward. I thought we learned our lesson from MongoDB. I still believe data is best modeled in sets, not objects. The solution isn't for databases to become more like object stores but for general purpose programming languages to be more amenable to seamless access of set-oriented data. More stuff like this: https://github.com/porsager/postgres https://github.com/launchbadge/sqlx reply RedCrowbar 10 hours agorootparentEdgeDB is NOT an object store. It is \"relational model enhanced\" instead where a set is a fundamental building block [1] so not just relations are sets, but attributes can be sets also. [1] https://www.edgedb.com/docs/edgeql/sets#ref-eql-everything-i... reply 1st1 17 hours agorootparentprev> Would love to see EdgeQL become adopted beyond EdgeDB We'll soon be announcing some interesting developments on that front, stay tuned :) reply dvdkon 19 hours agoprevDamn, now my bachelor's thesis will be less unique :) I'm working on a new language that compiles directly to Postgres' post-analysis structs. It's working out pretty well so far, but my chosen \"universal set\" (aggregation/array/subquery/... as one thing) semantics are sometimes a pain to encode. reply dangoldin 18 hours agoparentNot to change your direction but something I've been toying around is being able to support Algebraic types when defining tables. That way you can offload a lot of the error checking to the database engine's type system and keep application code simpler. reply dvdkon 16 hours agorootparentI'd like to do something like that too, if/when I ever get to replacing the DDL. In Postgres you could create custom types for tagged unions, but it might be better to translate table-level unions to a set of constraints, for performance and flexibility (you can't create referential integrity constraints using expressions IIRC). reply andyferris 11 hours agorootparentprevSounds wonderful. I actually think this is the highest value thing anyone could contribute to Postgres (assuming it could handle foreign key constraints inside the sum types). reply buremba 17 hours agoparentprevSounds interesting! What's the benefit of compiling directly to Postgres's internal structs over compiling to SQL? reply dvdkon 16 hours agorootparentThere's little direct benefit, since the internal structs pretty closely model SQL. But having the language compiler a part of the Postgres process does help. It gives you easy access to the database's structure, so you know the type of every identifier, what columns tables have, what functions are available, etc. You can then do your own (better) error reporting and, more importantly, move away from SQL's semantics. For example, I want to have universal broadcasting of operators on subquery results, array values, and aggregated columns. To do this, I need to know which of these the operand expressions represent, which is slow or impossible with transpilation. reply andy_ppp 21 hours agoprevVery interesting, it looks a lot like the Elixir package Ecto that has a DSL for writing SQL queries. Obviously there are some differences here and I wonder if the compiler can do further optimisations than Ecto can but interesting to see they align quite a bit. reply dang 17 hours agoprevRelated: PRQL as a DuckDB Extension - https://news.ycombinator.com/item?id=39130736 - Jan 2024 (47 comments) PRQL: Pipelined Relational Query Language - https://news.ycombinator.com/item?id=36866861 - July 2023 (209 comments) Calculate the Digits of Pi with DuckDB and PRQL - https://news.ycombinator.com/item?id=35153824 - March 2023 (1 comment) One Year of PRQL - a modern language for relational data - https://news.ycombinator.com/item?id=34690560 - Feb 2023 (1 comment) PRQL: a simple, powerful, pipelined SQL replacement - https://news.ycombinator.com/item?id=34181319 - Dec 2022 (215 comments) PRQL 0.2 ‚Äì a better SQL - https://news.ycombinator.com/item?id=31897430 - June 2022 (159 comments) PRQL ‚Äì A proposal for a better SQL - https://news.ycombinator.com/item?id=30060784 - Jan 2022 (292 comments) reply landingunless 15 hours agoprevNice to see extensions like this one developed in Rust/pgrx. Reminds me of https://github.com/tcdi/plrust reply kaspermarstal 13 hours agoparentYes, the PL/Rust code base was a very useful when developing this extension reply brikym 15 hours agoprevIt looks a lot like Microsoft‚Äôs Kusto query language which is a pleasure to use. Piping is better than nesting and from-first is the way to go as it‚Äôs necessary for autocomplete. reply billfruit 20 hours agoprevJust out of curiosity, did PRQL evolve from some theoretical innovation or did arise bottom up from practical utilities? Is it a new tool with great new powers or is it just syntactic sugar? reply xmonkee 19 hours agoparentit compiles to SQL so no extra powers. But it does make some common patterns more succinct. They have good documentation on the website. Personally, I was very excited about using it to write some complex queries in my application that does some fancy backtesting with sliding windows etc, but I reverted back to SQL pretty quickly because I found myself first thinking in SQL and translating back to PRQL :/ reply ttfkam 12 hours agoparentprevYep, just syntactic sugar. reply philipodonnell 20 hours agoprevI often wonder if NL-SQL tasks would benefit from an intermediate query language that is more compatible with the next-logical token approach that is used to generate the code. Obviously there is less of this in the training set, but if it transpires in a testable way, you could generate training data yourself from known good sql queries? Are there any languages that have been designed specifically for this? reply mikpanko 17 hours agoprevThere are very interesting improvements to SQL, which are much more ergonomic, extend functionality, and provide higher-level abstractions. Also backward compatible. PRQL and Malloy immediately come to mind but there are more. Anybody has good explanations why they struggle to get wide adoption? reply chobanilover 15 hours agoparentWhen it comes to data stack tooling, organizations aren't always optimizing for a better way to do things as much as minimizing the worst possible scenario. New syntax is nice, but it means that analysts and engineers need to learn something new and are more likely to make mistakes that could bubble up to production. There's always an argument to be made why shiny new tool XYZ is better, but unless it's 100X better, organizations are reluctant to switch from something like vanilla PostgreSQL that they know works 100% of the time. reply chaps 15 hours agorootparentNormally I would agree, except for the fact that this system works by converting PRQL to SQL. So it's not 100% throwing out the baby with the bath water, since there there are still means for newer engineers to learn SQL through this tooling! reply dataspun 13 hours agoprevWhy is this PRQL extension for Postgres limited to Mac and Linux? What dependencies on Windows are the obstacles, and is there an expected solution in the near-term? reply kaspermarstal 13 hours agoparentThis extension has been developed on top of pgrx and depends on the platforms that pgrx supports. From the pgrx readme: > Windows is not supported. It could be, but will require a bit of work with cargo-pgrx and figuring out how to compile pgrx's \"cshim\" static library. reply fkyoureadthedoc 18 hours agoprevPRQL looks interesting enough to at least give it an honest try. > PRQL allows for powerful autocomplete, type-checking, and helpful error messages (in progress) Without some kind of autocomplete though I'm a lot less motivated to do so. reply grepknfss 18 hours agoprevYou should make the first instance of ‚ÄúPQRL‚Äù in your readme a link to that project. reply kaspermarstal 17 hours agoparentGood suggestion, thanks reply mt_ 12 hours agoprevIs PRQL faster than the standard query model? reply Sammi 11 hours agoparentPRQL compiles to SQL. Why would it be faster? The intention is rather for it to be simpler, as it uses a linear direction of data handling. SQL jumps back and forth with its order of operations and can be confusing in this way. PRQL also has a more modern syntax that reuses more universal concepts with fewer keywords to learn. In contrast to SQL which has a unique keyword, syntax, and behavior for everything. reply zokier 10 hours agorootparentI think more relevant question would be is naive idiomatic prql faster than naive idiomatic sql? Of course you can tune any sql to hell and back, but the chances for some non-expert developer to land on anything nearly optimal sql are not so great. So if prql helps non-experts to get decent perf easier, I'd chalk that up as a win, and that is not so outlandish goal anymore. reply smabie 16 hours agoprevCan this be used inside Grafana? reply zoky 20 hours agoprevCan someone explain to me why ‚ÄúShow HN‚Äù and ‚ÄúAsk HN‚Äù posts are always, always in hard-to-read light gray? Is there some cabal that automatically downvotes such posts? Is there something intrinsic to The Algorithm that penalizes such posts? Or do I just have really bad luck and always manage to click through on posts that have gained traction despite having gotten enough downvotes to put them at risk of sudden termination? This is one of the great HN mysteries to me, and if anyone can shed some light on it, it would be much appreciated. reply murermader 20 hours agoparentI think I read here on HN some time ago that it is intentionally hard to read to discourage posts with text. I think the reasoning was that posting links to external blogs / websites is usually higher quality than someone creating a quick post on HN. Could be wrong though, just writing this from memory. reply throwaway143829 20 hours agorootparentHN has some really non-obvious UX, between this and the mysterious green usernames I still don't understand. And the fact that only some users can downvote. Or the weird logic behind which words cannot appear in titles. reply quesera 20 hours agorootparentGreen usernames are new accounts. Only users with 500 karma can downvote. Both are mechanisms to dull the potency of new users until they have a chance to learn how HN is expected to work. This might sound gatekeeping, and it literally is, but consider than HN signup is open and takes 15 seconds with no verification. HN likes the way HN works and these provide simple rate-limits on destructive or oblivious change. reply Izkata 14 hours agorootparent> Green usernames are new accounts. It made sense to me almost immediately, so since people aren't making the connection: https://www.oxfordlearnersdictionaries.com/us/definition/eng... >5 (informal) (of a person) young and without experience > The new trainees are still very green. reply Terretta 18 hours agorootparentprev> this might sound gatekeeping, and it literally is That term presumes unkept gates are preferable. I grew up farming. One keeps one's gates or both wildlife and livestock run amuck. reply saganus 20 hours agorootparentprevGreen usernames are for new users. reply PartiallyTyped 19 hours agorootparentprevA comment has a green username if at the time of commenting the author was new. The colour remains. If you look at your first comments, you will see them green as well. YCombinator founders have their own colours which are only visible to each other. There are plenty of undocumented features like this one. reply quesera 20 hours agoparentprevThey are not penalized, the grey text applies to all text posts. It's confusing, but greying out is used on HN for a single purpose: to discourage reading (and therefore writing). Low-quality comments are greyed by downvotes from other users and moderators. Text posts (including Ask and Show HN) are greyed automatically. Ask/Show posters are encouraged to post a comment on their own story, and to let that comment rise or fall according to its up/down votes. Meta-meta: Your comment might be downvoted for being meta to the post. It looks like it has already been \"detached\" from the comment tree so that it appears at the bottom instead of responding to up/downvotes. This is actually protective of your karma and this conversation. Your question is valid, but it's not germane to the post. If your comment was allowed to float to its normal location, it would be downvoted by others who considered it off-topic. reply pxeger1 20 hours agoparentprevThey aren‚Äôt downvoted; it‚Äôs just a styling thing. I believe dang gave some reasoning for deliberately making the post text less prominent, but I can‚Äôt remember it. reply lkuty 21 hours agoprev [‚Äì] I don't understand the need for libraries that abstract away SQL when you could just write SQL directly and have full access to the power of the language which is quite rich (recursive CTE, windowing, ... aka Modern SQL). You could also use stored procedures/functions for more complex stuff and e.g. JSON (or native types) to transfer data between the database and the application. Why limit ourselves with a sub-optimal language be it PRQL, Ecto, other ORMs, ... ? reply snthpy 21 hours agoparentThis comes up every time PRQL makes it onto HackerNews and is a fair question: Short answer: DX Slightly longer answer: Developer productivity and experience, especially for EDA and interactively writing complex analytical queries. Most people that have tried PRQL just find it more convenient to write their analytical queries in it. PRQL compiles to SQL so it can't express anything you can't already do in SQL, but you can probably express yourself much faster in PRQL. Just try the following query in the online PRQL Playground (https://prql-lang.org/playground/) to find the longest track per album: ```prql from tracks group album_id ( sort {-milliseconds} take 1 ) ``` How long would it take you to write the SQL for that? Disclaimer: I'm a PRQL contributor. reply quesera 19 hours agorootparentReformatted for readability (indent code blocks with two spaces): from tracks group album_id ( sort {-milliseconds} take 1 ) Editorializing: Data query specification is all about getting the details right. This does not look simpler than the corresponding SQL to me though. All components must be present -- scope, group, limit, order. SQL, for all its faults, is generally succinct at incorporating the required details. The PRQL sample here is succinct as well, but to me at least, not differentiating. reply GordonS 20 hours agorootparentprev> How long would it take you to write the SQL for that? I don't want to appear rude, but unless I'm missing something, this is a pretty simple SQL query, of the kind anyone with mimimal SQL experience could write off the top of their head in seconds. I like the idea of PRQL, but I think a better example is needed to sell it. reply audunw 19 hours agorootparentI have a moderate amount of SQL experince, but I could not write that query at the top of my head. Maybe you misunderstand what the PRQL query is doing? Here's the SQL it generates: WITH table_0 AS ( SELECT , ROW_NUMBER() OVER ( PARTITION BY album_id ORDER BY milliseconds DESC ) AS _expr_0 FROM tracks ) SELECT FROM table_0 WHERE _expr_0 \" query is kind of a poster-child for seemingly-simple but actually difficult to get right/performant sql. E.g. see: https://stackoverflow.com/questions/1313120/retrieving-the-l... reply quesera 19 hours agorootparentprevI think the logic here is: SQL is hard and most people don't know it well. So PRQL is perhaps easier to learn. The same logic is applied to TypeScript vs JavaScript. Or C vs assembly. Or Nano vs vim. Etc etc. It's a quandary though. SQL is clearly difficult for most people to get their heads around. It does require a different way of thinking about data, and you can get by with a minimal SQL knowledge for a long time, especially nowadays with ORMs. But like so many other things, making the investment is worthwhile and pays off in small and large ways, forever (so far). reply lowercased 18 hours agorootparent> making the investment is worthwhile and pays off in small and large ways And... 'making the investment' takes time, and means that time is not able to be invested someplace else. If the majority of your job is writing SQL or similar (data access, etc) then sure - yes, learn more of those tools. Some folks have a wider range of responsibilities that means you have to decide what to make more time investments in, and saying 'yes' to something is necessarily saying 'no' to other things. reply quesera 8 hours agorootparentEh, sure. Some people are not developers or don't work with systems. But 20 hours spent learning SQL pays off for a lifetime. Most people watch more hours of TV than that per week. We all make our choices, but very very few people could not find 20 hours somewhere in their lives, with zero net loss. reply setr 15 hours agorootparentprevThe hard part of SQL is thinking relationally. This obviously doesn‚Äôt do anything to affect that ‚Äî if they actually thought this, they‚Äôd be making the same mistake as SQL itself ‚Äúthe reason business users don‚Äôt program is because it‚Äôs not english enough‚Äù The problem this is resolving, if it successfully resolves anything at all, is that the SQL language is a mess of random keywords, inconsistent syntax requirements and generates some of the worst error messages known to man. It‚Äôs an attempt at making SQL a consistent, simple language ‚Äî ideally exposing the data model more directly and with less noise reply ttfkam 12 hours agorootparentI prefer \"thinking in sets\". Either way, folks try to map objects and structs to their databases (I blame ORMs personally) when the analogy just isn't so. It's like translating French to English word by word and wondering why folks have trouble understanding you. As for SQL being too much like English, making the syntax closer to a general purpose functional programming language isn't necessarily an improvement in my opinion. reply setr 4 hours agorootparent> making the syntax closer to a general purpose functional programming language It‚Äôs more about having a consistent, predictable and simple language. Being more functional-like or imperative-like or declarative-like or whatever is just a byproduct. I suppose LISP is the extreme of that goal, and probably too extreme, but SQL itself is on the other end, with every clause and function running its own subsyntax and special cases. I believe I‚Äôve read the ANSI SQL standard defines something like 1700 terminals ‚Äî it‚Äôs absolutely absurd. And of course every database extends that standard arbitrarily with a slew of new keywords. There‚Äôs a reason every RDBMS can only report errors like ‚Äúsyntax error on line 1: ‚Äù and it‚Äôs not because the devs are completely incompetent reply bvrmn 19 hours agorootparentprevI'm not a PRQL fun. But TBH I can't write this SQL from my head. I have thousand lines of written SQL. reply CuriouslyC 20 hours agorootparentprevThis seems like a micro-optimization to me. That's faster to write, but you pay for that with extra tooling and an abstraction layer that you now have to train or hire for, and I'm not sure that's a good payoff. This idea seems like it'd be better as an editor plugin that lets you write shorthand and have it automatically expanded into correct SQL rather than as a build time thing. reply sph 20 hours agorootparentYou can apply this logic to choosing a high level language vs writing assembly code. Yes the pipeline is more complex, there are more tools and more syntax to track, but the benefits are pretty clear (or we'd all be writing UI code in hand written asm) While leaky abstractions are a huge problem, the thing about abstractions is that, if they are any good, the benefits and improvements to productivity outweigh the negatives. You just have to figure out if the gains PRQL could give you are worth the effort. I rarely write SQL, so it's not worth it for me. But if PRQL were an actual query engine, not just a translation layer, and some database offered a native PRQL interface, I would immediately switch to it rather than to keep twisting my brain with SQL and it's inane syntax and rules. (I was a full time DBA in one previous life, so I should be more comfortable with SQL than most.) reply CuriouslyC 20 hours agorootparentI don't think the analogy of with js/etc and assembly is quite fair, the difference in readability between this and sql isn't on that order, and if it were I'd be a lot more bullish on this project :) reply hans_castorp 21 hours agorootparentprev> How long would it take you to write the SQL for that? select distinct on (album_id) * from tracks order by album_id, milliseconds desc; reply dewey 20 hours agorootparentWith the additional benefit that resources about SQL are everywhere and every question has been answered already multiple times. On top of that you are most likely to encounter SQL at a job than PRQL. reply da_chicken 18 hours agorootparentprevEh, `DISTICT ON` is a custom PostgreSQL extension. A standard* method would be: SELECT * FROM tracks QUALIFY row_number() over (partition by album_id order by milliseconds desc) = 1; But the QUALIFY clause is so new that it doesn't work on most RDBMSs. If you're on MS SQL Server, you're still using: SELECT * FROM ( SELECT * ,row_number() over (partition by album_id order by milliseconds desc) rn FROM tracks ) x WHERE x.rn = 1; That said, I still don't think PRQL is particularly amazing. I can't tell if it's merely syntactic sugar for SQL, or if it's actually meant to control query execution. If it's the former, it's likely to frustrate developers because it's actually just another layer of abstraction. If it's the latter, then it requires the developer to not only understand the data model well enough to be able to write SQL queries, they need to be able to understand the RDBMS impementation details well enough to be able to write queries that best take advantage of the current database's indexes, statistics, and configuration. Even something as simple as sorting before filtering or projecting can be a significant performance issue. Nevermind the fact that relational algebra done in the wrong order can be non-deterministic or not equivalent transformations, so even if the query processor is smart enough to do rewrites whatever the developer enters might be logically different unintentionally. Ultimately I think it's a tool that lets the developer thinking about the problem in the way they prefer, rather than thinking about the problem in the way that best suits the problem at hand. Like insisting on writing documentation in LaTeX instead of Word or Markdown. *: I believed this was in SQL 2023, but double checking it looks like it did not have make the final standard. I would be surprised if it didn't make it in the future, however. reply hans_castorp 12 hours agorootparentThe PRQL extension discussed here is for Postgres, so my solution was Postgres specific. Since when is QUALIFY part of the SQL standard? So far I have only seen it as a proprietary feature in Terradata. reply 6gvONxR4sf7o 17 hours agorootparentprevDamn. Reading your comment, i was about to be really glad that this pain would be a thing of the past before too long. Too bad it didn't make the standard :( reply da_chicken 16 hours agorootparentWell, even when it's part of the standard it will take about 6 years before your vendor chooses to implment it. And even then, it'll be another 6 years before your application vendor finally upgrades to it. And even then, it'll be another 6 years before the database feature is allowed to be enabled. And even then, your reporting software won't support it. reply reactivenz 11 hours agorootparentSnowflake has had it for years, and even then it is trivial to write as a nested SELECT. The bigger problem, is the \"over use of tools to boost developers\" the real problem, is putting poor developers into the pipeline. complex analytics is SQL is just simple, and lovely. The fact other struggle is not going to be helped by pretending the \"code\" looks more C like, they need to learn to think like a performant machine, and then be productive. reply hobs 20 hours agorootparentprevYou'd also need a LIMIT or a TOP, and you might need a subquery for that depending on your SQL of choice. reply hans_castorp 20 hours agorootparentThe question was \"the longest track per album\", so I'd expect one row per album as the result (which is what my query does). But adding a LIMIT would not do that. It might be that the question was \"the longest track across all albums\" which indeed would require a LIMIT reply gigatexal 20 hours agorootparentprevselect id from tracks qualify row_number() over partition by (album_id order by milliseconds desc) =1 That should work Look I‚Äôm a diehard SQL just use it guy but open to improvements. But I‚Äôm loathe to use abstractions for things when the underlying thing is so expressive. Autocomplete of fields in a good editor, schema help, etc go a long way to making SQL being written raw very nice. reply orlp 19 hours agorootparentQUALIFY is not part of the SQL standard. reply da_chicken 18 hours agorootparentI'm pretty sure QUALIFY was added in SQL 2023. Maybe it was only discussed and didn't make it. reply orlp 17 hours agorootparentI don't know, and I don't feel like paying $200 per chapter to find out. reply riku_iki 13 hours agorootparentprevmore importantly you can run prql on Postgres today, but can't use qualify yet reply gigatexal 18 hours agorootparentprevThen a subselect with the window function and an outer where clause where the window function column = 1 reply OJFord 16 hours agorootparentprevIn the real world, probably something like: select album_id ,first_value(id order by milliseconds desc) as longest_track_id from tracks group by album_id I agree the PRQL's pretty nice here, but I think such a generalised example (chances you actually want to `select *`?) overstates the advantage. reply da_chicken 15 hours agorootparentThis query would error: \"Field `id` is neither in the GROUP BY nor in an aggregate function.\" Since `id` is a key in `tracks` and `album_id` is a foreign key and not unique in `tracks`, the RDBMS wouldn't be able to use any implicit GROUP BY determinism, either. You could do this: select distinct album_id ,first_value(id order by milliseconds desc) as longest_track_id from tracks But this is unlikely to perform as well as the row_number() method because it will cause the RDBMS to generate a record for every track and then waste time sorting the intermediate results to find the unique records in the output. reply riku_iki 13 hours agorootparentprevyour query will return just album_id and id, not other track fields reply elbear 17 hours agorootparentprevI have a feeling this language will be more familiar to programmers who think functionally. I say this because it seems to consist of transformations applied to data and of `derive`, which defines new variables later used in other transformations. reply halayli 20 hours agorootparentprevThey can ask chatgpt to generate the SQL query and use the SQL output that everyone is familiar with rather than use an abstraction that is prone to versioning and behavioral changes and will consume everyone else's time to go learn a new language and become profficient in it. reply oulipo 20 hours agorootparentif they need ChatGPT to generate the query (costly and possibly error-prone), then it means that not everyone is familiar with the output... reply abirch 19 hours agorootparentprevThis would have been awesome for me 2 years ago. Currently much of my complicated SQL is generated by a LLM. reply buremba 19 hours agorootparentHmm, I would think that LLM helps adoption for the semantic layers such as PRQL, Malloy, and dbt since it's possible to generate/validate/iterate 5 lines of PRQL compared to 25 lines of SQL but considering none of them widely adopted yet, you might indeed be correct in a way that LLM makes it harder for the new tools to gain adoption by helping you to suffer less from the verboseness of SQL. reply staticautomatic 18 hours agorootparentIt‚Äôs a tough call. I run a small analytics team and am starting to train some analysts to code. Just the other day I basically told one of my reports to focus on learning Python and let ChatGPT teach him SQL by example because I think it‚Äôll be easier to grok the explanations. Now I‚Äôm looking at PRQL and Malloy and asking myself if it‚Äôs really a path I should send them down, and I‚Äôm not sure it‚Äôs a good idea. reply buremba 11 hours agorootparentI just tried ChatGPT to generate some Malloy snippets and compared to SQL, it‚Äôs very basic. It‚Äôs probably not a huge lift to teach it the library by scanning the docs but still the reasoning with SQL is much sophisticated given that there are tons of training data. reply beeboobaa 18 hours agorootparentprevWon't this just lead to developers wrecking performance because they don't understand what's happening? reply lkuty 20 hours agorootparentprevSorry to be that guy :-) The `introduction.prql` example on the playground gives an idea of the better readability of PRQL vs SQL and your small example an idea of the speed you may gain from writing PRQL vs SQL. It is interesting. Indeed, me writing the SQL would have taken more time than you writing the PRQL. reply vendiddy 20 hours agoparentprevThe thing I hate most about SQL is lack of composability. In most languages it's easy to pull out functions. In SQL you end up with a giant hard to comprehend mess. I think the underlying relational concepts in SQL are sound but I'd love to see ideas like PRQL that aim make SQL easier to write and maintain. Stored procedures and functions are nice but don't allow the basic idea of breaking a large query apart into smaller logical components. reply CuriouslyC 20 hours agorootparentYour statement about breaking large queries apart is wrong. You can write queries with CTES to improve readability, and extract CTEs into functions that can be selected as queries get too large and unwieldly. SQL is just as composable as any other language. The thing that's lacking right now is the tooling for managing/testing/deploying database code. There are solutions out there and the supabase folks have been working to make things better but database first development still has some hurdles in terms of DX. reply jpgvm 18 hours agorootparentOne issue with functions though is that they can change performance in unpredictable ways. For example a colleague of mine recently altered a function I had written that was used in multiple hot-path queries. The change he made accidentally caused the function to no longer be inline-able on PostgreSQL. Once the function couldn't be inlined then the PostgreSQL planner wasn't able to select the appropriate indices and the performance of several of the queries exploded by about 100x. So while it's true it can be composed etc the current state of the art planners struggle except under very simple/constrained scenarios. reply CuriouslyC 18 hours agorootparentIf a function is marked stable it should not impact the query plan at all, since stable functions are essentially in-lined before planning. If logic is unstable a view is probably going to be a better abstraction than a function. reply jpgvm 18 hours agorootparentWell I thought so but even with the functions marked IMMUTABLE, which is even more stringent than STABLE the in-lining was not successful, this was apparent in the query plan. This might be a special case however as the function called another function internally (also IMMUTABLE) which was essentially memoized using an expression index. This is the index that was no-longer hit when inlining failed. If you think this is bug I think I can create a minimal reproduction. reply zeroimpl 16 hours agorootparentAn immutable function cannot query a table because the table itself isn‚Äôt immutable. If your stable/immutable flags don‚Äôt match reality, the function can‚Äôt be inlined. reply jpgvm 12 hours agorootparentDetails in sibling but I dug a bit deeper, new version used to_char, turns out that is STABLE and not IMMUTABLE so because the volatility didn't match the whole way down anymore it broke inlining. I'm guessing switching the function that calls to_char to STABLE will fix the problem. reply zeroimpl 11 hours agorootparentMakes sense. You mentioned index so I thought maybe you were were querying a table. Would be nice if postgresql could tell you when the flags don‚Äôt match. I think anytime you deal with timestamps you can have problems since the expression may depend on the session‚Äôs time zone. reply CuriouslyC 17 hours agorootparentprevThat does sound like a bug, the planner should be inlining all of that. I would mention it on the postgres mailing list so a committer with more experience in how the planner marshals all that stuff together can weigh in. reply jpgvm 12 hours agorootparentI guess I got nerd-sniped. I dug a bit deeper into what was happening and the issue is the new version of the function called to_char() which turns out isn't IMMUTABLE which broke the inlining! reply da_chicken 17 hours agorootparentprevI'm not sure how adding another layer of abstraction helps with that. The problem you encoutered is that the rewritten function was either no longer table-valued, or else it was no longer deterministic (which is what that big list of rules for inlining really means). But that problem doesn't go away by adding a layer of abstraction. The need to understand relational determinism doesn't disappear. The need to understand SARGability doesn't go away. You can't really abstract the problem away. reply Izkata 14 hours agorootparentprev> You can write queries with CTES to improve readability, and extract CTEs into functions that can be selected as queries get too large and unwieldly. Personally I'd go for breaking them into views. IIRC as of around postgres 11-13 they're no longer a barrier for the query planner. reply CuriouslyC 14 hours agorootparentYou are right, views are a good choice in a lot of instances. Functions give you more behavioral flexibility, easy multiple version support (in postgres) and fewer issues with ddl dependencies, but views are semantically clearer, easier to work with and give good usage flexibility. reply vendiddy 16 hours agorootparentprevDo you have any links to a basic example on using CTEs and functions to keep SQL maintainable? I've used CTEs, but I had not tried breaking up an SQL query into functions. Didn't know that was possible! For whatever reason, I feel like I end up with a giant blob of SQL when writing SQL and it's incredibly frustrating. reply CuriouslyC 14 hours agorootparentYou can just use chatgpt to rewrite sql with ctes, and extract functions. It's quite good at it, particularly gpt4. That being said, CTEs are a really good way to write complex queries. They let you tag bits of query with meaningful names, and each thing you tag is accessible to every CTE after it so you can build up an almost imperative data flow by just doing select transforms one after another. That way you're building hard queries from the bottom up rather than the top down. reply ttfkam 12 hours agorootparentAgreed. It's so nice to be able to query intermediates to see what their output looks like and just check each step of the CTE during debugging. It was such an improvement over most subselects! reply ako 15 hours agorootparentprevComposition is available in sql, but works a bit different than in a procedural language. In sql you express sets of data, composition consists of defining subsets that you compose into more complex sets. Views and CTEs are the tools for composition in SQL. reply hosteur 21 hours agoparentprevSQL is incompatible with many types of autocompletion. For example columns in a select statement are not known here you write FROM. This alone justifies PRQL in my opinion. reply da_chicken 17 hours agorootparentIt's not a real problem in a practical sense. Yes, you have to write the FROM clause before autocomplete happens even though the SELECT must appear before it. Fortunately, however, text editiors used as IDEs allow for out-of-sequence code editing. You can just enter the FROM and move back to complete the SELECT. It's like complaining that you have to know the variable name you're going to assign something to before you start writing the expression that will set the value. int y = x * 2; The idea is to evaluate the expression and store it, but the expression doesn't actually read that way left-to-right. Wouldn't it make more sense for it to be: x * 2 assign to new int y; Technically, that's written more in execution order. In practice it just isn't that big of a deal. It only trips up beginners. reply CuriouslyC 20 hours agorootparentprevThere are tools in most languages to deal with this. For example, take https://jawj.github.io/zapatos/. It introspects your database schema to generate types, and gives you autocompletion inside tagged template strings. reply nwatson 19 hours agorootparentprevJetbrains tools (Datagrip, IntelliJ IDEA, PyCharm) deal well with this if you leave a placeholder column before FROM, and write the FROM part with proper joins. You can go back to the columns and autocomplete goes just fine. You can amend our extended any part and the autocomplete logic adapts well. reply haspok 20 hours agoparentprevOne notable reason is being database-agnostic. Like ORM's, if you can generate SQL you can generate database-specific SQL as well. SQL is also quite verbose in places (JOINs are the most trivial example), and lack a decent amount of abstraction (CTEs are relatively low level). Updating a large set of FK'd tables can be a nightmare (this is what ORMs shine at). Finally, some modern additions are quite unreadable, Postgres' JSON syntax, for example. I'm not saying that PRQL solves any of the above, but these are all legit problems with \"plain\" SQL. reply CuriouslyC 20 hours agorootparentDatabase agnosticism is so 2010. There's very little reason to choose a DB other than postgres, and if you have a reason to choose a specific niche db you're not probably not going to be migrating away from it any time soon. CTEs are a first step in structuring queries to make them decomposable. You can extract CTEs to functions and mark them stable and it's logically equivalent to the original query. reply haspok 20 hours agorootparent> There's very little reason to choose a DB other than postgres Sure, if you are a startup, or write your own code. But for most people the choice of database(s) is a given, and they are not in a position to challenge that. At the end of the day, Oracle has to make a living, too... reply ttfkam 18 hours agorootparentFrom parent: > you're not probably not going to be migrating away from it any time soon. Oracle has its own optimizations and foot-guns that extend well beyond what you can represent in a database-agnostic API. And once you're on that DB, you can write DB-agnostic and have performance be relatively horrible or require a careful rewrite of your schema and stored procedures when you migrate. There is not door number three. Writing a common layer for any and all relational databases is like using a Java UI library for all operating systems. Sure, it will work, but it will have obvious shortfalls, be immediately recognizable as such to anyone familiar with the underlying platform, be inconsistent with other apps on that platform, and leave any opportunities for efficiency and performance on the floor. Say you want a pivot table. In Oracle and MS SQL, it's built in. In Postgres, it's possible but noticeably more annoying. In MySQL, it's simply not possible. How would you represent this in a database-agnostic way? And yet performing in the app layer is very much slower/less efficient. Did you know Oracle supports parallel DML for enhanced performance and lower multi-query latency? You have to intentionally use though, and neither Postgres nor MySQL support it at all. What about global temporary tables? Those especially aren't found in Postgres or MySQL and are not easily swapped into the app layer without a massive performance penalty. Per-user namespaces are yet another Oracle-ism that just doesn't translate to other DB engines, but you definitely should know about. If you're making a living from Oracle, earn your pay. Make the most of what you've got. reply gaganyaan 11 hours agorootparentprevI like PostgreSQL quite a bit and pushed for using it by default at my company, but SQL pops up in a surprising number of places. You can use it in PowerBI, Snowflake, and pandas just to name a few. You don't always control which DB you're talking to, and it would be nice if the SQL interface that gets presented in those scenarios got an upgrade. PRQL looks like a really promising option for that. reply riku_iki 13 hours agorootparentprev> There's very little reason to choose a DB other than postgres Postgres is single server OLTP DB with complicated failover story, it is strong enough reason to consider some other contenders e.g. CocroachDB, SpannerDB for distributed OLTP or OLAP specialized ClickHouse, BigQuery, DuckDB. reply ttfkam 12 hours agorootparentAren't most of those wire-compatible with Postgres? reply riku_iki 12 hours agorootparentquery language likely not fully supported reply ttfkam 11 hours agorootparentYes, all engines are unique. Still wire compatible and still SQL though. reply riku_iki 11 hours agorootparentIt's obvious, but what is your point, why are you saying this? reply ako 15 hours agorootparentprevHow are joins verbose? It‚Äôs pretty straight to the point: combine these 2 tables on these columns‚Ä¶ what do you want to remove to make it less verbose? reply murkt 21 hours agoparentprevPRQL is not an ORM in any kind of way, it‚Äôs the less quirky SQL, basically. reply adrian_b 18 hours agoparentprevIn my opinion SQL is the sub-optimal language. Whenever I am writing SQL I am not thinking in SQL, but I am thinking in what I consider to be the mathematical sound way, which I translate into SQL while writing. I consider thinking in SQL a much greater mental handicap than having to translate mentally into it. I would prefer to write directly in what I would consider as a good query language and have it translated automatically into SQL, for compatibility with what is, for unfortunate historical reasons, the standard. I have not attempted previously to do or use something like this, but work like that discussed here seems like a step in the right direction. reply ttfkam 12 hours agorootparentCan you expand on this? I have always seen SQL as a DSL for set theory. How are you seeing things \"mathematically\" that do not comport with set manipulation in SQL? reply davedx 21 hours agoparentprevSQL is pretty great, but for some things (I thought the example in the readme was decent) it can be quite cumbersome. Stored procedures are very imperative (and hard to debug, depending on the platform IME). This seems like a more functional approach to stored procedures. I don't think it's suggested that this replaces SQL. Use the right tool (and abstraction) for the job? reply CuriouslyC 20 hours agorootparentI think the debuggability is the #1 issue with database-as-a-platform. Using notices to debug functions is such a poor workflow that even though I'm bullish on putting stuff in PG I avoid writing complex code might need to be debugged as an integrated unit. reply lkuty 21 hours agorootparentprevMaybe. I already think in terms of transformations (relational algebra and its closure property) when I write SQL and use a lot of CTEs. But I guess the functional way might help people see what's going on. reply baq 20 hours agoparentprev> Why limit ourselves with a sub-optimal language be it PRQL? Actually why limit yourself with SQL...? PRQL is a language compiled into SQL and makes certain hard-to-do things in SQL easy purely because it allows to streamline operations which SQL needs CTE joins or whatever hoop jumping to solve. My favorite example which sounds easy but isn't - select the row which is MAX(...). reply ttfkam 17 hours agorootparent> Actually why limit yourself with SQL...? Because it's everywhere, has extensive documentation and tutorials, all database tools support it, all relational engines support it, some non-relational engines support it, all programming languages have library support for it, it can be accessed through command line tools as well as graphical interfaces, etc. You think an industry is going to give up 50 years of infrastructure because some (typically junior) devs think the syntax is \"kinda icky\"? > My favorite example which sound easy but isn't - select the row which is MAX(...). If you look earlier in the comments you will see queries that have \"DISTINCT ON\" in them. It solves the problem that sounds easy, but actually is pretty easy if you know SQL. reply jeroen79 15 hours agorootparentI agree, sql is well documented and an industry standard, no need to make it more complex by adding preprocessors that do nothing but change the syntax. Just bite in and learn proper sql. reply ttfkam 12 hours agorootparentI would love relational database engines to adopt the same syntax when they support a feature. There is no compelling business case for them to do so though. Browsers converged largely in response to IE's dominance. If Firefox, Safari, and Opera had each gone their own way, no single one of them could ever get enough marketshare to get developers to care. Together they made a block significant enough that Microsoft was forced to join in or die. (Actually did effectively die once Edge was moved to the Blink engine. Now it's a satellite of a mostly standards-compliant effort.) But databases aren't browsers. Devs just have a single target usually: whatever database engine the company decided to use. Still it would be nice to have greater overlap to reduce the niggling details between them that only seem to exist today due to inertia rather than technical necessity. reply andix 19 hours agoparentprev [‚Äì] Because SQL can be cumbersome to write. It's often repetitive, requires nesting, aliases, and a specific order of statements. reply ttfkam 18 hours agorootparent [‚Äì] 1. Notice language is complicated for some tasks 2. Propose newer, simpler language to take care of these 3. Newer, simpler language lacks features of original language 4. Newer language adds features, making it more complicated 5. GOTO 1 reply andix 15 hours agorootparent [‚Äì] That's why we stopped innovation of programming languages at C89? Why use sql at all if you can also do it in C? reply ttfkam 13 hours agorootparent [‚Äì] Do you think SQL stopped at SQL-92? Arrays, JSON, CTEs, window functions, booleans, MERGE, temporal tables, regular expressions, foreign tables (aka SQL/MED), etc. SQL hasn't been sitting still, (though ORMs seem to lead folks to believe it is). Do you currently write K&R C or modern C (C11 or C17)? Don't get me wrong, I don't think SQL is perfect. Far from it. But PRQL isn't fixing the defects in SQL I care about. For example in DDL, NOT NULL should be the default rather than nullable. When I declare a column as a foreign key, I shouldn't have to specify the type again when the system already knows what the referenced type is. Then again, PRQL is for querying, not data definition, so it doesn't actually solve my biggest issue at all. SQL could perhaps be more terse. I agree with a lot of folks that FROM should have been first and SELECT near last. That's pretty uselessly subjective. The argument that it's not composable falls flat for me though. Views, CTEs, foreign tables, set-returning functions, etc. are all forms of composability within SQL. When you think in terms of sets, they all fit quite well together. If you're not thinking in sets, it doesn't belong in the database in my opinion. The underlying engines themselves have been innovating like gangbusters. Using the same wire protocol, folks can connect to a standard Postgres database, a massive CockroachDB cluster, Supabase, and all points in between without changing a client library. The same is true for MySQL, MariaDB, and PlanetScale. Time series DB? Just use SQL. Analytics? SQL. Document-oriented data? There's even standard JSON query syntax within SQL. It works. It doesn't generate huge amounts of CVEs like C has. (DB libraries have the SQL injection attacks, not SQL itself.) And it scales fairly seamlessly from Google Spanner all the way down to embedded SQLite. But folks assert it's irreparably broken and needs urgent replacement. Having trouble buying it. Perhaps I just haven't seen the right replacement yet. That may indeed be the case. I just don't see PRQL being that replacement. It feels a lot more like a lateral move to me at best, and that's just too disruptive compared to potential benefit. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed an extension to write PRQL functions in PostgreSQL, inspired by a concept on Hacker News.",
      "They integrated PRQL with PostgreSQL through the pgrx framework, collaborating with the friendly and supportive maintainers of both projects.",
      "-"
    ],
    "commentSummary": [
      "Hacker News discussion focuses on PRQL integration in PostgreSQL by user kaspermarstal, with interest in DSL languages like EdgeQL and comparisons between PRQL and SQL in terms of readability and performance.",
      "Users debate SQL query complexity, data modeling trends, database direction, and the pros/cons of using plain SQL versus ORMs for database management.",
      "Discussions mention limitations of Postgres and MySQL versus Oracle, potential SQL interface enhancements, ongoing SQL advancements, and considerations for a potential SQL replacement with PRQL."
    ],
    "points": 254,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1708342071
  },
  {
    "id": 39436358,
    "title": "Accidental Trigger Breaks IKEA's System (2023)",
    "originLink": "https://cohost.org/sirocyl/post/2891449-i-broke-ikea",
    "originBody": "sirocyl@sirocyl9/19/2023, 1:38 AM I broke IKEA. from some PBX in a LACK rack, I'd assume. - Voicemail recording of the delivery service call (Transcript in post) Voicemail recording of the delivery service call (Transcript in post) from some PBX in a LACK rack, I'd assume. 00:00 (or, well, one of their delivery services.) üîä Just a fair warning - there are some perhaps annoying glitch sounds in the attached recording. The volumes are normalized to limit loud spikes, as they were a lot worse in person. üòÖ so, my phone service has a rather clever anti-spam tactic, which works like this: I receive a phone call from an unknown number, and it goes through screening when I answer it. It rings until the fifth ring, the voicemail greeting plays out, then I've got 30 seconds to judge if it's a spam robocall or if it's genuine If it's okay, I press 1, and it interrupts the ring/voicemail sequence and I answer the call like usual. If it's spam, I press ### (the # key by itself normally opens my PBX menu, so it doesn't go through) and hang up immediately. Pressing ### and hanging up, will shove the call to voicemail, then launch a \"DTMF bomb\", which is a rapid sequence of over a hundred tones of DTMF keysmash, even including some of the \"ABCD\" keys. This has blown up spammers' cheapass PBXes, especially ones with poor security and too much trust given to the DTMF decoder on the call server. So, when IKEA called from a random 1-877 number to confirm my furniture shipment worth $1200 (that's the equivalent of bl√•haj!), the only thing it said is \"To continue in English, please press 1.\"... and I had no idea who it was, immediately thought it was spam, and did the ### gesture. Oops. What follows is a transcript of the call in the recording above. \"To continue in English, please press 1‚É£.\" [extremely rapid DTMF spam string] \"Your delivery is scheduled for Tuesday. Five. [A burst of digital static plays out here for about a quarter of a second.] $DeliveryDate between the hours of 2pm and 6pm. If an adult will not be available within the timeframe provided, or you have any other conflicts, please contact us at 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 Message repeat. ‚ö†. Your delivery is scheduled for-\" [total system breakdown occurs here... followed by dead line noise.] ............. [blerp] ............. [blerp] ............. [blerp] ............. [blerp] ... I should've just bought bl√•haj, instead. (Names, businesses, times, dates and phone numbers may be changed or redacted in order to protect the privacy of those involved.) #css crimes#screen reader friendly#robocalls#DTMF#software gore#glitch#windows xp see all 76 comments",
    "commentLink": "https://news.ycombinator.com/item?id=39436358",
    "commentBody": "I broke IKEA (2023) (cohost.org)219 points by jcurbo 10 hours agohidepastfavorite30 comments twothamendment 7 hours agoLong ago I got a Psion Series 5. One feature was that it could dial a phone number (output the DTMF) for you. Messing around I've day I realized a contact could have a very long phone number. This was also back in the day when answering machines existed and many had a 2 digit code you could punch in to get into the menu from the outside line. My contact called Answering Machine had a very long phone number that got me into more than one answering machine. Once in, it was fun to change their outgoing message. One friend was convinced that I must have climbed the back of his apartment building to get in the open 3rd story window to change the message. That would have been cool, but a string of DTMF was much easier! reply hibikir 4 hours agoparentBack when international phone calls were a real thing, messing with answering machines that had default settings was a typical fraud vector. People would change the message to say 'I accept' a couple dozen times. Then, they'd lace a collect call with a third party payer, pointed at said answering machine... which accepted the charges. Just not best done from one's home phone, as sufficient charges pointing to the same number would risk attention. reply skykooler 7 hours agoprevI read the text first, then listened to the audio, and was shocked at how good that transcription is. reply buffington 5 hours agoparentI know! I thought all the jibberish was just to be silly, but no, it visually looks like the wave forms of the audio. reply wrs 6 hours agoprevAs someone who‚Äôs had some incidents with DSP code, the end of the recording sounds like it may be playing some part of memory that isn‚Äôt an audio buffer. I wonder if there‚Äôs actually a ‚ÄúDTMF injection‚Äù possibility here‚Ä¶ reply iforgotpassword 2 hours agoparentYou can hear the windows XP message box sound right before that. Which surprises in two ways: a) they're still using windows XP (ok well we still do too at work for some appliance from the power company). b) it seems you're not hooked into the machine via some modem or virtual-something over lan, but something that connects to the sound card, otherwise I've no idea how system sounds that always play on the default card would end up in the phone call. That means there's one machine handling one call at a time. reply jensenbox 3 hours agoparentprevThere may be secrets in that audio - actual passwords and whatnot. reply malermeister 2 hours agorootparentMight be a fun follow-up to try and decode whatever's in there. reply lgats 1 hour agorootparentdidn‚Äôt have much luck with multiple baud rates and modulations on minimodem, no discernible ascii, but someone might have more luck looking at the binary output reply ajxs 15 minutes agorootparentIt wouldn't be encoded in a modem protocol. If that's indeed binary data, then most likely we're hearing binary data interpreted as being PCM wave data. reply throwanem 7 hours agoprevI'd love to know how that PBX is set up. reply buffington 4 hours agoparentFrom a comment on the blog post: > iirc it's generated from a script in asterisk, with the delay and tone durations set \"short\" (I think it was the minimum EIA/TIA DTMF mark/space numbers, not sure.) > My phone system was Google Voice, through an SIP bridge with Obihai (now defunct/discontinued). Asterisk then made the SIP connection and rang my other phones, a Lucent Partner ACS for my landlines, cellphones, ATAs and forwarding numbers, also over SIP. > Most of the hardware was lost in the housefire last year. This recording was from early-mid 2020 or so. https://cohost.org/sirocyl/post/2891449-i-broke-ikea#comment... reply 0xC0ncord 7 hours agoparentprevI was thinking the same thing! I don't get spam calls often but when I do I really want to punish them for wasting my time. reply Geezus_42 6 hours agorootparentJolly Rodger Telephone service has worked wonderfully for me. reply gffrd 3 hours agorootparentI'm on the floor just listening to the sample calls.\"Can you tell more about how ‚Ä¶ uh, how account holder services can help me? And by the way, do you have any tips for growing tomatoes? I've been trying to grow them in my garden but the just won't COOPERATE.\" reply throwanem 7 hours agorootparentprevI mean I already mute and mash till the line drops when they do come in, but they may not continue doing so at a low enough rate to keep that feasible, and boringly mechanical but necessary tasks are always prime candidates for automation in any case. reply isoprophlex 3 hours agoprevThat Windows alert sound in between the glitching binary-dump-as-audio sounds was just too funny. reply Severian 7 hours agoprevHaha, awesome. Would be good to get the uncompressed audio, I bet you could decode that binary stream into bytes. reply apimade 1 hour agoprevI wonder if those sounds are they sounds of bits/byte data. There‚Äôs some regularity to it so it‚Äôs likely somewhat structured. reply pavel_lishin 8 hours agoprevI'd love to know what actually happened back there. reply Kalabasa 3 hours agoprevI like how they can animate their posts in this cohost social networking site. (See the transcript section) reply wackget 6 hours agoprevWebsite doesn't like it if you block third-party content (using uMatrix). It loads and then disappears a few seconds later. reply Dwedit 4 hours agoparentWorking fine here with uMatrix (actually nuTensor), are you auto-blocking the first party content too? reply pmontra 3 hours agoparentprevWorks for me. I enabled the 1st party and cohost.org rows plus the css and images columns. Maybe it's because of another addon? reply BlueTemplar 3 hours agoparentprevWorks fine here. reply jakedata 7 hours agoprev...and I hope you've learned to sanitize your DTMF inputs reply baby_souffle 7 hours agoparentIs it too late to ret-con the name of john draper / captain crunch to bobby dials? reply bandergirl 4 hours agoparentprevI always sanitized my DTF inputs, as my last tests failed. reply jasonjayr 6 hours agoparentprevLittle Bobby Tables strikes again! reply iAMkenough 7 hours agoprev [‚Äì] Brilliant. Thank you for sharing. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A poster unintentionally activated a series of tones during an IKEA delivery service call, leading to a system breakdown and a message repeat loop, mistaking the call for spam.",
      "The post contains a call transcript and details an anti-spam tactic employed by the poster's phone service."
    ],
    "commentSummary": [
      "The post explores using a Psion Series 5 to dial phone numbers via DTMF tones and accessing answering machines with lengthy numbers.",
      "It highlights fraudsters exploiting default settings on answering machines for collect call scams and the technical process of decoding binary data from audio recordings.",
      "The discussion extends to setting up an Asterisk script in a phone system, spam calls, automation, and technical challenges related to website loading."
    ],
    "points": 220,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1708387182
  },
  {
    "id": 39428047,
    "title": "Exploring Fractal Boundaries in Neural Network Trainability",
    "originLink": "https://arxiv.org/abs/2402.06184",
    "originBody": "Computer Science > Machine Learning arXiv:2402.06184 (cs) [Submitted on 9 Feb 2024] Title:The boundary of neural network trainability is fractal Authors:Jascha Sohl-Dickstein Download PDF HTML (experimental) Abstract:Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations. Comments: 3 pages, mesmerizing fractals Subjects: Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Chaotic Dynamics (nlin.CD) Cite as: arXiv:2402.06184 [cs.LG](or arXiv:2402.06184v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2402.06184 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jascha Sohl-Dickstein [view email] [v1] Fri, 9 Feb 2024 04:46:48 UTC (36,948 KB) Full-text links: Access Paper: Download PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2402 Change to browse by: cs cs.NE nlin nlin.CD References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39428047",
    "commentBody": "The boundary of neural network trainability is fractal (arxiv.org)193 points by RafelMri 23 hours agohidepastfavorite51 comments otaviogood 17 hours agoThis is much more interesting if you see the animations. https://x.com/jaschasd/status/1756930242965606582 reply notfed 15 hours agoparentFractal zoom videos are worth infinite words. reply paulddraper 12 hours agorootparent> infinite I see you reply catlifeonmars 14 hours agoparentprevSo what exactly are we looking at here? Did the authors only use two hyperparameters for the purpose of this visualization? reply yazzku 13 hours agorootparentIt's explained in the post: > Have you ever done a dense grid search over neural network hyperparameters? Like a really dense grid search? It looks like this (!!). Blueish colors correspond to hyperparameters for which training converges, redish colors to hyperparameters for which training diverges. reply catlifeonmars 9 hours agorootparentI saw this, but am still not clear on what the axes represent. I assume two hyperparameters, or possibly two orthogonal principal components. I guess my point is it‚Äôs not clear how/which parameters are mapped onto the image. reply mchinen 22 hours agoprevReposting comment from last time since I'm still curious: This is really fun to see. I love toy experiments like this. I see that each plot is always using the same initialization of weights, which presumably makes it possible to have more smoothness between each pixel. I also would guess it's using the same random seed for training (shuffling data). I'd be curious to know what the plots would look like with a different randomness/shuffling of each pixel's dataset. I'd guess for the high learning rates it would be too noisy, but you might see fractal behavior at more typical and practical learning rates. You could also do the same with the random initialization of each dataset. This would get at if the chaotic boundary also exists in more practical use cases. reply londons_explore 19 hours agoparentI think if you used a random seed for weights and training data order, and reran the experiment enough times to average out the noise, then the resulting charts would then be smooth with no fractal patterns. reply baq 16 hours agorootparentAn interesting conjecture, well worth a paper in response. reply catlifeonmars 14 hours agorootparentprevDo you consider the random seed (or by extension the randomized initial weights) a hyperparameter? reply thomashop 20 hours agoparentprevexactly. if they always use the same initialization seed then this isn't very surprising. one would have to do many runs for each point in the grid and average them or something. but i didn't read the paper so maybe they did. reply pmayrgundter 16 hours agoprevOne of Wofram's comments is that there appears to be much more internal structure in language semantics that we had expected, contra-Chomsky. We also know the brain, cortex esp, is highly recurrent, so it should be primed for creating fractals and chaotic mixing. So maybe the hidden structure is the set of neural hyperparams needed to put a given cluster of neurons into fractal/chaotic oscillations like this. Seems potentially more useful too.. way more information content than a configuration that yields a fast convergence to a fixed point. Perhaps this is what learning deep NNs is doing: producing conditions where the substrate is at the tipping point, to get to a high-information generation condition, and then shaping this to fit the target system as well as it can with so many free parameters. That suggests that using iterative generators that are somehow closer to the dynamics of real neurons would be more efficient for AI: it'd be easier to drive them to similar feedback conditions and patterns Like matching resonators in any physical system reply catlifeonmars 14 hours agoparentTwo quibbles: (1) neural nets don‚Äôt necessarily operate on language, (2) they only loosely model biological neurons in that they operate in discrete space. All that is to say that any similarities are purely incidental without accounting for these two facts. reply pmayrgundter 13 hours agorootparent1) agreed. it's exciting seeing the same basic architecture broadly applied 2) not sure what you mean by \"operate in discrete space\" I'd emphasize the potential similarity to biological recurrence. Deep ANNs don't need to have this explicitly (tho e.g. LSTM has explicit recurrence), but it is known that recurrent NNs can be emulated by unrolling, in a process similar to function currying. In this mode, a learned network would learn to recognize certain inputs and carry them across to other parts of the network that can be copies of the originator, thus achieving functional equivalence to self feedback, or neighbor feedback. It takes a lot of layers and nodes in theory, but ofc modern nets are getting very big. reply sjwhevvvvvsj 13 hours agorootparentprevWell, in some sense they don‚Äôt operate on language at all - but on mathematical representation of tokens derived from language. I‚Äôm sure from your comment you are aware of the distinction, but it is an interesting concept for people to keep in mind. reply pmayrgundter 15 hours agoparentprevPer some offline discussion, I'll note that while this paper is about the structure of hyperparams, it also starts with the analogy between Mandelbrot & Julia sets, Julia as the hyperparam space for Mandelbrot, the param space Well, they both also have similar fractal dimensions. Mandlebrot's Hausdorff dimension is 2, Julia is 1-2. I won't argue it here but just suggest that this is an important complexity relationship and that the neural net being fit may also have similar fractal complexity, and that the distinction between param and hyper-param in this sense may be somewhat a red-herring reply taneq 9 hours agorootparent> Julia as the hyperparam space for Mandelbrot It‚Äôs kind of the opposite, no? The Mandelbrot set is the set of points where the Julia set for that point includes that point. reply blackbear_ 21 hours agoprevCould it be that this behavior is just caused by numerical issues and/or randomness in the calculation, rather than a real property of neural networks? reply sigmoid10 17 hours agoparentFractals are not caused by randomness. Fractals arise from scale invariance and self similarity, which in turn can come from non linear systems and iterative processes. It is very easy to generate fractals in practice and even the most vanilla neural networks trivially fulfill the requirements (at least when you look at training output). In that sense it would be weird not to find fractal structures when you look hard enough. reply nyrikki 15 hours agorootparentNot exactly, Newton's fractal, which is topological, specifically the wada property, is a boundary condition. It relates to fractal (non-integer) dimensions, which was first described by Mandelbrot in a paper about self similarity. Here is a paper that covers some of that. https://www.minvydasragulskis.com/sites/default/files/public... In Newton's fractal, no matter how small a circle you can draw, your circle will either contain one root or all the roots. The basins that contain one root are open sets that share a boundary set. Even if you could have perfect information and precision this property holds. This means any change in initial conditions that crosses a boundary will be indeterminate. There is another feature called riddled basins, where every point is arbitrarily close to other basins. This is another situation where even with perfect information and unlimited precision a perturbations would be indeterminate. A positive Laponov exponent which isn't sufficient to prove chaos, but is always positive in the presence of chaos may even be 0 or negative in the above situations. Take the typical predator prey model and add fear and refuge and you hit the riddled basins. Stack four reflective balls in a pyramid and shine different color lights in two sides and you will see the Wada property. Neither of those problems are addressable with the assumption of deterministic effects with finite precision. reply tomxor 17 hours agorootparentprev\"If we're built from spirals, while living in a giant spiral, then everything we put our hands to, is infused with the spiral\" ... Sorry I couldn't help myself. reply cactusfrog 11 hours agoprevEven the boundary for newtons approximation is fractal. This is a feature of non-linear optimization. reply FredrikMeyer 22 hours agoprevRelated: https://news.ycombinator.com/item?id=39349992 (seems to be the same content) reply ahdsr 15 hours agoprev> Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations. Reading this gave me goosebumps reply phaedrus 16 hours agoprevThe blog post would be a better link for this submission. https://sohl-dickstein.github.io/2024/02/12/fractal.html reply calibas 16 hours agoparentIt was posted on HN last week: https://news.ycombinator.com/item?id=39349992 reply subroutine 21 hours agoprevIn case it isn't obvious, you can tap on any of the figures in the PDF or HTML version to watch the video. reply jerpint 22 hours agoprevIt feels weird to me to use the hyper parameters as the variables to iterate on, and also wasteful. Surely there must be a family of models that give fractal like behaviour ? reply freeone3000 17 hours agoparentThe fractal behaviour is an undesirable property, not the goal :P ideally every network would be trainable (would converge)! this is the graphed result of a hyperparameter search, a form of optimization in neural networks. If you envision a given architecture as a class of (higher-order) function, the inputs would be the parameters, and the constants would be the hyperparameters. Varying the constants moves to a different function in the class, or, varying the hyperparameters gives a different model with the same architecture (even with the same data). reply amelius 21 hours agoparentprev> It feels weird to me to use the hyper parameters as the variables to iterate on Yes, I also think this is strange. In regular fractals the x and y coordinates have the same units (roughly speaking), but here this is not the case, so I wonder how they determine the relative scale. reply thfuran 17 hours agorootparentIs there really any meaningful sense in which real and imaginary numbers have the same units but two dimensionless real hyperparameters don't? reply xvedejas 14 hours agorootparentComplex numbers multiplied by an imaginary number rotate preserving magnitude. In this sense they have the same units. reply thfuran 13 hours agorootparentYou can rotate any vector while preserving its magnitude. reply xvedejas 13 hours agorootparentIf the units are different, you do need to come up with a conversation between them, or you're implicitly saying it's 1-to-1 reply scalablenotions 10 hours agoprevThe chaotic nature of these images, full of local minimas, makes me intuitively think that genetic algorithm approaches to optmizing hyper parameter searching are better suited than regression-based ones. reply bawolff 11 hours agoprevIts interesting how fluid-like this fractal is compared to other fractal-zoom videos i see on the internet. I have no idea how common it is for fractals to be like that. reply lawlessone 17 hours agoprevWhat does this mean? reply PaulHoule 17 hours agoparentTraining the network is a dynamic process similar to https://en.wikipedia.org/wiki/Julia_set or https://en.wikipedia.org/wiki/Newton_fractal reply idiotsecant 16 hours agoparentprevAs another poster pointed out, its much more intuitive with graphics. The parameters that you tweak to control model learning have a self-similar property where as you zoom if you see more and more complexity. Its the very definition of local maxima all over the place. reply romusha 16 hours agoparentprevNothing reply eapriv 14 hours agoprevNote that the paper does not provide a proof of this statement, only some experimental evidence. reply omginternets 16 hours agoprevI vaguely recall that in vivo neural oscillations also exhibit fractal structure (in some cases at least). reply ttoinou 18 hours agoprevI've produced some KIFS (Kaleidoscopic iterated function system) fractals that look like this reply kraig911 15 hours agoprevMakes me think of something I think I read from Penrose about consciousness. reply xcodevn 17 hours agoprevI am not trying to downplay the contribution of the paper, but isn't it obvious that this is the case? reply Buttons840 15 hours agoparentI'll defend the idea that it was obvious. (Although, it wasn't obvious to me until someone pointed it out, so maybe that's not obvious.) If you watch this video[0], you'll see in the first frame that there is a clear boundary between learning rates that converge or not. Ignoring this paper for a moment, what if we zoom in really really close to that boundary? There are two possibilities, either (1) the boundary is perfectly sharp no matter how closely we inspect it, or (2) it is a little bit fuzzy. Of those two possibilities, the perfectly sharp boundary would be more surprising. [0]: https://x.com/jaschasd/status/1756930242965606582 reply barbarr 11 hours agoparentprevI don't think it's obvious per se, but people who have studied numerical methods at the graduate level have likely seen fractal boundaries like this before - even Newton's method produces them [0]. The phenomenon says more about iterative methods than it says about neural networks. [0] https://en.wikipedia.org/wiki/Newton_fractal reply eapriv 14 hours agoparentprevNot only it is not obvious; it is not known to be true. reply teaearlgraycold 16 hours agoparentprevObvious to whom? reply bloaf 15 hours agorootparentI think the \"obvious\" comment was a bit snarky, but out of curiosity, I posed the question to the Groq website which currently happens to be on the front page right now. (It claims to run Mixtral 8x7B-32k at 500 T/s) And indeed, the AI response indicated that the boundary between convergence and divergence is not well defined, has many local maxima and minima, and could be quote: \"fractal or chaotic, with small changes in hyperparameters leading to drastically different outcomes.\" reply JL-Akrasia 18 hours agoprev [‚Äì] I'll add to this. It's not only the boundary that is fractal. We'll soon see that learning on one dataset (area of fractal) with enough data will generalize to other seemingly unrelated datasets. There is evidence that the structure neural networks are learning to approximate in a generative fractal of sorts. Finally, we'll need to adapt gradient descent to operate at move between different scales reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper delves into the fractal properties of the boundary separating stable and divergent training in neural networks, likening it to fractals such as the Mandelbrot set.",
      "It experimentally shows that this boundary exhibits fractal characteristics across various setups, emphasizing how neural network training is highly sensitive to minor hyperparameter adjustments.",
      "The findings indicate potential insights into the trainability of neural networks, shedding light on the implications of this fractal nature in training processes."
    ],
    "commentSummary": [
      "The debate on arxiv.org delves into the fractal properties influencing neural network trainability boundaries, focusing on hyperparameters, randomness, and internal structure impacting language semantics.",
      "References to Mandelbrot and Julia sets are made, drawing parallels with biological recurrence and intricate fractal configurations.",
      "The importance of hyperparameters in molding neural networks, along with discussions on the efficacy of genetic algorithms for optimization, is emphasized in the conversation about observed fractal boundaries' relevance and clarity in neural networks."
    ],
    "points": 193,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1708338472
  },
  {
    "id": 39432085,
    "title": "Decoding Erlang/OTP Framework for Efficient Development",
    "originLink": "https://adoptingerlang.org/docs/development/otp_high_level/",
    "originBody": "Erlang/OTP is different from most programming environments out there, even those that also use a virtual machine. Erlang has a strong opinion about how your applications should be structured, the level of isolation they should have, and a separation between what Erlang‚Äôs VM can do, and what your software can do. It‚Äôs not just a programming language, it‚Äôs a whole framework for building systems. Understanding its core principles is the key to getting started fast without having to rewrite everything later: it ensures that all applications can fit well together, that updates can be done live, and that your code is easy to instrument and make observable. In this chapter, we‚Äôll cover the Erlang virtual machine and the core concepts of OTP at the highest level. The Erlang Run-Time System The foundational block for everything is the Erlang virtual machine itself, called BEAM. BEAM is technically a single implementation of the Erlang virtual machine, as there could be others. For example, Erllvm is an implementation over LLVM (using some custom patches to make everything possible), and an older implementation in the 90s was called JAM. The Erlang VM is implemented in C, and contains a lot of fancy stuff: schedulers to run processes, garbage collection, memory allocators, a timer wheel for events, a bunch of smart switches to abstract over operating system features and provide unified interfaces (such as over time management, file-handling drivers, and so on), a few built-in functions that go faster than what Erlang can do on its own (BIFs) and an interface for functions implemented natively in other languages (NIFs) along with special schedulers for them. There‚Äôs obviously a lot more, but you can think of all that stuff the way you would with the kernel in BSD or Linux: low level stuff that you need in order to build fancier stuff. If all you have is the virtual machine with nothing else, you can‚Äôt run Erlang code. You don‚Äôt have a standard library, you don‚Äôt have libraries to even load code. To get it all going, there‚Äôs some tricky bootstrapping going on that we don‚Äôt need to understand. Just know that there‚Äôs a limited set of pre-loaded Erlang modules that ship with the virtual machine, and those can be used to set up networking and file-handling stuff, which is in turn used to further load and run modules. If you‚Äôre interested in knowing more though, please consult The BEAM Book or BEAM Wisdoms. If you take the virtual machine and the pre-loaded stuff, along with all the little utilities that make code-loading possible, you have what is essentially called the Erlang Run-Time System (ERTS). The Run-Time System, when starting, follows the instructions of a thing called a boot script (which nobody writes by hand) that specifies what to start. Erlang, by default, provides boot scripts that load a minimal amount of code required to start a shell and write your own applications. Once this is done, we can start thinking about Erlang, and not just the virtual machine. Erlang/OTP What we have described so far is equivalent to an operating system‚Äôs kernel. We now need the foundational blocks for the userspace components. In Erlang, this is essentially what OTP is about. OTP specifies how ‚Äúcomponents‚Äù that run on the virtual machine should be structured. There‚Äôs more to the language than just ‚Äúprocesses and messages‚Äù: there‚Äôs one well-defined way to structure your code. note OTP stands for _Open Telecom Platform_, which is literally a meaningless name that was used to get the stuff open-sourced back in the old days of Erlang at Ericsson. Erlang/OTP systems are structured through components named OTP Applications. Every Erlang version you have installed or system built with it that you use ships with a few OTP Applications. There are basically two variants of OTP applications: Library Applications, which are just collections of modules, and Runnable Applications, which contain a collection of modules, but also specify a stateful process structure stored under a supervision tree. For the sake of clarity, we‚Äôre going to use the following terminology for OTP Applications for this entire book: Library Applications: stateless collections of modules Runnable Applications: OTP applications that start stateful supervision tree structures with processes running in them OTP Applications: either Library or Runnable Applications, interchangeably By default, the two OTP applications everyone includes are called stdlib, which is a library application that contains the core standard library modules such as list or maps, and kernel, which is a runnable application and sets up the core structure for an Erlang system that relies on OTP applications to work. When a node boots, the modules from all required OTP applications are loaded in memory. Then kernel is started. kernel manages the lifecycle of the system from this point on. All other OTP applications and their configuration are handled through it, and so are unique features like distribution and hot code updates. If we go back to the operating system comparison, you can think of the kernel OTP application a bit like you could think of systemd for the Linux kernel (or init if you hate systemd or use a BSD ‚Äì Windows users can think of it as the service that runs other services) In fact, kernel and stdlib are the only two applications you need for a basic working Erlang shell. When you type in erl (or start werl on Windows), this boots up the VM, along with kernel, with stdlib pre-loaded. Everything else is optional and can be loaded at a later time. The standard Erlang distribution contains applications such as: kernel stdlib crypto (cryptographic primitives) ssl (TLS termination library) inets (network services such as FTP or HTTP clients) ct (Common Test framework) wx (graphic toolkit) observer (a control panel to manage your Erlang node, building on wx) compiler (the Erlang compiler to build your own project) and so on All of these are put together into what is called an Erlang release. A release is a collection of OTP applications, possibly bundled together with a full copy of the virtual machine. As such, when you download and install Erlang, you just get a release whose name is something like Erlang/OTP-21.3.4. You‚Äôre free to build your own releases, which will take some of the OTP applications in the standard distribution, and then bundle them with some of your own apps. So if we were to write an app named proxy that relies on ssh and ssl (which themselves depend on public_key, crypto, stdlib, and kernel), we would make a release with all of these components in it: ERTS kernel stdlib crypto public_key ssl ssh proxy A visual representation of this can be seen in Figure 1. Figure 1: Visual representation of building the proxy release Essentially, building an Erlang system is re-bundling the VM, along with some standard applications provided with the default distribution, together with your own apps and libraries. Living in Erlang/OTP Standard tools developed and used by the community such as Rebar3 operate on the idea that what you write and publish are OTP applications, and as such contain all the functionality required to deal with them. That‚Äôs a big shift from a lot of programming languages that only ask of you to have a function named main() somewhere in one of your files. This is why the programming language is often called Erlang/OTP rather than just ‚ÄòErlang‚Äô: it‚Äôs not just a programming language, it‚Äôs a general development framework that mandates some basic structure for everything you do. And everyone follows it, whether they are writing embedded software, blockchain systems, or distributed databases. It‚Äôs OTP or nothing. Whereas other languages usually mandate nothing specific to get started, but then add some requirements later on (such as when integrating with a package manager), Erlang‚Äìand its entire community‚Äìexpects you to just write OTP applications, which the rest of the tools can handle. So the key to getting started fast in Erlang is to know the framework, which is often kept as more advanced material. Here we‚Äôre going to do things upside down and start from a fully functional release, and then dig down into its structure. The next chapters will be dedicated to understanding how to work within these requirements. ‚Üê Prev Next ‚Üí",
    "commentLink": "https://news.ycombinator.com/item?id=39432085",
    "commentBody": "OTP at a High Level (2019) (adoptingerlang.org)188 points by todsacerdoti 16 hours agohidepastfavorite54 comments asaph 15 hours agoAm I the only one who saw the title and thought OTP was \"one time password\"? reply bmitc 7 hours agoparentYes. OTP is engrained in my head as the One True Programming (way of building large systems). reply andsoitis 2 hours agoparentprevThe domain name right along the title should give you a hint‚Ä¶ reply fatkam 8 hours agoparentprevprobably not. here is an answer from mistral-next: > \"OTP\" can stand for different things depending on the context. However, in the context of transportation, it commonly stands for \"One-Time Password\". In the context of fanfiction, it can stand for \"One True Pairing\". In the context of Atlanta, Georgia, OTP stands for \"Outside the Perimeter\", referring to the areas outside of the Interstate 285 loop that encircles the city. Please provide more context if you meant a different usage. Erlang OTP makes no sense... just like X, or Alphabet as company names reply hn_throwaway_99 6 hours agorootparentThe article itself has a blurb that says what it stands for: > OTP stands for _Open Telecom Platform_, which is literally a meaningless name that was used to get the stuff open-sourced back in the old days of Erlang at Ericsson. reply eindiran 12 hours agoparentprevI clicked thinking it was going to be about one-time programmable memory. reply LoganDark 15 hours agoparentprevNope. Should really say \"Erlang OTP\" or something reply Jtsummers 15 hours agorootparentThe URL kind of gives away the theme of the site. And the first sentence starts with ‚ÄúErlang/OTP‚Äù. reply chefandy 11 hours agorootparentSure: you could just have a list of links with the text \"click here\" that you hover over to see the url, too. If you haven't worked in the BEAM world those terms packed into a URL might not grab your attention enough to trigger recall, and what are headlines for if not telling us what's on the other side of the click? reply fuzztester 2 hours agorootparent>and what are headlines for if not telling us what's on the other side of the click? Wow, good way of putting it. reply LoganDark 11 hours agorootparentprevSure, if I spent more than half a second deciding if I want to click something, which I usually don't. This isn't \"I can't tell this is Erlang without clicking\". This is \"there are no obvious signs that it's Erlang without clicking\". reply Jtsummers 11 hours agorootparent‚Äúadoptingerlang.org‚Äù shows up on the discussion page and the HN main list. No need to click, just to read the word ‚Äúerlang‚Äù that is literally right there. It‚Äôs pretty easy actually. reply chatmasta 8 hours agorootparentWhat's ingerlang? Some hip new Norwegian programming language? reply fuzztester 2 hours agorootparentParsingAmbiguityError. adopt ingerlang OR adopting erlang. reply LoganDark 11 hours agorootparentprevDude, not everyone knows everything. I'd have to have run into this situation before in order to know to look for erlang in the URL here. Plus, do I ever look at the URL before I click something on HN? no. I had assumed \"OTP\" was so incredibly obvious that there was no need to look at the URL, that's all. > It‚Äôs pretty easy actually. Hindsight is 20/20... blaming user error is pretty cheap when my entire point is that it's easy to make user error. reply Zondartul 15 hours agorootparentprevQuoting the article: \"OTP stands for _Open Telecom Platform_, which is literally a meaningless name that was used to get the stuff open-sourced back in the old days of Erlang at Ericsson.\" reply LoganDark 11 hours agorootparentThen maybe the acronym should just be expanded. reply LorenzoGood 15 hours agoprevJust out of curiosity for anyone starting a new Erlang project: Why use Erlang over Elixir? reply rdtsc 14 hours agoparentI find Erlang is a bit simpler. Some of the macros and so many ways of doing things in Elixir is a tad too much for me. I may be in the minority here, but I also prefer immutable variables. Other factors: Erlang/OTP base libraries are written in Erlang, so it feels like there is less friction when using them. With a lot of recent language improvements in Erlang like maps, better command line completion, sigils, etc., I feel like I am not missing too much from Elixir's ergonomics. One caveat might be if I had do a web app, then Phoenix just has a lot of goodies that are nice to have, and maybe then only then I might consider Elixir. And not make it sound like I am berating Elixir, and to balance things out, I'd like to say the Elixir community is amazing and the collaboration and improvement they've brought to Erlang/OTP are significant in the recent years. It's nice to collaborate and be part one one large BEAM VM community. reply lolinder 13 hours agorootparent> but I also prefer immutable variables Just to clarify in case anyone gets the wrong idea about Elixir‚ÄîElixir data is immutable (you can't modify structures, only make new ones) and variables cannot be reassigned, but Elixir does allow shadowing variable names. The difference between shadowing and reassignment is significant, because it means that this code outputs \"10\", not \"20\": x = 10 closure = fn () -> x end x = 20 IO.puts closure.() # 10 Contrast that with the otherwise equivalent JavaScript: let x = 10; const closure = () => x; x = 20 console.log(closure()) // 20 Shadowing allows you the convenience of reusing names without the risk of a variable changing out from under you via spooky action at a distance. reply rdtsc 12 hours agorootparentIndeed, thanks for clarifying, lolinder. I assumed the audience would be familiar with the differences as it's Elixir vs Erlang here. So it's immutable variables vs immutable data. Both Erlang and Elixir have immutable data. That's the stuff variables refer to. But in Erlang the variables are also immutable, so X = X + 1, just like in basic math, doesn't make sense. reply ramchip 12 hours agorootparentIt's called rebinding, rather than mutation. Good background here: https://dashbit.co/blog/comparing-elixir-and-erlang-variable... reply rdtsc 11 hours agorootparentThanks, but I prefer immutable data vs variables as opposed to introducing a new term. I think it's a bit simpler. reply lolinder 11 hours agorootparentIt's simpler, but it's also incorrect. There are very real differences between rebinding, reassignment, and mutation. All three are distinct from one another in meaningful ways, so reusing the same term is actually wrong and misleading [0]. Besides, \"rebinding\" is not a new term. A quick search on Google Scholar turned up numerous results that predate Erlang itself [1]. The concept of \"binding\" a variable dates back at least to Alonzo Church with the lambda calculus, and \"rebinding\" is the natural term for what happens when you bind the same name again. [0] As an example of the potential for confusion, see JavaScript's `const` keyword, which prevents reassignment but does not prevent mutation. Sloppy use of the terminology here has lead to real confusion among new JavaScript developers. [1] For example, 1986: \"If the problems illustrated by Mesa are to be avoided, the language should provide some method of unbinding or rebinding a previously bound name.\" https://dl.acm.org/doi/pdf/10.1145/324634.325436 reply rdtsc 6 hours agorootparent> There are very real differences between rebinding, reassignment, and mutation. Hmm, what would be the difference between rebinding and reassignment in Erlang? To avoid complication, it's simpler to just use \"change\" and understand that there are variables and data. The variables themselves may change or may not change. In Erlang they don't, while in Elixir they do. The data won't change in either language, unless we are playing with NIFs or use atomics or counters. > All three are distinct from one another in meaningful ways, so reusing the same term is actually wrong and misleading If we're talking in the context of Erlang is quite simple, it's enough to just say variables are immutable. When it comes to Elixir we need to add a distinction, and that complicates things. When talking about Javascript or C++ or other languages, then more nuances are needed. reply lolinder 5 hours agorootparent> To avoid complication, it's simpler to just use \"change\" and understand that there are variables and data. If you're just talking about Erlang, sure. But as soon as you're comparing languages to each other (which you are) then you have to use the precise terminology if you don't want to confuse people. You can complain about the need for the terminology, and you're certainly welcome to prefer the language that doesn't allow rebinding, but that doesn't actually make the terminology unnecessary. reply bmitc 8 hours agorootparentprevBinding is not a new term. And here, it is the term. No object is getting mutated. The name is rebound. This is also known as shadowing. reply bmitc 12 hours agorootparentprevI think the more accurate thing to say is Elixir allows you to rebind variable names whereas Erlang doesn't. Elixir will allow you to pin a name though such that the pattern matches against the already existing variable name's value. Talking about mutability I think confuses this. reply dudinax 7 hours agorootparentprevThis would make me choose erlang over Elixer. It's convenient to find an assignment to a variable and not have to worry there might be another. reply sbrother 6 hours agorootparentI've worked with both extensively and I agree that Elixir's rebinding is a bit of a wart. FWIW you can get the Erlang behaviour by using the \"pin\" operator: iex(1)> x = 4 4 iex(2)> ^x = 3 ** (MatchError) no match of right hand side value: 3 reply lolinder 7 hours agorootparentprevYep, I can see why someone would prefer preventing shadowing even though I don't. I just wanted to be clear on what the actual behavior was. reply cmullaparthi 3 hours agorootparentprevI agree with this sentiment. I programmed in Erlang for more than a decade before Elixir came along and I couldn't see the point of it. When I first picked up Erlang in '99 it was a breath of fresh air. The simplicity of the language blew me away (or maybe I just had a very good tutor). reply Miner49er 14 hours agoparentprevI can speak on this a bit. I'm fairly new to Erlang, but the team I'm on has been using Erlang since before Elixir was created. So we've been doing Erlang just because it's what we've been using, and the team knows it, not Elixir. However, we recently just decided to start new projects in Elixir rather then Erlang. One argument a former team member would make against Elixir is that it has almost too many ways of doing things, in some cases. Erlang is a much simpler language. One example he used was sigils. Sure, they're nice, but it also means there's multiple ways to define a binary string, for example. Sometimes it is nice to have a single way to do something and not have to worry about making a decision about it. This example isn't great anymore though since Erlang is getting sigils in OTP 27. Another argument I can think of is that Erlang is learning from Elixir. Erlang took its maybe expression from Elixir's `with`, and, IMO, the maybe expression is better. They're also adopting sigils. But, like I said, we ended up deciding to go with Elixir for new projects, so I would be curious to see what reasons other people have for picking Erlang. reply square_usual 14 hours agorootparentWhat do you think is better about the maybe expression? Just curious, haven't looked into it too deeply and I thought it behaved exactly like when. reply Miner49er 14 hours agorootparentWell, I haven't written any Elixir yet, so I'm probably saying that without really knowing. The main thing, and maybe I just am missing something, but I don't really understand what the point of the 'do' in Elixir's 'with' is? In Erlang you just have the maybe and anything that would go in that do block would just go inside the maybe expression. That seems a lot better to me. I also just prefer the look of Erlang's maybe. I think maybe is a clearer name for what the statement is doing and the lack of the 'do' block looks better to me and makes it easier to read. reply nesarkvechnep 2 hours agorootparentSo you say `maybe` is better than `with` without ever using Elixir, and `with`? I hope your team is not full of people making blanket statements like this. reply troupo 12 hours agorootparentprev> I think maybe is a clearer name for what the statement is doing and the lack of the 'do' block It's because basically everything in Elixir is a macro :), and the do block is an argument/parameter to that macro: maybe x do ... end # is really maybe x, do: ... # which ends up being a macro call with a named parameter maybe(x, do: ...) Same goes for many (all?) other things in Elixir: if x do ... end # is really if x, do: ... # which ends up being a macro call with a named parameter if(x, do: ...) And so on. But it took me a very long to come to grips with this, too :) reply square_usual 12 hours agorootparentI think GP is talking more about the fact that you can have a do block, not about what it is. reply Miner49er 11 hours agorootparentYeah exactly, in Erlang the 'do' portion would just go in the same code block as the conditionals. reply pmw 14 hours agoparentprevThose two aren't even the only options. There's also Lisp-Flavored Erlang (LFE), and maybe other languages targeting the virtual machine BEAM. https://lfe.io/ It's similar to a multitude of languages targeting the Java Virtual Runtime (JVM) -- functional, imperative, object-oriented, actor-based -- whatever you want, but they all produce interoperable bytecode. reply lawn 14 hours agorootparentI'm personally a big fan of Gleam. Bringing static typing to the BEAM is like a dream come true! It's a young language to be sure, but I see the potential. (Maybe don't bet your startup on it.) reply lpil 14 hours agorootparentI would definitely bet my start up on Gleam! reply slekker 12 hours agorootparentCongrats on the upcoming v1 release! reply vegabook 12 hours agorootparentprevbut does it have a REPL? Lots of BEAM's runtime interaction capability is lost without one. reply gregors 10 hours agorootparentprevalso Lua in Erlang https://news.ycombinator.com/item?id=29703949 reply bmitc 7 hours agoparentprevI have been solidly in the just use Elixir camp, but I'm debating that on some new projects. For one, if you want a library to be used in both, it's best to write it in Erlang. Then Elixir can just directly call the functions or you could build Elixir-specific wrappers of the Erlang functions. The other is that Erlang just has a bit of simplicity and rawness about it that I like, not to mention it is the OG language. I would miss piping and variable name rebinding though. At the end of the day, I do think that Elixir has the better tooling ecosystem. Mix, Hex, the formatter, Credo, ExUnit, Livebook, etc. are really nice. reply LorenzoGood 6 hours agorootparentAlso, Phoenix and Liveview. reply mekoka 14 hours agoparentprevDepends on the project, but let's say you're writing something that should or could be made a part of other projects. I might be mistaken and correct me if I'm wrong, but I think it might be easier for an Erlang project to be integrated into an Elixir one than the reverse. reply schneebyte 10 hours agoparentprevIf your working on a distributed system, i think Erlang still has better tooling compared to Elixir. For example common test for integration/system testing and snabbkaffe for trace based testing. reply denvaar 14 hours agoparentprevI think that it would come down to personal/team preference. Maybe you need to create a library that can be used in both Erlang/Elixir, so it may make more sense to do it in Erlang. Everything in this article about OTP, Supervision trees, etc. carries over to Elixir, just with slightly different syntax. reply lpil 14 hours agoparentprevI prefer the simplicity of Erlang, Elixir is a much larger language and has some quirks. reply waynesonfire 14 hours agoparentprevTLDR; I prefer the Erlang syntax. Since I use Erlang VM for hobby / side projects, it's easier for me to come back to after some time away. You can't go wrong. Inevitibly, which every you start with, you'll learn both. My personal journay started with learning Elixir. I soon discovered that I was reading Erlang docs, encountering Erlang stack traces, and using many fantastic Erlang lirbaries. I decided I wanted to be build stronger Erlang experience and havn't looked back. I still create Elixir projects from time to time, especially to leverage Ecto (for databases). Also, on my todo list is to play with the Phoenix (web framework) project. I'm suspect my journey is rather common, I see Elixir developers comfortable with Erlang and vice-versa. Though, I'm sure each has a perference like a dominant hand. You'll find yours as you explorer this fantastic technology. reply sho 5 hours agorootparent> I see Elixir developers comfortable with Erlang and vice-versa I agree with this. I was an \"in theory\" fan of erlang for a long time, but was always put off by its syntax, which I don't find too pleasing. But after a lot of elixir, and kind of rubbing up against erlang anyway, you sort of can't help but learn it, or at least learn to read it. Once you start using things like ETS, for example, you're going to be reading erlang docs. By the time you even know about ETS, though, you've probably seen enough erlang that it's not scary anymore. I do prefer elixir - but I admit it's purely a subjective, aesthetic preference. They're both good and I can imagine jumping back and forth as needed. One thing I will say though is that once you've worked with the BEAM enough you ain't going back. You will rip my genservers out of my cold, dead hands. reply crowdhailer 14 hours agoprev [‚Äì] This is a great series of posts. I get so used to the erlang environment I forget to share the cool bits because they become normal reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Erlang/OTP is a distinct programming environment with a virtual machine and a framework for system development, with the BEAM virtual machine and Erlang Run-Time System as its core components.",
      "OTP outlines how components should be organized in the virtual machine, dividing applications into Library and Runnable Applications, with stdlib and kernel as default applications in an Erlang system.",
      "Rebar3 is a tool that aids in supporting OTP applications within Erlang, emphasizing the importance of understanding Erlang/OTP's framework for effective development across different applications."
    ],
    "commentSummary": [
      "The discussion delves into the disparities between Elixir and Erlang concerning immutable variables, data, rebinding vs. reassignment, and selecting between the two for new projects.",
      "Users exhibit diverse inclinations towards syntax, tooling, and personal/team requirements when deciding between the languages.",
      "The BEAM environment is lauded for its robust capabilities in this comparison."
    ],
    "points": 188,
    "commentCount": 54,
    "retryCount": 0,
    "time": 1708362418
  },
  {
    "id": 39435397,
    "title": "Endgame: Xbox Dashboard Exploit Unleashed",
    "originLink": "https://github.com/XboxDev/endgame-exploit",
    "originBody": "ENDGAME - A Dashboard Exploit for the Original Xbox Overview ENDGAME is a universal dashboard exploit for the original Microsoft Xbox. This exploit has been carefully engineered to be compatible across all retail kernel and dashboard versions released for the original Xbox. It does not require a game, or even a working DVD drive -- only a memory card. Special credit belongs to @shutterbug2000 for the initial discovery of this vector within the dash and the first to demonstrate code execution against it. With further research, ENDGAME was developed by @gaasedelen leveraging an adjacent vulnerability that offered greater control and facilitated a more ubiquitous exploitation strategy. Disclaimer This project does NOT use any copyrighted code, or help circumvent security mechanisms of an Xbox console. Upon success, ENDGAME will launch a habibi-signed XBE from the root of the memory card. It does not patch kernel code or allow you to launch retail-signed executables. By using this software, you accept the risk of experiencing total loss or destruction of data on the console in question. Building The exploit files can be generated from scratch using Python 3 + NASM on Windows. Example usage is provided below: python main.py Successful output should look something like the following: [*] Generating ENDGAME v1.0 exploit files -- by Markus Gaasedelen & shutterbug2000 [*] Assembling shellcode... done [*] Un-swizzling payload... done [*] Compressing payload... done [*] Saving helper files... done [*] Saving trigger files... done [+] Success, exploit files available in ENDGAME/ directory A pre-built zip of the exploit and sample payload XBE is available on the releases page of this repository. Usage Copy the contents of the generated ENDGAME/ directory to a Xbox memory card such that the root directory of the memory card has the following structure, where payload.xbe can be any habibi-signed XBE of your choosing: /helper/ /trigger/ /payload.xbe To trigger the exploit, plug the memory card into a controller and navigate to it while in the dashboard. After a few seconds, the system should begin cycling the front LED to green/orange/red to indicate success. This is followed by it launching the payload.xbe placed on the memory card. FAQ Q: Is this a softmod? A: No, by itself, ENDGAME is not a softmod. But it will make softmodding significantly more accessible as the community integrates it into existing softmod solutions. Q: What is new about this exploit? A: This exploit will enable people to softmod any revision of the original Xbox without needing a specific game. It will also allow people to easily launch a homebrew XBE (such as the Insignia setup assistant, or content scanning tools) by simply inserting a memory card into an unmodded Xbox. Q: I don't have a memory card, can I use something else? A: Yes, any FATX-formatted compatible USB device and controller port dongle should work. Q: Why am I getting Error 21 after placing my own XBE on the memory card? A: Your XBE must be signed using the habibi key. Several tools can do this, xbedump being the most popular. Q: Why does my habibi-signed XBE result in a black screen with ENDGAME but not on a modded xbox? A: The most common explanation is that your XBE may be using the Debug/XDK kernel thunk & entry point XOR keys rather than the retail ones, resulting in a crash. Q: I triggered ENDGAME but my system quickly rebooted to the dash rather than my XBE... A: While this should be uncommon, it means the exploit probably crashed. It's recommended to navigate straight to the memory card on a cold boot for successful exploitation. Q: My XBE requires multiple files and external assets to run, will it work with ENDGAME? A: No. Currently, ENDGAME is only structured to copy & execute a standalone XBE. Q: How does this exploit work? A: The exploit targets an integer overflow in the dashboard's handling of savegame images. When the dash attempts to parse the specially crafted images on the memory card, ENDGAME obtains arbitrary code execution. Authors shutterbug (@shutterbug2000), discovery and initial exploitation efforts Markus Gaasedelen (@gaasedelen), root-cause-analysis & ENDGAME development xbox7887 (@xbox7887), minor contributions and assistance with testing",
    "commentLink": "https://news.ycombinator.com/item?id=39435397",
    "commentBody": "Endgame: A dashboard exploit for the original Xbox (github.com/xboxdev)183 points by Lammy 11 hours agohidepastfavorite67 comments azalemeth 10 hours agoIn many ways I think it is really quite sad that we're at a point where blown e-fuses, encrypted everything and properly done public/private key crypto means that it's entirely possible that next generation of consoles really might actually be unhackable and unbackupable, barring some tiny bootloader exploit, cheap TEM and an understanding of \"physically uncloneable functions\", or cheap and plentiful qubits. Things like this are just cool, prevent e-waste, and preserve cultural heritage. reply auguzanellato 10 hours agoparent> In many ways I think it is really quite sad that we're at a point where blown e-fuses, encrypted everything and properly done public/private key crypto means that it's entirely possible that next generation of consoles really might actually be unhackable Pirates will always find a way, the Nintendo Switch had all the countermeasures you mentioned, the latest revisions also have a patched bootloader but someone figured out that they could still boot a custom firmware via power fault injection on the SoC. reply jsheard 10 hours agorootparentNintendo (or rather Nvidia) was behind the curve on fault injection counter-measures, Sony and Microsoft did their homework and it's never been achieved on their last two generations of systems as far as I know. Microsoft openly talked about it here: https://www.youtube.com/watch?v=U7VwtOrwceo Seriously, console security is getting really good. The Switch had some major hardware blunders between the bootloader bug and fault injection, but the developer of the main Switch CFW project is on record that their custom microkernel is absolutely bulletproof and he expects there will never be another software exploit. If Switch 2 gets the hardware right, there's no way in. https://twitter.com/SciresM/status/1486787208333774848 https://twitter.com/SciresM/status/1327721899862888448 The Playstation is the weakest of the bunch, since they run FreeBSD on bare metal there's a large attack surface and a lot of eyes on the code. The Xbox uses some variant of NT but each application gets its own Hyper-V virtualized NT instance, so to jailbreak you need to escape Hyper-V, which is easier said than done and has a $250,000 bug bounty so MS would probably be the first to be informed. reply WirelessGigabit 6 hours agorootparentI felt called out in that YouTube video when the guy said 'This is what people do to not spend $60 on games'. reply grishka 2 hours agorootparent$60 for a game is very expensive though. Especially so with digital distribution which means it can't be resold. reply ChrisKnott 40 minutes agorootparentIt's not expensive compared to what games used to cost. reply bowsamic 42 minutes agorootparentprevIt‚Äôs really not‚Ä¶ reply JauntyHatAngle 5 hours agorootparentprevWhile I'm not saying most people by far aren't doing it to save money, I have bought switch games and then emulated them just to try the graphics at 4k 60fps that isn't possible on the console. That's the part I'm sad about - that we won't get emulated games that look and feel better due to faster hardware in the future. Money isn't an issue for me. reply doctorpangloss 4 hours agorootparentprevOn the other hand the Switch is the best platform to make money on as an indie game developer. reply graphe 10 hours agorootparentprevXbone has no hacks. It has dev mode, no exclusives both which killed hacking and piracy. reply Jerrrry 7 hours agorootparentyup, the \"cat-and-mouse\" game ended when they embedded the decryption key physically to the CPU. You can't read it without breaking it. reply coretx 9 hours agorootparentprevThe PS3 had \"OtherOS\" for lucrative import/export levies. Saving a dime on removing it post sales and export/import is what lead to hacking & piracy. reply coretx 9 hours agorootparentprev\"console security is getting really good.\" - It's not nor will it ever be. There is no point in investing extra millions for let's say anti electron microscope measures for example. Developers aim for secure enough and don't care much about a single nerd who cracked his individual system using exotic specialists tools more valuable than the console and the entire game collection combined. It's more lucrative to just send a lawyer. ( Until it goes wrong. ) reply jsheard 9 hours agorootparentI would class making it uneconomical to hack a console for any practical purpose as \"really good security\". Could someone with nation-state resources hack the Xbox? Maybe? But who cares, they're not going to. reply kjellsbells 5 hours agorootparentFunny, I was just wondering if such an entity would not actually quite like the xbox and Playstation as targets. I mean, they are powerful machines, well connected to the internet and the ownership class by and large dont blink when they power them on and are told they need to download a system update or game patch. Feels like a fertile place to build a bot net? I wonder if you really even need to hack the console. It might be easier to, say, subvert Rockstar's supply chain to put some code in the next GTA that spent a few cycles doing whatever botnets need to be doing. reply danielheath 4 hours agorootparentSurely cheaper to mandate that microsoft ship the patch, if they wanted to do that. reply DANmode 3 hours agorootparentprevnot going to and publish it. reply charcircuit 8 hours agorootparentprev>There is no point in investing extra millions for let's say anti electron microscope measures for example. TPM designs that protect against such an attack will become a commodity reducing the cost to include it onto a console. Every desktop, phone, laptop, tablet, microwave, fridge, server, etc will all need TPMs. The demand and scale exists for this to become cheap over time. reply jedrek 57 minutes agorootparentprevThe Playstation 4 was launched in 2013 and there is no modern hack for it. reply rusk 10 hours agoparentprevit saddens me as a vintage games conservator that my present day PS4 will be a brick in a few years. When I have consoles and games going back 40 years. It just doesn‚Äôt seem right. reply deergomoo 1 hour agorootparentThe thing that has me concerned is that even if you keep the consoles working, and even if you have an extensive library of physical media, these days what‚Äôs on the disc is probably an unpatched, incomplete, possibly unplayable mess. That‚Äôs not intended as a criticism of game development, just an expression of fear for the preservation implications. It‚Äôs no longer enough to preserve what‚Äôs in your hands‚Äîwhich is difficult enough where electronics are concerned‚Äîbut now someone must find some way to extract, archive, and reapply all the day one patches, bug fixes, DLC packs etc. reply theodric 10 hours agorootparentprevI mean this as earnestly as possible: remember that pain and don't buy a PS5, or a PS6, or [...] Don't build any more emotional relationships with products built by rentiers that refuse to respect your ability to enjoy the perpetuity of your license (and god forbid even considering actually owning anything). Take on an allotment and breed a new hybrid chili pepper, or brew some beer and share it with your neighbors. Beat Galaga for the 11,000th time. Whatever it takes. To hell with Huxley's Brave New World-- a better one is possible if we force it to exist by refusing to continue enabling the system enslaving us. reply codetrotter 10 hours agorootparentprevOTOH, has there really been any actually culturally significant video games released in recent years? I enjoyed for example the Uncharted games a lot. But I don‚Äôt think anything significant will have been lost if future generations of people don‚Äôt get to play the Uncharted games, no matter how much I enjoyed them myself. Whereas, things like Pong, and Pac-Man and the first Super Mario games and the Pok√©mon Red and Blue and Yellow games feel much more important for me, and of course the original Doom, all of these are important on a whole other level. And thankfully they are all sure to be able to be preserved at least in some form. reply p1necone 10 hours agorootparentThere's been plenty of amazing and culturally relevant games recently, you just have rose tinted glasses for the time you grew up in (or you haven't been gaming much). I'll just list a handful of really well known ones. * Basically everything FromSoft has put out (With Dark Souls 1 and Elden Ring probably being the most important - Dark Souls effectively kicked off an entire genre because of it's popularity (technically Demon's Souls came first, but Dark Souls was the first one to blow up).) * Breath of the Wild * Escape from Tarkov also kicked off a new genre in the extraction shooter, although noone else has quite nailed it yet imo. * The Witcher 3 (heavily influentual towards the design of Breath of the Wild, and a number of other open world rpgs recently) * As someone else pointed out, Baldurs Gate 3 * Skyrim * Minecraft * Dwarf Fortress (not exactly recent, but the Steam release propelled it into being much more well known) * Path of Exile arguably revolutionized the diablo-style ARPG genre - any ARPG that comes out now is influenced by it - see Last Epoch for probably the biggest example. Whether a game is \"culturally relevant\" or not really just hinges off of whether a significantly large portion of the population plays and talks about it for long enough though. These days we get so many more quality game releases than we ever did in the 90s/2000s so it's much harder for a single game to grab everybody's attention in the same way that the stuff you mentioned did. reply xnorswap 35 minutes agorootparentHow are we defining recent? ( you're right they're all recent compared to Pong and the OP's point. ) Except BG3, most of what you've listed are around 10 years old. BoTW: 7 Tarkov: 8 ( Technically, but was slow burning so hard to place exactly when it took off. ) Witcher3: 9 Skyrim: 13 Minecraft: 14? ( 13 if we're counting retail, but it was massively popular before then ) DF: 18? Path of Exile: 11. reply ascagnel_ 7 hours agorootparentprevWe‚Äôre also finally getting games that have genuinely good writing in them, not just writing that‚Äôs good for the sliding scale that usually applies to interactive media. - Disco Elysium is a role-playing game largely about self-discovery and identity - Kentucky Route Zero is a reflection on mortality and the true costs of poverty - NORCO is a meditation on death and survivorship with meanderings into environmentalism and groupthink/cult thinking reply ngcazz 1 hour agorootparentthat's not really what Disco Elysium is about ;) reply estiaan 9 hours agorootparentprevTo be fair many of the games on this list would be trivial to preserve as long as windows remains backwards compatible with older software, and not too hard even after that. Breath of the wild and Tarkov stands out as exceptions reply mikepurvis 8 hours agorootparentUntil recently I would have said Spider-Man and God of War 2016 though both are now released on Windows as well. reply harry8 9 hours agorootparentprevlinux & proton? reply badsectoracula 10 hours agorootparentprev> OTOH, has there really been any actually culturally significant video games released in recent years? Yes, like -random example here- Uncharted :-P. TBH i do not think what is \"culturally significant\" is something that we can answer right now, but something that will be up to future people to decide. Though i do think that Uncharted is culturally significant in how it was part of a series of influences from the original Indiana Jones, to Tomb Raider, to Uncharted and back to Tomb Raider (in that each one of those influenced the next) - with some Prince of Persia thrown in for good measure. But for people to be able to tell, judge and discover these things, the games must be available to them - preferably in their original condition (remasters, rereleases, remakes, etc often tend to change things - though those should also be available so these changes can also be seen and perhaps talked about in the future). Also not only \"good games\" or \"influential games\" are worthy of preservation - even bad or just mediocre game are worthy of that, for a variety of reasons. Games are often a snapshot of the times they were made at and multiple \"snapshots\" are always better than a single one. Even from a design perspective, a bad game can be interesting: for example the original Prince of Persia 3D seems to have been largely lost in the sands of time and yet it is an interesting game in its own right despite its flaws, in how its interpretation of (classic) Tomb Raider's controls combined with ideas from the original 2D games around realistic/captured animation ended up creating something inferior to its clear inspirations (but that is clear only really if you know about/have played the games that inspired it and know about/have played the game itself). Of course if you just want to have fun, whatever is available on Steam will more than suffice. reply happymellon 56 minutes agorootparentCompletely agree with this take. What is culturally significant? Things that impact culture more than \"is it a good game\". Advanced Lawnmower Simulator probably has more cultural significance than \"Another Call Of Duty 7\", since I remember it from my childhood and an article gets posted that mentions it at least once a year. Does that make it a good game? Definitely not. reply softfalcon 10 hours agorootparentprevBased on the massively positive response both inside and out of the gaming community‚Ä¶ I‚Äôm gonna say Baldur‚Äôs Gate 3 Seriously, it‚Äôs converting people I never in 100 years would have expected to become gamers. reply Lammy 8 hours agorootparentprevThe problem is that the lockout and anti-privacy tech developed for consoles tends to metastasize over to general-purpose computers, like with signed executables, gatekeeper, secure boot, etc. People would oppose development of it otherwise, like they did for the Clipper Chip and the original Palladium proposal. Having an appliance where software \"\"\"needs\"\"\" to be \"\"\"protected\"\"\" from the machine's owner allows that stuff to fly under the radar until it's already bulletproof. reply yellowapple 8 hours agorootparentprev> OTOH, has there really been any actually culturally significant video games released in recent years? Fortnite certainly seems to be, for better or worse. reply ShamelessC 9 hours agorootparentprevHow recent are we talking about? Did you miss Breath of the Wild? Super Mario Odyssey? The Last of Us 2? Red Dead Redemption 2? Hollow Knight? Elden Ring? Half Life Alyx? Hell, Pokemon Go qualifies for cultural recognition. reply kaashif 8 hours agorootparent> Hell, Pokemon Go I think Pok√©mon Go was a much more visible and impactful phenomenon (in certain places) than anything else on your list. This might be different to \"cultural importance\" depending on your definition. I mean did random churches and libraries have \"Playing Red Dead Redemption 2 is not allowed here\" signs up? Were there flash mobs in NYC playing Half Life Alyx? reply cmur 6 hours agoparentprevI think there‚Äôs some hope in the console market via the likes of things like the Steamdeck and other handheld PC devices. Hopefully they‚Äôll get cheaper as time goes on and compete at the same price point as the switch. reply Cloudef 7 hours agoparentprevAs long as consoles have web browser there is a way. Ps5 was broken recently reply graphe 10 hours agoparentprevThey're all x86 so don't despair. Plus they haven't made a good exclusive since Bloodborne. No Xbox exclusives afaik. Don't worry, gaming peaked a long time ago. There's a huge backlog of great games that are better than their remakes like XCOM. Aside from graphics, I'd say AAA gameplay has even regressed in many ways. Indie games do their own thing and run on PC. reply nebula8804 11 hours agoprevThis soothes my soul seeing exploits for the consoles I grew up with. For a second it takes me back to a simpler time. The homebrew scene for the XBOX was more fun than the console itself! A while back I saw this clickbait video of \"Gen Z trying out the original XBOX\" and man that was a reality check as to how much time has passed. reply cheeze 11 hours agoparentHacking the original Xbox + x360 is what I credit as getting me \"really into computers\" For OG xbox, I was a definition script kiddie but I learned how to use FTP, how to connect via crossover cable, how to solder, the basics of what exploits are and how they work, and a ton of other skills that I use daily. X360 required me to take the damn thing apart (which was terrifying as a 14 year old who wouldn't get a replacement if I broke things). I learned about firmware, SATA connections, the bits (I forget the name) that specify whether something was retail or not, why the exploit wouldn't let you run unsigned code, etc. I was just a broke kid who wanted to watch movies and play games I couldn't afford, but it literally got me into tinkering and hacking, even though what I was doing wasn't much more than \"follow a guide, script kiddy\" reply chadkeck 10 hours agorootparentSame here for the OG Xbox. Lots of TSOP flashes for friends over SSH after booting a saved game exploit for 007: Agent Under Fire. Reading lots of the forums on xbox-scene.com. Learned to solder with a soldering iron I picked up from the local Radioshack. reply nebula8804 10 hours agorootparentprevDitto for me with the OG XBOX. I was in high school and had no clue what I was doing with static ip addresses and crossover cables. We didn't even have a router hence the static ip address. Each cool homebrew XBE I FTP'ed over to my XBOX was another exciting adventure! My computing journey started earlier on DOS but XBOX kept me going through a period of post 9/11 depression where the excitement of childhood and an amazing future was slowly disappearing. I stopped even playing video games after the end of the XBOX lifecycle (despite continuing to buy new gen hardware just so I could take it apart and admire the internals). Now living in the post ChatGPT days and wondering about my future, i'm looking back wishing I could live through those late nights one more time. reply Jerrrry 7 hours agorootparentprevMicrosoft actually just tricked an entire generation of us into learning C# and .NET. They were playing the long-con game. reply samuria 2 hours agorootparentFor me it was creating ‚Äúgift card code generator‚Äù for games like Habbo Hotel and RuneScape in C# and distribute it among my friends to get into their accounts. 14 year old me was proud of my ‚Äúhacking‚Äù skills. reply echelon_musk 6 hours agorootparentprevA +1 for me too. The Xbox modding scene taught me the foundation of my systems knowledge despite already having a Windows PC. reply dadrian 9 hours agoprevThis is awesome. I remember back in 2006 I bought every copy of MechAssault that my local Game Stop had and hoped that one was the correct revision such that it was compatible with the Krayzie Ndure softmod exploit, and used it to install Xbox Media Center (XBMC). It's from that setup that I learned how computer networking works. I credit basically my entire career with reading about softmods at the back of a Popular Science magazine. Anyway, it's awesome that you'll eventually be able to softmod an Xbox without a specific game once the mods are integrated with this exploit. Good work! reply harry8 9 hours agoprevAside: og xbox is not so expensive 2nd hand and a great way to set up a kids lan party. 1 old router, 3 xboxes, 3 tvs, 12 controllers, Halo gets you 10 year-olds' mayhem. Noisy fun. Arguably easier if xboxes are modded. MotoGP, crash team racing, powerdrome, crimson skies all work great https://www.teamxlink.co.uk/wiki/Supported_Games then sort by both total players and players per console descending Is this single location lan something that has been lost since those days? Any better ways to set up a temporary lan party for kids? [1] https://www.aliexpress.com/item/1005005142269614.htm reply Farbklex 2 hours agoparentI have the same use case but with Xbox 360s since they are cheaper and easier to get here in Germany. Haven't found a better \"easy LAN party\" setup than this. A PC gaming LAN setup for 20 year old games is also dirt cheap but mostly requires 1 PC per player. The splitt screen support on PC is unfortunately very bad. reply Jerrrry 7 hours agoparentprevmodded oG xbox's or even X360s are a great living room centerpiece. oG Xbox, Xlink Kai is still a thing. It was my first experience playing online as a poor 12 year old script kiddie myself. For X360 LAN parties, It took a long time to find, but the hard-coded 4ms ping limit for \"LAN\" mode was removed, so LAN parties can be split into neighborhood LAN parties without problems reply karlgkk 11 hours agoprevThis is pretty cool. For anyone interested, it's an int overflow in savegame image handling code. reply TheJoeMan 6 hours agoparentI saw a little pessimism from other comments about a bleak future with fully secured consoles, but I expect image parsing bugs are forever! reply jsheard 12 minutes agorootparentThe difficulty is in turning a userland exploit into a useful jailbreak, there will always be parser bugs and WebKit bugs but those are only the first step in the chain. From there you need to escape the sandbox and that's the part which keeps getting more difficult. The \"unhackable\" Xbox One has had at least one userland exploit via attacking the web browser, but nothing came of it because they weren't able to break out of Hyper-V. reply bradhe 11 hours agoprevHow many things named \"endgame\" are there? reply boomboomsubban 11 hours agoparenthttps://en.wikipedia.org/wiki/Endgame reply HtmlProgrammer 11 hours agoparentprev6 reply bl4kers 11 hours agoprevWhy would I want to do this? It doesn't seem to mention that in the README reply breakingcups 10 hours agoparentThe original Xbox was locked down so only code signed by Microsoft was allowed to run on it. The executables also had a flag specifying which media it was allowed to run from. This meant that you couldn't run a game from a copied disc (different media-type) or the internal hard drive (again, different media-type). There were plenty of hacks that allowed you to circumvent this. Once you get some form of code execution on the Xbox, it's over, everything runs in Ring 0, meaning you get full access. You can use exploitable executables to maintain persistence without having to do any hardware modifications. However, all these methods relied on an initial entry through either: A. Installing new hardware (a modchip) to run an altered version of the Microsoft Xbox kernel that doesn't perform the usual checks. B. Transferring an altered save game to the Xbox through the memory card (internally it's just a USB drive), running the game and loading the save, which triggers an exploit C. Booting the console (which unlocks the hard drive), waiting until the hard drive activity ceases, \"hotswapping\" the IDE cable to your computer, modifying the files, swapping it back and turning the console off so the drive locks again. (If the hard drive loses power, it locks itself again too, hence why the hotswap-which is not supported by the PATA standard obviously-is needed) Now, with this, there is no more reliance on bespoke modchip hardware, aging vulnerable game discs or machines that you can still hotswap an IDE drive to. Which means that Xbox modding has gotten more reliable for the coming years. Which is good news in my book. reply ace2358 4 hours agorootparentOh man! That hot swap method! I remember my computer, running Linux (which freaked 14 year old me out) from a live cd, case open, and plugging in the hdd while the Xbox and pc was on. I felt like a warlock when I could rip my games after that. Great memories. And my mate at school saying there is no such thing as a soft mod! Haha reply geor9e 10 hours agoparentprevI guess the younger generation grew up with unmoddable consoles, so the concept is less known nowadays. The older audience immediately knows that it's for. You can run free homebrew apps, games, and if you really wanted, with a little more code, run pirated copies of retail games, and cheats. If you had a PS1 or Xbox, you got it modchipped by some classmate, everyone did in the 2000s, it was rampant, at least at my school. I still remember bunnie, who was famous online for first hacking the xbox. Hardmods required some soldering, Softmods just required a particular game and flashdrive. This new one just requires a memory card. They didn't fully softmod it for legal reasons but they did the hard part. Since it's such an old console, it's really just a novelty that they found another method. reply graphe 10 hours agorootparentPS3, PS4, Xbox, PSP, DS, vita, switch and maybe PS5 all have hacks. Phones came out and killed homebrew. Now the developers developed cool super compatible stuff without needing you to hack the hardware plus games became stagnant on consoles. The PSP when I bought it was known as a boring rehash console with nothing but re-releases but the hardware made it worth it. reply badsectoracula 9 hours agorootparentConsidering how often ModernVintageGamer uploads videos about new (and old) homebrew hacks, i do not think homebrew is that dead :-P. reply scheeseman486 5 hours agorootparentBack in the 2000s homebrew served a practical purpose, today it's just a novelty. I can run Sega games on a WiiU, sure, or virtually anything else. reply mhh__ 10 hours agoparentprevBeyond the usual (i.e. loading ones own code on something one owns), is \"because it's there\" not sufficient? reply rusk 10 hours agoparentprevIt‚Äôs a great platform for emulating anything up to the original playstation reply mschuster91 8 hours agoprev [‚Äì] > The exploit targets an integer overflow in the dashboard's handling of savegame images. It's truly fascinating how often image format parse bugs end up being the entry point for attackers... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ENDGAME is a universal dashboard exploit for the original Xbox, enabling users to launch a habibi-signed XBE from a memory card, bypassing the need for a game or functioning DVD drive.",
      "The exploit, generated with Python 3 + NASM or through a pre-built zip, allows softmodding any original Xbox revision, facilitating the launch of homebrew XBEs without game restrictions.",
      "It targets an integer overflow in the dashboard's savegame image processing to execute arbitrary code but does not alter security measures or kernel code."
    ],
    "commentSummary": [
      "Discussion on Github about the original Xbox dashboard exploit and hacking modern consoles raises security, piracy, and evolving console security concerns.",
      "Participants delve into the cultural importance of video games, preservation issues, evolving game genres, and game design impact.",
      "Experiences shared include modding consoles, skill development, and LAN party organization, contemplating the practicality and relevance of homebrew hacks in modern gaming consoles."
    ],
    "points": 183,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1708380343
  },
  {
    "id": 39435458,
    "title": "Sony's PlayStation Portal hacked to emulate PSP games",
    "originLink": "https://overkill.wtf/sony-playstation-portal-hacked-emulate-psp-games/",
    "originBody": "Well, that didn‚Äôt take too long. Sony‚Äôs remote-play PlayStation Portal handheld has been hacked ‚Äî meaning it can now be used for emulation, perfect for playing back your PSP collection perhaps? The $200, Android-powered companion device was first launched in November of last year, and since then folks have been curious as to what the device could do beyond its oddly limited PS5 streaming capabilities. The PlayStation Portal, being used as Sony intended Long-time PlayStation hacker Andy Nguyen is one of a handful of enthusiasts who has been working hard on efforts to bring popular PSP emulator PPSSPP to the device ‚Äî and now, following a month long effort, the PlayStation Portal can do more than just stream whatever is playing on your PS5. The PlayStaton Portal running a PSP game via the PPSSPP emulator Yep, according to Nguyen the Portal can now run the PPSSPP emulator, meaning you can theoretically spin up Lumines or some Vice City Stories on the curious handheld. Currently the trio behind this hack state that \"there's no release planned in the near future\" adding that there is still \"much more work to be done\". Whether we can easily replicate this or not, it‚Äôs good to know it's possible all the same. üíå Did you find this post helpful? We publish a free weekly newsletter where we feature a mix of content from overkill.wtf, links to exciting gaming reads we found across the internet, and more. Sign up here or read it first. How the surprisingly delightful PlayStation Portal redefined my gaming limits Here‚Äôs how Sony‚Äôs PlayStation Portal remote play handheld delivers a flawless PS5 experience anywhere. overkill.wtfChelsea Beardsmore The Best Android Gaming Handhelds What options do you have when it comes to portable Android gaming? We take a look and share our recommendations for the best Android handhelds. overkill.wtfBen Kuchera Via: Liliputing Source: Andy Nguyen on X",
    "commentLink": "https://news.ycombinator.com/item?id=39435458",
    "commentBody": "Sony's Playstation Portal hacked, can now emulate PSP games (overkill.wtf)177 points by rcarmo 11 hours agohidepastfavorite75 comments zerocrates 10 hours agoWhy not just sell something like the Portal as open in the first place? There are no Portal games to worry about protecting. What's the percentage in locking it down? reply jsheard 10 hours agoparentIf they're selling it at cost or at a loss (I don't know if they are) as a value-add to the PS5 then they wouldn't want people buying it just to play emulators. reply RajT88 10 hours agorootparentNotably they still sell retro games on PSN. As long as retro sales exceed the cost to lock the device down you will see stuff like this. I recall one time paying for FF7 which I played on PS3/PSP. That game alone probably makes it worth it. reply xp84 2 hours agorootparentI‚Äôd modify that to ‚Äúas long as the IP owner wishes retro sales exceeded the cost to lock the device down.‚Äù After all, lots of games are completely unpublished today, making $0 in sales, but they‚Äôre just as zealous about stopping you from accessing or playing those ones too. reply mjevans 27 minutes agorootparentA reasonable compulsory license fee (all paid to the copyright office, none to the corp) and/or a revocation of copyright and entry to public domain for failure to maintain published access would be good sticks. reply Farbklex 2 hours agorootparentprevAnd the retro games on PSN are very dissapointing. I would love to have Tekken 3 on the PS 5 just because it is convenient. Instead they only offer Tekken 2, which is only available via the PSN subscription. reply nightowl_games 9 hours agorootparentprevNice. I played they FF7 on my hacked PSP. Hacked PSP was a great time to be a gamer. Played through all the greats I missed like Harvest Moon, a bunch of Final Fantasy, some mega man. Nostalgia overload thinking about it. reply Talanes 8 hours agorootparentPlus it was cool to be around for a time when the hack to get boot access was as simple as popping the battery pack open and penciling in one line on the PCB. reply jsheard 8 hours agorootparentAs I recall Sony just forgot to enable the signature checks on the PSPs 1.00 firmware, and they could be bypassed on 1.50 by giving the executable a malformed filename. Simpler times. reply pmalynin 6 hours agorootparentMy favorite was the tiff exploit to install virtual CFWs by scrolling through an album of digital chickens. Good times reply spike021 7 hours agoprevNews like this makes me so nostalgic for the PSP homebrew and \"hacking\" scene of the 00's. It's what got me into software in the first place. reply conradev 7 hours agoparentSame! The PSP was the first mobile device I used with WiFi and a web browser. reply cultofmetatron 4 hours agoprevIts a cool hack but I dont' see why anyone would go through the trouble when you can do the same with a steam deck (no hacking required) there's also the neo which will come out https://neo.manjaro.org/ all the same benefits and without having to play this cat and mouse game with a company who just wants to turn you into a stream of income. reply anonymousab 2 hours agoparentMaybe they just like the specific form factor or Sony controller more than the steam deck's. reply Larok00 1 hour agoprevI still have my original PSP with its discs, and it works like a charm. The build quality is very impressive. I can use it as a plug-in controller for my PC. reply colonwqbang 10 hours agoprevWhat's the market for a (non-hacked) device like this, when the Switch and the Deck already exist? Surely the appeal of a handheld is that you can bring it with you on the train, airplane, etc. What am I missing? reply solardev 8 hours agoparentI have Logitech's version of this (https://www.logitechg.com/en-us/products/cloud-gaming/cloud-...), one of the earlier entrants into the field, and I use it both at home and when traveling, though only when there's good wifi (so no trains or planes). In conjunction with GeForce Now, it's better than the Steam link in a few ways for my use case: better screen (1080p and slightly bigger), quite a bit lighter and easier to hold, no fan noise, no heat, much better graphics (since it's not running on a local APU, but a 4080 in the cloud). At the time, it was also significantly cheaper than the Steam Deck, although that's gotten cheaper over time too. There are also a bunch of similar $200 ish streaming Android handhelds if price is the primary consideration. There are a few downsides, though: Its control inputs are way cheaper than that of the Steam Deck and it feels like crap and has no gyro input. It can't run local games (obviously) except cheap Android junk like Torchlight Infinite. It's co-developed with Tencent, which means China is probably spying on all your games (and your Google login). But overall... it's fine for what it is? I mostly just play Path of Exile on it (and hopefully Diablo 4 soon) before falling asleep in bed. I actually got a Steam Deck at one point, tried it for a couple days, and then sold it. It just wasn't as ergonomic as the GCloud for casual play. It was a pain waiting for the OS to update, games to patch and then slowly load, etc. reply rcarmo 57 minutes agorootparentYep. I got one recently too: https://taoofmac.com/space/blog/2024/02/03/2000 But you're wrong about the gyro input. I have it working with Yuzu -- I am very happy with it, and am running Steam on Bazzite (https://bazzite.gg) inside Proxmox. reply MBCook 10 hours agoparentprevYou could use it remotely, but I doubt it would feel great due to latency and bandwidth. In home, a great use is to play when the TV the PS5 is hooked up to is in use. You can play and someone else can watch a movie. Also the Switch and Deck don‚Äôt play PS5 games you already own. And both are more expensive. It‚Äôs 100% a companion device. If you owned a deck you could just run the PS remote app on it too. reply jonhohle 9 hours agorootparentThere‚Äôs at least one open source remote play client that could work on the Steam Deck, and the official one if booted into windows. reply ShamelessC 9 hours agorootparentIndeed I recently replayed The Last of Us Part 2 (PS4 Pro) using chiaki on my Steam Deck. Software also supports PS5. reply hiccuphippo 9 hours agoparentprevI thought it was to play PS5 in the bathroom. reply bebna 3 hours agoparentprevIf you mean emulation on handhelds: Multiple vendors with multiple new models per year, different quality and price levels. Some come with Linux based systems others with android or even Windows. So you can choose between a selection of models for roughly 50-100USD, 150-300 or from a section that is even more expensive than a steamdeck but also provide more on some or all fronts for they bigger price tag. There are some devices that are basically the same concept, just in new generation, but there are also many that are more unique. The space seems to require it, button layouts or screen ratio / resolution play a mayor role how well the emulation experience is, but there are so many different consoles over the years that they can't just do one thing fits all, especially if they want to keep it a handheld or sometimes pocketable. There was just a windows device released with clamshell dual screen design like the DS, but thicker to run basically everything on a modern AMD Windows setup. The portal wasn't an unknown device in that bubble, bc some people use there emulation handhelds for streaming, as in in-home streaming or like the nvidia shield, cloud gaming streaming, often Xbox game pass these days. There is a whole section of controllers that make a smartphone a portal/switch/steamdeck like device, by holding it in the middle and connect via usb c, lightning or Bluetooth. There is a good set of emulators for phone, snapdragon is quite good in emulating switch for example, but there is a also next to streaming, android native games that support controllers or apps that produce virtual touch inputs to map the controller. There is also a scene that makes handhelds like the DSi, psp, vita, etc emulators who work quite well if you want to put in the effort to set it up. I recommend retro game corps on yt for a start, he doesn't do quick bait or mind numbing flash cuts. reply pjmlp 1 hour agoprevAs usual, it ends up being about emulators instead of creating new stuff. reply makin 1 hour agoparentThere isn't much incentive in making new stuff when it can be shut down by Sony or whatever company at any moment. See A2MR. reply dvdkon 1 hour agoparentprevWhy create new games for a proprietary platform that users would have to hack instead of just writing it for PC? Homebrew like this is about the challenge or about making existing hardware more useful, porting emulators is an effective way to do that. reply pjmlp 1 minute agorootparentWhy buy this in first place then? Just use a PC and save the money for something that really matters. reply hnlmorg 10 hours agoprevThe moment the hack is public Sony will release an update to block it. And so the game of cat and mouse continues. reply jsheard 10 hours agoparentDepends on the nature of the jailbreak, some of them are unpatchable if they attack the system early enough in the boot flow. Those are much less common than they used to be though, the Switch is obviously a notable example of a permanent jailbreak but it's been some time since Sony or Microsoft made a blunder like that. OTOH who knows how much of the Portal was actually designed by Sony, since it's running Android rather than SIEs usual FreeBSD it might have been outsourced or derived from some generic ODM design. reply solardev 8 hours agoparentprevIt's stupid that in 2024, Sony is still like this :( Meanwhile Microsoft has gone the other direction, opening up many Xbox games for crossplay/cross-save with Windows, partnering with GeForce Now to offer Game Pass games there (in addition to their own xCloud, which is total crap by comparison because it streams from actual Xbox consoles), and they don't care where you end up streaming it to (including web browsers). reply hx8 7 hours agorootparentIn 2023 I looked into these options. The Game Pass games are not compatible with proton and they did not support streaming to Firefox running on Linux. It wasn't exactly an open ecosystem. reply solardev 7 hours agorootparentNot open, but more so than Sony's. Also, have you tried GeForce Now? Not sure if it works on Linux/Firefox. It runs on ChromeOS at least, so if you're okay with that distro, it ought to be a much better experience than GamePass (but only a subset of GamePass games are supported). reply rcarmo 56 minutes agorootparentIt does, with a little tweaking. reply hx8 7 hours agorootparentprevGeForce Now does not support Firefox. GamePass also works on ChromeOS. reply solardev 6 hours agorootparentIt's so weird. I thought they were just streaming a video; wonder what specifically Firefox doesn't support? reply doubled112 10 hours agoparentprevI did enjoy this cat and mouse game when I got my first PSP. That was quite a few years ago. reply scheeseman486 7 hours agorootparentThat wasn't so much a cat and mouse game, rather a bunch of hackers playing keep away with Sony's hat. The PSP's security was \"defeated\" literally from day one since the v1 firmware launched unsigned executables, then after that there was exploit after exploit, then the whole pandora battery thing and finally the host keys got leaked and it was game over. I bought my PSP at launch and it spent the vast majority of it's lifespan with some kind of custom firmware installed. reply alias_neo 2 hours agorootparentPSP was the golden age for me. I started university the year it launched in the UK but I already had a Japanese import PSP1000 I'd bought the year before. Studying CSEE a few friends and I were all over the hacking and homebrew and wrote a few apps/tools ourselves. We spent all of the boring lectures gaming on our PSPs, modifying them, dumping and swapping games etc. Later we got Compaq iPaQs and modified those, installed Linux etc and dreamed of a Linux phone, then came the first iPhone, I imported one of those the year before it released here (UK) and everyone who saw it in the EE department were amazed. Then of course came Android, the dream came true. Fast forward over a decade and Google has well and truly put a bolt through the head of that dream. It was a different time in my life, but I definitely feel it's (tech/gadgets) all a lot less interesting these days. reply jimmyl02 10 hours agoparentprevthis will always be true of jailbreaks as they fundamentally break the security of the device. this means that it can be used for cool things like shown in the article but it could also be used maliciously by an attacker to completely take over the device which the manufacturer should definitely patch. reply brokenmachine 5 hours agorootparentAfter the last update to my expensive DSLR broke third-party batteries for no reason beyond pettiness and greed, at this point I trust random hackers more than I trust Sony. *the battery ruining was not mentioned in the changelog for the update. reply hnlmorg 1 hour agorootparentprevthese exploits are never closed for the security of their users, though the changelogs might often claim that‚Äôs the case. It‚Äôs always about licensing costs for the manufacturer. Every game or application released for consoles (and phones alike) have to pay a licensing fee to the maintainers of that platform to publish on that platform. If you can jailbreak the device then you can publish software to that device without paying Sony, Apple or whoever else. The security aspect is just a convenient side effect from locking people in. But if it was only security that these manufacturers were concerned about then they‚Äôd have found a way to allow people to tinker with their hardware without needing to crack any security mechanisms in the first place. You can see this concern for licensing being a big thing in consoles as early as the Gameboy and MegaDrive (Genesis). And back then security wasn‚Äôt even a concern. reply extraduder_ire 4 hours agorootparentprevIt's a shame that such exploits are needed in the first place, when allowing developer mode to be toggled like a normal android device (ideally easier than on an xbox/quest) would be enough for most of these cool things. It's also a strangely perverse incentive to stay on an exploitable firmware if you want to hack your own hardware. reply coretx 9 hours agoparentprevThat might not be true since there is no need for Sony codeif you can replace it with AOSP & the Android remote play client... reply ejj28 9 hours agoprevIf running a moonlight client on one becomes possible, I might be actually interested in buying one, just for casual gaming at home. The Steam Deck looks great but is pretty expensive and a little overkill when I could just stream from my desktop to a less powerful device. reply solardev 9 hours agoparentFYI you can probably just use a Logitech GCloud (https://www.logitechg.com/en-us/products/cloud-gaming/cloud-...), Razer Edge (https://www.razer.com/mobile-handhelds/razer-edge), Abxylute (https://abxylute.com/products/abxylute-handheld-console), or any generic Android portable gaming thing to do that. Not sure if you even need Moonlight; wouldn't Steam Link work too? Personally I use a GCloud and stream from GeForce Now, and that works great. reply rcarmo 56 minutes agorootparentThat's what I do too: https://taoofmac.com/space/blog/2024/02/03/2000 reply ejj28 7 hours agorootparentprevThe GCloud and the Razer Edge are both significantly more expensive than the PS Portal if I recall correctly though. The Abxylute does seem similar in price, although I'm guessing the PS Portal likely has better controls (being essentially a Dualsense split in half) and maybe a better screen. Also yeah, Steam Link would work too but Moonlight/Sunshine generally seems to work better. reply rcarmo 55 minutes agorootparentYou can get second-hand G Cloud devices on eBay without a lot of trouble - that's what I did, and it was pretty cheap considering the excellent screen: https://taoofmac.com/space/blog/2024/02/03/2000 reply solardev 6 hours agorootparentprevI see. Yeah, $200 seems like a great price point for that device and build quality. Too bad it's locked to PS games! Just Sony being Sony, sigh. reply yurishimo 2 hours agorootparentIt doesn't even play games though; it streams them from your console over wifi. reply ErneX 2 hours agorootparentprevIt‚Äôs a streaming device to play the games from your PS5 remotely. Just an accessory. reply radiKal07 1 hour agoparentprevJust get an Odin 2: https://www.ayntec.com/collections/odin/products/odin-2 reply Shad0w59 8 hours agoparentprevThere are many devices that can do this that are cheaper than a Steam Deck and have great screens and controls. They come in all shapes and sizes. I'll recommend Retro Game Corps as a great gateway into this scene: https://www.youtube.com/c/retrogamecorps If you just want a huge list of what's available: https://docs.google.com/spreadsheets/d/1irg60f9qsZOkhp0cwOU7... I'll plug my own channel as well as I own many of these devices: https://www.youtube.com/@forthenext reply ejj28 7 hours agorootparentVery impressive spreadsheet, I appreciate the link. reply jokoon 9 hours agoprevI would pay money to play a silent hill remaster no other choice than to emulate it, it seems reply a_gnostic 9 hours agoparentnext [‚Äì]reply gentleman11 8 hours agoparentprevI‚Äôd love to play silent hill 1 remastered. It was more concrete than 2, and felt more real and meaningful because of it. Also, didn‚Äôt finish it, since it was a rental Give us rentals back while you‚Äôre at it, if the larger industry is reading this reply KryptoKnight 1 hour agoprevSweet reply spiritplumber 10 hours agoprevCave Johnson, we're done here reply a_gnostic 9 hours agoprevWhy did it have to be hacked to do this? reply kotaKat 8 hours agoparentBecause Sony won't let you run your own code on a product you paid good money for. reply bitwize 8 hours agoprevFirmware patch to negate this in 3, 2, 1... reply musha68k 9 hours agoprevCool that they've at least got something out of that NOLED device. reply PlutoIsAPlanet 10 hours agoprevnext [2 more] [flagged] pfundstein 10 hours agoparentGo back to reddit reply yieldcrv 9 hours agoprevthe audacity, of Sony reply Fire-Dragon-DoL 10 hours agoprev [‚Äì] I like the concept of the playstation portal, but given its cost similar to a steam deck, wouldn't be better to use that for this purpose? reply zerocrates 10 hours agoparentThe Deck is twice the price of the Portal, at minimum. Of course it's also vastly more capable. They're really not competitors, except for the Deck's use case as a streaming sink from a PC; this is more a \"you've got these nice controls and screen, sure would be nice if you could use them for more\" situation. reply cooper_ganglia 10 hours agorootparentYou can buy a refurbished 256 GB Steam Deck for $319, and there‚Äôs an app to easily use Remote Play with your PS5! For not too much more money than the PS Portal, you can also download your entire Steam collection + non-Steam games (Minecraft w/ mods + shaders), and have a fully-functioning Linux machine! Personally, I‚Äôm very confused on who exactly the PS Portal is for, but the only important thing is that people are having fun with their games! :) reply timfsu 9 hours agorootparentI returned the Steam Deck when I received it - it was larger than expected and just didn't feel ergonomic. The Portal on the other hand almost feels like holding the DualSense. It's a nice, though limited, device. reply jwells89 9 hours agorootparentprevThere‚Äôs savings to be had over time on Steam sales too, where even AAA titles eventually become dirt cheap. With new games now costing $70, it doesn‚Äôt take long for the price difference between a Deck and a Portal (or other similar streaming device) to pay for itself. If all I wanted was a PS Portal-like experience I think I‚Äôd get one of those USB-C phone game controller things to put my smartphone in, which would be cheaper (albeit with a smaller screen), be able to stream from more than just a PS5, and would get better every time I upgrade my phone. At $200, without hacks the PS Portal is a bit too much of a one-trick pony. reply cooper_ganglia 6 hours agorootparentI pick up more games every Steam sale that end up getting put into The Backlog‚Ñ¢. But when you can buy the entire Arkham series for $10, how can any reasonable person say no?? In all seriousness, Steam sales are the only reason I‚Äôve been able to try out so many new games I‚Äôd otherwise not be able to afford, unless I was using Game Pass or something. I grew up on consoles, and didn‚Äôt even get into PC gaming until adulthood, but man, Steam has saved/cost me so much money, haha reply johnnyanmac 8 hours agorootparentprev>For not too much more money than the PS Portal 60% more money is signifigantly more. That was about the price gap between the 3DS and Vita and we saw how that ended (even though Vita hardware was far superior). And of course we're comparing used tech to brand new MSRB price as well. reply dotnet00 6 hours agorootparentTo be fair the bigger issue with the Vita was that Sony didn't really support it with games the way the PSP got. I remember watching E3 every year back then and being disappointed that they spent barely any time on Vita stuff. The price of the console itself would've been fine if the memory cards were cheaper (or if they had just used a regular standard) and if there were as many good exclusives as the PSP had. It basically just ended up being an indie and niche weeb gaming machine. I guess it was kind of foreboding of the then upcoming console generations. reply MBCook 9 hours agorootparentprevPlenty of PS people don‚Äôt have a Steam library. So that‚Äôs not a benefit. reply ThatPlayer 9 hours agoparentprev [‚Äì] Playstation Portal is still better at PS5 Remote Play. Compared to the Deck it's got a 1080p vs 720p display. It's got a controller with all the Dualsense features like resistive analog triggers and fancy rumble. If the only usage a person is looking for is PS5 Remote Play, then it is superior. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The PlayStation Portal handheld, known for remote play, has been hacked to run PSP games using the PPSSPP emulator, thanks to hackers like Andy Nguyen.",
      "This hack expands the capabilities of the device beyond streaming PS5 games to allow users to enjoy playing PSP games on it.",
      "Although no official release has been announced, the potential to run PSP games on the PlayStation Portal is now achievable, opening up new gaming possibilities."
    ],
    "commentSummary": [
      "Users are discussing hacking and emulating games on Sony's Playstation Portal, considering alternative handheld gaming devices like the Steam Deck and the Neo for better graphics and portability.",
      "Nostalgia for past hacking scenes on devices like the PSP is expressed, along with discussions on challenges and risks of jailbreaking and homebrew on proprietary platforms.",
      "Recommendations for devices and resources are shared, comparing gaming devices such as the Steam Deck and the PSP in features, cost, and game library, emphasizing the value of alternative platforms for gaming experiences."
    ],
    "points": 177,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1708380752
  },
  {
    "id": 39428640,
    "title": "GlobalFoundries Secures $3.1B for NY, Vermont Chip Plant Expansion",
    "originLink": "https://www.syracuse.com/business/2024/02/globalfoundries-wins-31-billion-in-chips-act-subsidies-for-upstate-ny-vermont-expansion.html",
    "originBody": "An aerial view of the GlobalFoundries computer chip plant in Malta, N.Y. The company wants to triple its production capacity by building a second plant.GlobalFoundries photo By Mark Weinermweiner@syracuse.com GlobalFoundries would receive $3.1 billion in federal grants and loans to expand its Upstate New York computer chip plant and modernize a factory in Vermont, according to a preliminary deal the White House plans to announce today. The massive package of subsidies would be the largest award to date to a manufacturer competing for aid from the $52 billion federal CHIPS and Science Act. GlobalFoundries would receive $1.5 billion in grants and $1.6 billion in federal loans under a preliminary, non-binding agreement, according to the U.S. Commerce Department. In return for the subsidies, GlobalFoundries said it would invest about $12.5 billion in the projects, most of which would be spent to expand the company‚Äôs Fab 8 in Malta, in Saratoga County. The big subsidies for GlobalFoundries offer a preview to the kind of announcement that President Joe Biden‚Äôs administration is expected to make soon for Micron Technology, which plans to build a much larger complex of computer chip plants in Central New York. The GlobalFoundries expansion would create up to 1,500 manufacturing jobs and 9,000 construction jobs over the next 10 years, federal officials said. Most of the new jobs would be created in Upstate New York, where GlobalFoundries is building a new plant to triple production capacity, said U.S. Sen. Charles Schumer, D-N.Y. The jobs would be on top of the 2,500 at the company‚Äôs existing plant in Malta, he said. GlobalFoundries is one of only four companies in the world outside of China ‚Äì and the only one in the United States ‚Äì with the capacity to operate large-scale foundries that produce semiconductors used in both consumer and military applications. The Saratoga County plant produces chips considered vital to national security, supplying more than 200 customers. GlobalFoundries has signed a long-term deal to produce chips at its Upstate New York campus for Lockheed Martin, the nation‚Äôs largest defense contractor. The plant also has a long-term agreement to supply chips to General Motors for its cars and trucks. ‚ÄúThis is a huge day for not just Upstate New York and the Capital region and New York state, but for America as well,‚Äù Schumer said in a White House conference call with reporters. ‚ÄúBy doing this, we will never have the kind of (chip) shortages we once had.‚Äù U.S. Commerce Secretary Gina Raimondo said the Saratoga County plant will make ‚Äúhigh-value chips‚Äù that are currently not made in the United States. The 300-millimeter chips are essential for national security because U.S. defense companies and auto manufacturers have no other domestic sources, Raimondo said during the call with reporters. ‚ÄúI can‚Äôt emphasize how important this is,‚Äù she said. ‚ÄúAs we all remember, very recently during the pandemic we faced a shortage of chips, which led to shutdowns and layoffs and furloughs of thousands of hard-working Americans in auto manufacturing sites all across the country.‚Äù As part of today‚Äôs agreement, GlobalFoundries would receive a $10 million grant to expand a workforce development program in Saratoga County for manufacturing and construction workers, a Commerce Department official said. Schumer said the deal includes a commitment from GlobalFoundries to continue to provide a $1,000-per-employee annual subsidy for childcare and to extend the benefit to construction workers who build the new plant. The massive financial deal is a milestone in the CHIPS program that could provide clues as to how a deal would be structured between Micron Technology and the federal government for a complex of chip plants in Central New York. Micron applied to the U.S. Department of Commerce in August for subsidies to support construction of up to four manufacturing plants at White Pine Commerce Park in the town of Clay. Micron said it would invest up to $100 billion in the plants over 20 years, creating up to 9,000 direct jobs and up to 40,000 more construction and spinoff jobs in other industries. If GlobalFoundries receives a $1.5 billion federal grant for a roughly $12 billion investment, Micron might expect subsidies totaling at least $10 billion for its $100 billion project if the same percentages are applied. Those subsidies are expected to come as federal grants, loans and loan guarantees. Other big semiconductor companies could be in line for similar financial incentive packages. Intel, which plans to invest up to $100 billion in an Ohio project similar to Micron‚Äôs in Central New York, is negotiating with federal officials on a deal for $10 billion in loans and grants, Bloomberg reported Friday, citing sources it did not identify. Separately, the Treasury Department is in the process of establishing rules for the companies to apply for a 25% investment tax credit. The tax credit would apply to construction and equipment costs. Under the CHIPS and Science Act authored by Schumer, the Senate majority leader, the limit for grants is $3 billion per project. But those limits could be waived by President Biden if officials determine the project is essential for national security. Micron and Commerce Department officials have declined to discuss the status of secret negotiations over the company‚Äôs application. Raimondo said earlier this month that she expects her department to announce several large CHIPS grant awards over the next six to eight weeks. The grants for GlobalFoundries would come from a separate $2 billion fund in the CHIPS Act set aside for legacy chips, older generation computer chips used in large quantities in autos, airplanes, home appliances and other consumer electronics. Micron makes leading-edge or cutting-edge memory chips that are more sophisticated and used in smartphones, data centers, quantum computing and artificial intelligence applications. The CHIPS and Science Act set aside $39 billion in subsidies for leading-edge chip subsidies. Other leading-edge chip makers include Intel, Samsung and Taiwan Semiconductor Manufacturing Co. ‚Äì all of which have committed to building new plants in the United States. All told, the Commerce Department says it is negotiating with about 160 large companies who have applied for grants and loans. An additional 600 companies have submitted statements of interest in CHIPS funding. Schumer said he wrote the CHIPS and Science Act with the idea of transforming Upstate New York into a global hub for semiconductor manufacturing, research and development. A series of chips projects are underway in the Thruway corridor stretching from Albany to Buffalo. Some industry officials have complained about the slow pace of the negotiations for federal subsidies. Until today, only two companies had struck preliminary deals for smaller amounts of CHIPS Act money. BAE Systems would receive up to $35 million in incentives to modernize its plant in New Hampshire. Separately, Microchip Technology Inc. agreed to a deal for about $162 million in subsidies to support expansions of plants in Colorado and Oregon. Earlier this month, Intel said it would delay construction on the $20 billion chip manufacturing plant it started to build near Columbus, Ohio. Intel blamed the decision on slowing demand for its chips and delays in securing federal grants from the CHIPS Act. The company is building the first two of its four planned chip plants near Columbus. READ MORE ABOUT MICRON TECHNOLOGY IN CLAY Granting tax breaks for all housing is irresponsible public stewardship (Your Letters) Proposal to expand tax breaks for new housing in Onondaga County receives pushback Micron offering paid summer internships for Central NY college students Cost to build sewer line in Clay more than doubles ‚Äì and it‚Äôs not because of Micron China‚Äôs open-source chips threaten US semiconductor push (Guest Opinion by David Paterson) Got a tip, comment or story idea? Contact Mark Weiner anytime by: EmailTwitterFacebook571-970-3751 If you purchase a product or register for an account through a link on our site, we may receive compensation. By using this site, you consent to our User Agreement and agree that your clicks, interactions, and personal information may be collected, recorded, and/or stored by us and social media and other third-party partners in accordance with our Privacy Policy.",
    "commentLink": "https://news.ycombinator.com/item?id=39428640",
    "commentBody": "GlobalFoundries wins $3.1B in CHIPS Act subsidies for NY, Vermont (syracuse.com)163 points by geox 22 hours agohidepastfavorite155 comments beezle 12 hours agoFrom Vermont reporting - \"Meanwhile, part of the money will be used to modernize the Essex Junction campus. Officials say the upgrades will enable the Vermont operation to become the first high-volume manufacturing facility capable of making gallium nitride semiconductors, which are used in electric vehicles, power grids, smartphones and more.\" reply csomar 21 hours agoprev> The GlobalFoundries expansion would create up to 1,500 manufacturing jobs and 9,000 construction jobs over the next 10 years, federal officials said. So assuming the construction jobs are temporary and the manufacturing jobs is what remains, this works out at around $2.066 million per employed personnel. Assuming a federal income tax rate of 20% and a 30 year tenure, this means the personnel should have a take home pay of $344k/year. And this is just for the scheme to break-even. I think this IRA (the act) and CHIPS act are going to be just another wealth transfer where the \"work\" being done just to check boxes to be eligible for the money getting sloshed around. reply Nevermark 21 hours agoparentThe point of all these breaks is to alleviate a critical dependency on two foreign advanced chip makers. Especially one that is under military threat by a huge political economic and military rival. Losing out on a big percentage of advanced chip making capacity could be catastrophic for the US economy and technological advancement. And military. This is not an \"investment\" in future tax revenue via jobs, although jobs are nice. reply csomar 20 hours agorootparentI understand the point of these subsidies. I am just doubting (a personal opinion) that they'll actually work and calculating the wealth transfer happening here. reply myrmidon 19 hours agorootparentDo you think that the gain from this (investment into Saratoga Fab) is useless or that the state pays too much for it? Because I personally believe that this is a decent deal; I hate the idea and it still reeks of protectionism to me, but I think this serves US (taxpayers!) interests decently well. I especially like the investment matching aspect, where the necessary commitment from the company is much larger than the grants, so in the end, they are dropping money into that fab (fudging numbers only gets you so far), and its in their best interest to make it work out. I'd be very careful with characterising this as \"wealth transfer\", because it seems significantly less questionable than most defense spending to me and even that I wouldn't label as \"wealth transfer\". Out of personal curiosity: Are you similarly opposed to agriculture subsidies, or do you consider those less problematic (magnitude is somewhat comparable at ~20billion$/year)? reply mschuster91 18 hours agorootparent> Out of personal curiosity: Are you similarly opposed to agriculture subsidies, or do you consider those less problematic (magnitude is somewhat comparable at ~20billion$/year)? European here, we could and should close our borders for agricultural goods that we can grow ourselves (and you Americans should ban alfalfa exports from California, while you're at it). Ag subsidies are only required because there's no way an US or European farmer can compete with Russian, Ukrainian, South American or African labor and land costs, but we still need farmers to ensure we're self-sufficient and don't end up like many developing nations did once Russia blockaded Ukrainian grain exports. reply nyokodo 14 hours agorootparent> there's no way an US or European farmer can compete with Russian, Ukrainian, South American or African labor and land costs For certain agricultural goods perhaps, but the Midwest is the most productive region for agriculture on the planet. Russia and Ukraine have vast areas of land, they focus on wheat because they mainly have marginal land and a limited growing season and wheat is perfect for that. Most of Africa and much of South America (Argentina being a notable exception) have terrible soil and require significantly more fertilizer to grow crops at scale. Labor is cheaper in those regions but that‚Äôs only one factor and it‚Äôs why so much farm labor has been automated in the US. reply myrmidon 16 hours agorootparentprevI agree with you in principle, but I believe this is basically impossible in any democracy because you just get un-elected immediately. Just consider the Polish protests against Ukrainian grain, or the recent German protests against cutting fuel subsidies for agriculture. It just costs too much political capital to do anything about this... reply user_7832 17 hours agorootparentprevThat is an interesting take. If I'm not mistaken in India at least (which one would imagine has fairly cheap labour) sugar and corn is still cheaper imported, although in sugar's case I think the production costs are higher due to water requirements. The sugar import situation had become highly political some years back. reply tuatoru 17 hours agorootparentprevAs a New Zealander: please don't. reply delfinom 15 hours agorootparentprevWell, agricultural subsidies are also needed because agriculture has become too efficient. The dairy industry is prime example in both US and EU of agriculture that overproduces so much milk that the US has farmers demanding we sanction Canada for their milk quotas (Canada using the quotas on all their farmers to prevent overproduction) and the EU dumps their milk on Africa and prevents any development of the dairy industry in Africa. The same applies to so many other types of agriculture. Nobody wants to let farmers go out of business for decades now in the Western world. And the rate of \"natural attrition\" hasn't matched the rate of technological efficiency improvements. There is of course agriculture that isn't as efficient because higher human labor required for things like picking and sorting. But the automation for that is slowly coming. reply earthwalker99 17 hours agorootparentprevIf only it were protectionism! reply myrmidon 16 hours agorootparentWhich parts of the CHIPS act would you like to see changed? I personally think that its quite difficult to make protectionism happen in a way where you don't prop up an inefficent industry with continuous infusions of tax dollars in the end... reply graeme 15 hours agorootparentprevYour comment doesn't address the parent comment's point though. You'd have to make an argument that the chip factories won't get built at all. reply KoftaBob 19 hours agorootparentprevWhy do you doubt they'll accomplish their goal of removing dependency on foreign chip makers? reply rusticpenn 19 hours agorootparentGlobal Foundries promised 7nm tech to IBM and dropped it after winning the contract. https://www.datacenterdynamics.com/en/news/ibm-sues-globalfo... reply phone8675309 19 hours agorootparentprevNot parent poster, but there's a long track record of these sorts of subsidies to big tech companies already rolling in money to go bust with little oversight, investigation, or prosecution. The big one everyone remembers is the set of grants giving to incumbent providers to expand access to broadband (25 Mbps down/3 Mbps minimum) to everyone in the country - a promise that, despite three rounds of grants in the billions, has not materialized.[1] [1] https://www.jsonline.com/in-depth/news/2021/07/14/weve-spent... reply bee_rider 19 hours agorootparentChips and telecom are pretty different markets, right? GlobalFoundries needs a leg up, most of the big telecom companies are already abusive monopolies. reply csomar 19 hours agorootparentprevBecause they do not fix the underlying causes; and also they create perverse incentives. Most companies are in the business of extracting money and will see this as such: an opportunity to extract money and the product is bureaucracy checkboxes. It would make more sense, in this personal view, to restrict sales to domestic production only and make sure the field is even for competitors to out-compete one another. This guarantee the dependency aspect at least (as you already restricted yourself!). Another solution: Move the manufacturing to a near-shore and friendly country (ie: Mexico or Puerto Rico) and keep the design part in the US. But of course, that wouldn‚Äôt be as politically popular and wouldn‚Äôt create the extraction opportunity (which lots of companies will want to benefit from). reply dragontamer 18 hours agorootparent> to restrict sales to domestic production only You think people will stop buying NVidia GPUs, AMD GPUs, PS5, XBox, Nintendo Switch, iPhone, Pixels, Samsung Galaxies, and a litany of major automotive parts from Microchip/STMicro/etc. etc.? Because none of these are made in the USA. The only major chips made here are Flash/RAM by Crucial, CPUs by Intel, and some lower-tech (but still important) chips by TI. The vast majority of chips, well over 60% of the worldwide supply, is made by TSMC IIRC, and another huge chunk are made by other companies located in Taiwan. -------------- Furthermore, Taiwan is a close ally who has extremely friendly relations with USA. Yes, there's a war-risk with China here, but Taiwan fairly competes with us. I don't think anyone has a single piece of anti-Taiwan hate inside of them. Its not a protectionist thing. Its purely the \"what if China attacks Taiwan\" thing. Economically speaking, trade with Taiwan and USA is a good thing. Geopolitically, trade with Democracies is a good thing, and trade with a century+ old friendly nation is a good thing. ------------- Taiwan can create this tech because they are smarter than us and outcompeted us on this subject. Its not \"low tech\", its extremely difficult chemistry, high-end PH.d stuff here. Taiwan is basically the Vulkans from Star Trek. They're smaller, more technologically advanced, but weaker militarily. They're so obviously good allies that its unthinkable to do anything against them. We need a \"just in case\" plan if trade between our nations gets interrupted because chips are a strategic good, but... cutting them off would mean that they ally with someone else and that's even worse. reply vel0city 17 hours agorootparent> some lower-tech (but still important) chips by TI TI makes plenty of pretty high-tech chips, just not necessarily bleeding edge digital logic processors. Their real high-tech stuff are analog devices, something far less sexy to people on HN but still incredibly crucial. reply dragontamer 15 hours agorootparentAnalog is important for cutting another 20% off of MOSFET resistances or whatever, which means even more efficient electric engines or other electric components. Yeah, important but not as much of a headline as 20% more GHz or FLOPs for some reason. reply therealbilly 15 hours agorootparentprevNot really. It‚Äôs because the government of Taiwan poured a lot of money into these chip companies. Which is the smart thing to do. reply dragontamer 15 hours agorootparentA lot of countries shoved tons of money into chips. In fact, a ton of countries have more money than Taiwan. And yet, it's Taiwan who won. We gotta give them props for that, and we can also thank the gods that it was a close ally of the USA who got so good at it. reply politician 16 hours agorootparentprevIt‚Äôs an accident of history and the hubris of Intel that they passed over an opportunity to buy the last generation machines from ASML leading to TSMC taking the advantage. It‚Äôs not that they are intrinsically superior Vulkans as you‚Äôve described. It‚Äôs a temporary distortion caused by Intel trying to rest on their laurels and drip feed advancements. reply dragontamer 16 hours agorootparentEven *IF* Intel reaches parity with TSMC (which seems unlikely...), Intel doesn't have much experience with external customers (like Apple, Qualcomm, AMD, NVidia) like TSMC does. Even if we magically wave a wand and give Intel next-generation lithography today, they still don't have the capabilities to take orders from customers and translate them to designs. ----------- Its not Intel who needs to catch up to TSMC. Its a 3rd party fab like GloFo who needs to catch up for USA to recapture that market share. I'm happy that GloFo is getting some scraps and investment from this CHIPS act, its the right move. But I have my doubts that we'll reach the technological level of TSMC / Taiwan any time soon. But whatever we get its still better than nothing. reply jacobsimon 19 hours agorootparentprevThis is a compelling perspective. But putting sales or import restrictions in place right now would be damaging in the short term both for the manufacturing partners and the US. They‚Äôve already put some export restrictions in place (ie preventing nvidia from selling their most powerful chips to china). In terms of nearshoring - that‚Äôs a good idea too but I think the workforce training would be even more difficult than it has been in the US and it would be less popular politically as you said. (Side note: PR is part of the US - maybe you meant Costa Rica) reply coredog64 15 hours agorootparentIntel does (or used to do) wafer packaging in Costa Rica. On paper, CR is an attractive near shore location: Good amount of English speakers, time zone parity with the US, and not difficult to travel to. reply edgyquant 19 hours agorootparentprevIf a certain former president is reelected he will likely continue to push for the type of tariffs that will incentivize domestic production over foreign imports. If that happens the chips act will have put us on the footing required for that ramp up to begin. reply KoftaBob 18 hours agorootparentprev> Move the manufacturing to a near-shore and friendly country (ie: Mexico or Puerto Rico) Puerto Rico isn't a foreign country, it's a territory of the United States. Regardless of that, semiconductor fabrication is one of, if not THE most complex form of manufacturing. It's not enough to simply build a fab in Mexico, the supply chain of materials and highly skilled labor that runs these fabs needs to exist there too. That's not to say that it can't exist somewhere like Mexico, but currently it doesn't and it would take longer to build up. So while moving it to Mexico would remove the geopolitical risk problem, putting it there instead of the US would only give the advantage of cheaper labor but at the cost of taking much much longer to ramp up. reply coredog64 15 hours agorootparentNot everything needs to be fabbed at 7 or 3 nm. Malaysia does well as a place where industrial nations can put mature manufacturing processes and get high quality outputs. reply chrinic839400 19 hours agorootparentprev> I am just doubting (a personal opinion) that they'll actually work and calculating the wealth transfer happening here. The goal is not wealth transfer. If you want that, move to Europe. reply dukeyukey 19 hours agorootparentEurope does a lot of income transfer, but not much wealth transfer. reply myrmidon 20 hours agoparentprevIt's always good to question subsidies like this, but the question you ask (\"does income tax on the created jobs pay for the subsidies\") is the wrong one: The state is not investing into this for a monetary return from income taxes-- he is paying for strategical reasons/supply chain security, and the \"price\" is to compensate for the fact that labor costs and regulations make building chip foundries in the US more expensive. So what you would need to ask would be more like: Is the subsidiy higher than it needs to be to lure chipmakers into the US? And that is much harder to answer... reply Shrezzing 19 hours agoparentprevOxford Economics estimates that there's a 6.7x multiplier on each new role created in the semiconductor industry[1]. That is, each role in the semiconductor industry supports an additional 5.7 jobs indirectly in other domestic industries. The only two sectors that are higher are Computer Storage manufacturing and Aircraft Manufacturing. You'd need to split the pay between all 6.7 newly created roles, meaning they only need ~$60k take-home or so each - which is right around the US mean salary. [1] https://www.oxfordeconomics.com/resource/chipping-in-the-us-... reply ericmay 21 hours agoparentprevBut those employees also do things like buy cars and lattes, and they also pay state and local taxes, sales tax, etc. as does the foundry. There are other economic benefits too. But even if your simple calculation was correct and all there was to this story, what you‚Äôre missing in your post is that America as a nation has decided we need to produce semiconductors here for economic and national security purposes and so therefore subsidizing their manufacture is a normal thing that would occur. reply gosub100 20 hours agorootparentthey would buy cars, lattes and pay tax on non-subsidized jobs, too. Why should the latte shop owner have to pay tax to run their business, while the chip mfg doesn't? it just creates two classes of people. > America as a nation has decided we need to produce semiconductors here for economic and national security purposes a few decades ago we decided to move manufacturing jobs overseas because paying people who had families and bought \"cars and lattes\" here was just too expensive. Why reneg on that commitment? reply bee_rider 19 hours agorootparent> a few decades ago we decided to move manufacturing jobs overseas because paying people who had families and bought \"cars and lattes\" here was just too expensive. Why reneg on that commitment? Because it was dumb, changing away from a path when you see it is a dead end is a good thing to do. Also I‚Äôm not sure ‚Äúwe‚Äù (to the extent that there even is an ‚Äúwe‚Äù here, there are always winners and losers with these sorts of policies so I bet the whole country didn‚Äôt agree) ever intended to move high-tech manufacturing away from the US. I think we want to outsource the bad jobs and keep the good ones. Of course it is a game everybody is playing, we‚Äôre shifting our strategy to hopefully more effectively keep the high-tech stuff. reply octacat 1 minute agorootparentThe game the manufactures are playing is to get extra good conditions for them by pressing the government that they would produce abroad. Local manufacturing in any reasonable volume is not coming back. Highly automated - yes, maybe, if there are dotations/tax cuts/high import tariffs. Highly automated means 1.5k people working on a giant fab and rest are robots. jdewerd 19 hours agorootparentprevEveryone involved with floating the dollar and running a deficit understood that it would pump the asset sector and dump the export sector. There were fights about it. Keynes got himself ejected from elite circles by lobbying a bit too hard for his pet alternative. If you want a more visible and recent datapoint, look to the perennial fights in the UN over the opposite, where a country that promised not to devalue their currency devalued their currency. Note that these discussions generally start from a point where everyone understands why it might be desirable to devalue one's currency, pumping exports & the real economy while dumping assets. This is macroeconomics 101 stuff. The surprising thing is not that high tech manufacturing left, it's that for a while it was actually able to compete with a money printer for access to talent and resources. reply gosub100 19 hours agorootparentprevwhy is building a washer that keeps peoples clothing clean a 'bad job' and building tech components a 'good job'? reply bee_rider 18 hours agorootparentBecause computer chips are cool and washing machines are boring. reply Dalewyn 19 hours agorootparentprev>I‚Äôm not sure ‚Äúwe‚Äù ever intended to move high-tech manufacturing away from the US. Until the recent turnaround, it was understood the west was moving to a service industry. Someone Else(tm) (read: China) would manufacture everything (the \"dirty\", \"cheap\" work) and we would provide the service (the \"clean\", \"expensive\" work). Of course, an economy can't be valuable if it can't actually make anything. As it stands, the US can't even make chopsticks by itself let alone microprocessors. I actually have significant doubt CHIPS will be successful in bringing back microprocessor manufacturing, and more broadly bringing back manufacturing in general to the west at all, but we have to start somewhere if we don't want to be beholden. reply bee_rider 18 hours agorootparentI‚Äôm not sure what you mean by ‚Äúcan‚Äôt even make chopsticks by itself,‚Äù which is to say, obviously that is an exaggeration but on the other hand, lots of products do touch multiple countries‚Ä¶ Anyway, US manufacturing output still seems to be second in the world, around 1/2 that of China with 1/4 the population. Microprocessors are already made in the US, Intel (despite their struggles) is still making plenty of chips here. I‚Äôm not really sure what the plan is with Global Foundries, in the sense that they presumably won‚Äôt keep up with TSMC or even Intel‚Ä¶ but maybe 7nm will be the next or next-next generation automotive chip node or something. reply Dalewyn 18 hours agorootparentIntel designs their chips here, but Intel makes their chips in Israel, Vietnam, Malaysia, Costa Rica, etc. This is what the west was trying to do before: Providing services like microprocessor design for the manufacturing industries overseas. But of course, simply drawing stuff on paper for others to peruse doesn't actually produce value. It's the act of making something that produces value, and we finally realized that grave error. Not to mention the people doing the manufacturing work will inevitably accrue the know-how to also just do the designing themselves too, and then we in the service industry really are worthless fools. >I‚Äôm not sure what you mean by ‚Äúcan‚Äôt even make chopsticks by itself,‚Äù I mean that quite literally. I know Georgia (the state, to be clear) has (or had) a robust disposable chopstick manufacturing industry, but nearly every disposable chopstick I see sold or given out are Made In China these days. If we can't (or won't) even figure out the means to making some wooden sticks, we definitely aren't manufacturing microprocessors. reply BeetleB 17 hours agorootparent> Intel designs their chips here, but Intel makes their chips in Israel, Vietnam, Malaysia, Costa Rica, etc. > But of course, simply drawing stuff on paper for others to peruse doesn't actually produce value. Intel actually manufactures quite a bit in the US. The bulk of their fabs are here, and they ensure they can produce the wafers with good yields in the US. Once they've got the process down, the other fabs kick in. And even then, I believe a fair amount of the mass production is still in the US. It's the packaging that is mostly offshored. So no, not just designing on paper. reply Dalewyn 8 hours agorootparentIf that's true Intel needs to advertise that better, especially in today's geopolitical climate. I'm aware they have fabs (mostly R&D) in Oregon and have manufacturing fabs planned in Arizona, New Mexico, and I recall Ohio, but I've mostly heard praise of their Israel fabs and all the CPUs I've ever bought from them have said some variation of \"Made in Costa Rica/Malaysia/Vietnam\". To be fair my latest purchase, a 14700K, says \"Manufactured in US with global content.\" which is a slight improvement, but that could just as well mean \"We shipped chips in from Israel and soldered a heat spreader on it.\" and it still ultimately says \"Final assembly in VN.\" (Vietnam) right under it. Made in USA this is not (yet?). reply bee_rider 17 hours agorootparentprevIntel has production fabs in the US. https://www.intel.com/content/www/us/en/support/articles/000... >> I‚Äôm not sure what you mean by ‚Äúcan‚Äôt even make chopsticks by itself,‚Äù > I mean that quite literally. I know Georgia (the state, to be clear) has (or had) a robust disposable chopstick manufacturing industry, but nearly every disposable chopstick I see sold or given out are Made In China these days. But these are different things. ‚ÄúCan‚Äôt make chopsticks by itself‚Äù is clearly not true, I could just go make a pair of chopsticks if I wanted. You can find expensive hand-made chopsticks from the US. The US might not have a huge production of disposable chopsticks, but making really simple things at scale is a place where we don‚Äôt have a competitive advantage, and I wouldn‚Äôt be surprised if we have a relatively low demand for chopsticks, so this doesn‚Äôt seem surprising or indicative of anything in particular. reply Vt71fcAqt7 17 hours agorootparentprevThat is false. All the advanced Intel fabs are located in the U.S. And in fact, the design has actually been done in Israel in previous generations.[0] Much of the equipment needed to run advanced fabs are made in the US as well. [0] eg. https://en.wikipedia.org/wiki/Intel_8088 reply ericmay 19 hours agorootparentprev> they would buy cars, lattes and pay tax on non-subsidized jobs, too The point here was that your assumption of the economic effects of the subsidies didn‚Äôt include other meaningful factors. reply edgyquant 19 hours agorootparentprevWhy continue to ignore the answer given in favor of purely economic arguments? We need chips built at home so we‚Äôre making it happen, it isn‚Äôt complicated. reply gosub100 17 hours agorootparent\"we\" is doing a lot of work there. a multiple-trillion-dollar company could do the investing (since their products depend on this silicon) instead of the American taxpayer. But lets blur the lines with the word \"we\". reply melling 20 hours agoparentprevThis has nothing to do with jobs. Read Chip War by Chris Miller https://en.m.wikipedia.org/wiki/Chip_War:_The_Fight_for_the_... reply dubcanada 19 hours agoparentprevIs the only reason governments subsidize things for employment? Could it not just be here, have $3.1B and further us as a species by producing chips needed for AI, ML, cars, cellphones, etc. We give giant science grants all the time, tons of money for infrastructure, etc, what makes this one immediately go into X dollars / jobs created * tax generated? reply imgabe 14 hours agoparentprevThe 9,000 construction jobs will also pay taxes while they're working for 2-3 years. The 1,500 people will buy houses and go to restaurants and dry cleaners and have lawn care and create much other economic activity that will also generate tax revenue. Simply diving the subsidy by the 1,500 permanent employees doesn't capture the value that's created, along with the strategic value of bringing more chip manufacturing to the US. reply mattmcknight 19 hours agoparentprevIn addition to the point of this not being to make money, more than half of this is loans, so is intended to be paid back. 1.5B is grants, so closer to $1M per manufacturing job in your framing. The other economic benefit is moving one part of the supply chain here may enable more of the supply chain to be completed here. For some of the hardware companies I work with, if you have to make a few parts in China, it's vastly cheaper and more flexible to do the whole assembly there. reply BeetleB 17 hours agoparentprevAt $344K/year, I can assure you the federal income tax will be higher! Also, did you not account for capital costs? I know that at Intel fabs, labor costs are significantly lower than capital costs. Consider, for example, that ASML charges $0.5B for a single unit. reply zitterbewegung 20 hours agoparentprevYou are absolutely right but missing the point. The CHIPS act is to make domestic foundries for companies where the manufacturing is outside of the United States. It is to make us resistant to supply chain problems and also the possibility of tampering of computer components. Foundry creation is incredibly expensive and to get to the state of the art lithography is hard but global foundries can make chips for things that don't require that. The subsidies should reflect that (Intel / TSMC have had this grant also). On the other hand they could just not complete what they promised and we would be in a bad position: https://www.theverge.com/2019/4/10/18296793/foxconn-wisconsi... Others have said about trickle down economics (this can be debated). reply zer00eyz 20 hours agoparentprevIts not about this being a \"winner\" it's about being able to make the dam things domestically. Full Stop. We have national defense concerns and zero faith in intel. EDIT: This makes it \"defense spending\" in a civilian line item. reply Dalewyn 20 hours agorootparentIntel is one of the recipients/beneficiaries.[1] [1]: https://www.intel.com/content/www/us/en/corporate/usa-chipma... reply edgyquant 19 hours agorootparentYes but the point stands. ‚ÄúZero faith in intel being able to fill the gap if left to its own devices,‚Äù would be a better wording reply Dalewyn 19 hours agorootparentTSMC, SMIC, and Samsung among others all have the financial and political will of entire countries behind them. It's unreasonable to expect anyone to fight equally if not better without similar terms like CHIPS. reply patmorgan23 15 hours agoparentprevThe government is not a business, it's \"investments\" do not have to \"break-even\" in raw financial terms. Maintaining supply chain diversity is critical for national security. The US Military does not want to be dependent on foreign manufacturers for its equipment. Furthermore the business and employees will be paying sales and property taxes, in addition to federal income tax, and will have 2nd & 3rd order effects from their spending. reply tqi 17 hours agoparentprevNational security considerations and second order economic stimulus effects aside, rounding 9000 construction jobs down to 0 is also doing a lot of work here. If you assume the average duration of those jobs is just 5 years, that doubles the number of job-years... reply konschubert 20 hours agoparentprevThis isn't about creating jobs. The US has more Jobs than people who're willing to work. This is about creating strategic independence. reply edgyquant 19 hours agorootparentNo a big part of the chips and inflation reduction acts are definitely the creation of jobs and the bringing of manufacturing back to parts of America that has been deindustrialized due to offshoring. reply patmorgan23 15 hours agorootparentWhile policies can have multiple goals, the primary goal of the CHIPS Act is definitely supply chain independence for the US Military, Job creation is a nice side effect. reply dev_tty01 16 hours agorootparentprevThat is just what they say to get support from constituents who don't understand international power struggles and the strategic importance of chip production capabilities. The overarching motivation is global strategic power and security. Read Chip War: The Fight for the World's Most Critical Technology. reply dukeyukey 19 hours agoparentprevI imagine that $2 million also includes land purchases, materials, and equipment on top of salaries. reply woah 16 hours agoparentprevDon't you think they have to pay for the machines? reply lulznews 18 hours agoparentprevIt‚Äôs a money dump no question about it. America simply can‚Äôt do these things. reply formerly_proven 20 hours agoparentprevThe value of a job for the state is limited to ... federal income tax? That's certainly a view to hold. reply dangus 11 hours agoparentprevBut the jobs aren't the only nor primary benefit. Don't forget that TSMC was essentially a government enterprise when it started. Be cynical if you want, but the one thing that has a 0% track record of effective results is calling every government function wasteful, corrupt, and untrustworthy and responding by dismantling government institutions and investments. reply seventytwo 20 hours agoprevA geo-strategic win for securing the semiconductor supply chain, and the top parent comments in here are complaints about the parking lot and how much tax money it‚Äôs going to take. Sheesh. reply alephnerd 20 hours agoparentIt might be because GloFo always had foundaries in NY and VT (because it's the old IBM+AMD foundary spinoff). IMO it's still fine to subsidize - everyone else (Taiwan, SK, China, MY) is doing it against WTO regulations, so we may as well reply Always42 20 hours agoparentprev^it's crazy to me usa hasn't brought chip mfg in sooner, but easy to say in hindsight reply seventytwo 20 hours agorootparentWell, we‚Äôve been of the collective mindset that internationalizing supply chains is best, for multiple reasons. And, under the right conditions, that may be true. However, the world is becoming morr multi-polar, so the calculations are changing. Lots of political, economic, and social inertia‚Ä¶ reply edgyquant 19 hours agorootparentNo ‚Äúwe‚Äù haven‚Äôt. Half of the ultra rich and the cadre of their supporters have, while most Americans have always though this was stripping them of their wealth and destroying their communities. To the point when offshoring began they were told every lie about how this would be a benefit to them. reply Workaccount2 18 hours agorootparentGlobalization has been a boon for just about anyone with a white collar job. It's not limited to the ultra rich at all. While the middle class sure has shrunk, the life those in it live is markedly more materialistic than in the past. Fifty years ago there was no Temu, Alibaba, or Amazon TYEVRRTY brands to deck out facet of your life once you clear the cost of basic living. Houses are expensive as ever, but filling them has never been so cheap. reply twoodfin 17 hours agorootparentThe middle class has shrunk largely because the upper class has grown: https://www.aei.org/carpe-diem/yes-the-us-middle-class-is-sh... reply edgyquant 17 hours agorootparentThis is so false I can‚Äôt believe you‚Äôre posting it. Yes the amount of people making over 100k has grown since 1969 because inflation has eaten away how far that money goes. The top ten percent of earners today make >140k while the upper middle class is the top 10-20% (which includes those earning 100k.). Meanwhile the lowest earners as a class has definitely grown since 1969. It cannot be stated enough that people who aren‚Äôt domain experts should not be using stats to push narratives because lots of nuance is lost when just numbers are shown. reply latency-guy2 15 hours agorootparent> Yes the amount of people making over 100k has grown since 1969 because inflation has eaten away how far that money goes. Real dollars. > It cannot be stated enough that people who aren‚Äôt domain experts should not be using stats to push narratives because lots of nuance is lost when just numbers are shown. Agreed, so its really important for everyone right now that you personally refrain from commenting further. reply twoodfin 15 hours agorootparentMore narrative pushing from the Federal Reserve: Per capita disposable real income https://fred.stlouisfed.org/series/A229RX0 Real median family income https://fred.stlouisfed.org/series/MEFAINUSA672N reply edgyquant 17 hours agorootparentprevBecause you make this claim doesn‚Äôt imply at all that most white collar workers supported such a thing, which is what I was commenting on. reply DinoCoder99 17 hours agorootparentprevTurns out people want more from the market than a 401k and a cheap tv. Specifically a stable place to live with reliable employment and decent healthcare is worth sacrificing a hell of a lot for. reply Workaccount2 17 hours agorootparentMost white collar workers have that. HN is packed with people who own homes, are highly employable, have excellent healthcare, and disposal income to splurge on consumerism. The internet/social me is so hyper focused on those at the bottom that they have lost sight of what its like just a few levels up. But it's critical to understand in order to craft functional solutions. reply DinoCoder99 17 hours agorootparentOk, who gives a damn about that? White collar workers are still workers who are fucked by the current globalized market. Furthermore most folks don't have cushy salaried positions and they collectively matter a lot more than the folks on this forum. reply WarOnPrivacy 19 hours agorootparentprev> we‚Äôve been of the collective mindset that internationalizing supply chains is best, for multiple reasons. Ultimately (multimately?) the collective mindset is of the investors and execs who directly, financially benefit from those reasons. I know this is a narrow lens with which to view pub+corp financial decisions. But it keeps producing such stunningly accurate pictures and I find it hard to not use it. reply pm90 19 hours agorootparentI think thats a very simplistic view. Yes labor is cheaper in Asia. But the other factor is the availability of a certain kind of labor force. You need a lot of people that are highly skilled to operate these facilities. There isn‚Äôt that kind of workforce available in NA today. reply pxeger1 19 hours agoprev> Gina Raimondo said the plant will make ‚Äúhigh-value chips‚Äù that are currently not made in the United States. The 300-millimeter chips are essential for national security [...] 300mm? What dimension is this measuring? I would expect feature size to be 300nm, but 30nm or 3nm is surely more likely. reply Alpha3031 19 hours agoparent> What dimension is this measuring? Twenty bucks says wafer size, since 300 mm wafers are standard. Edit: Also, here's the press release for the announcement: https://www.nist.gov/news-events/news/2024/02/biden-harris-a... reply Kirby64 16 hours agoparentprevThat‚Äôs a shorthand for modern processes. Older process technology was made before 300mm wafers were standard, and retrofitting old processes to use larger wafers is generally considered to be not worth the cost since all the tooling is depreciated and paid for in 200mm or smaller wafers. Anything made today from the ground up would use 300mm wafers, though. Even slightly older process nodes (55nm, for instance). reply stn8188 19 hours agoparentprevThis is talking about pre-diced wafer diameter. https://en.m.wikipedia.org/wiki/Wafer_(electronics) reply bee_rider 19 hours agoparentprevCertainly not the transistor size unless they‚Äôve invented some new Gate All Around Ingot technology, hahaha. reply dubcanada 19 hours agoparentprevI suspect they mean wafer size, 300mm is a common wafer size. reply dragontamer 15 hours agoparentprev300mm is the new wafer standard. 200mm was the standard a few years ago. reply georgeburdell 14 hours agorootparentI was in the industry for a decade and never saw 200mm reply dragontamer 13 hours agorootparent200mm was also called 8-inch wafers. reply georgeburdell 13 hours agorootparentI think you‚Äôre missing my point. I have since come and gone from a decade-long career in semiconductors, and it‚Äôs always been 300mm for logic. 200mm is early 2010s at the latest reply capital_guy 18 hours agoprevAs a native upstate new yorker, I root for the success of GlobalFoundries. But it was exceedingly frustrating to me that they went and did layoffs in 2022 despite their booming revenues and incoming subsidies [0]. It seems to me there should be some kind of rule to prevent these layoffs for companies that receive billions of federal dollars. [0] - https://vtdigger.org/2022/12/01/globalfoundries-to-cut-up-to... reply gizmo 17 hours agoparentLayoffs are necessary from time to time to keep org chart bloat down. It's not just about the money. I don't mean this in a callous way and I understand that layoffs are painful for those getting fired. But for a business to thrive it has to make the correct strategic choices. reply capital_guy 17 hours agorootparentI guess I am too humanist to believe this sort of rhetoric is not callous. I know people who lost their jobs in that layoff. It hurts a lot of families. Companies should think about organizational bloat during hiring. If their revenues are soaring and they‚Äôre getting free money from the feds, they should shuffle the people they have into more efficient roles. I know there are fiscal reasons why this kind of thing happens. But workers in the US do not have the protections to balance it out. Healthcare, unemployment insurance, etc are not sufficiently strong to keep people afloat for the sake of ‚Äúreducing organizational bloat.‚Äù reply gizmo 17 hours agorootparentI'm in favor of a stronger social safety net (and the taxes to pay for it). reply nos4A2 17 hours agoparentprevSuch a rule can have the opposite effect though, a company may choose to hire/grow more conservatively if it is subject to a condition that it cannot layoff. reply viscountchocula 17 hours agorootparentIsn't their layoff a tacit admission that they should have grown more slowly, though? reply epistasis 17 hours agorootparentWhen the Fed raises interest rates, it slams on everyone's brakes. Should GlobalFoundries somehow predicted the macro environment, better than everyone else, and followed their gut on reading macro tea leaves and pre-anticipated what the Fed would do? Ideally, yes, they would know the future, but in reality the best laid plans must change to meet reality, as current events always has surprises that can not be 100% derisked. reply _giorgio_ 16 hours agoprevBillions... Just imagine what they could do with trillions, eh Sam? reply downrightmike 14 hours agoparentApple and Microsoft seem to be doing pretty well reply garunski 9 hours agoprevwhat a crock of shit! no. none of this money will be spent wisely. these companies are requiring the government to subsidize their profits without having to spend their own money. the promise of jobs is all fluff. billions to buffalo didnt send even one billion to buffalo and certainly didnt improve any lives in that region. as someone who lives here, I wish I could trust these people but the reality is that the state is so corrupt and US government is on the same level, where all the money will be forced out of the people and nothing will be delivered. reply garunski 8 hours agoparentthe 3.1 billion could be spent on Education, Libraries, Updating Infrastructure that is destroyed by harsh winters, investing in restoration of natural resources, helping the failing farmers, updating rail and lake/river deliveries. But no. we should def give it to people that will spend exactly 1% pretending to develop a factory and claim the rest for themselves because they encountered problems. reply rasz 20 hours agoprevThe 'we give up R&D' GlobalFoundries? https://www.anandtech.com/show/13277/globalfoundries-stops-a... reply icegreentea2 20 hours agoparentFrom the Anandtech article, you'll note that a major challenge GloFo cited in bringing 7nm online was getting the fab space for it, citing that they would need to sink 10-15 billion to bring 7nm (and lower) nodes online. You'll note that in the submitted article, GloFo is promising to do exactly that (\"In return for the subsidies, GlobalFoundries said it would invest about $12.5 billion in the projects, most of which would be spent to expand the company‚Äôs Fab 8 in Malta, in Saratoga County.\"). Basically it seems like GloFo made a pretty hard nosed decision based on financing (guess they were just a few billion short last time) back in 2018, and now that they have those missing billions, they're going to go ahead. Based on these numbers, it does seem likely this would allow GloFo to bring online 7nm. But getting much past that (5nm and lower) seems uncertain. That said, having additional domestic 7nm production (crucially, 7nm that does not displace existing 14nm+ production) would be useful. reply seventytwo 20 hours agoparentprevIf it‚Äôs not TSMC, it‚Äôs bad? reply jeanlucas 19 hours agorootparentHow's that related at all to the link? It's just a news article pointing out that GlobalFoundries gave up on making 7nm chips. reply minroot 21 hours agoprevman, the parking lot! couldn't they make it underground? reply infecto 21 hours agoparentWhy? That would probably cost many multiples more than what exists above ground. I am guessing upstate New York and where this plant is located, is not incredibly dense. reply ambrose2 19 hours agorootparentI used to work there, and can confirm that there‚Äôs plenty of space there for more parking lot, there‚Äôs woods in all 4 directions and it sits on an old super fund site (former rocket fuel testing). reply jprd 20 hours agoparentprevWhen I worked at Malta 5y ago, that parking lot was too small. If you didn't get in at an early hour, you were parking in the gravel lots that are off this picture to the right. Combine that with the actual size of the Fab, hella lot of walking :) As for solar panels, the Fab operates 24/7/364 (usually 1 downtime day a year). Which means even with the large amounts of snow that will build-up in the winter, there are always cars in the lot - clearing that is a big job and not sure how to do it with solar panels. reply Alpha3031 5 hours agorootparent> Which means even with the large amounts of snow that will build-up in the winter, there are always cars in the lot - clearing that is a big job and not sure how to do it with solar panels. I'm not sure how much sense it'll make in this specific case, but in general, interestingly, with a high albedo (e.g. snow covered or white for some other reason) ground surface, vertically mounted bifacial panels produce very close to equator facing panels with the appropriate tilt. The value of that generation might actually be greater, as it obviously spreads out the profile towards the evening peaks. For the regular horizontal/tilted ground mounts, NY is quite far north, so you're looking at a mount angle of 43¬∞ or so to optimise overall power production, and you can go about 15¬∞ more tilt and only lose maybe 1 or 2% of summer production, but increase production in winter when the sun is angled lower. If you go for like 55¬∞ to 58¬∞ tilt, there's probably not going to be very much snow accumulating on the panels themselves. reply vitro 21 hours agoparentprevOr at least put solar panels on top of it. Provides electricity and shades cars. It should be a norm. reply sofixa 19 hours agorootparentIt's the law in France over a certain size of parking lot, which is great. reply mschuster91 21 hours agoparentprevIntel in Germany is even worse. The parking lot is larger than the plant [1]! For fucks sake, large commercial (both office and industry) zones should only be permitted when the developers show a viable mass transit solution for their employees. Even if it's just buses, that's still better than thousands of cars. [1] https://www.mdr.de/nachrichten/sachsen-anhalt/magdeburg/magd... reply geraldwhen 21 hours agorootparentMost office complexes have parking lots larger than buildings. People are smaller than cars, commute one person to car, and buildings are often multi story where parking is not. Hence campuses that are 85% parking lot. reply mschuster91 21 hours agorootparent> Hence campuses that are 85% parking lot. Yeah and that is nuts. We lose ridiculous amounts of valuable land for barely any productive use - every day Germany loses 39 ha for buildings and 8 ha for transportation infrastructure [1], which means streets and parking lots. Loss of land is an externality that doesn't get priced in anywhere. [1] https://www.umweltbundesamt.de/themen/boden-flaeche/bodenbel... reply InvaderFizz 19 hours agorootparentLoss of land does get priced in as prices rise with a lower supply and higher demand. It just takes a while to really feel it. reply bee_rider 18 hours agorootparentprevI mean, they have to buy the land for the parking lot, so it does get priced in. But we should also have a Land Value Tax to push people toward using it more productively. reply vel0city 17 hours agorootparentThe Land Value Tax on that site would still have been incredibly low and probably still justify paving a parking lot. It is a toxic superfund site way out in the boonies. You're not going to develop some kind of dense mixed use town center in that space anytime soon. reply vel0city 18 hours agorootparentprevLook at Malta, NY on satellite view. Then tell me mass transit makes sense for that location. Tell me a few good bus routes that would actually make sense for people to take instead of driving their car. Not every spot on Earth is a dense urban area. reply raverbashing 21 hours agorootparentprevThere probably is mass transit access to the Magdeburg plant (or the state can guarantee an expansion there) I'm thinking that \"excess parking\" is usually due to wanting to secure a site with enough expansion potential but also putting it to use in the most cheap way possible (like, you won't verticalize the parking until needed and that would require more planning, permits, etc) reply edgyquant 19 hours agorootparentprevNo we definitely should not enforce a ridiculous regulation like that. Again people like you miss the entire point by thinking everything should exist only in big cities where public transportation makes sense. We need to bring these factories back to small towns and regulations like you‚Äôre proposing completely kill that. reply mschuster91 18 hours agorootparent> We need to bring these factories back to small towns and regulations like you‚Äôre proposing completely kill that. For an Amazon parcel sorter or a wholesaler distribution facility, sure, there are enough towns with desperate populations that can be exploited (and Amazon in particular is infamous for using that leverage). But a company like Intel that needs highly educated staff? They won't go to some random village hours away from civilization. reply twoodfin 17 hours agorootparentPeople who work at a wholesaler distribution facility don‚Äôt want the same lifestyle comforts as the highly educated? That‚Äôs a strange claim. I know lots of highly educated folks who love rural life, and blue collar workers who enjoy urban amenities. reply ciabattabread 16 hours agorootparentprevAmazon builds warehouses where their customers are. That‚Äôs why they have a lot of warehouses in Red Hook, Brooklyn [1] - an industrial neighborhood located off of a major highway within NYC. [1] https://www.consumerreports.org/corporate-accountability/ama... reply vel0city 17 hours agorootparentprevHours away from civilization? This example fab is 7mi from downtown Saratoga Springs, an absolutely delightful small town. It is alsoFree markets create monopolies. I have never thought this to be the case, could you provide some backing to your claim? reply jgeada 13 hours agorootparentIt is described in many places, including in detail here: https://archive.org/details/cu31924096224799 Some other info here https://www.economicshelp.org/blog/21726/concepts/why-does-c... In actuality though, this is almost religious. It doesn't matter than monopolies have always appeared in markets and been dealt with by regulation; believers in free markets will always argue that the actual cases weren't really monopolies or that they'd not have lasted. /Shrug reply kredd 15 hours agoparentprevYou can't free-market your way in an industry that's heavily subsidized by other governments. Whatever local company you have will obviously lose to some other company that's getting the levies somewhere else to design a cheaper product which will win in the \"market\". So, you end up having two options: a) Offer subsidies yourself b) Let other governments dictate the industry that you're crucially depended on reply onlyrealcuzzo 15 hours agoparentprev> eg: cars with 100% US made chips, Isn't this about being able to build military aircraft and guided missiles w/o any foreign dependencies, not cars? reply DinoCoder99 17 hours agoparentprev> while I agree with the geo-political arguments, I'd prefer to see regulations that allow the freemarket to design a solution, rather than a government contract to choose the winner. I think the market did choose and the choice was \"not in the us\". reply Vt71fcAqt7 17 hours agoparentprev>that is very much contrary to free markets or capitalism Free markets and capitalism are related but not the same. The government using capital as an incentive for companies to increase output is an example of capitalism. reply maerF0x0 15 hours agorootparentCorrect, but I'd claim that there is a \"freemarket\" ethos in the US, despite the lack of them. People here often say they want freemarkets and hold it as an ethical and idealized system. Their government behaves very much in contradiction to that though. reply dragontamer 15 hours agorootparentWhen exactly? The bulk of USAs technological progress is Department of Defense investments. Such as ARPANET, computers, supercomputers, pressurized cabins for bombers (converted into passenger jet tech today), NASA projects and more. USA was only free market in the 1800s. When the 'Robber Barons' turned out to be a bunch of assholes, we regulated away a lot of that bullshit they did. reply maerF0x0 14 hours agorootparentI agree with your assessment of how the government looks, in actuality, here. The ethos i was describing was my own anecdata of what people claim to value. IE, they give at least lip service (maybe more actual effort as well) to wanting free markets, but their government does not actually do that reply Vt71fcAqt7 14 hours agorootparentprevI'm not sure you can declare everything that ever received government funding as invented by the government. Jets are \"just\" pressurized cabins? Why ignore the Wright brothers in this story? Aren't jets really \"just\" fast planes? And ARPANET is \"just\" telegrams with computers. On that note, computers were not invented by the government, although they recieved a lot of funding from them. I don't think oversimplifying this issue helps us discuss it seriously. reply thoughtstheseus 11 hours agoprevPlease stop the wasteful spending. Concentrate semi development geographically to realize synergies and economies of scale. Building redundant and competitive production thousands of miles apart in the Northeast/Great lakes, Texas, and the west Coast is not the best idea when you are resource and labor constrained. reply bobbylarrybobby 9 hours agoparentWith climate change getting more and more severe, I can't imagine a higher risk strategy (for anything) than geographic concentration. Talk about putting all your eggs in one basket. reply dangus 11 hours agoparentprevWasteful? What is a better national investment than the semiconductor industry? Look into the history of TSMC. It became what it is because of direct government investment. I'm sure to some people it seemed like a gamble at the time. This amount of money is basically trivial on the national level and the upside is tremendous. reply fxd123 10 hours agoparentprev> Building redundant and competitive production You do realize these fabs already exist? There is no redundant or competitive production currently. reply 802er 21 hours agoprev [‚Äì] >Schumer said the deal includes a commitment from GlobalFoundries to continue to provide a $1,000-per-employee annual subsidy for childcare and to extend the benefit to construction workers who build the new plant. So pacing inflation on the cost of childcare? What a joke. Childcare in Vermont is ranging between $15,000-$25,000 per year among families I know. That subsidy covers less than a month. As the largest private employer in Vermont I think they can do more than that. It‚Äôs not like they pay well. I see on their advertisements that night shift operators are paid 21.93 hourly with a shift differential. So day shift makes even less. Looks like shifts are 7 to 7 am and pm night and day. What a schedule. It‚Äôs one thing if you‚Äôre a nurse making at least low six figures. But this? Grueling manual labor in those suits for twelve hours a day for a sub livable wage in Vermont and New York. https://livingwage.mit.edu/states/50 reply verdverm 19 hours agoparentI worked the overnight there when IBM still owned it, it was 4/4-3/3 on/off with 12 hours shifts, 8 overtime per paycheck It was really, really nice having 4 days off. It's a super easy job, basically babysitting machines, more reading the Internet than anything else reply fallingknife 16 hours agoparentprev [‚Äì] Or maybe they can mind their business, not concern themselves with how employees spend their salary, and pay the same to workers with and without children. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GlobalFoundries is poised to receive $3.1 billion in federal grants and loans to expand its chip plant in Upstate New York and upgrade a factory in Vermont.",
      "The investment aims to create 1,500 manufacturing jobs and 9,000 construction jobs in the next decade, aligning with the national security-focused CHIPS and Science Act to boost domestic semiconductor production.",
      "GlobalFoundries' $12.5 billion investment will triple production capacity, establish a new plant in Upstate New York, and potentially pave the way for similar agreements with companies like Micron Technology in Central New York."
    ],
    "commentSummary": [
      "GlobalFoundries receives a $3.1 billion subsidy through the CHIPS Act to modernize facilities in NY and VT, focusing on gallium nitride semiconductors, aiming to generate numerous jobs in the coming years.",
      "Debates arise on subsidy effectiveness, agricultural subsidies, trade relations with Taiwan, Intel and TSMC challenges, and the prospect of relocating semiconductor production to Mexico.",
      "Discussions range from boosting domestic chip manufacturing for security reasons, layoffs' effects, and expanding Fab 8 in Malta to incorporating solar panels in parking lots and governmental involvement in industries."
    ],
    "points": 163,
    "commentCount": 155,
    "retryCount": 0,
    "time": 1708342286
  },
  {
    "id": 39428677,
    "title": "GoPro Ride Through Electron Beam Irradiator at Full Power",
    "originLink": "https://www.youtube.com/watch?v=Uf4Ux4SlyT4",
    "originBody": "GoPro Ride Through an Electron Beam Irradiator at Full Beam Power (GOPR0016trim) Watch later Share 0:00 0:00 / 2:41‚Ä¢Live",
    "commentLink": "https://news.ycombinator.com/item?id=39428677",
    "commentBody": "GoPro Ride Through an Electron Beam Irradiator at Full Beam Power [video] (youtube.com)162 points by firebaze 22 hours agohidepastfavorite86 comments ethbr1 12 hours agoI didn't realize how much Valve mentally linked miniature rail cars, physics, and Half Life for me... reply totetsu 12 hours agoparentThe tripod is a trigger too reply XorNot 7 hours agoparentprevMy brain went straight to \"Residue Processing\" too. reply proactivesvcs 15 hours agoprevHuh, so those factory and warehouse levels in Quake II were accurate? Stuff does just get ferried around on conveyor belts for seemingly no reason. Pleasing taste, some monsterism. reply ooterness 13 hours agoparentIn this case, the long twisty tunnel is necessary for operator safety. Even if you're not in the beam itself, anything within direct line-of-sight is getting irradiated by various secondary effects. Having a few right-angle turns in the entry and exit tunnels mitigates this to acceptable levels. reply guitarsteve 11 hours agorootparentIt also provides a natural place to hide loading screens between levels! ;-) reply shadowgovt 10 hours agorootparentYou just know that snow effect is an asset-swap. ;) reply cgannett 12 hours agorootparentprevYou can tell that's absolutely what its for because right after the turn you start seeing the static from the electrons messing with the camera. reply tantalor 10 hours agorootparentprevSecondary effect like electron ricochet? reply Arrath 15 hours agoparentprevSeems safer than having a PhD push the sample into the electron beam with a trolley. reply perihelions 14 hours agorootparentThe PhD's cheaper. reply csours 12 hours agorootparentThe first grad student is cheaper. The lawsuit ... ehhh reply giantrobot 11 hours agorootparentLawsuits require survivors. The implications are clear. reply dylan604 13 hours agorootparentprevBut no PhD would do physical labor. That's what undergrads, PhD candidates, or even the interns are for. reply dmd 7 hours agorootparentAh, yes, no postdoc has ever been ground down by endless repetitive labor. reply toxik 13 hours agorootparentprevThey‚Äôre waiting for you Gordon, in the test chamberrr... reply LeoPanthera 14 hours agoparentprevI recently watched a short YouTube video detailing the process of making high-end rice cookers in a Japanese factory: https://youtu.be/xLCwr8qG1p4 The process includes parts being moved around on some of the shortest conveyer belts I've ever seen, in some cases moving just a few feet before being picked up by another person. reply jasonwatkinspdx 12 hours agorootparentThe tightest bottleneck sets the pace for the whole line, so yeah high volume manufacturing can get into really detailed optimization. reply pests 12 hours agorootparentYep. Even in the linked video around 3:20 the assembly makes a short 3foot trip on a conveyer. There's a cut there but you could imagine instead of walking the part that distance and back now they have half the next item already done. Looks like some are used as a small queue for the next station as well to give some slack to timing variations. reply patcon 11 hours agorootparentprevAh just like with the \"PhD student\" joke in another thread: the value is likely in the \"workers compensation case\" savings when you can show that you did everything humanly possible to mitigate risk of injury, should there be a claim reply icegreentea2 10 hours agorootparentprevYou can actually see that in many of those cases all of those short belts/rollers are actually funneling different major sub-assemblies together. Because you have lines coming together the conveyer/rollers provide a bunch of benefits: a) It's a short term buffer. While the lines are going to balanced pretty well, it's still super useful to have a small buffer. b) Lets you spread out the lines a bit, giving you more flexibility with the design and layout of your lines. This is probably one reason why they don't try to pack the lines closer so that you can just hand off. c) Having people shuffling the parts around is just inefficient. You don't want to hire a person just to shuffle the parts over such a small distance. You don't want to interrupt the last guy on the line's job with such a task. You could try to minimize the cost of runners by batching up your work before moving them (and having your runner rotate through different things to haul), but now you introducing probably more buffer than you want, and also potential latency spikes into your process. reply hbossy 7 hours agorootparentI'm not sure if you are talking about real factory or making a Factorio reference. reply evanjrowley 13 hours agoprevFor those of you who chucked reading the Half Life references in that video's comments, here's a video for you: https://youtu.be/XfgN-EzthJM?si=_yilEGjcCdEf1sYP reply animatethrow 15 hours agoprevAliens could plausibly sterilise earth from light years away using an electron beam, according to a great Kurzgesagt episode: https://www.youtube.com/watch?v=tybKnGZRwcU An electron beam seems to be the stealthiest of the three extermination options, the other two being a star laser and antimatter \"rods from God.\" reply perihelions 14 hours agoparentThat's probably physically impossible for a charged-particle beam (like electrons). It would defocus itself from its own self-interaction, as well as from interaction with intervening magnetic fields. On this theme: we know several types of natural, astrophysical accelerators of charged particles‚Äîbut none of those are observed as a localizable source of charged particles, from the perspective of astronomy. We just see secondary photons. reply JumpCrisscross 13 hours agorootparent> probably physically impossible for a charged-particle beam (like electrons) That‚Äôs the point of the relativistic part of a relativistic electron beam. Time dilation doesn‚Äôt give the beam internal time to self interact. reply FirmwareBurner 15 hours agoparentprev>An electron beam seems to be the stealthiest of the three extermination options Or they could just false flag a nuke strike and watch us sterilize ourselves. reply gridspy 14 hours agorootparentA lot of effort is currently put into tracking missile launches and predicting ballistic trajectories. The goal is to give some warning if a nuclear strike is launched. It would be obvious if a strike came from an extraterrestrial source, there would be no terrestrial launch detection. In addition, such a attack is unlikely to succeed. It would take a long time to arrive on earth and by the time the result of the attack was known there could be angry Earthlings counterattacking. reply consumer451 50 minutes agorootparent> A lot of effort is currently put into tracking missile launches and predicting ballistic trajectories. The goal is to give some warning if a nuclear strike is launched. Less advanced nations such as North Korea and Pakistan have nukes. Do you think that their monitoring systems are really that good? reply krisoft 11 hours agorootparentprev> It would be obvious if a strike came from an extraterrestrial source, there would be no terrestrial launch detection. There would be no launch detected, that is true. That doesn‚Äôt mean that it would be obvious that the attack was extraterestrial. The alternative hypothesis would be the that the known terrestrial enemies developed some technique to confuse your sensors, or cloak themselves, or bribe your watchdogs, or pre-position warheads in space, or any other similar deception. People would be sooner thinking that their enemies smuggled nukes in overland than to think that aliens attacked them. reply dylan604 13 hours agorootparentprevThe aliens would have superior tech, so hacking into our systems would be trivial. They could just launch an actual nuke rather than lobbing one in from space. reply throwanem 13 hours agorootparentPeople assume this and I never understand why. Even if we postulate highly complex realtime computing as a necessity for controlling a superluminal engine, why assume an entire technological history unrelated to ours would make it trivial, or for that matter possible, that even FTL-capable extraterrestrials can compromise earthly systems? 'The thing about aliens is they're alien.' - why assume this only works in one direction? reply krisoft 1 hour agorootparent> People assume this and I never understand why. Of course it is not a guarantee. It is possible that the alien invasion fleet arrives, they land, unload their main battle tanks and a passing puppy laps up their whole fleet accidentally. What we know that by virtue of them being here they are either very good at faster than light travel, or they are good at traveling slow. If they are good at FTL what else are they good at? We currently think that is impossible. What other things we think as impossible are practiced by them? If they traveled slow, they must be also good at maintaining their equipment on crazy long timeframes. It also shows that they are the patient sorts who plan and execute things on the order of timelines our empires crumble. How long have they been with us then? How much preparation did they do beforehand? Did Pham Nuwen code the intel management engine? But sure, it is possible that the Aliens arrive. They broadcast a TV signal threatening us, but unfortunately the sync is a bit off so basicaly no-one understands it. Then they enter our atmosphere. The high oxygen concentration rusts their equipment and they all die. reply throwanem 0 minutes agorootparentSeeing something occur that we thought was impossible tells us our understanding of physics is incomplete, which we already know. Seeing how it is done would probably tell us more, but we haven't actually done that yet. Until then we're guessing, and to assume FTL mastery confers godlike powers is as much an assumption as any other. Turtledove addresses this in the story that a sibling commenter mentions; I can also recommend that story, which rewards the reader with considerable entertainment while making its point about what can reasonably be assumed in the total absence of information. The same goes for the sublight option, only a lot more so. Unless they have FTL communications, which I believe we also consider effectively impossible, by the time they get here anything they think they know about us will be wildly outdated, in technological terms at the very least. Possibly also in terms of the dominant terrestrial species, but we can be generous here. We can be generous about their information latency because that also doesn't have to matter. Any species which wants us dead and doesn't care what state the planet's left in - a reasonable assumption, if we're talking about them popping our nukes at us - doesn't need to come close to landing, or even to orbit to do it; a kinetic energy bombardment in passing will amply suffice to depopulate Earth to more or less any degree desired. For subluminal interstellar travel to be even remotely feasible, even for an individually long-lived species, implies access to the kind of delta-V budget where the only limiting factor in such a bombardment is the time it takes to accelerate impactors, which may be zero if those are released before or during deceleration to match velocity with our solar system. In the former or FTL case, we don't know how FTL works or even could, and we therefore can assume anything we like - with all assumptions at equal risk of bankruptcy. In the latter or high-sublight case, they don't need to be more clever to kill us if that's all they're after, and it may be unreasonably charitable to assume we would even get a chance to see it coming. throwanem 12 minutes agorootparentprevSeeing something occur that we thought was impossible tells us our understanding of physics is incomplete, which we already knew. Seeing how it is done would probably tell us more, but we haven't actually done that yet. Until then we're guessing, and to assume FTL mastery confers godlike powers is as much an assumption as any other. Turtledove addresses this in the story that a sibling commenter mentions; I can also recommend that story, which rewards the reader with considerable entertainment while making its point about what can reasonably be assumed in the total absence of information. The same goes for the sublight option, only a lot more so. Unless they have FTL communications, by the time they get here anything they think they know about us will be wildly outdated, in technological terms at least. But that also doesn't have to matter, because any species which wants us dead and doesn't care what state the planet's left in - a reasonable assumption, if we're talking about them popping our nukes at us - doesn't need to come close to landing, or even to orbit. For subluminal interstellar travel to be even remotely feasible implies access to the kind of delta-V budget where the only limiting factor in such a bombardment is the time it takes to accelerate impactors, which may be zero if those are released before or during deceleration to match velocity with our solar system. reply tzs 5 hours agorootparentprev> People assume this and I never understand why There's at least one science fiction writer who had similar thoughts and wrote a story [1] where the FTL is pretty much the only technology in which the FTL aliens who visit Earth are ahead of us. https://en.wikipedia.org/wiki/The_Road_Not_Taken_(short_stor... reply jorticka 9 hours agorootparentprevThey would compromise the systems by dumping nanobots into them which rewire the whole thing, not by breaking the encryption. reply throwanem 7 hours agorootparentOkay, but if they can do that to our networks, why assume they can't about as easily do it to us? reply dylan604 8 hours agorootparentprevI always assumed they'd just use the gov't mandated encryption backdoor. Then again, since the master code was 000000, it wouldn't really be to difficult to break without a backdoor reply dylan604 13 hours agorootparentprevYou're assuming that the aliens are alien. reply throwanem 13 hours agorootparentIf there's a reason to assume otherwise, I've yet to become aware of it. Or are you getting at something more earthly in scope? reply nextaccountic 13 hours agorootparentA common trope in sci fi is that aliens are actually people from the future traveling back in time reply HankB99 7 hours agorootparentThat does not seem so far fetched to me. So many of the purported alien sightings are beings with bilateral symmetry, two eyes, two arms, humanoid face and so on. The only way I could see that happening is visitors from a distant future. Or, more likely, the alien \"creators\" have created them more or less in our image. reply shiroiushi 5 hours agorootparentIt's quite possible that the humanoid lifeform is optimal to have a technological species that can travel between stars. An aquatic species would have huge difficulties just building technology and civilization, because of the habitat. A species without arms and thumbs would have a hard time manipulating its environment (just look at all the 4-legged animals now). A species with more than 4 limbs would likely either be too small (insects) to accomplish much, or would need too much energy (and probably evolve to lose the extra limbs over time). There's good reasons to think that alien species might not look all that different from animals on this planet, simply due to physics. Animals here didn't simply spring to life in their current form; they evolved from single-celled organisms to best suit their environments. reply throwanem 45 minutes agorootparentThe word 'parochialism' positively vaults to mind. reply dylan604 12 hours agorootparentprevOr from the same planet but spread out via FTL travel either ships or wormhole gates. After all, we were made in the image of THE creator, right? reply throwanem 12 hours agorootparentSpeaking of Golden Age twists! And not fully thought-through ones, at that. It requires two assumptions: first that there exists a yahweh-style creator deity, and second that Genesis 1:26 is accurate to fact. Even taking both as axiomatic, this approach still further assumes that this likeness, namely the one in which we as humans are made, must also be the only likeness in which a mortal could be made after its creator. Given the assumptions of faith under which we here labor, it may also be wise to heed 1 Cor. 2:11, in which the convert Roman makes one of his few worthy statements in warning men against imagining they can know the mind of God. In that light, the proposition lacks soundness even under its own axioms. reply dylan604 11 hours agorootparent>It requires two assumptions: You're kidding right? It's all SciFi. In case you're confused, the Fi is short for Fiction. Stuff that's not real. So of course we're making assumptions on the entire thing. Including The Book as the greatest selling book of fiction of all time. You're also now assuming that we Earthlings are the original source. Some scifi tropes state we're more Martian fleeing their dying planet or with things like panspermia. I like the SciFi where everyone is searching for the nearly mythical planet that turns out to be Earth. Ice Pirates is a goofy one. reply throwanem 11 hours agorootparentYou may labor under a misapprehension here; if I met Yahweh on the road, I would do my level best to kill it. But I was raised with that book, and still remember enough to play with the toys in it when I want to; if we're talking 1950s sf twists like \"the aliens were fellow children of God all along!\" then those are the toys with which we're playing. That aside, of course we're making assumptions. But if we don't choose to either be bound by the assumptions we've already made or re-evaluate them, then we're playing with dolls rather than worldbuilding. Your pastimes are of course your own business, but it's been a long time indeed since I graduated from the former to the latter. (Not that I mind space opera, when it focuses on the character-driven stories it's best suited to tell - trying to figure out how a TARDIS works misses the point entirely, while \"The Doctor's Wife\" is beautiful. But you mentioned science fiction, and my current standard there is set by Children of Time and Blindsight.) reply throwanem 12 hours agorootparentprevThat's fair; I suppose I am assuming this story doesn't have that kind of Golden-Age twist in it... reply AtlasBarfed 13 hours agorootparentprevThe state actor would probably assume a surreptitiously placed space-borne nuke, like on a \"space telescope\" or other larger satellite. You don't need all that fuel to get it into space, it's already there. You just drop the warhead. reply sneak 8 hours agorootparentTo deorbit things requires delta-v, not ‚Äújust drop it‚Äù. reply Onavo 14 hours agorootparentprevIf they entered the atmosphere stealthily and launched from sea level, then it would appear to be similar to a submarine launched missile. reply randomname93857 11 hours agorootparentprevThey probably can do that much cheaper: just come up with two opposing conspiracy theories, and people will naturally divide into 2 camps and will eagerly kill each other. reply shkkmo 14 hours agorootparentprevOr just nudge a couple of big asteroids into collision courses reply cgijoe 13 hours agoprevSo... what would happen if you took a ride with it? Instant death? Painful death? Slow death over several days? I need details! reply jasonwatkinspdx 12 hours agoparentThere's a Russian Physicist that was in an accident where a proton beam went through his head: https://en.wikipedia.org/wiki/Anatoli_Bugorski He lived but with some brain injury. I'm not sure what the differences in effect electrons vs protons and a focused vs diffuse beam would be though. reply chasil 9 hours agoparentprevIt already happened. \"...the Therac-20, which could produce X-rays or electrons of up to 20 MeV. \"This dual accelerator concept was applied to the Therac-20 and Therac-25, with the latter being much more compact, versatile, and easy to use. It was also more economical for a hospital to have a dual machine that could apply treatments of electrons and X-rays, instead of two machines. \" https://en.m.wikipedia.org/wiki/Therac-25 This has been posted many times. https://news.ycombinator.com/item?id=38458448 reply jhallenworld 7 hours agoparentprevDon't stick your hands in the particle accelerator beam either: https://www.youtube.com/watch?v=8UgKki1tCKI reply acjohnson55 10 hours agoparentprevIf it happened after a series of humiliations and misfortunes, you would become a supervillain. reply Analemma_ 13 hours agoparentprevDepending on the voltage, you might die immediately from being electrocuted, or a few days later from radiation poisoning. The stories of the people who died from the Therac-25 machine are probably instructive: https://en.wikipedia.org/wiki/Therac-25#Radiation_overexposu... reply lordfrito 16 hours agoprevI never thought I'd see a Resonance Cascade, let alone create one. reply AnarchismIsCool 9 hours agoprevI spent some time doing testing in a similar facility doing things with gamma irradiation and I'm amazed that they all look like they were constructed by crazy people in their garage. Like really janky mechanisms for moving samples and containing radiation, really ugly facilities, stuff laying around everywhere. They all look like the lab of a mad scientist. reply omgJustTest 10 hours agoprevBlue light just before entry - very likely Cherenkov radiation. Apparently some phosphorescent materials or just electron-induced heating on the cart in front. White speckles are electrons ! reply preisschild 8 minutes agoparentCherenkov radiation only appears under water. In this case it is because the electrons ionize the air. reply joarv0249nw 19 hours agoprevhttps://en.wikipedia.org/wiki/Food_irradiation reply tracerbulletx 7 hours agoprevIt's crazy how the GoPro survived, and the little lcd gauge got fried. Those things are so impressively tough. reply InitialLastName 7 hours agoparentA half-inch thick lead box is a lot more help than the gauge gets. reply geocrasher 15 hours agoprevThat looks a lot like several levels of the first Portal. That conveyor system didn't run on GladOS, did it? reply sandworm101 12 hours agoprevLike the original Pirates of the Caribbean, but with science! Fyi, if you start seeing those little single-pixel stars in your VR headset, you have probably wandered out of the play area. reply conjecTech 11 hours agoprevDidn't expect to see a familiar face at the end! Andrew, if you're reading this, greetings from a fellow RoboJacket! reply SomeoneFromCA 13 hours agoprevHa, now I see why I suddenly got it in my youtube recommendations. reply wkat4242 21 hours agoprevI wonder what that thing is actually used for reply chankstein38 17 hours agoparentI believe I was actually at this facility around the time this video was taken! I was there with a large group mixing scientists and others who were sending acrylic through it to make captured lightning/lightning sculptures/lichtenberg figures! It was a really cool experience! I met this guy there: https://www.capturedlightning.com/ reply m463 7 hours agorootparentThat seemed familiar - I have some shrunken coins: https://www.capturedlightning.com/frames/interesting1.html reply nhecker 14 hours agoparentprev\"The irradiation facility is the Mercury Plastics NeoBeam facility which is used for cross linking plastics\" -- http://www.rtftechnologies.org/physics/neobeam-m12d10y2016.h... reply myself248 15 hours agoparentprevOne common use is cross-linking polymers. If you've ever used cable with XLPE insulation, it was initially extruded onto the wire as normal polyethylene, then spooled back and forth between drums passing under an e-beam system like this, to perform the cross-linking. It's phenomenally tough stuff after that; a lot of cars use XLPE (in its thinwall TXL spec) wiring. reply r2_pilot 21 hours agoparentprevIn this video, it's for examining calcite according to the description; I believe I've also seen similar devices used for making lichtenburg figures in acrylic. reply GordonS 20 hours agorootparentI believe they are used to irradiate cannabis too (to kill mould & yeast) - electron-bean irradiation retains much more terpenes than gamma irradiation does! reply GordonS 12 hours agorootparentYes, a lot of commercially grown cannabis is, unfortunately, irradiated. Different countries (abd different US states) have different laws about the amount of mould and yeast that are permitted on sold cannabis flowers (as well as other things, such as moisture content). The target is quite hard to meet got some markets, including the UK and EU. To get to the required levels, irradiation isn't necessary... but it's cheap and easy. I get the sense that it's often used because of poor growing practises, more than anything else. I can't speak for other countries, but in the UK most cannabis has until recently been gamma irradiated - which often led to practically terpless flower that smelt of damp hay. More recently, e-beam (electron-beam) irradiation has largely taken over - and it seems to be much, much kinder on terpenes! Honestly, most is now impossible to tell apart from non-irradiated flower. I don't have actual data, I'm afraid. There's a single paper talking about cannabinoid and terpene content of cannabis after gamma irradiation, which was paid for by Bedrocan - but I haven't found anything about how it compares to e-beam. reply Ballas 21 hours agorootparentprevYou might be correct on both counts: https://fusor.net/board/viewtopic.php?t=8666 (I'm not sure if this is the same run, the link in my link seems to be dead) reply joarv0249nw 19 hours agorootparentprevCould be for food irradation: https://en.wikipedia.org/wiki/Food_irradiation reply ein0p 15 hours agoprevI‚Äôm surprised it not only survived this, but also did not even stop filming. reply i80and 13 hours agoparentI wondered about that, but from the description: > GoPro is enclosed in a 3/8\" thick lead pig with a 1/2\" thick, 50% lead glass window. Additionally there is a 1/4\" thick lead plate above the camera box to provide shielding from direct irradiation from the beam. reply scaredginger 15 hours agoparentprevIt was shielded reply orbital-decay 15 hours agoprev [‚Äì] Related: https://en.wikipedia.org/wiki/Anatoli_Bugorski https://en.wikipedia.org/wiki/Tr%E1%BA%A7n_%C4%90%E1%BB%A9c_... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A GoPro video demonstrates a ride through an electron beam irradiator operating at full beam power for 2 minutes and 41 seconds.",
      "The footage provides a unique perspective on the inner workings of an electron beam irradiator.",
      "This post offers an exciting visual journey into the operation of such a high-powered scientific device."
    ],
    "commentSummary": [
      "The discussion covers a wide range of topics on electron beam irradiation, from comparing it to video games and industrial uses to debating the viability of using charged particle beams for sterilization.",
      "Participants delve into the effects of radiation exposure, applications in manufacturing, cannabis irradiation, and studying calcite using electron-beam technology.",
      "The conversation also delves into speculative scenarios about extraterrestrial life, alien invasions, and the technological capacities of advanced civilizations, along with the risks of mishandling radiation equipment and personal anecdotes on the subject."
    ],
    "points": 162,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1708342600
  }
]
