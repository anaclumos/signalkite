[
  {
    "id": 38153573,
    "title": "Exploring the Technical Issues and High System Requirements of Cities: Skylines 2",
    "originLink": "https://blog.paavo.me/cities-skylines-2-performance/",
    "originBody": "Why Cities: Skylines 2 performs poorly The teeth are not the only problem 2023-11-05 Table of contents One of the most highly anticipated PC games of the year Cities: Skylines 2 was released last week to a mixed reception. My impression is that gameplay and simulation-wise it seems to be a step in the right direction, and at least on paper the game seems more well-rounded in terms of features than the original was at launch. There are however significant issues with the game, ranging from balance problems and questionable design choices to bugs rendering a lot of the gameâ€™s economic simulation almost pointless. Whether or not the game is a worthy successor to the original is an open question at this point, but one thing is almost universally agreed upon: the titleâ€™s performance is not up to par. (This is not) a performance review There were warning signs. Under a month before release date there was an announcement informing that the gameâ€™s recommended system requirements had been raised, and the console release was delayed to 2024. Quite a few YouTubers and streamers got early access to the game, but they were explicitly forbidden to talk about performance until the regular review embargo was lifted. This wasnâ€™t exceptional as games tend to receive frequent performance optimizations and other fixes in the last few weeks before release, but it wasnâ€™t a good sign either. Then, just a week before release Colossal Order issued a statement which Iâ€™d describe as a pre-emptive apology for the poor performance of the game. And then the game was released. Simulation-heavy games like city builders can be surprisingly hard to run at a good framerate, but what makes Cities: Skylines 2 stand out is that on most systems and in most situations the game is GPU-bound â€” rather unusual for a game of this genre, as most games like this tend to be really heavy on the CPU (such as the original Cities: Skylines), but relatively light on graphics. Visually the game is an improvement on most aspects compared to the 2015 original, but nothing really justifies the game being harder to run than maxed out Cyberpunk 2077 with path tracing enabled. Personally Iâ€™d even argue that C:S2 looks quite unpleasant; while individual models are relatively detailed and the sense of scale is impressive, the shading is decidedly last-gen and the screen is absolutely covered in rendering artifacts and poorly filtered textures. Comparing the gameâ€™s graphics to a relatively close competitor Anno 1800 (which was released in 2019) doesnâ€™t do it any favours. Anno goes for a slightly more stylized look and in my humble opinion manages to look more polished and consistent, while offering decent performance even on hardware that was considered low-to-mid range in 2019. Thereâ€™s a lot going on this with this character: oddly smooth hair, weirdly chunky beard, suspicious moustache, 100% opaque drug dealer sunglasses, a seam between the head and the rest of the body, and painted-on clothes. Iâ€™m not going to waste my time meticulously benchmarking the game because many others have done that already and to a significantly higher standard than I could. If you are interested in that, check out this written article from PC Games Hardware (in German) or this video from Gamers Nexus (in Americanese). Iâ€™ll summarize the results as such: when the settings are turned above the bare minimum (the â€œvery lowâ€ graphics preset completely disables decadent luxuries such as shadows and fog) you need a graphics card that costs about 1000 to 2000 euros to run at the game at 60 frames per second at 1080p. As a comparison similar hardware in Alan Wake 2 â€” which was released the same week as C:S2 and is considered by some to be the best looking game of this console generation â€” reaches comparable average framerates with all settings cranked including path tracing, either at 1440p without any upscaling magic or at 4K with some help from DLSS. I think thatâ€™s a good illustration of how bizarrely demanding C:S2 is. The game ran so poorly that Windows Game Bar refused to acknowledge there even was a framerate. Some personal experiences to cap off this introduction: when I started the game for the first time on my relatively beefy gaming PC (equipped with an NVidia RTX 3080 graphics card, an AMD Ryzen 7 5800X CPU and a 5120x1440 super ultrawide monitor) I was greeted by a frame rate of under 10 FPS in the main menu. After tweaking the settings as instructed by the developer (which involved disabling depth of field, motion blur and volumetric effects) my FPS reached almost 90. What made this especially bizarre was that the menu features just a static background image and a few buttons. Loading into an empty map gave me about 30 to 40 FPS, and the frame rate stayed around that level after playing for about an hour, with the occasional stutter. Letâ€™s investigate. 100% GPU usage and 7 frames per second, everyone. Cropped from 32:9. Pulling back the curtain Cities: Skylines 2 like its predecessor is made in Unity, which means the game can be decompiled and inspected quite easily using any .NET decompiler. I used JetBrains dotPeek which has a decent Visual Studio -like UI with a large variety of search and analysis options. However static analysis doesnâ€™t really tell us anything concrete about the rendering performance of the game. To analyze whatâ€™s going with rendering I used Renderdoc, an open source graphics debugger which has saved my bacon with some of my previous GPU-y personal projects. Engine and architecture Letâ€™s go through some of the technical basics of the game. Cities: Skylines 2 uses Unity 2022.3.7, which is just a few months old at the time of writing. The most notable aspect of Unity 2022 was the stabilization of the DOTS set of technologies which Unity has been working on for several years, and interestingly C:S2 seems to be largely built on top of this stuff, including the newfangled Entity Component System (ECS) implementation and Burst compiler. Iâ€™ve been interested in the ECS architecture for a few years and have experimented with implementations such as Specs, Legion and most recently Bevy, and if it werenâ€™t for the many issues in this game Iâ€™d rather be writing about how ECS is basically the ideal architecture for a game like this. Cities: Skylines 2 seems to use DOTS to great effect as the game makes use of multiple CPU cores much more efficiently than its predecessor. Unfortunately a lot of the graphics-related issues are indirectly caused by the gameâ€™s use of DOTS. Iâ€™ll expand on that later. Judging by the code there are about 1200 different systems powering practically all of the game logic. The game also utilizes a bunch of third party middleware and some custom / forked libraries. Unlike DOTS, Unityâ€™s UI Toolkit is apparently still not ready for production as C:S2 uses HTML, CSS and JavaScript based Coherent Gameface (what a name!) for its user interfaces. A brief glance at the JS bundle reveals that they are using React and bundling using Webpack. While this is something that is guaranteed to make the average native development purist yell at clouds and complain that the darned kids should get off their lawn, I think at least on paper this will make the gameâ€™s UIs significantly easier to maintain and modify than before. Other notable bundled libraries include InstaLOD, Odin Serializer, and a DLL file for NVidia DLSS 3, even though the technology is not currently supported by the game. For graphics rendering the game makes use of Direct3D 11 and Unityâ€™s High Definition Rendering Pipeline, also known as HDRP. Unityâ€™s regular rendering system only works with traditional MonoBehaviour-based game objects, so a game built using DOTS & ECS needs something to bridge the gap. Unity has a package called Entities Graphics, but surprisingly Cities: Skylines 2 doesnâ€™t seem to use that. The reason might be its relative immaturity and its limited set of supported rendering features; according to the feature matrix both skinning (used for animated models like characters) and occlusion culling (not rendering things that are behind other things) are marked as experimental, and virtual texturing (making GPU texture handling more complex but hopefully more efficient) is not supported at all. Instead it seems that Colossal Order decided to implement the glue between the ECS and the renderer by themselves, utilizing BatchRendererGroup and a lot of relatively low level code. Iâ€™ll cover this and its many implications in more detail later. Attachment issues Getting Renderdoc attached to a process and collecting rendering events is usually quite trivial. Normally you just need to provide Renderdoc the path of the executable, the working directory and some command line arguments, and then Renderdoc starts the binary and injects itself to the game process. However, my issue was that I had the game on Xbox Game Pass, which does some weird sandboxing and / or NTFS ownership magic to limit what you can do with the game files. Renderdoc was not allowed to read the gameâ€™s executable, even when running as administrator. Before I knew Game Pass was the problem I also tried to use NVidia Nsight Graphicsâ„¢ instead (a tool similar to Renderdoc from NVidia), but it had the same issue. Ultimately I ended up solving that particular problem with my credit card: I bought the game again on Steam at full price, despite knowing it had severe issues. Sorry. However, the Steam version didnâ€™t immediately start co-operating either. This time the problem was Paradox Launcher, a little piece of bloatware used in most big budget Paradox-published titles. The launcher binary is also included in the Game Pass version, but at least on release it seemed to be completely unused. Basically when you start C:S2 from Steam it pops up Paradox Launcher, you click either Resume or Play, and then it actually runs the game binary. I tried to attach Renderdoc by running Cities2.exe directly but that didnâ€™t work â€” it creates the game window, but then runs for a few seconds, opens the launcher and then exits. There is an option in Renderdoc called â€œCapture child processesâ€ which should in theory make Renderdoc inject itself to all processes started by the target process â€” so it should attach itself to the launcher started by the game binary, and then get injected to the game binary again â€” but I think there was some extra layer of indirection which unfortunately prevented that from working. I configured Renderdoc to start Paradox Launcher directly, but in short that didnâ€™t work either, as Steam and the launcher do some communication to select which game to start and to handle authentication / DRM thingies. Some of that communication happens through command line arguments which I was able to extract using Process Explorer, but reusing the same arguments didnâ€™t work either, so I gave up on that approach as well. Ultimately I was able to finally attach Renderdoc by using the Global Process Hook option, which the program hides by default and advices against using. It is a very invasive method of hooking as it injects a DLL to every single process that is started on the system, but hey, it worked! We can finally see whatâ€™s going on. This place is a message... and part of a system of messages... pay attention to it! Sending this message was important to us. We considered ourselves to be a powerful culture. This place is not a place of honor... no highly esteemed deed is commemorated here... nothing valued is here. â€” Renderdoc when you try to enable global process hooking. I was later able to get NVidia Nsight Graphicsâ„¢ working as well. Instead of trying to start the game or the launcher, I opened Steam from Nsight and then started the game from Steamâ€™s UI as usual. Ultimately I wasnâ€™t able to get much more information out of Nsight than I already had from Renderdoc, as it seems that many of NSightâ€™s profiling and performance focused features are not supported with D3D11. Renderdoc analysis Iâ€™ll preface this section by admitting that Iâ€™m not a professional graphics programmer nor even a particularly proficient hobbyist. I do graphics programming occasionally and have spent quite a lot of time toying with game engines, but Iâ€™m not an expert on either subject. I have never implemented Actual Proper Graphics Things like deferred rendering or cascaded shadow mapping, though I think I know how they should work in theory. So as difficult it might be to believe, there is a chance that Iâ€™m wrong about some of the things Iâ€™m about to say. If you think Iâ€™m wrong, please let me know! Letâ€™s begin by analyzing the following frame (click to open it as a new tab): This is a decently complex frame, but itâ€™s far from the scale the game can actually reach. This was captured in a town of about 1000 inhabitants under an hour into a new save. Thereâ€™s rain and itâ€™s night time, but in my experience neither moves the needle much in terms of performance. The game version was 1.0.11f1, so the first post-release hotfix is included. It should be noted that latest patch at the time of publication (1.0.12f1) was released during the making of this article and it includes some improvements for the issues Iâ€™m about to describe, but itâ€™s far from having solved all of them. Renderdoc reports that the frame took about 87.8 milliseconds to render, which would average to about 11.4 FPS. The game was running at 30-40 FPS on average at the time, so either this frame is an outlier (which â€” as weâ€™ve learned from the Gamers Nexus video â€” is ironically quite common) or perhaps more likely Renderdoc adds a bit of overhead in a way that affects the measurements, as all of the frames Iâ€™ve captured have reported slightly higher frame times than what Iâ€™ve seen in-game when playing normally. Iâ€™m making the assumption that even if Renderdoc does add some overhead, it adds it in a way that doesnâ€™t completely invalidate the measurements, like making specific API calls take 10x longer than they normally would. For reference, at consistent 60 FPS the frametime should always be about 1000 / 60 = 16.666... milliseconds. Here are some basic rendering statistics reported by Renderdoc: Draw calls: 6705 Dispatch calls: 191 API calls: 53361 Index/vertex bind calls: 8724 Constant bind calls: 25006 Sampler bind calls: 563 Resource bind calls: 13451 Shader set calls: 1252 Blend set calls: 330 Depth/stencil set calls: 301 Rasterization set calls: 576 Resource update calls: 1679 Output set calls: 739 API:Draw/Dispatch call ratio: 7.73796 342 Textures - 3926.25 MB (3924.10 MB over 32x32), 180 RTs - 2327.51 MB. Avg. tex dimension: 1611.08x2212.36 (2133.47x2984.88 over 32x32) 4144 Buffers - 446.59 MB total 6.48 MB IBs 43.35 MB VBs. 6700.34 MB - Grand total GPU buffer + texture load. Thereâ€™s not much we can deduce from these figures alone. 6705 draw calls and over 50000 API calls both sound like a lot, but without further context their cost is hard to evaluate. 6.7 gigabytes of used video memory is a lot for a relatively simple scene like this, especially considering there are still current generation mid-tier graphics cards with only 8 gigabytes of VRAM. Since the game uses HDRP its documentation might serve as a good starting point for understanding the different rendering and compute passes the game perform on each frame. Iâ€™m not going to do a fancy graphics study like these legendary ones for DOOM 2016 and GTA V, but Iâ€™ll go through most of the rendering process step by step and highlight some of the more interesting things along the way. DOTS instance data update Almost every draw call the game makes uses instancing, which is required in a game of this scale. To make instancing work the game has a single large buffer of instance data which contains everything necessary for rendering any and all objects. The contents and size of per-instance data varies by the type of entity but it seems that regular game objects like buildings take about 50 floats per instance, roads significantly more. I havenâ€™t fully figured out how the buffer is managed because itâ€™s a very complex system, but essentially the instance data for every visible object is updated to the buffer each frame, and the changes are then uploaded to the GPU. The buffer starts at about 60 megabytes, and is reallocated to a larger size when necessary. The buffer is used for practically every draw call the game makes, and according to Renderdoc itâ€™s at least available in every vertex and pixel shader, though I would assume itâ€™s primarily only used in vertex shaders. It would be interesting to know how this buffer affects GPUâ€™s cache as I would assume instances are not laid out in the buffer in the same order they are rendered and that could be a problem for caching, but I lack the expertise to figure that out. Regardless there is a certain cost associated with looking up data from this buffer for every vertex, and it might explain some of the issues regarding high poly meshes Iâ€™ll get to soon. Simulation Several compute shaders are used for graphics-related simulations, such as water, snow and particles, as well as skeletal animation. These take about 1.5 milliseconds in total, which is under 2% of frame time. One early theory regarding the gameâ€™s poor performance was that maybe it was offloading a lot of the actual game simulation to the GPU, saving CPU time but taking processing power away from rendering. However, I can conclude based on both decompiled code and GPU calls that this is simply not the case. Virtual texturing cache update Remember how I mentioned that virtual texturing is not supported by Entities Graphics? Well, it seems that C:S2 implements its own virtual texturing / texture streaming system. I first assumed that the game is using Unityâ€™s built-in solution for that, but in traditional Unity fashion even though it was added to the engine in 2020 following an acquisition it remains as experimental and unsupported as ever (if not more so). What is virtual texturing, anyway? My understanding is that virtual texturing is an approach for loading and managing texture data in a potentially more memory efficient way than the mÃ©thode traditionnelle of using one GPU texture per texture asset. Textures are stored in texture atlases, which are basically fancier versions of sprite sheets (which I also happened to cover in my GPU tile mapping article). Atlases consist of tiles of a fixed size, and each tile can contain one or more textures. The trick which can save memory is that large textures can be split into multiple tiles, so if you have a large texture that is only visible in a small portion of the screen, you only need to load the tiles that are actually visible. Virtual texture visibility information is produced as a side product of normal rendering in a later pass, and the visibility information is used on the CPU side to determine which tiles need to be loaded and which can be unloaded. If you want to know more, Unreal Engineâ€™s documentation seems to offer a great description of the technique in more detail. The game seems to use virtual texturing for all static 3D objects except the terrain. One of the tile atlases used in the rendering of my example frame. Massively downscaled; the original is 16368x8448. This approach to texturing is quite elegant in theory but it comes with many tradeoffs, and the gameâ€™s implementation still has some teething issues, such as high resolution textures sometimes failing to load even when the surface is close to the camera. The use of virtual texturing is also likely the culprit for the gameâ€™s lack of support for anisotropic texture filtering, a standard feature in PC games since the beginning of the millennium. The pass took about 0.5 milliseconds. Skybox generation The game uses Unity HDRPâ€™s built-in sky system, so it generates a skybox texture (a cubemap) every frame. This takes about 0.65 milliseconds which is not a lot compared to everything else, but if the game was targeting 60 FPS it would be almost 4% of the total frame time budget. Pre-pass Now we get to the actual rendering. C:S2 uses deferred rendering, which basically means that rendering is done in many phases and using several different intermediate render targets. The first phase is the pre-pass, which produces per-pixel depth, normal and (presumably) smoothness information into two separate textures. This pass is surprisingly heavy as it takes about 8.2 milliseconds, or roughly about far too long, and this is where some of the biggest issues with the gameâ€™s rendering start to appear. But first we need to talk about THE TEETH. The teeth controversy One bizarre yet popular talking point about Cities: Skylines 2â€™s performance is the fact that the character models have fully modelled teeth, even though thereâ€™s literally no way to see them in-game, unless we count using the photo mode and clipping the camera inside a characterâ€™s head. Reddit user Hexcoder0 did some digging using NVidia Nsight Graphicsâ„¢ and posted their findings in to a thread in the official subreddit (which inspired me to do my own research and write this pointlessly long article). It was revealed that not only does the game have fully modelled teeth, they are rendered literally all the time at maximum quality. More importantly this is the case for everything related to characters: none of the character meshes have any LOD variants. Colossal Order was quick to acknowledge this publicly, and they even referenced broader problems with LOD handling. Ignore all the weird rambling about simulating citizensâ€™s teeth and whatnot; this is not Dwarf Fortress so they are not doing that, and even if they were that obviously wouldnâ€™t require rendering the teeth. Colossal Order has also told us that that they are using a middleware called Didimo Popul8 to generate the character models. If I recall correctly the teeth controversy began even before the game was released when someone noticed that the Didimo character specification includes separate meshes for things like teeth and eyelashes. I had originally assumed that the game is using Didimoâ€™s default character meshes â€” because to be honest they look very generic and soulless â€” but now Iâ€™m not so sure. The meshes in the game in fact have even more polygons than Didimoâ€™s defaults: the infamous mouth / teeth model for example consists of 6108 vertices, significantly more than the default meshâ€™s 1060. A single character even before we add hair, clothing and accessories is about 56 thousand vertices, which is a lot. For context the average low-density residential building uses less than 10 thousand vertices before yard props and other details are added. In this example frame the game renders 13 sets of teeth, and their visual impact on the frame is zero: not a single pixel is affected. Even the characters themselves contribute basically nothing to the frame except for noise and artifacts. At this distance and rendering resolution an individual character (the purple Tetris block) affects a literal handful of pixels. Scaled 4x to make the individual pixels visible. Pre-pass continued, featuring the high poly hall of shame The egregiously unoptimized character models are not the sole cause of the gameâ€™s poor performance (because itâ€™s never that easy), but they are an indicator of the broader issues with the gameâ€™s assets and rendering. The game regularly draws too many objects with too many polygons that have quite literally zero impact on the final image. This is not specific to the pre-pass, as the same issues seem to affect all rendering passes which rasterize geometry. I think there are two main causes for this: Some models donâ€™t have any LOD variants at all. The gameâ€™s culling system is not very advanced; the custom rendering code only implements frustum culling and thereâ€™s no sign of occlusion culling at all. There is some culling based on distance but itâ€™s not very aggressive, which is great for avoiding pop-in but bad for performance. Here are a few other examples besides the character models. This highly detailed pallet of gas tanks consists of over 17K vertices. These densely packed clotheslines are made from 25K vertices per piece and feature dozens of individually modelled clothespins, or laundry boys as we call them in Finland. Thereâ€™s also an even more dense variant featuring over 30K vertices. This parking booth mesh is technically not used in my example frameâ€™s pre-pass, but it is still present in the scene and later used in the shadow mapping pass. This mesh consists of over 40K vertices with no LODs, and features luxurious details you donâ€™t even get in most AAA games, like individually modelled cables connecting screens and keyboards. They are even routed through a (relatively round) hole in the desk! Combining the building and the furniture into one mesh saves on draw calls, but it also means that the props canâ€™t be culled individually. This mesh of a pile of logs is similarly only used in the shadow rendering pass, and features over 100K vertices. It is the highest poly model in the game Iâ€™ve encountered so far, though I havenâ€™t played for more than a few hours. Now you might say that these are just cherry-picked examples, and that modern hardware handles models like these just fine. And you would be broadly correct in that, but the problem is that all of these relatively small costs start to add up, especially in a city builder where one unoptimized model might get rendered a few hundred times in a single frame. Rasterizing tens of thousands of polygons per instance per frame and literally not affecting a single pixel is just wasteful, whether or not the hardware can handle it. The issues are luckily quite easy to fix, both by creating more LOD variants and by improving the culling system. It will take some time though, and it remains to be seen if CO and Paradox want to invest that time, especially if it involves going through most of the gameâ€™s assets and fixing them one by one. To be clear having highly detailed models is not a problem in itself, especially if you are intending to make a self-proclaimed next generation city builder. The problem is that the game is struggling to handle this level of detail, and that polygons are used inefficiently and inconsistently. For every character model with opulently modelled nostril hairs there are common props with surprisingly low polycounts. I think if the game ran well people would be celebrating these highly detailed models and making hyperbolic social media posts and clickbait videos titled â€œOMG the devs thought of EVERYTHING ðŸ¤¯ðŸ¤¯ðŸ¤¯â€ and â€œI canâ€™t believe they modelled the cables in the parking booth ðŸ˜±ðŸ˜±ðŸ˜±â€ and â€œCITY SKYLINES 2 MOST DETAILED GAME EVER CONFIRMED?â€. Instead we are here. Oh yeah I was talking about rendering at some point, wasnâ€™t I? Letâ€™s continue. Motion vectors The game renders per-pixel motion vectors as a separate pass, which can be used for anti-aliasing and motion blur. I think motion vectors are slightly broken now, which is also the reason the game doesnâ€™t support DLSS or FSR2 at the time of writing. There is a temporal anti-aliasing option hidden in the advanced settings menu and it improves the rendering quality to some extent, but things animated using vertex shaders like trees are just covered in artifacts and ghosting. This pass takes about 0.6 milliseconds. Roads and decals Now we are finally rendering something recognizable: roads! And lawns, and other things that follow the surface of the terrain. This pass takes about 1 millisecond. Main pass This is the meat (vegan alternatives are available) of the deferred rendering process. This pass takes in all the intermediate render targets produced so far alongside the virtual texture caches and some seemingly hardcoded textures and produces several more buffers, including ones for albedo, normals, different PBR properties and depth. It also produces the virtual texture visibility information I mentioned earlier. It is rendered at half horizontal resolution, presumably as an optimization. Terrain doesnâ€™t use virtual texturing, so it is rendered at full resolution and with a constant color regardless of the actual terrain texture. This pass takes 16.7 milliseconds, or about as long the entire frame should take if we were aiming for 60 frames per second. The pass rasterizes all of the geometry again, so the same reasons for the pre-pass being slow apply here as well. The additional cost is probably explained by the number of additional outputs, plus the cost of virtual texture cache lookups and texture mapping itself. Ambient occlusion Next the game produces an ambient occlusion buffer using motion vectors, normals and the depth buffer plus copies of the last two from the previous frame. Judging by the debug names of the shaders the algorithm is GTAO. This takes about 1.6 milliseconds. Cascaded shadow mapping C:S2 uses cascaded shadow mapping, and in my opinion not very well. Shadows are full of artifacts and constantly flickering especially when either the sun or any foliage are moving (and they are, all of the time). Even when the screen isnâ€™t completely covered with artifacts, the resolution of the shadows is quite low, and the jump in quality between the different shadow cascades is very noticeable. The game uses four cascades with a resolution of 2048x2048 pixels per cascade. Thereâ€™s a directional shadow map resolution setting in the advanced graphics settings menu, but at the time of writing itâ€™s not connected to anything in the code; neither the individual setting nor the overall shadow quality setting alters the resolution of the shadow map. This is the reason why the medium and the high shadow setting presets are literally identical. I donâ€™t know whether this is an oversight or if the setting was hastily disabled because it was causing issues. The low preset differs from medium and high in that it disables shadows cast by the terrain. Despite the low quality, it is by far the slowest rendering pass, taking about 40 milliseconds or almost half of total frametime. It also dwarfs all other passes in terms of the number of draw calls: in my test frame 4828 out of 6705 draw calls were for shadow mapping, a staggering 72%. This is why thereâ€™s such a huge performance gain when shadows are disabled. The reasons behind this passâ€™s slowness are mostly the same as with the pre-pass and the main pass: too much unnecessary geometry rendered with way too many draw calls. Renderdocâ€™s performance counters view indicates that many of the draw calls affect between zero and under 100 pixels in the shadow map, and the teeth are back again. The game seems to treat every single 3D object as a potential shadow caster on all quality settings regardless of size or distance. Thereâ€™s a lot of room for optimization here, and in theory general improvements to LODs and culling should have a large impact on shadow mapping performance as well. Hopefully after performance has been improved CO (or modders) can turn the shadow quality setting back up, and raise the shadow map resolution to something more 2023. Letâ€™s end this part on a positive side note: when digging into shadow handling code I stumbled upon the fact that the game computes the positions of the sun and the moon using the current date, time and coordinates of the city. Thatâ€™s a really neat detail! Screen space reflections and global illumination The game uses Unity HDRPâ€™s built-in implementations of screen space reflections (SSR) and screen space global illumination (SSGI). I wonâ€™t be covering them in detail because Unityâ€™s documentation is already decently comprehensive, plus Iâ€™m not going to pretend I fully understand them. Global illumination uses ray-marching and is evaluated by default at half resolution. Denoising and temporal accumulation are used to improve the quality. It would be nice if the game as a self-proclaimed next generation city builder supported hardware accelerated ray tracing in addition to these screen space solutions, but Iâ€™m not holding my breath. These two effects combined took about 3 milliseconds. Deferred lighting This is where it all comes together. Most of the intermediate buffers produced so far are combined to render the near-final image. Not much more to say about this pass, except that it takes about 2.1 milliseconds. Weird clothing pass Thereâ€™s a small rendering pass just for the clothes of the Didimo characters, in this case 3 dresses, 1 jumpsuit and 1 set of suit trousers. The remaining 8 characters are either naked or their clothes use different shaders. This pass affects almost no pixels at this zoom level. Luckily it takes just 0.2 milliseconds. Sky rendering The sky is rendered next using the previously generated skybox texture, though it is not visible in my example frame. This pass takes about 0.3 milliseconds. Transparent objects pre-pass Traditional deferred rendering doesnâ€™t work with transparent objects, so they are rendered separately. Transparent objects are rendered in two phases, starting with this pre-pass which only updates the normal and depth buffers. There are not many unique transparent objects in the frame, so this pass takes about 0.12 milliseconds. Water rendering The game does some pre-processing in compute shaders to prepare for water rendering and then produces several downscaled and blurred versions of the almost-final image. These inputs are fed to the main water rendering shader which renders the water surface. This takes about 1 millisecond. Particles, rain and transparent objects This pass handles most things transparent, including particles, weather effects and 3D objects made of glass and other transparent materials. No particles are visible in the frame, but the game still tries to render the smoke from the industrial zoneâ€™s chimneys, as well as the stream of goop produced by the sewage pipe. Rain is rendered next, using 20 instances of 12K vertices each. Interestingly the remaining transparent objects are rendered after the rain, causing some weirdness when transparent objects (like greenhouses and power lines) and rain overlap. All of this takes about 0.56 milliseconds. VT feedback processing The virtual texture visibility buffer we got earlier is processed with a compute shader, resulting in an output texture 1/16th of the original resolution. For the visualization I nearest neighbor scaled the output 8x to make it more readable. This is the information the game ultimately gets back from the GPU to decide which texture tiles to load and unload. Renderdoc reported very little time spent on this, well under 0.1 milliseconds. Bunch of post-processing The game uses many of Unityâ€™s built-in post-processing effects, including temporal AA (which is a bit broken as I previously mentioned), bloom and tonemapping, plus DOF and motion blur if enabled. I canâ€™t be bothered to sum up the timings of all of these, but itâ€™s about 1 to 2 milliseconds in total. Outlines, text and other UI The last remaining draw calls are used to render all of the different UI elements, both the ones that are drawn into the world as well as the more traditional UI elements like the bottom bar and other controls. Quite a lot of draw calls are used for the Gameface-powered UI elements, though ultimately these calls are very fast compared to the rest of the rendering process. The names of roads are rendered into the scene using 2D signed distance fields. The depth buffer is used to blend the text with the scene if the text is behind a building or other object, which is a nice touch. This final pass takes an irrelevant amount of time. And we are done! I tried not to make this into an in-depth graphics study, but I think I failed. Hope you learned something new. Summary and conclusions So why is Cities: Skylines 2 so incredibly heavy on the GPU? The short answer is that the game is throwing so much unnecessary geometry at the graphics card that the game manages to be largely limited by the available rasterization performance. The cause for unnecessary geometry is both the lack of simplified LOD variants for many of the gameâ€™s meshes, as well as the simplistic and seemingly untuned culling implementation. And the reason why the game has its own culling implementation instead of using Unityâ€™s built in solution (which should at least in theory be much more advanced) is because Colossal Order had to implement quite a lot of the graphics side themselves because Unityâ€™s integration between DOTS and HDRP is still very much a work in progress and arguably unsuitable for most actual games. Similarly Unityâ€™s virtual texturing solution remains eternally in beta, so CO had to implement their own solution for that too, which still has some teething issues. Hereâ€™s what I think that happened (a.k.a this is speculation): Colossal Order took a gamble on Unityâ€™s new and shiny tech, and in some ways it paid off massively and in others it caused them a lot of headache. This is not a rare situation in software development and is something Iâ€™ve experienced myself as well in my dayjob as a web-leaning developer. They chose DOTS as the architecture to fix the CPU bottlenecks their previous game suffered from and to increase the scale & depth of the simulation, and largely succeeded on that front. CO started the game when DOTS was still experimental, and it probably came as a surprise how much they had to implement themselves even when DOTS was officially considered production ready. I wouldnâ€™t be surprised if they started the game with Entities Graphics but then had to pivot to custom solutions for culling, skeletal animation, texture streaming and so on when they realized Unityâ€™s official solution was not going to cut it. Ultimately the game had to be released too early when these systems were still unpolished, likely due to financial and / or publisher pressure. None of these technical issues were news for the developers on release day, and I donâ€™t believe their claim that the game was intended to target 30 FPS from the beginning â€” no purebred PC game has done that since the early 2000s, and the graphical fidelity doesnâ€™t justify it. While I did find a lot to complain about the gameâ€™s technology, this little investigation which has been consuming a large share of my free time for the past 1.5 weeks has also made me appreciate the gameâ€™s lofty goals and sympathize more with the developers of this technically ambitious yet troubled game. Iâ€™ve learned a lot about how Cities: Skylines 2 & Unity HDRP work under the hood, and Iâ€™ve also gotten some good practice with Renderdoc. If you liked this article, good for you! I donâ€™t have anything to sell you. Write a comment or something to the media aggregator or social media of your choice. Subscribe to my Atom feed if it still works. Stay tuned for my next article in a couple of years.",
    "commentLink": "https://news.ycombinator.com/item?id=38153573",
    "commentBody": "Why Cities: Skylines 2 performs poorlyHacker NewspastloginWhy Cities: Skylines 2 performs poorly (paavo.me) 840 points by paavohtl 23 hours ago| hidepastfavorite538 comments dang 19 hours agoHey all: this is an interesting article. Can we please discuss what&#x27;s specifically interesting here?Threads like this tend to become occasions for responding generically to stuff-about-$THING (in this case, the game), or stuff-about-$RELATED (in this case, the framework), or stuff-about-$COMPARABLE. There&#x27;s nothing wrong with those in principle but each step into genericness makes discussions shallower and less interesting. That&#x27;s why the site guidelines include \"Avoid generic tangents\" - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html thrillgore 21 hours agoprev\"And the reason why the game has its own culling implementation instead of using Unityâ€™s built in solution because Colossal Order had to implement quite a lot of the graphics side themselves because Unityâ€™s integration between DOTS and HDRP is still very much a work in progress and arguably unsuitable for most actual games.\"This sadly tracks with my own experiences with Unity&#x27;s tooling, where DOTS did ship but its implementation rots on the vine like every other tool they acquired. The company is woefully mismanaged, its been mismanaged, and given the very public pricing incident from a few weeks back, they aren&#x27;t focusing on improvements to the engine, but on any way to scrap money from its users.Bevy&#x27;s ECS implementation is really good, and I want to see it succeed here, in addition to Godex. reply torginus 7 hours agoparentTbh, there are many ways to skin a cat, using ECS is not necessary to ship a performant game, not is it necessarily the modern thing to do. It&#x27;s one of the many architectural patterns you can build a game on top of, and has many advantages as well as detractors.For example, Godot isn&#x27;t really build around ECS, but the idea of servers which are mostly autonomous game subsystems that can process their area of expertise (rendering, physics etc.) mostly independently, and are loosely coupled to the general game logic.ECS architectures originate in the PS2&#x2F;PS3 era, when CPUs were awful. Tiny caches, horrible branch mispredict penalties, slow memory, fragmented memory spaces and lack of a random access storage forced developers to build their games around predictable memory access patterns, that in general resulted in streaming architectures, where data necessary for the game data to be streamed in tiny chunks to be processed.While generally this is good practice even nowadays, with the advent of superfast CPUs with amazing speculative execution, great branch predictors, and tens of megabytes of cache, this is no longer strictly necessary, especially considering that most modern games haven&#x27;t really increased that much in terms of stuff going on on the screen compared to say, a decade or two ago. It&#x27;s still uncommon for the player to fight more than a dozen dudes in an action game.And in games with thousands of things on screen at the same time, often specialist logic and handling is necessary. reply nvm0n2 3 hours agorootparentSure, but surely a city simulation is the perfect textbook use case for a cache-friendly architecture like ECS? reply torginus 1 hour agorootparentWell, ECS only makes some operations cache friendly&#x2F;parallelizable - namely linear iteration through the array of entities, where not all fields of each entity are used.I&#x27;d imagine a city simulation involves a lot of graph lookup and traversal, spatial lookups and is generally a hairy and messy affair, not sure how easy it is to adapt that to ECS.Then again, SimCity and Transport Tycoon simulated complex cities on, by today&#x27;s standards, very frugal hardware, so city simulation might not even be such a CPU hog. reply johnnyanmac 13 hours agoparentprev>Bevy&#x27;s ECS implementation is really good, and I want to see it succeed here, in addition to Godex.Godex is ultimately putting lipstick on a pig. it can improve performance a bit, but ECS isn&#x27;t some magical optimazation to slap on as a plug-in. cache coherency in the gameplay layer can&#x27;t fix engine level bottlenecks. reply Sakos 18 hours agoparentprevI don&#x27;t understand how Unity burns through up to a billion in revenue every single year, yet their engine still feels so half-baked and unpolished. Where&#x27;s all that money going? reply djbusby 17 hours agorootparentLots of cash can be burned in meetings and by managers of managers. I&#x27;m sure my case is not unique here but, meetings that cost $100k are common at $BigCo. And there dozens of those per day. reply paulddraper 16 hours agorootparentYou have 5-hour long meetings with 40 people each costing $500&#x2F;hr? reply EwanG 16 hours agorootparentLet me introduce you to the world of PI planning where two days of every quarter are spent with approximately 100 people (developers, program managers, etc) for each line of business to plan out the next quarter of work - even if 90% of it is continuing the work from the last quarter... reply azemetre 15 hours agorootparentTwo days sounds lucky. I&#x27;ve been in SAFE plans where it took an entire week to plan things, it was unreal. Like you I met people I&#x27;ve never seen or heard of from the company all giving their input on software I&#x27;m creating. I&#x27;d never see them again until the next SAFE planning, only this time it felt like 1 SWE out of maybe 15 (yes there were only 3 teams of 5 devs, put 100+ in these SAFE meetings) contributed anything meaningful.Company wasted so much money, then the org shut down for spending $500 per customer per year in maintenance (this was a health insurance company) whereas the main company would only spend $70 per customer per year. You&#x27;d think leadership of the org would get fired for this but they were rewarded with other positions within the company to do the same thing.Unreal. Why do shareholder&#x27;s put up with this? I guess the healthcare monopoly is the only reason to. reply KingMob 9 hours agorootparentUgh, I still shudder at our weeklong SAFe planning. At least I got a couple free trips to Europe for the meetings. reply tter3 15 hours agorootparentprevHalf a day is spent on collecting and analyzing the confidence vote. The results of the confidence vote have no impact on the plan except as a baseline for the next PI. reply edwardsdl 15 hours agorootparentprevâ€¦ and inevitably three weeks later priorities change and it all goes out the window. reply spenczar5 12 hours agorootparentprevAWS has (or had) a weekly 2-hour operational status meeting. It has over 200 people in it, a mix of distinguished and principal engineers, very senior managers, etc. That meeting easily crossed the $100k-per-pop threshold. Worth it, though, that was actually a pretty great institution. reply paulddraper 3 hours agorootparent> 200 people> 2 hours> weeklyKill me---EDIT: also, 200 people for 2 hours is only 100k if average salary is $500k&#x2F;year reply Tangurena2 3 hours agorootparentprevI was involved in a large software project with a large financial institution. Every weekday, there was an hour long status meeting. Based on what my company billed my time at (and presuming the other 40 companies billed their staff time likewise), that morning status call cost about $50k. reply djbusby 16 hours agorootparentprevSometimes the meet has folk that are even more expensive (not me). Sometimes theres more than 50. Four hours of meetings, six hours of wall-clockEdit: some companies have >20k employees. My first years at MS (late 90s) had these \"war room\" like 2x a week, 50 people, multiple VPs in the room, 2h. But there were other groups doing their war rooms too. reply animal531 7 hours agorootparentprevTheir worst problem (compared to Epic) is that they don&#x27;t finish projects.They&#x27;ll want a new feature, for example new scriptable graphics pipelines which will allow them to modify the core rendering a lot easier. Ok that&#x27;s fine, but then instead of implementing one they try to implement two, while at the same time completely ignoring the existing pipeline which everyone is currently using. The time to get the new pipelines working properly and onboard everyone is counted in years in this case.Those features they actually \"finished\", but there are so many experimental packages that just sit there forever, or work in a half-baked manner. reply DonHopkins 2 hours agorootparentThey hired the TextMesh Pro dude Zoltran, what must have been six or so years ago, with the promise that he&#x27;d integrate TextMeshPro into the \"new\" UI system, which I was looking forward to. I was just recently reading Unity&#x27;s web page about what&#x27;s new in the UI system, which announced that TextMesh Pro is finally built in! That took a while.https:&#x2F;&#x2F;unity.com&#x2F;features&#x2F;ui-toolkitI really love TextMesh Pro and Zoltran&#x27;s mesmerizing tutorials, I learned a lot about Unity reading his code, and he was totally into what he was doing and hyper productive when he was a highly regarded indie developer, but I got the impression (just guessing, no inside knowledge) that he was wasn&#x27;t 1&#x2F;100th as productive working inside Unity as he was on his own, or maybe they put him on something else than he originally went there to do.It&#x27;s been my experience that user interface toolkits are often turf war shit shows. I hope he&#x27;s doing ok there!https:&#x2F;&#x2F;www.youtube.com&#x2F;@Zolran&#x2F;videosi.e. the classic 9-year-old Unite 14 demo:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=q3ROZmdu65o reply intelVISA 17 hours agorootparentprevAgile shaman took the wheel reply psunavy03 16 hours agorootparentAgile done properly is literally the opposite of this. Big Important Exec spouting Agile terms they don&#x27;t understand and cluelessly forcing top-down crap on the teams, more likely. Normalize your story points, prole! Daddy needs metrics! reply torginus 8 hours agorootparentPersonally, I&#x27;ve never seen &#x27;Agile&#x27; done well. Every project where we made good progress was engineer driven. We talked to customers about what they wanted so see and we created tasks and implemented them.Every project managed by a dedicated servant-leader Six Sigma Kanban certified Scrum ninja-coach project has been a bureaucratic shitshow, with constant cargo culting to agile priniciples and forced ceremonies. reply andreasmetsala 7 hours agorootparent> Personally, I&#x27;ve never seen &#x27;Agile&#x27; done well. Every project where we made good progress was engineer driven. We talked to customers about what they wanted so see and we created tasks and implemented them.> Every project managed by a dedicated servant-leader Six Sigma Kanban certified Scrum ninja-coach project has been a bureaucratic shitshow, with constant cargo culting to agile priniciples and forced ceremonies.The engineer-led project sounds more agile than the latter Agile insanity. reply red-iron-pine 3 hours agorootparentprev> Agile done properlyNo True Scotsmen would do programming this way! reply wilg 19 hours agoparentprevDOTS is homegrown isnâ€™t it? reply thrillgore 17 hours agorootparentRight, it is. I intended to talk about Bolt and ProBuilder, tools they bought, added to the engine, and then left to rot.Although to build DOTS they did poach a lot of ECS and Data-Oriented folks like Mike Acton, who left earlier this year. reply torginus 11 hours agoprevUnity is a clown engine. I remember some guy benchmarked DOTS, against plain old C++ code with OpenGL, just like your dad used to write, and it was no contest, DOTS couldn&#x27;t keep up.You have to remember, Unity usually sets the bar very low for themselves, comparing their ancient Mono implementation (they still use the Boehm GC, written in the 1980s!), and when a shiny new performance &#x27;fix&#x27; like Burst&#x2F;DOTS drops, they proudly proclaim how much faster they managed to make things, never mind a modern .NET implementation, like Microsoft&#x27;s CoreCLR would achieve the same performance without any proprietary tech, without any code modification. reply maeln 6 hours agoparent> Unity is a clown engine. I remember some guy benchmarked DOTS, against plain old C++ code with OpenGL, just like your dad used to write, and it was no contest, DOTS couldn&#x27;t keep up.I don&#x27;t think it is necessarily a fair or useful comparison. I also wrote several toy 3d engine a few years back, and some where performing better than some \"commercial\" engine, but it is not hard to do when you don&#x27;t have to handle cross-platform, and a large number of feature. What is a fairer criticism is that Unity has been unable for the last decade to actually stick to a few features and make them actually mature and production ready. DOTS has been introduced like 4-5 years ago ? And it still feels like a polished tech-demo, not a production ready-system, properly integrated with the rest of the stack. And the same could be said about so many Unity feature. The \"legacy\" systems are just plain simply more reliable than almost any new feature they started for their replacement. reply torginus 6 hours agorootparentI don&#x27;t think we disagree on much. Unity has failed to stick the landing on most features developed in the past decade. If you make small to medium sized games, and treat it like 2015-era Unity, it still works decently, but progress since then has been slow going. All the new-ish features since then, like Scriptable Render Pipelines, Networking and DOTS had a rough development path, with constant bugs, breakages and stillborn features, sometimes packages got deprecated before reaching production status.As for the C++&#x2F;OpenGL comparison, I could&#x27;ve just recommended any common sense approach using industry standard tools, like Unreal.I&#x27;m just saying, in terms of code performance, DOTS promised a paradigm shift, this cool new way of structuring code and writing a dialect of C# promised heretofore unseen heights of performance. The reality was that Unity Burst code is barely faster than CoreCLR .NET code, and is slower than straightforward no-frills C++ (but at least you get Unity vendor lock-in). This performance matters when you&#x27;re trying to code a core game system, like procedural shattering, CSG, or inverse kinematics, doing it in regular C# will bite you in the butt. I&#x27;d rather do C++ for the few things where performance matters (not to mention, I&#x27;m 99% sure that well-tested high performance open-source C++ implementations of the above problems exist), than let Unity take me for a ride with DOTS.Just take a look at Unreal. I&#x27;m pretty sure most of the stuff Unity&#x27;s still working on wasn&#x27;t a problem circa UE4 release@2014, and most of their 2014 demos would work in a modern version of Unreal (not sure how much of a breaking change UE5 is, but definitely in the latter UE4 releases). reply Varriount 11 hours agoparentprevHuh, I never knew Unity used its own CLR implementation. Any idea why? reply torginus 10 hours agorootparentI think its implementation is a fork of a 2006-era Mono (a clean room reverse engineered .NET implementation). After a few releases, they had a falling out with the Mono team over licensing, meaning they got stuck on an old release.Besides that, they made a bunch of proprietary changes to Mono to make it run on their engine, and to be able to export to literally any platform under the sun.A lot of platforms, like iOS and consoles have (or had) a strict no-JIT policy, so they needed to come up with statically compiling code to said platforms. One of the methods they used was IL2CCP, which turned .NET bytecode, into horrible looking C++, full of goto-s, weird labels and structs getting passed around.Considering some platforms had limitations like you had to compile the game solely with the C++ compiler the platform supplied, not sure if they had a better solution, but it&#x27;s still horribly hacky.They&#x27;ve been manually syncing up changes from the more recent versions, but I can&#x27;t really tell, at what pace.But the thing is, even the official Mono has never really kept pace with MS&#x27;s implementation, and recently, after the acquisition, Mono was dropped by MS in favor of the CoreCLR. reply neonsunset 8 hours agorootparentAt the moment, Unity is in the middle of the move to \"vanilla\" flavour of .NET aka CLR. Once they do, it should dramatically improve GC performance in Unity and enable an easier route for targeting AOT-requiring platforms like iOS (it is officially supported as a fully-fledged target for NAOT starting with .NET 8, and there&#x27;s even rich interop directly with Swift in the plans). reply nvm0n2 2 hours agorootparentprevWhich platforms require you use a single C++ compiler? reply heywhatupboys 10 hours agorootparentprevUnity is older than the Core runtime. They had to use Mono as their base. reply fulafel 9 hours agorootparentAssuming they had C# or .NET as a constraint. I&#x27;d wager not, they just wanted something that would work for \"scripting\". reply geon 7 hours agorootparentC# and their own custom clr language was the only supported languages, so yes. reply fulafel 7 hours agorootparentThere were lots of other languages with support available.I don&#x27;t think Unity had anyone mandating they had to use .NET or C#. reply speps 6 hours agorootparentIt used to be C# along with JS and Boo (a Python for .NET). All docs had examples in order of preference for JS, C#, Boo. replycube2222 22 hours agoprevTip for those wanting to play it: change resolution scaling from dynamic to constant.I have a 3080 and it basically moves it from \"unplayable 10fps in the main menu\" to \"works just fine, no issues in game\" with medium-high graphics. reply dawnerd 10 hours agoparentThatâ€™s not what fixes it. Disable motion blur and depth of field. Depth of field kills the menu in particular. reply stouset 21 hours agoparentprevOr off, entirely. On my 3080 it seems to cause lots of rendering artifacts. reply olaulaja 21 hours agoprevFor a bit of reference, a full frame of Crysis (benchmark scene) was around 300k vertices or triangles (memory is fuzzy), so 3-10 log piles depending on which way my memory is off and how bad the vertex&#x2F;triangle ratio is in each. reply paavohtl 18 hours agoparentAuthor here: I never bothered counting the total vertices used per frame because I couldn&#x27;t figure out an easy way to do it in Renderdoc. However someone on Reddit measured the total vertex count with ReShade and it can apparently reach hundreds of millions and up to 1 billion vertices in closeups in large cities.Edit: Checked the vert & poly counts with Renderdoc. The example scene in the article processes 121 million vertices and over 40 million triangles. reply jiggawatts 13 hours agorootparentâ€œIf youâ€™ve used more triangles than there are pixels on the screen, youâ€™ve messed up.â€I call this kind of thing the â€œkilobyte ruleâ€ because a similar common problem is shoving tens of megabytes of JSON down the wire to display about a kilobyte of text in a browser. reply Sohcahtoa82 10 hours agorootparentI&#x27;d think today&#x27;s problem is tens of megabytes of JavaScript to display a couple kilobytes of text. reply kgabis 5 hours agorootparentprevWell, not anymore, Epic&#x27;s Nanite is basically a triangle per pixel rendering. reply westurner 9 hours agorootparentprev> The issues are luckily quite easy to fix, both by creating more LOD variants and by improving the culling systemHow many polygons are there with and without e.g. AutoLOD&#x2F;InstaLOD?An LLM can probably be trained to simplify meshes and create LOD variants with e.g. UnityMeshSimplifier?Whinarn&#x2F;UnityMeshSimplifier: https:&#x2F;&#x2F;github.com&#x2F;Whinarn&#x2F;UnityMeshSimplifier :> Mesh simplification for Unity. The project is deeply based on the Fast Quadric Mesh Simplification algorithm, but rewritten entirely in C# and released under the MIT license.Mesh.Optimize: https:&#x2F;&#x2F;docs.unity3d.com&#x2F;ScriptReference&#x2F;Mesh.Optimize.htmlUnity-Technologies&#x2F;AutoLOD: https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;AutoLOD\"Unity Labs: AutoLOD - Experimenting with automatic performance improvements\" https:&#x2F;&#x2F;blog.unity.com&#x2F;technology&#x2F;unity-labs-autolod-experim...InstaLOD: https:&#x2F;&#x2F;github.com&#x2F;InstaLOD\"Simulated Mesh Simplifier\": https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;AutoLOD&#x2F;issues&#x2F;4 :> Yes, we had started work on a GPU-accelerated simplifier using QEM, but it was not robust enough to release.\"Any chance of getting official support now that Unreal has shown off it&#x27;s AutoLOD?\" https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;AutoLOD&#x2F;issues&#x2F;71#issu... :> \"UE4 has had automatic LOD generation since it first released - I was honestly baffled when I realized that Unity was missing what I had assumed to be a basic feature.*> Note that Nanite (which I assume you&#x27;re referring to) is not a LOD system, despite being similar in the basic goal of not rendering as many polygons for distant objects.\"Unity: Feature Request: Auto - LOD\" (2023-05) https:&#x2F;&#x2F;forum.unity.com&#x2F;threads&#x2F;auto-lod.1440610&#x2F;\"Discussion about Virtualized Geometry (as introduced by UE5)\" https:&#x2F;&#x2F;github.com&#x2F;godotengine&#x2F;godot-proposals&#x2F;issues&#x2F;2793UE5 Unreal Engine 5 docs > Rendering features > Nanite: https:&#x2F;&#x2F;docs.unrealengine.com&#x2F;5.0&#x2F;en-US&#x2F;RenderingFeatures&#x2F;Na...Unity-GPU-Based-Occlusion-Culling: https:&#x2F;&#x2F;github.com&#x2F;przemyslawzaworski&#x2F;Unity-GPU-Based-Occlus... reply ReactiveJelly 19 hours agoparentprevSounds right. I remember seeing \"1M Triangles\" in the performance HUD and thinking, that&#x27;s crazy, a million triangles. Probably very few shared vertices once you account for edge splits, billboards, etc. reply gardnr 17 hours agoprevI really appreciate the writing style:> This pass is surprisingly heavy as it takes about 8.2 milliseconds, or roughly about far too long, ... reply promiseofbeans 15 hours agoparentI wish more people would write like this, sprinkling little bits of humor in a somewhat serious piece reply getwiththeprog 12 hours agorootparentYou might then like RockPaperShotgun and The Register reply promiseofbeans 10 hours agorootparentThanks! reply vGPU 22 hours agoprev> This mesh of a pile of logs is similarly only used in the shadow rendering pass, and features over 100K vertices.Butâ€¦ why? reply clnq 21 hours agoparentBecause it is one of the 1,000,000 things to pay attention to in game development. Someone or some software probably just made a mistake in setting up its LOD. Or some dynamic LODding code didn&#x27;t properly cull the LOD0 mesh. Or that code couldn&#x27;t be finished in time. Or it was something else.It&#x27;s completely normal in AAA games to have a few imperfect and in-optimal things. Budgets are always limiting, and development times short. Plus, it&#x27;s a hit-driven industry where payoff is not guaranteed. There are some things you can do (which are usually management-related and not dev-related) to make the game a success, but estimated bookings are rarely on-point. So trade-offs have to be made to de-risk - corners cut where possible, the most expensive part - development - de-prioritized. These are much bigger trade-offs than a single mesh being unoptimized. A single mesh is nothing.It&#x27;s a fun fact that this mesh is LOD0, and so is the teeth mesh. But that alone doesn&#x27;t tank the performance of the game and is probably unlikely to be addressed in lieu of actual performance fixes. The fixation on these meshes in the thread is kind of excessive.A lot of these comments are quite galvanized so I don&#x27;t want to add to that - just giving more context. reply mvdtnz 19 hours agorootparent> It&#x27;s completely normal in AAA games to have a few imperfect and in-optimal things.No, mate, stop. The state of C:S2 is well beyond anything we should accept as \"completely normal\". It&#x27;s a defective product that should not have been released. Stop normalising this crap. reply johnnyanmac 13 hours agorootparentyou&#x27;re missing the forest for the badly rendered tree in some E3 showcase that everyone nitpicked to death. Can we not treat this discussion like a Reddit rant, please?Gamedev has many shortcuts, and some things simply fall through the cracks. Some are caught and fixed, some are caught and not fixed, some just aren&#x27;t caught at all. I imagine it&#x27;s the 2nd case here; There&#x27;s a unfortunate large amount of bgs these days caught by QA but not given time to fix before publisher mandates. reply dingnuts 16 hours agorootparentprevGood grief, I have a mid tier AMD card and I&#x27;m having a blast with almost 40 hours in the game already. Can we quit with the \"defective\" propaganda?The game runs fine and it&#x27;s really fun. This \"controversy\" really drives home for me how detached from reality online discourse often is reply hwillis 15 hours agorootparentThe most common GPU on steam stats is a 3060. The AMD 7800&#x2F;7700 do 90% and 70% better on benchmarks. So if you have either of those, you&#x27;re getting nearly twice the FPS that the most common steam user would see. reply beowulfey 4 hours agorootparentI have a 1660Ti. I get 40+ fps on 1080p. It&#x27;s functional. Now, whether it is really any better than the previous generation is the more relevant question in my opinion. There are a few things improved, and a whole lot missing. The performance issues are only a small fragment of the game&#x27;s issues. reply red-iron-pine 3 hours agorootparentyeah this here is the real point.runs alright for me, too. just alright, but for what it is that&#x27;s fine.however it&#x27;s not really much better than the predecessor, and needs to give me a reason to give up on the HUGE mod community and well documented approaches from CS1. reply squeaky-clean 12 hours agorootparentprevI&#x27;m running on a laptop 2070 and doing just fine. Calling it defective is just blatantly lying.It should be better optimized, but calling it \"defective\" does nothing but make people dismiss your comment. reply programcookie 7 hours agorootparentprevWhile I tend to agree that there are a few people bashing it just because everyone else seems to be doing it I would also like to ask you to consider that not all the complaints are invalid.Case in point: I get 6-10 fps on a 7900XT with the default settings, 17 if I use the lowest preset, all while seeing around 55-ish percent system utilization.Something is amiss here and the game is definitely not running fine for everyone.As someone seeing this weird performance issue I wish both sides would focus less on screaming at each other and make it easier to figure out the root cause so everyone can enjoy the game. reply unaindz 12 hours agorootparentprevIt works on my machine reply taneq 18 hours agorootparentprevWhat trade off would you choose between fixing a performance issue which is somewhat avoidable and fixing a game breaking crash bug? Because the latter gets priority and thereâ€™s never enough time to fix all of those before launch, let alone work your way up to closing out every last frame rate drop. reply Aeolun 17 hours agorootparentI think if you get 100k poly models in your (city builder) game in the first place (for any but the most amazing wonders) your process has failed spectacularly at some point. reply Guthur 16 hours agorootparentprevThey pay for it they accept it, it&#x27;s that simple.All this crying when they could have simply returned the product or not buy it at all. Colossal Order themselves warned about the performance before it was even released. There were plenty of reviews that said the same thing.So to get up in arms about the performance means they are just being exceptionally stupid and entitled, and they should just grow up and stop crying over their toys.Colossal Order can release whatever garbage they want to. And you can choose to buy it or not, or even buy it and return it (fight for better return policies if you want something positive). reply Retric 19 hours agorootparentprevTheir point is that specific mesh could be left alone and the game still be playable as long as other issues were fixed.Chances are a nearly complete version of C:S2 was playable and they â€œbroke itâ€ at the last minute by not finishing the optimization process. reply mvdtnz 19 hours agorootparentThat&#x27;s speculation based on nothing but vibes. reply Retric 19 hours agorootparentItâ€™s speculation based on these mesh sizes being so arbitrary in the game development process and whatâ€™s broken being unnecessarily window dressing for gameplay. Itâ€™s the kind of thing that could be delayed to the last minute with some simple placeholder.â€œNow you might say that these are just cherry-picked examples, and that modern hardware handles models like these just fine. And you would be broadly correct in that, but the problem is that all of these relatively small costs start to add up, especially in a city builder where one unoptimized model might get rendered a few hundred times in a single frame. Rasterizing tens of thousands of polygons per instance per frame and literally not affecting a single pixel is just wasteful, whether or not the hardware can handle it. The issues are luckily quite easy to fix, both by creating more LOD variants and by improving the culling system. It will take some time though, and it remains to be seen if CO and Paradox want to invest that time, especially if it involves going through most of the gameâ€™s assets and fixing them one by one.â€IE: The the game would have looked nearly complete even if none of these meshes where in use. Meanwhile the buildings themselves are optimized. reply ethbr1 15 hours agorootparentAgreed.This really smacks of late asset delivery, which probably happened because delivery dates to the rest of the dev team kept being bumped.Then, by the time the assets were finally delivered, it was recognized they weren&#x27;t optimized (as expected), but there wasn&#x27;t any time to fix that.Or the same thing with an internal asset team.Although honestly, you&#x27;d think after seeing the performance numbers they would have implemented an \"above X zoom level, people &#x2F; anything smaller than this are invisible\" sledgehammer fix, until LOD could be properly addressed.Better to deal with pop-in than have everything be unexpectedly slow. replyDylan16807 15 hours agorootparentprev> Because it is one of the 1,000,000 things to pay attention to in game development.Finding objects with ridiculous triangle counts is one of the easiest things to do when you have known performance issues.If they didn&#x27;t have time to do the first, easiest chunk of the work, then something was far more dysfunctional than \"it&#x27;s one of a million things to deal with\". reply jiggawatts 13 hours agorootparentThis is on par with: â€œSure, thereâ€™s a raging fire in that corner of the office, but itâ€™s just one of a million things I had to deal with that day. Thatâ€™s why I didnâ€™t call the fire department, okay!?â€Staying on top of poly count budgets is like game dev 101. reply johnnyanmac 13 hours agorootparentHappen more than you think, despite the absurd metaphor. Maybe there was an inferno in the neighbohood and no time to worry about the fire in the corner of the apartment. Maybe your publisher doesn&#x27;t care if your house burns down but wants to make money now for their own earnings.I don&#x27;t think it&#x27;s uncommon even outside of gaming for modern software to have these sort of \"hidden fires\". Games just get a lot more conversation and a lot of niche problems as a 3D real time application. reply clnq 4 hours agorootparentprevNo, friend, poly counts are rarely a raging fire these days. Not 10k poly counts. That was in the 90s. reply 3seashells 10 hours agorootparentprevBut gamedev usually means, all new faces teams, as the old one quits in lockstep. So it&#x27;s very likely a bunch of youngsters running around yelling \"premature optimization\" is the death of all good things. Mistaking that with needing no optimization plan at all until game is almost done. reply smolder 19 hours agorootparentprevYou&#x27;re right that this kind of stuff is sort of par for the course. As in other cases, it&#x27;s indicative of (IMO) a bad development process that they didn&#x27;t budget the time to polish before shipping. I save my games budget for stuff that is \"done when it&#x27;s done\", not rushed out, mostly out of principle.If you aggressively min-max development cost & time vs features, there are big external costs in terms of waste (poorly performing software carries an energy and hardware cost,) end-user frustration, stress on workers, etc., which is how I justify voting with my money against such things. reply account42 1 hour agorootparentprev> It&#x27;s a fun fact that this mesh is LOD0, and so is the teeth mesh. But that alone doesn&#x27;t tank the performance of the game and is probably unlikely to be addressed in lieu of actual performance fixes. The fixation on these meshes in the thread is kind of excessive.So I assume you have a better explanation of the excessively slow G-buffer and shadowmap passes? reply eloisant 19 hours agorootparentprevI get that you can leave a bunch of things unoptimized, as long as it works fine.What I don&#x27;t understand is - how did they not notice that the performances was horrible even high end hardware? How did they not decide to take the time to investigate the performances issues and find the causes we&#x27;re talking about now? reply clnq 17 hours agorootparentSo you know the saying â€œpremature optimization is the root of all evil?â€ Producers love that statement because it removes half of the complaints around work being rushed.Optimization is not done throughout the process and later thereâ€™s not enough time. Assets are made with bad topology and it would take time to redo them. Or it would take time to write a tool that retopologizes them automatically.What Iâ€™m saying is by the time itâ€™s â€œtimeâ€ to optimize, thereâ€™s not enough time to optimize. It happens very commonly. But the alternative is taking development slower to do things right. And you simply donâ€™t get investment for schedules like that in most companies. Not to mention that itâ€™s goddamn hard to do when the execs lay off people, ask them to RTO, and induce serious attrition otherwise. Sometimes the team just canâ€™t settle into a good process as people leave it too much. So youâ€™re between a rock and a hard place â€” on the one hand: attrition and low morals, on the other hand: a tight schedule. This doesnâ€™t apply to Colossal Order from my knowledge, but it does apply to many AAAs.There is a problem at the root of this - extremely over-ambitious production schedules as norm. Most other things are symptoms. Most of what I described is a symptom. reply athrow 17 hours agorootparentExcept there really isnâ€™t a better product to back up these issues. There were some important improvements made to the gameplay, traffic system, certain things were reworked, nicer graphics etc. It feels like an iteration and not a ground-breaking game rhat would justify the performance issues weâ€™re seeing. reply clnq 14 hours agorootparentThe design was iterated, but the game assets are redone almost completely, and the systems appear largely reworked, too. This is evident when you play the game.The scope of work done for this game was exceptionally large for a company with 40 employees, assuming it was done within the usual AAA timeframe. reply tbillington 19 hours agorootparentprevI _guarantee_ they knew about it.They even posted on social media 1 week before launch warning people to expect lower than expected performance, and raised the system requirements.If companies have to decide between prioritising features that they&#x27;ve advertised, show stopper bugs, and performance, guess which one always takes the back seat :) reply dimgl 18 hours agorootparentprev> Because it is one of the 1,000,000 things to pay attention to in game development.This is a cop-out. This doesn&#x27;t seem like an oversight but rather blatant incompetence. You don&#x27;t just \"not pay attention\" to this. reply johnnyanmac 13 hours agorootparentyou call it incompetence, devs call it \"publishers told us to ship now\". You&#x27;d think a technical community like this would sympathize with such mandates given across the industry. reply mook 9 hours agorootparentCall it incompetence on the part of the management, then? reply jameshart 18 hours agoparentprevWell, because when youâ€™re modeling a pile of logs, each additional log you add doubles the number of vertices.This is a well known property of log scaling. reply amluto 1 hour agorootparentIf you make natural looking logs, itâ€™s a factor of 2.7 or so. reply function_seven 17 hours agorootparentprevIâ€™m too dumb to know if youâ€™re making a math joke or if this is a real 3D modeling thing.Or both? reply Aeolun 17 hours agorootparentItâ€™s a joke reply matsemann 21 hours agoparentprevIt could have been like a hundred vertices and a clever normal map. Just insane. reply ripper1138 21 hours agoparentprevThe studio that made this has like 30 devs. reply harrid 21 hours agorootparentThis doesn&#x27;t fly with a one man team and not with a 1000. It&#x27;s just badly done, there&#x27;s no sugarcoating. Those meshes should never end up in the game files. reply johnnyanmac 13 hours agorootparent>This doesn&#x27;t fly with a one man team and not with a 1000sounds like someone never worked on a 1000 dev team. random quirks either go unnoticed or are de-prioritized all the time. Most are minor, more and more moderate to major ones are getting through. That&#x27;s definitely a publisher issue. reply kamray23 8 hours agorootparentrandom quirks do. this is not random quirks. it&#x27;s a systematic and expected issue of underoptimisation caused by releasing a product before it was even slated to be ready. one bad mesh is not the issue, it&#x27;s never the issue. we&#x27;re talking of thousands of terrible meshes and a near-total lack of basic optimisations applied at the last stage of development to most games. the manpower was not enough to release within the deadline, likely due to running into a lot of technical difficulties working with unity. instead of going into valve time, they released it anyway, which means that you skipped the entire polish and optimisation part not only for the game itself but for half the engine as well. poor performance was not only expected, i&#x27;m certain that every member of the team saw it as the only possible outcome. reply johnnyanmac 8 hours agorootparentI&#x27;d call 4 examples of \"this needed LODs\" random quirks in the grand scheme of things. It&#x27;s not like every single mesh is 100k vertices. Grossly underoptimized, yes. But the devs pre-empting their announcement with \"we&#x27;re not satisfied\" tells me they were too busy slaying dragons to worry about the annoying barking Chihuahua in the room.It was expected, yes. It does not mean they weren&#x27;t trying to fix it in the 11th hour. I woildnt be surprised if some core tech was unfinished or inadequate that lead to this.>instead of going into valve time, they released it anyway, which means that you skipped the entire polish and optimisation part not only for the game itself but for half the engine as well.Yup, welcome to game development when you have deadlines and no benevolent (or at least, apathetic) dictator paying your bills. It&#x27;s unfortunate that we can trace this back to the 80&#x27;s with ET, but this is simply the business realities. Game code isn&#x27;t mission critical (and until recently, does not care about maintainability), and also isn&#x27;t what sells the product.So it never gets the time to be cultivated like other indistries. And people still buy anyway. It&#x27;s a two way street of apathy and every publisher hopes it can slip under the cracks and not become the next Superman 64. Most manage to slip.There&#x27;s not much you can do about it with the current publishing structure, where most funders don&#x27;t work in nor care about games. And the ones that do still see their money draining whenever the talk of delays come up. That won&#x27;t be solved except with time as more industry trailblazers retire and shift to management (remember, Todd Howard is only in his 50&#x27;s. Gabe and Kojima are 60. So many pioneers are still well under retirement age). Or for more truly indie teams to arise and learn how to scale up projects while staying lean. The latter is what I hope to do. reply kamray23 7 hours agorootparentIt&#x27;s pretty clear that with the performance that the game has it&#x27;s a lot more than four models with bad LODs. They would probably have identified that issue within days. There&#x27;s likely a more fundamental issue, not only with the technology but also with LODs for most detail models. These are only examples since you can&#x27;t list every model out of hundreds being rendered.I really think that the performance and scale indie developers can squeeze out puts AAA developers to shame nowadays, and I really hope that that&#x27;ll continue to happen. All it takes is time, organisation, and a lot of time. reply johnnyanmac 7 hours agorootparentSure, more than 4, less than however many assets there are in the game. I mostly want to emphasize that a systematic issue implies that this was an acceptable development pipeline, which I doubt any engineer on the team would say.>I really think that the performance and scale indie developers can squeeze out puts AAA developers to shame nowadays, and I really hope that that&#x27;ll continue to happen. All it takes is time, organisation, and a lot of time.Yup, I agree. Indies don&#x27;t tend to have money, so the budget comes from. Elsewhere. We can already utilize some amazing tools to cut down the time of scaling up environments. Not as much with actor assets. But I don&#x27;t think it&#x27;s too far off (more just locked off in acedemics white papers). replydimgl 18 hours agorootparentprevThank you! This is what I was getting at in another comment. This isn&#x27;t just a case of \"oh no, I forgot to switch a button\". reply LarsDu88 13 hours agorootparentprevI&#x27;m solodeveloping a game, and there&#x27;s no fucking way a 100,000 vert mesh is getting in the game without a LOD. My game is running on the Quest 2 at 72 fps stable reply johnnyanmac 13 hours agorootparentsure, it&#x27;s easy to catch major hiccups when one person has 100% knowledge base. Not so much when 30 devs each have different responsibilities and the optimizer guy is drowining in other fires. reply Tijdreiziger 21 hours agoparentprevHey, ya gotta have logs &#x2F;s reply PeterStuer 10 hours agoprevSo the game uses extremely detailed models, then fails to have an intelligent way of abstracting&#x2F;culling away those things that will never make it into pixels, is that a fair summery?Also, does that mean that easy fixes are available or is this so core that solutions would require going back to the drawing board? reply sbergot 9 hours agoparentWe can only speculate one the difficulty of the fixes. I am not an expert but I believe culling and LoD are two big things for engines. Not being able to use existing implementations is really unfortunate. I guess the \"tricks\" are well-known but I don&#x27;t think the implementation is easy. reply Ciantic 7 hours agorootparentI&#x27;m not a GPU programmer, but I became interested to dig out what is the state of art of culling, and found out Arseny Kapoulkine [1] has implemented those in MIT licensed C++ code base:Cluster cone culling, frustum culling, Automatic occlusion culling, triangle culling, Meshlet occlusion culling, Optimizing culling. On paper sounds impressive, and maybe Colossal Order could learn from those.https:&#x2F;&#x2F;github.com&#x2F;zeux&#x2F;niagara reply arendtio 1 hour agoprevI am surprised that the issues are so basic. When I heard, that Cities Skylines 2 had a bad performance I expected something like they were using the GPU for graphics as well as for the simulation, but didn&#x27;t have a good scheduler or so (something new&#x2F;unproven).However, after reading this, it seems to be mostly a rendering issue with even basic stuff missing, like LOD models or occlusion culling?!? Feels like someone had to optimize the rendering engine after Unity didn&#x27;t deliver, but skipped the &#x27;101 of how to optimize a renderer&#x27; in the first place. reply mrcwinn 20 hours agoprevI loved every bit of this post, especially the final few sentences. Thank you. reply lowbloodsugar 21 hours agoprevDOTS is the brain child of Mike Action. See his 2014 CppCon \"Data-Oriented Design and C++\" [1]. But Mike has left Unity, according to his twitter.[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rX0ItVEVjHc reply frogblast 19 hours agoparentDespite the original post talking about DOTS rough edges, I didn&#x27;t see anything in that article that actually suggested DOTS was the cause: that would cause CPU overhead, but it seems like they simply have a bunch of massively over-detailed geometry, and never implemented any LOD system.Maybe they could have gotten away with this with UE5&#x27;s Nanite, but that much excessive geometry would have brought everything else to its knees. reply baazaa 18 hours agorootparentThe author&#x27;s point is that poor support for DOTS meant the devs had to roll their own culling implementation which they screwed up. reply iMerNibor 19 hours agorootparentprev> Maybe they could have gotten away with this with UE5&#x27;s NaniteExactly.If unity actually delivered a workable graphics pipeline (for the DOTS&#x2F;ECS stack, or at all keeping up with what UE seems to be doing) these things probably wouldn&#x27;t be an issue. reply frogblast 19 hours agorootparentDOTS&#x2F;ECS has nothing to do with geometry LODs. Those are purely optimizing CPU systems.Even if DOTS was perfect, the GPU would still be entirely geometry throughput bottlenecked.Yes, UE5 has a large competitive advantage today for high-geometry content. But that wasnâ€™t something Unity claimed could be automatically solved (so Unity is in the same position as every other engine in existence apart from UE5).The developer should have been aware from the beginning of the need for geometry LOD: it is a city building game! The entire point is to position the camera far away from a massive number of objects! reply iMerNibor 18 hours agorootparentTo quote from the blog post:> Unity has a package called Entities Graphics, but surprisingly Cities: Skylines 2 doesnâ€™t seem to use that. The reason might be its relative immaturity and its limited set of supported rendering featuresI&#x27;d hazard a guess their implementation of whatever bridge between ECS and rendering is not capable of LODs currently (for whatever reason). I doubt they simply forgot to slap on the standard Unity component for LODs during development, there&#x27;s got to be a bigger roadblock hereEdit: The non-presence of lod&#x27;ed models in the build does not necessarily mean they don&#x27;t exist. Unity builds will usually not include assets that aren&#x27;t referenced, so they may well exists, just waiting to be used. reply johnnyanmac 11 hours agorootparentprevmain issue is that DOTS and the whole HDRP&#x2F;URP stuff started at about the same time, but the goals were completely different. So it would have been nearly impossible to get them working together while DOTS was a moving target. Devs already have multiple breaking updates from the alpha versions of DOTS, an entire GPU pipeline sure wasn&#x27;t going to rely on that.>Unity has a package called Entities GraphicsWell that&#x27;s news to me. Which means that package probably isn&#x27;t much older than a year. Definitely way too late for game that far in production to use.oh, so they rebranded the hybrid renderer. That makes a lot more sense: https:&#x2F;&#x2F;forum.unity.com&#x2F;threads&#x2F;hybrid-rendering-package-bec...I&#x27;m fairly certain the hybrid renderer was not ready for production. reply lowbloodsugar 15 hours agorootparentprevOh sure. I did not mean to imply that. Sorry. reply tuna74 19 hours agoparentprevThe same Mike Action that claimed that 30 fps games sold better than 60 fps games using a very questionable data set (excluding sports games etc). reply alkonaut 19 hours agoprevMaking a AAA game sounds horrible. And making one in Unity sounds doubly so. All of these things sound like fixable issues. They&#x27;ll likely be fixed. Hopefully the developers made these oversights because they focused on what makes the game worth fixing to begin with: that the gameplay is fun. In many recent games I feel developers have focused on the wrong things and totally forgot the core, meaning if they fix the bugs - the game is still quite hollow (cough, Starfield). A game like this should of ocourse have a continous perf process and if it doesn&#x27;t run ok (min 30fps on median hardware 60 on enthusiast hardware for example) then it just shouldn&#x27;t ship. I wish more studios would stop having crunchtime for meaningless deadlines such as holiday seasons. Someone has said \"it&#x27;s ok to be just 10fps on beefy hardware, we can fix that later, let&#x27;s ship it now\". reply brucethemoose2 17 hours agoparent> then it just shouldn&#x27;t shipPart of the problem is that AAA is just (IMO) too big and expensive. Devs might actually have to ship a broken game around holiday time just to get enough sales to survive.And the other extreme can be dangerous too, like how Mass Effect Andromeda&#x27;s development dragged on forever, and EA let it happen because its such a golden IP.I think the ultimate solution is to just scale down most studios a little bit, so the studio and publisher can afford to delay. Medium sized studios are the sweetspot, especially going forward with GenAI. reply chii 16 hours agorootparentIf a studio downsizes, they might be able to survive making lower budget games. And while it&#x27;s true that if every studio does this, the industry would get better, there&#x27;s incentive for _one_ studio to pump up their funding and make a super-high-fidelity game, and grab all marketshare.This makes it a &#x27;race to the bottom&#x27; style (or a race to the top?) competition, where higher funding gets you more marketshare, but only against lower funded studios. It&#x27;s akin to advertising budgets. Mostly a zero sum game in the end. reply brucethemoose2 16 hours agorootparent> super-high-fidelity game, and grab all marketshareThis is not necessarily true any more. I think smaller studios chipping away for a long time are making better games than most big ones. reply BeFlatXIII 3 hours agorootparentprevHopefully this dynamic creates a cascading market crash to wipe out all the biggest players and force the industry back to its proper size. reply SXX 9 hours agorootparentprevIt&#x27;s not like this is unique to AAA. Unless gamedev is your hobby and not full-time job you simply have no choice other than ship game and try to fix it.Any small indie studio have to deal with it. reply 3seashells 10 hours agorootparentprevEA were the culprit for those delays by forcing bioware to use frostbite. Blame rests solidly with some synergies savings suitsuperman. reply brucethemoose2 2 hours agorootparentGoing by the allegations in the reports, it wasn&#x27;t just frostbite. Bioware chose to dump a lot of time into features like procedural generation (and having to use Frostbite exhasterbated this problem even more). reply Dah00n 19 hours agoparentprev>Someone has said \"it&#x27;s ok to be just 10fps on beefy hardware, we can fix that later, let&#x27;s ship it now\".Well, yes, because it doesn&#x27;t matter. It is on the top seller list on Steam. I agree with you, but we can discuss fixes till our fingers bleed. In the end, the problem is capitalism.https:&#x2F;&#x2F;store.steampowered.com&#x2F;search&#x2F;?supportedlang=english... reply mcmoor 18 hours agorootparentI&#x27;ve learnt that (initial) success of a sequel is 95% because of its prequel. True performance may be seen if it has another sequel, or DLC, or heck, another 3 months. reply johnnyanmac 12 hours agorootparentThat&#x27;s the benefit about continual updating games. By the time CS3 is ready people won&#x27;t remember CS2 2023 but whatever 3-6 years of updates does to CS2. For a modern example: who&#x27;s still complaining about Cyberpunk in 2023?It&#x27;s not like Sonic 2006 that is forever broken, sold decently at launch and then cratered the series for the next decade to come. reply alkonaut 8 hours agorootparent> For a modern example: who&#x27;s still complaining about Cyberpunk in 2023?Those who bought it and played it in 1.0! Many of them never saw the fixed game. They werenâ€™t helped at all by the fact that the game was quite good 6 months later. Worst case theyâ€™ll never buy a CDPR game again, best case theyâ€™ll buy the games after 6 months (which by the logic of rushing releases is disaster because apparently not selling millions the first holiday season is a failure). reply mcmoor 12 hours agorootparentprevThere&#x27;s still a risk of the game cratering completely like Imperator, although it&#x27;s not a sequel to an established title. Sometimes I&#x27;m still worried how Victoria 3 &#x27;s fate would be. reply alkonaut 17 hours agorootparentprevThatâ€™s still not a sufficient condition for success. Longer term the studio brand will be hurt, not least. reply christophilus 18 hours agorootparentprev> In the end, the problem is capitalism.Is this sarcasm? Iâ€™m asking seriously. If not, then how is a poorly running game the result of capitalism, and what is the alternative economic model that would produce only high-performance &#x2F; efficient games? reply johnnyanmac 12 hours agorootparentI don&#x27;t think it&#x27;s sarcastic. The game runs poorly because it needed more time for optimizations. It doesn&#x27;t get more time for optimiztions because the publisher said it needed to ship now. The publisher can say that it needs to ship because they can advertise for good launch sales, because the a large portion of the customer base will buy it at launch as long as there aren&#x27;t obvious showstoppers.It&#x27;s a bit trite to sum that down to \"capitalism\", but sure. it&#x27;s the underlying societal buzzword issue reply gruez 6 hours agorootparentCorner cutting doesn&#x27;t happen in socialist countries? reply johnnyanmac 6 hours agorootparentIn a truly socialist structure, corner cutting comes from prioritizing other duties to the people, directly or indirectly. Capitalistic pressures come from money.But there is no pure capitalistic structure nor socialist one. I did already mention it was a trite comparison. reply gruez 6 hours agorootparent>In a truly socialist structure, corner cutting comes from prioritizing other duties to the people, directly or indirectly. Capitalistic pressures come from money.You can argue that&#x27;s the same under capitalism, just replace \"the people\" with \"customers\". In both cases corner cutting is only an issue if there&#x27;s deception involved, wether that&#x27;s a Party member using lower quality materials for a construction project, or a video game company spending less time polishing a game. I don&#x27;t see a problem with releasing unfinished&#x2F;poorly optimized games as long as it&#x27;s clearly disclosed. Early releases are basically this. replyheywhatupboys 10 hours agorootparentprev> In the end, the problem is capitalism.surely, it is the underlying economic system ruining the art of modern video game development! reply drstewart 18 hours agorootparentprevThe problem is European capitalism, as the game developer here is Finnish and publisher is Swedish. Important clarification since you seem very invested in calling out the US in every other post of yours, so I&#x27;m sure you appreciate the calling out of systemic European failures here. reply heywhatupboys 10 hours agorootparentThe Soviet Union produced some of the best video games!! reply Stevvo 9 hours agoprevUnity has built in profilers that show exactly what Renderdoc + NSight do, and far more. The blame lies squarely with the studio for failing to do basic optimization that is required for any game. The many issues with Unity&#x27;s engine development and features are unrelated. reply inoffensivename 23 hours agoprevI spent 40 minutes trying to eke out more than a handful of fps on an empty map with the resolution set at 1080p with Proton Experimental. I gave up and got a refund, I&#x27;ll try again if they fix the awful performance.I got a tremendous amount of enjoyment out of the first instalment of the game, it&#x27;s a big bummer that I can&#x27;t give this one a go reply solardev 23 hours agoparentIt&#x27;s pretty playable on GeForce Now, for what it&#x27;s worth. Still a big laggy, but I was able to play for many hours without major issues... just the occasionally annoying but livable stutter. reply JCharante 21 hours agorootparentGeForce Now has been amazing as a mac only user reply solardev 21 hours agorootparentSame.I have a M2 Max and GFN is much much easier than trying to set something up with GPT (Game Porting Toolkit) and Whisky, and much faster & quieter too. An RTX 4080 running in their data center means no local heat and noise. reply Unfrozen0688 20 hours agorootparentprevYes because you have no other options. reply solardev 20 hours agorootparentThere&#x27;s lots of options? GPT, WINE Crossover, Luna, Boosteroid, Shadow.tech... none of them run as well as GeForce Now, though. Or a dedicated gaming PC. reply Unfrozen0688 20 hours agorootparentprevDosent really count as it is not rendering on your machine... ofc its good there. reply solardev 20 hours agorootparentSo? That&#x27;s even better. Doesn&#x27;t use my battery life or create noise & heat. Netflix isn&#x27;t run on my machine either. reply Unfrozen0688 20 hours agorootparentSure, but then it does not have any relevance to the article. reply jodrellblank 17 hours agorootparentI wasn&#x27;t claimed to be relevant to the article, it was a reply to \"it&#x27;s a big bummer that I can&#x27;t give this one a go\" with a suggestion of how they could play it. reply solardev 17 hours agorootparentprevWasn&#x27;t supposed to challenge anything in the article. Just an option for those who are struggling to play it on their current hardware.Even with GFN it&#x27;s laggy and stutters. Totally agree with the article. reply smolder 19 hours agorootparentprevIt just uses natural resources to outfit and power data center stuff to create heat and noise somewhere further away. Netflix... is fairly efficient, though being on-demand, perhaps much less so than broadcast TV. reply solardev 17 hours agorootparentYes, but better that, in a shared environment with center-wide cooling and such, then each individual household needing to do that on their own.Also way fewer cards needed this way, with users being able to share cards through the day instead of each needing their own.Basically mainframes all over again :) reply Chaosvex 11 hours agorootparent> Yes, but better that, in a shared environment with center-wide cooling and such, then each individual household needing to do that on their own.I don&#x27;t think this actually tracks, unless the heat is actually being put to use. You don&#x27;t need HVACs when you have the machines distributed. reply solardev 5 hours agorootparentWhy don&#x27;t you need distributed HVAC? A gaming desktop can use a kilowatt of power, which is what a small space heater might put out. It often will make the room uncomfortably hot. (Anecdotally, this is part of why I switched to Geforce Now, myself. My apartment got incredibly uncomfortable from the heat. It&#x27;s an older unit with no air conditioning.)In data centers, sometimes (not always, and perhaps not often?) the heat can be more efficiently handled through central heat exchangers, more efficient commercial HVACs, etc. replydeanCommie 18 hours agoparentprevThe fix fits into a tweet -> https:&#x2F;&#x2F;twitter.com&#x2F;ColossalOrder&#x2F;status&#x2F;1716883884724322795> If you&#x27;re having issues with performance, we recommend you reduce screen resolution to 1080p, disable Depth of Field and Volumetrics, and reduce Global Illumination while we work on solving the issues affecting performance.This is all I had to do to get smooth performance on an AMD Radeon RX 5700 XT reply DonHopkins 20 hours agoparentprevTry not fluoridating the water, defunding the dentistry college, and subsidizing sugar, so everyone&#x27;s teeth fall out. Runs much faster then![Chill: it&#x27;s a tooth joke, not a conspiracy theory.] reply dieortin 18 hours agorootparentWater is not fluoridated in most of the world, thatâ€™s one of many weird things in the US reply dimgl 18 hours agoprevJust wanna give a shout out to the brief mention in the article of Anno 1800. Quite possibly one of the best games I&#x27;ve ever played. reply sundvor 8 hours agoparentI bought it based on this comment, on sale now at Steam! It looks great, thanks. reply kookamamie 9 hours agoprevThe real summary: Unity is used as rendering engine and various rendering options are toggled on without much worry about the big picture. Add in some Unity \"packages\" and the patchwork of rendering things with unpredictable (and hard-to-fix) performance is ready. reply hooby 8 hours agoprevThe TL;DR:Analysis showed that most frame time was lost due to lack of LOD levels and culling - causing the engine to render insanely high amounts of polygons that weren&#x27;t even visible on screen.In conclusion it seems that using the brand new engine feature DOTS (which is a perfect match for this game), successfully solved the CPU bottlenecks the first game had.But because of Unity&#x27;s DOTSHDPR integration still being WIP (with several key features being experimental&#x2F;missing) - they had to implement a lot of stuff on their own (including, but not limited to culling), which cost development time and caused the game to be not quite ready at release (explains missing LODs&#x2F;optimization). reply CooCooCaCha 23 hours agoprevI hope these issues come from the game being rushed and not from a lack of rendering expertise.Luckily it seems like there are pretty simple reasons for the poor performance so I&#x27;m hopeful they can at least do something even if they don&#x27;t have a ton of rendering expertise. reply capableweb 22 hours agoparentI think the guess in the article is pretty close to the truth, I&#x27;ve seen stuff like that happen countless of times. You make a bet on a early technology (Unity DOTS + ECS in this case) which gives you a lot of benefits but also, it&#x27;s immature enough that you get a bunch of additional work to do, and you barely have time to get everything in place before publisher forces you to follow the initial deadline. reply lamontcg 22 hours agorootparent100,000 vertices for pile of logs isn&#x27;t really a bad bet on tech, though. That is just piling vastly more onto any tech stack than it can handle, with nobody having the time or the political okay to do a perf pass through the code and put all these ideas on a diet.But that means that everything is solvable. There&#x27;s no need in this game for 100,000 vertices for a logpile, so that should be a relatively straightforward task to fix. And someone can rip out all the teeth and put \"Principal Tooth Extraction Engineer\" on their resume. reply capableweb 21 hours agorootparent> 100,000 vertices for pile of logs isn&#x27;t really a bad bet on tech, though. That is just piling vastly more onto any tech stack than it can handle, with nobody having the time or the political okay to do a perf pass through the code and put all these ideas on a diet.I can easily see this happening though.Artist starts making assets, asks \"What&#x27;s my budget for each model\" and engineering&#x2F;managers reply with \"Do whatever you want, we&#x27;ll automatically create different LODs later\" and the day gold master is being done, the LOD system still isn&#x27;t in place so the call gets made to just ship what they have, otherwise publisher deadline will be missed. reply CountHackulus 19 hours agorootparentThat sounds like exactly what happened. I&#x27;ve been in that position many times in games I&#x27;ve worked on and seen it happen. reply munificent 18 hours agorootparentprevAnyone in management or engineering who tells an artist they have no texture or mesh budget at all gets exactly what&#x27;s coming to them. reply johnnyanmac 10 hours agorootparentThat&#x27;s essentially what Epic is trying to tell devs these days. Don&#x27;t know if it will ever truly live up to such lofty goals, though. replyok_computer 16 hours agoprevThis is my favorite video using the poor rendering in the game to make cool art effects. Full disclosure there may be an artistic license and post processing but itâ€™s hilarious you can get these visuals in a modern game.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=FQ13OFmRF-4 reply LtdJorge 14 hours agoparentWrong video? I think that one is using Mirror&#x27;s Edge Catalyst reply hombre_fatal 13 hours agorootparentThey are referring to the video&#x27;s focus on low-detail game assets that are only rendered in the distance. Though apparently something the game in TFA sorely lacks. reply pierrec 13 hours agorootparentprevThat right there is the city from the original Mirror&#x27;s Edge (2008). I can tell from having played it a lot. And what a great idea for a music video. reply ok_computer 3 hours agorootparentprevWrong video on my part. reply bhaak 22 hours agoprevIs there any Paradox game that doesnâ€™t have lots of obvious bugs and terrible UI at release? And only the former gets somewhat addressed over time.I really wonder how they develop at that place. And what kind of QS they have.I think even applying a crude pareto would improve their games a lot.Edit: I stand corrected. I wasn&#x27;t aware that Paradox is also a publisher and even such a big company (over 600 employees!). Still makes you wonder how they go about their business. reply frozenfoxx 21 hours agoparentI mean unrelated since Paradox is the publisher, but Gauntlet ran like greased lightning, Helldivers ran like greased lightning, Magicka 2 ran like greased lightningâ€¦â€¦of course those were all built on Dragonfly&#x2F;Bitsquid instead of Unity so that might be a clue about where the issue lies. reply Dah00n 19 hours agoparentprev>Still makes you wonder how they go about their businessIn a way that sent them straight to the top sellers list on Steam. Sadly, today, it just doesn&#x27;t matter.Edit: spelling reply adventured 22 hours agoparentprev> Is there any Paradox game that doesnâ€™t have lots of obvious bugs and terrible UI at release?It&#x27;s not a Paradox game, they&#x27;re the publisher. Colossal Order is the developer.It&#x27;s a small developer out of Finland, 30-50 employees. reply Dalewyn 21 hours agorootparentIt has or will have twelve dozen DLCs or more, so it&#x27;s a Paradox game. reply getwiththeprog 9 hours agorootparentI think that the Paradox model IS DLC. They definateley comprimise user experience through their money-grabbing, and this cash above quality bleeds through to the way their games are published. They blur the line between developer and publisher. reply brainzap 18 hours agoparentprevParadox just doesn&#x27;t do testing. The CEO even admitted it. reply capableweb 22 hours agoparentprevSvea Rike II was pretty bug free, but it was released quite some time ago... reply izacus 21 hours agoparentprevMost of them. reply davbo 5 hours agoprevLaundry boys is a superb name for clothes pegsGreat article @paavohtl! reply honkycat 20 hours agoprevUnity has been stalling on it&#x27;s DOTS and network stack re-implementation for like 5 years now.There is no excuse other than leadership are cashing the checks and squeezing the juice out of the company until they close it, which would make sense looking at their semi-recent merger and poor behavior by the CEO.Seriously, I was looking into Unity at the start of Covid while laid off, and DOTS was \"around the corner\" even THAT far back!They still don&#x27;t have an answer for a network stack, and now LOD is broken? LMAO.Unity has been a dirty word for me for a number of years. This is the pay-off for dismissing people&#x27;s concerns and insisting it will buff out eventually. reply kristianp 18 hours agoparentWhen reading sections of the article about Unity&#x27;s permanently experimental features, I was wondering why they didn&#x27;t use a different engine (probably because their expertise is in Unity). Does Unreal for example have support for this kind of game?Oh and I have to mention the cascaded shadow mapping: \"taking about 40 milliseconds or almost half of total frametime. \". - 40ms is 25fps all by itself! reply munificent 18 hours agorootparentUnless you&#x27;ve worked in games, it&#x27;s really hard to understand just how massively tied to the engine a game is.Imagine you have a web app written in Ruby using Rails with data stored in Postgres. You have a few hundred tables, millions of rows. Millions of lines of Ruby, CSS, and HTML. Thousands of images carefully exported as JPEG, GIF, or PNG depending on what they contain.Now imagine being told you need to simultaneously port that to:- Not run on the web. Instead be a native application using the OS&#x27;s rendering.- Not be written in Ruby. Instead using a different language.- Not use Rails. Instead, use a different application framework.- Not use Postgres. In fact, don&#x27;t use a relational database at all. Use an entirely different paradigm for storing data.- Not use CSS, HTML, and raster images. Instead, everything needs to be, I don&#x27;t know, vector images and Processing code.That&#x27;s about the scale of what moving from one game engine to another can be like. Depending on the game, it&#x27;s isn&#x27;t always that Herculean of a task, but if often is. A mature game engine like Unity is a programming language, build system, database, asset pipeline, application framework, IDE, debugger, and set of core libraries all rolled into one. Once you&#x27;ve picked one, moving to another may as well just be creating a whole new game from scratch. reply bluescrn 17 hours agorootparentprevUnity&#x27;s killer feature is its C# support. It&#x27;s enabled small teams to do far more in less time that they could ever hope to do using C++, and avoiding some of the nastier types of bugs that C++ game developers have to deal with.And if you&#x27;ve got a team of experienced Unity developers, some who&#x27;ve been working with C#&#x2F;Unity for their entire game dev career, switching to C++&#x2F;Unity isn&#x27;t the most practical option. While many concepts are similar between engines, you&#x27;re going to be back at the bottom of the learning cliff for a while.It sounds like Unity isn&#x27;t really the problem in this case, it&#x27;s more about too many polygons, poor use of LOD, and sub-optimal batching (too many draw calls). It was probably more of a time pressure issue than a tech issue, as game development is usually a race against the clock. reply honkycat 18 hours agorootparentprevunreal absolutely does have support for this type of game. It is really a smattering of different loosely coupled tools that you can bring into the project, or roll your own. The star is absolutely the rendering engine and visual programming tools IMO.Unreal really excels at action games, but you can absolutely implement custom camera RTS controls and such. Batteries included but replaceable.The question \"does X engine support this kind of game\" is a bit.. off. You would be amazed at how many features these engines pack in. However: You would also be amazed at how NOT \"plug and play\" they are. It still takes a TON of effort and custom code to make a sophisticated game. reply johnnyanmac 9 hours agoparentprev>I was looking into Unity at the start of Covid while laid off, and DOTS was \"around the corner\" even THAT far back!to be fair, DOTS 1.0 did \"release\" in 2022 I believe. IIRC the core came out and the biggest ommision was DOTS animation. But many core parts of DOTS has been deemed \"production ready\" for about a year now.>They still don&#x27;t have an answer for a network stack, and now LOD is brokenbroken implies that Unity had a robust LOD system to begin with. That&#x27;s always been very primitive since Unity&#x27;s most successful projects were mobile or 2D.But yes, the Netcode has been in absolute shambles for years. Very confusing since that is the backbone of mobile. I&#x27;m guessing there&#x27;s some popular 3d party they defer to that integrates well with Unity. reply wilg 22 hours agoprevIt sounds like these issues are relatively fixable. It&#x27;s a classic victim of the Unity engine&#x27;s tech debt though. I use Unity myself and they desperately need to decide on how they want people to make video games in their engine. They can&#x27;t have three rendering pipelines and two ways of adding game logic that have a complicated matrix of interactions and missing features. And not great documentation and a bad bug reporting process. reply Someone1234 22 hours agoparentIt makes one wonder what their internal employee incentives are and if they&#x27;re problematic.Microsoft has a similar problem where nobody gets promoted from fixing bugs or maintaining stuff, everyone gets rewarded for new innovative [thing] so every two-three years there&#x27;s a completely new UI framework or similar.Although I feel like wanting to start-a-new is a common tech problem, where there are problems and everyone wants to just reboot to \"fix\" it rather than fixing it head-on inc. backwards compatibility headaches. reply cipheredStones 22 hours agorootparent> Microsoft has a similar problem where nobody gets promoted from fixing bugs or maintaining stuff, everyone gets rewarded for new innovative [thing] so every two-three years there&#x27;s a completely new UI framework or similar.Is there any big (or even medium-sized) company where this isn&#x27;t true? I feel like it&#x27;s just a rule of corporate culture that flashy overpromising projects get you promoted and regularly doing important but mundane and hard-to-measure things gets you PIP&#x27;d. reply throw3823423 22 hours agorootparentIt&#x27;s a matter of letting things degrade so that the maintenance becomes outright firefighting. I am currently working on a project where a processing pipeline has a maximum practical throughput of 1x, and a median day&#x27;s for said pipeline is... 0.95x. So any outage becomes unrecoverable. Getting that project approved 6 month from now would have been basically impossible. Right now, it&#x27;s valued at a promotion-level difficulty instead.At another job, at a financial firm I got a big bonus after I went live on November 28th with an upgrade that let a system 10x their max throughput, and scaled linearly instead of being completely stuck. at their 1x. Median number of requests per second received in dec 1st? 1.8x... the system would have failed under load, causing significant losses to the company.Prevention is underrated, but firefighting heroics are so well regarded that sometimes it might even be worthwhile to be the arsonist reply piaste 21 hours agorootparentIntuitively, \"fixing life-or-death disasterss is more visible and gets better rewards than preventing them\" doesn&#x27;t seem like it should be a unique problem of software engineering. Any engineering or technical discipline, executed as part of a large company, ought to have the potential for this particular dysfunction.So I wonder: do the same dynamics appear in any non-software companies? If not, why not? If yes, have they already found a way to solve them? reply harimau777 21 hours agorootparentOutside of software, people designing technology are engineers. Although by no means perfect, engineers generally have more ability to push back against bad technical decisions.Engineers are also generally encultured into a professional culture that emphasizes disciplined engineering practices and technical excellence. On the other hand, modern software development culture actively discourages these traits. For example, taking the time to do design is labeled as \"waterfall\", YAGNI sentiment, opposition to algorithms interviews, opposition to \"complicated\" functional programming techniques, etc. reply ghaff 20 hours agorootparentThat&#x27;s a very idealistic black-and-white view of the world.A huge number of roles casually use the \"engineer\" moniker and a lot of people who actually have engineering degrees of some sort, even advanced degrees from top schools, are not licensed and don&#x27;t necessarily follow rigid processes (e.g. structural analyses) on a day to day basis.As someone who does have engineering degrees outside of software, I have zero problem with the software engineer term--at least for anyone who does have some education in basic principles and practices. reply rat9988 19 hours agorootparentI have yet to see, with the exception of the software world, engineering with such loose process. reply ghaff 19 hours agorootparentAs someone who was a mechanical engineer in the oil business, I think you have a very positive view of engineering processes in general. reply nvm0n2 2 hours agorootparentprevIt&#x27;s common to start constructing buildings before the design is even complete. And there can be huge \"tech debt\" disasters in civil engineering. Berlin Airport is one famous example. reply dmoy 21 hours agorootparentprev> If yes, have they already found a way to solve them?A long history of blood, lawsuits, and regulations.Preventing a building from collapsing is done ahead of time, because buildings have previously collapsed, and cost a lot of lives &#x2F; money etc. reply harimau777 21 hours agorootparentI remember my very first day of studying engineering, the professor said: \"Do you know the difference between an engineer and a doctor? When a doctor messes up, people die. When an engineer messes up LOTS of people die.\" reply nostrademons 20 hours agorootparentprevHow do you think we got into this climate change mess? reply userinanother 20 hours agorootparentprevYeah but if you had a release target of dec 15 and it crashed dec 1st and you could have brought it home by the 7th you would have been a bigger winner. Tragedy prevented is tragedy forgotten. No lessons were learned reply bluedino 22 hours agorootparentprevI spent a few weeks migrating and then fixing a bunch of bugs in 20-year old Perl codebase (cyber security had their sights set on it). Basically used by a huge amount of people to record data for all kinds of processes at work.Original developer is long gone. Me and another guy are two of the only people (we aren&#x27;t a tech company) who can re-learn Perl, upgrade multiple versions of Linux&#x2F;Apache&#x2F;MySQL, make everything else work like Kerberos etc...Or maybe I&#x27;m one of the only people dumb enough to take it on.Either way, nobody will get so much as an attaboy at the next department meeting. But, they&#x27;ll know who to go to the next time some other project is resurrected from the depths of hell and needs to be brought up to date. reply dymk 22 hours agorootparentprevFacebook was pretty good about this on the infra teams. No, not perfect, but a lot better than the other big companies I was exposed to.If anything, big companies are better about tech-debt squashing, and it&#x27;s the little tiny companies and startups that are, on average, spending less time on it. reply hutzlibu 22 hours agorootparentprevI think it is a bit tricky to get the incentives right ( since the bookkeeping people like to quantize everything). If you reward finding and fixing bugs too much - you might push developers to write more sloppy code in the first place. Because then those who loudly fix their own written mess gets promoted - and those who quietly write solid code gets overlooked. reply xctr94 21 hours agorootparentGoodhartâ€™s law at work, or â€œwhy you shouldnâ€™t force information workers to chase after arbitrary metricsâ€. Basecamp has been famously just letting people do good work, on their terms, without KPIs.I will preemptively agree that this isnâ€™t possible everywhere; but if you create a good work environment where people donâ€™t feel like puppets executing the PMâ€™s vision, they might actually care and want to do a solid dayâ€™s work (which weâ€™re wired for). reply ajmurmann 22 hours agorootparentprevIs it only big companies? The fact that many companies in our industry need to do \"bug squash\" events because we are unable to prioritize bugs properly speaks books to meet. reply Jochim 21 hours agorootparentTop down decision making, typically by non-technical people who often have no idea what software development even involves.Eventually things get so bad that there&#x27;s no choice but to abandon feature work to fix them.The business loses out multiple times. Feature work slows down as developers are forced to waste time finding workarounds for debt and bugs. The improvements&#x2F;fixes take more time than they would have due to layers of crap being piled on top, and the event that forces a clean up generally has financial or reputational consequence.Collaborative decision making is the only way around this. Most engineers understand that improvements must be balanced with feature work.I find it very strange that the industry operates in the way it does. Where the people with the most knowledge of the requirements and repercussions are so often stripped of any decision making power. reply ghaff 20 hours agorootparentThis is pretty much a universal thing--whether it&#x27;s software development or home maintenance. It&#x27;s really tempting to kick the can down the road to the point where 1.) You HAVE to do something; 2.) It&#x27;s not your problem any longer; or 3.) Something happens that the can doesn&#x27;t matter any more.I won&#x27;t say procrastination is a virtue. But sometimes the deferred task really does cease to matter. reply 3seashells 10 hours agorootparentAt least we wouldn&#x27;t do that on a planetary scale, right? reply brucethemoose2 21 hours agorootparentprev> Is there any big (or even medium-sized) company where this isn&#x27;t true?Valve? reply uolmir 21 hours agorootparentFrom everything I&#x27;ve read Valve has exactly the same problem. Stack rating isn&#x27;t immune. New features still get rewarded the most. reply flukus 20 hours agorootparentprevIt seems endemic, especially everywhere that&#x27;s not a product company. I think it was mythical man month (maybe earlier) that pointed out the 90% of the cost of software is in maintenance, yet 50 years on this cost isn&#x27;t accounted for in project planning.Consultancies are by far the worst, a project is done and everyone moves on, yet the clients still expect quick fixes and the occasional added feature but there&#x27;s no one familiar with the code base.Developers don&#x27;t help either, a lot move from green field to green field like locusts and never learn the lessons of maintaining something, so they make the same mistakes over and over again. reply bandrami 14 hours agorootparentprevAviation. Software will often spend ten times as long in QA and testing as it will in principal development. reply Mistletoe 20 hours agorootparentprevhttps:&#x2F;&#x2F;www.thepeoplespace.com&#x2F;practice&#x2F;articles&#x2F;leadership-...Itâ€™s very rare, this is one of the only places I can imagine something like that happening. reply johnnyanmac 9 hours agorootparentprev> It makes one wonder what their internal employee incentives are and if they&#x27;re problematic.not a very strong one until recently. They implemented the FAANG esque \"levels\" in late 2021. Promotion lines weren&#x27;t too atypical from FAANG, but the new system was not around long enough to cause the problems people complain about today.The biggest issue IME was that teams were isolated. Maybe DOTS should have talked more with strongly integrating HDRP&#x2F;URP into its workflow, but DOTS was busy getting off the ground itself. HDRP and URP are two very different teams and they were trying to solve very different problems. Perhaps that was a bad move for the people it was serving, who&#x27;d want to have the flexibility to migrate to&#x2F;from HDRP and URP. There weren&#x27;t really much product management that was trying to make the engine cohesive, so you end up with a rendering core and a bunch of different plugins with different philosophies and whatnot.>I feel like wanting to start-a-new is a common tech problem, where there are problems and everyone wants to just reboot to \"fix\" it rather than fixing it head-on inc. backwards compatibility headaches.to some extent, yes. I think the one huge downside of Unity compared to modern companies is its antiquated CI&#x2F;CD. Getting changes into the core c++ engine took 10x longer than it really needed to. iterations on repo builds were slow because Unity simply didn&#x27;t cough out the money for proper server farms, and the interface to interact with the status of PRs felt like it was from 2005. Much of DOTS was iterated upon separately on with modern Jira&#x2F;Github&#x2F;etc. pipelines and the DOTS repo was very lean (and it was public too... until it wasn&#x27;t. I think they moved it in 2022?), moving to make a change to the core engine was like traveling back in time 15 years ago.Legacy code is a pain as is. And Unity definitely needed to revamp some non-feature workflows before it could really dig into the core Unity engine issues. reply capableweb 22 hours agorootparentprevThe developers of Cities Skylines has less than 50 employees in total, it&#x27;s a small developer based in Finland (Colossal Order), I doubt they have those sort of issues at that scale, that&#x27;s usually something that happens with medium&#x2F;large companies.Edit: seems I misunderstood, ignore me reply wilg 22 hours agorootparentTalking about Unity, not Colossal Order. reply Epa095 22 hours agorootparentprevUnity, not cities skylines. reply hnthrowaway0315 20 hours agorootparentprevIt&#x27;s a combination of team not given enough time amd headcount to maintain and develop a product and another team&#x27;s manager wants to grab a fief.So old products are thrown away while new products with similar functionalities are being created.Both teams are happy. The users suffer. reply thrillgore 21 hours agorootparentprevWe&#x27;re weeks past a very public pricing change that cost Unity market reach amidst competitors and open source projects; and that led to a CEO change. There are problems beyond what the employees can realistically fix. reply frozenfoxx 21 hours agoparentprevI worked at Unity on Build Automation&#x2F;Cloud Build for nearly a decade. Let me assure you, that tech debt is NOT being fixed any year soon. Itâ€™s due to a fundamental disconnect between executive leadership wanting to run the company like Adobe (explicitly) and every engineer wanting to work like a large scale open source project (Kubernetes, Linux, and Apache are pretty close in style). The only way anything gets built is as a Skunkworks project and you can only do so much without funding and executive support. reply johnnyanmac 9 hours agorootparent> I worked at Unity on Build Automation&#x2F;Cloud Buildmy condolances. the Build iterations and ancient CI workflow was by far the biggest complaint for the teams I was on and talked to. it takes so long getting stuff properly landed into trunk that I can&#x27;t really blame the teams wanting to break off if possible (it fortunately was for my division).But I&#x27;m guessing it was hard to convince product or execs that dev velocity matters, so we were all just wading in muck. I heard things were improving... but I heard that every month I was there.>executive leadership wanting to run the company like AdobeI think I know what you mean by this, but can you clarify? reply LeanderK 20 hours agorootparentprev> run the company like Adobe (explicitly)what does this mean? reply robterrell 18 hours agorootparentprevCan you elaborate on this? reply dexwiz 22 hours agoparentprevSounds like every other enterprise software platform. Unity has reach the IBM level of â€œno one gets fired for choosing X,â€ even though X only makes the business people happy. reply KronisLV 20 hours agoparentprevHonestly, automatic LOD generation would solve at least some of the performance issues: add the functionality, make it opt-out for those that don&#x27;t need LODs and enjoy performance improvements in most projects, in addition to some folks getting a simpler workflow (e.g. using auto-generated models instead of having to create your own, which could at the very least have passable quality).Godot has this: https:&#x2F;&#x2F;docs.godotengine.org&#x2F;en&#x2F;stable&#x2F;tutorials&#x2F;3d&#x2F;mesh_lod...Unreal has this (for static meshes): https:&#x2F;&#x2F;docs.unrealengine.com&#x2F;5.3&#x2F;en-US&#x2F;static-mesh-automati...Aside from that, agreed: the multiple render pipelines, the multiple UI solutions, the multiple types of programming (ECS vs GameObject) all feel very confusing, especially since the differences between them are pretty major. reply mardifoufs 19 hours agorootparentI&#x27;m pretty sure unity already has that. reply KronisLV 19 hours agorootparentOut of the box, it only has manual LOD support for meshes: https:&#x2F;&#x2F;docs.unity3d.com&#x2F;Manual&#x2F;importing-lod-meshes.html (where you create the models yourself)They played around with the idea of automatic LOD, but the repo they had hasn&#x27;t gotten updated in a while: https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;AutoLODThe closest to that would be looking at assets on the Asset Store, for example: https:&#x2F;&#x2F;assetstore.unity.com&#x2F;packages&#x2F;tools&#x2F;utilities&#x2F;poly-f...An exception to that is something like the terrain, which generates the model on the fly and decreases detail for further away chunks as necessary, but that&#x27;s pretty much the same with the other engines (except for Godot, which doesn&#x27;t have a terrain solution built in, but the terrain plugins do have that functionality). I guess in Unity&#x27;s case you can still get that functionality with bought assets, which won&#x27;t be an issue for most studios (provided that the assets get updated and aren&#x27;t a liability in that way), but might be for someone who just wants that functionality for free. reply mardifoufs 17 hours agorootparentOk that&#x27;s pretty surprising. I didn&#x27;t know it was still this bad reply wilg 19 hours agorootparentprevIt doesnâ€™t which is really annoying. reply AuryGlenz 21 hours agoparentprevIt&#x27;s honestly a bit insane.Just the other night I wanted to know what it&#x27;d take to do some AR development for the Quest 3 using Unity. 10 minutes in I was straight up confused. There&#x27;s AR Foundation, AR Core, AR Kit, and I think at least one other thing. I have no idea the difference between those, if they&#x27;re even wholly separate. That&#x27;s on top of using either the OpenXR or Unity plugin for the actual headset. reply andybak 21 hours agorootparentAR Kit is Apple&#x27;s thing. AR Core is Google&#x27;s thing. Neither of those are Unity&#x27;s fault. AR Foundation is a Unity layer to present a common interface. Which of my books is a good thing.Open XR is also an an attempt to make a cross platform layer for vendor specific APIs. Again not Unity&#x27;s fault. The Unity plugin system is a common interface for all XR devices.I&#x27;d generally support your sentiment but in this case you&#x27;re picking on things where Unity had mostly got it right. reply AuryGlenz 14 hours agorootparentThis comment made things more clear to me than the documentation that I saw, perhaps because I was looking at it from a Quest 3 perspective and their main page about it doesn&#x27;t mention that at all.I know what Open XR is all about but again, it&#x27;s not clear which you should actually use for development if you&#x27;re only targeting Quest devices, for instance. A little extra documentation would go a long way.The same goes for all of the other things so frequently mentioned like their renderers. reply squeaky-clean 11 hours agorootparentHonestly, like a lot of Unity functionality, what you do there is pay $50-100 for an asset from the store that handles things in a sane way. reply araes 20 hours agoparentprevIt sounds like they need to implement easy to use Level of Detail (LOD) and progressive meshes. 100,000 vertices on far away objects will break most rendering pipelines that do not somehow reduce them. 100,000 complicated matrix interactions instead of the like, 8, it probably takes really far away.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Level_of_detail_(computer_grap...[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Progressive_meshes reply moffkalast 21 hours agoparentprevI think it&#x27;s a good thing in the long run, one more reason to switch away from Unity to add to the ever growing pile. reply tus666 22 hours agoparentprevIt&#x27;s a classic victim of shitty, shitty software developers who blames tools rather than taking ownership.Or shitty software dev companies that push out crap to meet marketing deadlines.Either way, take your money elsewhere. reply gamblor956 21 hours agorootparentGiven that a number of other Unity-based games have had the same or similar performance issues, including KSP1, the Endless games, and others, it seems the problem is very much that Cities Skylines 2 is hitting up against the performance limits that the Unity engine is capable of without custom modifications to the engine-layer codebase. reply MrLeap 20 hours agorootparentI have personally been responsible for optimizing unity games you haven&#x27;t heard issues like this about ;)This write-up really points the finger at not solving occlusion culling or having good LOD discipline.Give a person a dedicated optimization mandate and you can avoid most of this. One of the first things I do when I&#x27;m profiling is to sort assets by tris count and scan for excess. I wonder if they had somebody go through and strategically disable shadowcasting on things like those teeth? I am guessing that they made optimization \"everybody&#x27;s responsibility\" but nobody had it as their only responsibility. reply hellotomyrars 19 hours agorootparentYeah I mean regardless of any of Unityâ€™s limitations, this is entirely upon the developer.However, I also find the suggestion that because there are other high profile examples of unity projects with performance issues, it must be a problem with unity.You donâ€™t hear that about Unreal Engine, despite the fact that there are poorly optimized UE games.Such a bizarre set of assumptions. reply johnnyanmac 8 hours agorootparentThere is definitely a lot of public bias amongst the two engines. UE4 couldn&#x27;t solve this problem either (UE5 might, but Nanite isn&#x27;t quite as \"it just works\" as you&#x27;d expect as of 5.1).It definiitely has much to do with how UE&#x27;s PR constantly shows off and ships new and exciting features that blend in the engine. Meanwhile, Unity has been criticized for some 6+ years minimum for its package management and lack of cohesion. reply gamblor956 20 hours agorootparentprevOcclusion culling and LOD should be handled by the engine, not the game logic, so the write-up really points to the problem being Unity&#x27;s new and very incomplete rendering pipeline for ECS. reply vasdae 19 hours agorootparentGranted I know next to nothing about game development, but aren&#x27;t LOD models made by hand? reply MrLeap 19 hours agorootparentThere are tons of answers to this! I&#x27;m going to say that in projects I&#x27;ve worked on, LODs have been hand made about 60% of the time.There are tools for creating automatic LODs that come with their own pro&#x27;s and con&#x27;s. A bad LOD chain can express itself as really obvious pop-in while you&#x27;re playing the game. There&#x27;s also these things called imposters that are basically flipbook images of an object from multiple angles that can be used in place of the true 3d geometry at a distance. Those are created automatically. They tend to be like 4 triangles but can eat more vram because of the flipbook sizes.Unreal engine has nanite, which is a really fancy way to side step needing LOD chains with something akin to tessellation, as I understand it. Tech like that is likely the future, but it is not accurate to describe it as the \"way most games are made today\" reply LarsDu88 13 hours agorootparentprevThis is simply not correct. City Skylines 2 even went through the trouble of using DOTs which is something you cannot take advantage of in Unreal Engine or Godot. To get more optimal than that on the CPU utilization side, you will be writing your own engine in C++ or Rust.The fuck up here is whoever was handling the art assets. You simply do not ship a game with such detailed graphics and no LODs. They must&#x27;ve simply been downloading things off the asset store and throwing them in without any regard for performance. reply raincole 21 hours agorootparentprevI&#x27;ll be really surprised if City Skylines&#x27;s team didn&#x27;t have access to Unity&#x27;s source code. reply kimixa 20 hours agorootparentAnd do they have the number of engineers with the required skills to rewrite half the engine? Especially if the reason why they developed using those tools and engine is they expected not to have to do it themselves in the first place?It&#x27;s not like there&#x27;s just some \"go_slow=true\" constant that just needs changing. reply bananaboy 18 hours agorootparentprevIt&#x27;s pretty unlikely. A source code license is negotiated with the sales team directly and costs at least USD100k last I heard (the price is not publicly disclosed). They&#x27;re also reluctant to give source code licenses at all. reply285 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Cities: Skylines 2, a simulation-based game, has released with numerous problems, including weak performance and gameplay bugs.",
      "This game possesses high system requirements due to its GPU intensity, coupled with its usage of the Entity Component System implementation and Burst compiler, part of the Unity 2022 engine's DOTS technologies, which is unusual for such games.",
      "Despite the developers acknowledging and indicating a need for significant time to address them, these issues offer a useful insight into the intricacies of game development."
    ],
    "commentSummary": [
      "Cities: Skylines 2 has launched with several performance and gameplay issues largely tied to its use of the Entity Component System (ECS) implementation and Burst compiler, part of the Unity 2022 engine's DOTS technologies.",
      "The game has high system requirements; it is GPU-intensive for a simulation game and requires an expensive graphics card for 60 FPS gameplay, which has been a point of criticism.",
      "Beyond performance issues, problems also stem from experimental texturing and excessive detail in character models. Developers have recognized these issues but underscore their resolution requires significant effort."
    ],
    "points": 840,
    "commentCount": 538,
    "retryCount": 0,
    "time": 1699206847
  },
  {
    "id": 38162435,
    "title": "Resource Collection Aims to Boost Developers' Proficiency in DOM Manipulation with Vanilla JavaScript",
    "originLink": "https://phuoc.ng/collection/html-dom/",
    "originBody": "Mastering DOM manipulation with vanilla JavaScript Star me on GitHub â†’ 5400 â­ Web development moves at lightning speed. I still remember when I first started using libraries like jQuery, Prototype, script.aculo.us, Zepto, and many more. Even with modern tools like Angular, VueJS, React, Solid and Svelte, we still have to deal with the Document Object Model (DOM). While these frameworks encapsulate and hide direct DOM management, they still give us access to work with the DOM via refs and event handlers. Whether you're developing or using a web component in any framework, you need to work with the DOM at a certain level. Knowing the browser DOM APIs and how to use them is crucial to web development. A website that introduces the APIs, highlights common problems, and provides answers to popular questions can be incredibly useful. That's why I've put together this collection of resources: No external libraries, just native browser APIs Small, easy-to-understand examples Live demos Tips and best practices included Real-life use cases Works with modern browsers and even supports Internet Explorer Get ready to master DOM manipulation with vanilla JavaScript. Level 1 â€” Basic Change the favicon dynamically based on user's color scheme preference 17 Oct, 2023 Select a given line in a text area 24 Sep, 2023 Calculate the reading time of a webpage 23 Sep, 2023 Dynamically update CSS root variables 21 Sep, 2023 Insert HTML at the current position of a contentEditable element 20 Sep, 2023 Insert text into a text area at the current position 19 Sep, 2023 Move the cursor to the end of a contentEditable element 18 Sep, 2023 Get or set the cursor position in a text field 17 Sep, 2023 Display a confirm modal when closing the browser 15 Sep, 2023 Make a textarea auto-expand 03 Sep, 2023 Check if users click outside of selected text 02 Sep, 2023 Prevent the page from scrolling to an element when it is focused 19 Oct, 2021 Detect the dark mode 03 May, 2020 Check if the code is running in the browser 03 May, 2020 Detect mobile browsers 25 Apr, 2020 Check if the touch events are supported 25 Apr, 2020 Detect internet explorer browser 24 Apr, 2020 Select the text content of an element 19 Apr, 2020 Get the selected text 19 Apr, 2020 Prevent body from scrolling when opening a modal 18 Apr, 2020 Scroll to an element 08 Apr, 2020 Calculate the mouse position relative to an element 02 Apr, 2020 Press Shift and Enter for a new line 01 Apr, 2020 Distinguish between left and right mouse clicks 01 Apr, 2020 Check if an element is in the viewport 28 Mar, 2020 Swap two nodes 18 Mar, 2020 Count the number of characters of a textarea 18 Mar, 2020 Upload files with Ajax 17 Mar, 2020 Submit a form with Ajax 17 Mar, 2020 Serialize form data into a query string 17 Mar, 2020 Insert given HTML after or before an element 16 Mar, 2020 Detect mac OS browser 15 Mar, 2020 Strip HTML from a given text 10 Mar, 2020 Prevent the default action of an event 10 Mar, 2020 Get size of the selected file 10 Mar, 2020 Check if the native date input is supported 10 Mar, 2020 Detect if an element is focused 09 Mar, 2020 Determine the height and width of an element 03 Mar, 2020 Toggle password visibility 28 Feb, 2020 Toggle an element 28 Feb, 2020 Put cursor at the end of an input 28 Feb, 2020 Select the text of a textarea automatically 27 Feb, 2020 Create one time event handler 24 Feb, 2020 Check if an element is a descendant of another 22 Feb, 2020 Resize an iframe to fit its content 21 Feb, 2020 Replace broken images 21 Feb, 2020 Redirect to another page 21 Feb, 2020 Go back to the previous page 21 Feb, 2020 Get the size of an image 21 Feb, 2020 Get the position of an element relative to another 21 Feb, 2020 Get or set the document title 21 Feb, 2020 Get the position of an element relative to the document 19 Feb, 2020 Scroll to top of the page 18 Feb, 2020 Reload the current page 18 Feb, 2020 Prepend to an element 18 Feb, 2020 Get the document height and width 18 Feb, 2020 Detect clicks outside of an element 18 Feb, 2020 Trigger an event 17 Feb, 2020 Show or hide an element 17 Feb, 2020 Set CSS style for an element 17 Feb, 2020 Get siblings of an element 17 Feb, 2020 Get, set and remove data attributes 17 Feb, 2020 Get, set and remove attributes 17 Feb, 2020 Get or set the HTML of an element 17 Feb, 2020 Get CSS styles of an element 17 Feb, 2020 Attach or detach an event handler 17 Feb, 2020 Append to an element 17 Feb, 2020 Wrap an element around a given element 16 Feb, 2020 Unwrap an element 16 Feb, 2020 Select the children of an element 16 Feb, 2020 Select an element or list of elements 16 Feb, 2020 Replace an element 16 Feb, 2020 Remove an element 16 Feb, 2020 Remove all children of a node 16 Feb, 2020 Loop over a nodelist 16 Feb, 2020 Insert an element after or before other element 16 Feb, 2020 Get the text content of an element 16 Feb, 2020 Get the parent node of an element 16 Feb, 2020 Get the closest element by given selector 16 Feb, 2020 Execute code when the document is ready 16 Feb, 2020 Create an element 16 Feb, 2020 Clone an element 16 Feb, 2020 Check if an element has given class 16 Feb, 2020 Check an element against a selector 16 Feb, 2020 Add or remove class from an element 16 Feb, 2020 Level 2 â€” Intermediate Remove indentation in a text area using the Shift+Tab key combination 03 Oct, 2023 Truncate the content of an element 02 Oct, 2023 Jump to the beginning or end of the current line in a text area 25 Sep, 2023 Count how many lines a given string takes up in a text area 24 Sep, 2023 Enter full-screen mode 22 Sep, 2023 Indent content in a text area using the Tab key 20 Sep, 2023 Build a spin input 18 Sep, 2023 Get or set the cursor position in a contentEditable element 17 Sep, 2023 Build an OTP input field 17 Sep, 2023 Display a modal when users leave website 15 Sep, 2023 Waiting for an element to become available 13 Sep, 2023 Automatically resize an iframe when its content height changes 13 Sep, 2023 Add a special message when users copy text 27 Aug, 2023 Scroll an element to ensure it is visible in a scrollable container 03 May, 2020 Position an element absolutely to another element 02 May, 2020 Check if an element is visible in a scrollable container 02 May, 2020 Get the direction of the text selection 24 Apr, 2020 Show a custom context menu at clicked position 21 Apr, 2020 Save and restore the text selection 19 Apr, 2020 Print an image 08 Apr, 2020 Show a loading indicator when an iframe is being loaded 06 Apr, 2020 Show a ghost element when dragging an element 31 Mar, 2020 Allow to enter particular characters only 31 Mar, 2020 Paste as plain text 19 Mar, 2020 Paste an image from the clipboard 18 Mar, 2020 Export a table to csv 17 Mar, 2020 Scale a text to fit inside of an element 16 Mar, 2020 Resize an image 15 Mar, 2020 Highlight an element when dragging a file over it 09 Mar, 2020 Communication between an iframe and its parent window 08 Mar, 2020 Placeholder for a contenteditable element 04 Mar, 2020 Detect if the caps lock is on 03 Mar, 2020 Calculate the size of scrollbar 03 Mar, 2020 Resize the width of a text box to fit its content automatically 29 Feb, 2020 Measure the width of given text of given font 28 Feb, 2020 Get the default value of a CSS property 28 Feb, 2020 Change the website favicon 28 Feb, 2020 Get the first scrollable parent of an element 27 Feb, 2020 Check if an element is scrollable 27 Feb, 2020 Copy text to the clipboard 24 Feb, 2020 Preview an image before uploading it 21 Feb, 2020 Download a file 21 Feb, 2020 Load a JavaScript file dynamically 18 Feb, 2020 Load a CSS file dynamically 18 Feb, 2020 Level 3 â€” Advanced Create a custom cursor 21 Sep, 2023 Sanitize HTML strings 20 Sep, 2023 Detect if users open another tab of the current page 18 Sep, 2023 Show an addition toolbar after users selects text 02 Sep, 2023 Create a custom scrollbar 03 May, 2020 Scroll to an element smoothly 26 Apr, 2020 Show or hide table columns 24 Apr, 2020 Drag to scroll 20 Apr, 2020 Copy highlighted code to the clipboard 19 Apr, 2020 Zoom an image 18 Apr, 2020 Create a range slider 07 Apr, 2020 Create an image comparison slider 06 Apr, 2020 Create resizable split views 04 Apr, 2020 Drag and drop table column 03 Apr, 2020 Drag and drop table row 02 Apr, 2020 Drag and drop element in a list 01 Apr, 2020 Sort a table by clicking its headers 08 Mar, 2020 Resize columns of a table 05 Mar, 2020 Make a resizable element 03 Mar, 2020 Make a draggable element 03 Mar, 2020 Tip Avoid shifting layout when opening a modal 13 Sep, 2023 Get the bounding rectangle of a text node 30 Aug, 2023 Attach event handlers inside other handlers 24 Apr, 2020 Recent posts âš¡ Scroll by dragging 03 Nov, 2023 Create an image comparison slider 02 Nov, 2023 Create a drawer 01 Nov, 2023 Create resizable split views 31 Oct, 2023 Create a range slider 30 Oct, 2023 Create a custom draggable hook 29 Oct, 2023 Make an element draggable on touchscreen devices 28 Oct, 2023 Make a given element draggable 27 Oct, 2023 Build a tooltip component 26 Oct, 2023 Pass a ref to a child component using forwardRef() 25 Oct, 2023 Expose methods of a component using useImperativeHandle() 25 Oct, 2023 Merge different refs 24 Oct, 2023 Make an element draggable 23 Oct, 2023 Create a custom hook returning a callback ref 22 Oct, 2023 Save the element passed to a callback ref as a state 21 Oct, 2023 See moreâ†’ Newsletter ðŸ”” If you're into front-end technologies and you want to see more of the content I'm creating, then you might want to consider subscribing to my newsletter. By subscribing, you'll be the first to know about new articles, products, and exclusive promotions. Don't worry, I won't spam you. And if you ever change your mind, you can unsubscribe at any time. PhÆ°á»›c Nguyá»…n",
    "commentLink": "https://news.ycombinator.com/item?id=38162435",
    "commentBody": "Mastering DOM manipulation with vanilla JavaScriptHacker NewspastloginMastering DOM manipulation with vanilla JavaScript (phuoc.ng) 277 points by srirangr 4 hours ago| hidepastfavorite82 comments snide 3 hours agoExcellent list and examples. Browsing through a dozen or so of these I&#x27;m amazed at the care and detail in the examples. They are not only functional, but provide very solid UI work as well.As I&#x27;ve shifted away from platforms like React to smaller, minimalist implementations, I often have trouble finding ways for how to do complex patterns in standard JS. It&#x27;s funny when you look at the code and think... huh, that&#x27;s actually a lot easier than importing a huge library with way too many props :)Thanks to the author for putting this together. reply bob1029 3 hours agoparent> It&#x27;s funny when you look at the code and think... huh, that&#x27;s actually a lot easier than importing a huge library with way too many propsI had this realization after the 3rd or 4th RiotJS major version update. It started getting harder to do it the opinionated way. I realized that every minimalistic JS framework would eventually suffer this fate as totally innocent feature requests gradually accumulate into a monstrous pile of hooks, events and other side-effect bandaids.I don&#x27;t even use jQuery anymore. I will use things like ES modules to help organize code, but you don&#x27;t need to run a goddamn npm command to use any of that technology. All of this is a global browser standard, not something you have to vendor in.I look at MDN as the Bible these days. If you take a leap of faith, it will carry you almost 100% of the way. The frameworks will cripple you after a certain point. I can&#x27;t say this would have been true ~5 years ago. Things have shifted rapidly with browser APIs converging on a powerful set of primitives (modules, flexbox, grid, etc) that are now very consistent across platforms. reply reactordev 3 hours agorootparentI like the fact that you said â€œI canâ€™t say this would have been true ~5 years agoâ€.React et al. exist because there werenâ€™t global standards around these things and composability suffered. Now that we are all playing the same chorus, these frameworks provide little more than component libraries to reuse. This can be done with ES modules. JSX is why most React folks stick with React, not knowing they can use Preact or just NakedJSX if they wish.I just wanted to verbally concur that vanilla JS is more than capable of doing everything you need. Custom tags. Shadow dom. Composed UI. etc. if you are ok with returning HTMLElement vs a JSX closure. reply efdee 2 minutes agorootparentNo, jQuery existed for that reason. React has other reasons, one of them being that manual DOM tracking and manipulation is brittle and annoying and the way React works turns that around completely.I&#x27;m not sure what Preact or NakedJSX offers me that would make it a better choice than React. reply wayfinder 1 hour agorootparentprevNo, React exists because there was a need for a template engine thatâ€™s easy to use.There is still no global standard for templates.Just like mustache, Twig, Java Server Pages, Jinja, ERBâ€¦ every language has this problem. You have a bunch of components written in HTML and you need to mix and match them together in a bigger unit. React+JSX let you do that easier than appending DOM components. Before React, youâ€™d use different libraries that did what React did.The ONLY major language I can think of that has templating baked in is PHP. reply Joeri 19 minutes agorootparentHTML has built-in templates now, and custom elements (web components) provide composability. If the built-in templating is not quite powerful enough (it has no logic or variables) thereâ€™s developit&#x2F;htm which is basically JSX but parsing in the browser.React is still easier, but it comes at a maintainability tax because you have to keep upgrading to new react versions and port to new react wrapper framework versions if you want to keep a codebase in active development. Vanilla web development has no such problem because web standards remain backwards compatible.For a small project where I donâ€™t care about SEO &#x2F; server-side rendering I absolutely prefer vanilla web dev over React now. reply nicoburns 2 hours agorootparentprev> JSX is why most React folks stick with React, not knowing they can use Preact or just NakedJSX if they wish.The large ecosystem of component libraries and the backing of a large corporate company is why people continue to use React. That, and that the extra 100kb that comes with React is generally dwarfed by the actual application in any large app. reply explaininjs 2 hours agorootparentI&#x27;m using preact for a few projects and the integration with React stuff is generally seamless, but there are enough rough edges encountered that looking back I wouldn&#x27;t mind taking the difference of a few dozen kB to avoid them all. The vast majority of my load time is taken by TTFB, even from the Cloudflare Pages local cache, so worrying about kilobytes seems misguided. reply sesm 1 hour agorootparentprevReact exists because Facebook needed to sync DOM elements in different parts of webpage with a single source of state. There is still no global standard that solves this problem. reply marcosdumay 1 hour agorootparentYou mean state handlers? Those are one of the main reasons people adopted OOP at the 80&#x27;s.The observer pattern got pretty much standardized by the GoP book, but people overwhelmingly prefer to run some non-standard specialized thing, as it&#x27;s usually much simpler. It&#x27;s a pretty much solved problem since way before a lot of current developers were even born.All of (ok, a lot of) the React&#x27;s complexity comes from it being a generic library that must support every kind of usage. Any standard would have to be as complex. reply gedy 1 hour agorootparentprevYeah I think some people glorify \"simple\", low level solutions, which work fine for sprinkles of interactivity. But if I&#x27;m building a non trivial UI application - give me a framework please! reply snide 3 hours agorootparentprevAnd this is absolutely true in CSS now as well. I find a lot of people are needing to rediscover the standards now that they&#x27;ve upped their featuresets. reply nine_k 2 hours agorootparentprevOne of the points of React vs e.g. jQuery was the ability to do efficient updates in large DOM.trees.Did something arrive to plain JS APIs to allow this in a simple enough way? reply wruza 12 minutes agorootparentthe ability to do efficient updates in large DOM.treesIt was the ability to do efficient updates based on large VDOM diffs. The speed of updates is the same between a React VDOM-diff update plan and a direct update of the same size based on e.g. observers. reply kevindamm 2 hours agorootparentprevTemplates and shadow DOM help a lot with creating and cloning of large subtrees, but time to first render and caution around page re-layout still require attention to do well at scale. reply r-spaghetti 2 hours agorootparentprevIt depends on the type of relations between the updates you want to make, for example adding&#x2F;removing a class can group DOM elements. Updating all elements with this class is easy. reply irrational 2 hours agorootparentprevI though React, et al were originally created as a better way to manage DOM updates (shadow DOM). Has that need gone away? reply nicoburns 2 hours agorootparentNot at all. Without React (or a similar framework) you are still left with the options of:- Manually keep track of UI state (which is complex, and often leads to hard to maintain spaghetti code)- Recreate an entire DOM tree every time you re-render (which is slow to the extent that it will often lead to performance problems in practice). Apparently this is not as slow as it used to (so you could probably get away with this sometimes - and you could before), but it&#x27;s still generally a better idea to use a framework as you get better performance for very low cost.There are plenty of non-React frameworks that will you these benefits too. And many of them are much smaller. I think the reason to use React specifically is more \"business reasons\" such assurances that the framework will continue to be maintained, library ecosystem, developer pool, etc. Other frameworks are often technically superior, the difference just isn&#x27;t that great. reply evan_ 2 hours agorootparentprevReact et al use a virtual DOM, not shadow DOM (unless you specifically render it to shadow DOM).They are a better* way to manipulate DOM in that you no longer need to manipulate the DOM, you just build a function that returns what the DOM should look like and it figures out what transforms need to happen.*Theyâ€™re pitched as a better way, and I think itâ€™s better, but people can reasonably disagree reply hombre_fatal 2 hours agoparentprevNothing in TFA competes with a web client framework, though, since the framework is solving the general questions of how to manage state and then update the UI when state changes.For example, pretty much all of the TFA examples are code you&#x27;d have to write even if you were using React. reply danielvaughn 3 hours agoprevSo Iâ€™m working on this side project. I donâ€™t have a name for it yet but Iâ€™ve been describing it as â€œvim for web designersâ€. The idea is that you can build a website (or components) in the browser, similar to Webflow, but itâ€™s entirely driven by a command language.When I began the project, I told myself I wasnâ€™t going to use a framework. After all, Iâ€™ve been doing this since 2009, I was working with JS for a long time before I ever touched a framework. And besides, the native DOM APIs have long been good enough. Right?My god, it was an absolute slog. Marshalling data into and out of the DOM is so tedious and error prone that I began writing my own mini framework to save my own sanity. And then I remembered why I began using frameworks to begin with - because ultimately youâ€™re going to be using a framework of some kind regardless, itâ€™s just whether you want your own custom one or a community-built industry standard.I do still think native DOM is great, if youâ€™re working with small amounts of interactivity. Most sites that use React probably donâ€™t need it. But if your product has even a little bit of nuance or complexity, Iâ€™d go with a framework and avoid the hassle.In the end I migrated to Svelte, and Iâ€™m much happier as a result. reply the__alchemist 1 hour agoparentYou are building application - I could estimate from your description alone that you&#x27;re better off wrapping the DOM interactions.For websites that need interaction, DOM manipulation is IMO a better move due to it using a more fundamental&#x2F;universal skill, light or no build process, faster execution, and explicitness.This is a categorization, with lots of room for grey areas! I bring it up because your use case falls squarely in the category that benefits from a framework. reply danielvaughn 1 hour agorootparentYeah, agreed. I was hoping I could lean into a quasi-HATEOAS approach, where the state of the editor is reflected in the state of the html output. My thinking was that if it&#x27;s only in the html, then in theory it would handle very large numbers of nodes better, since you don&#x27;t have to correspondingly increase the size of JS object in memory.I&#x27;m still attracted to that idea, but it&#x27;s not worth it in the short term. reply dmix 1 hour agoparentprevHyper minimalist projects always end up re-implementing half of jquery or lodash. Much easier to just find a good library that does treeshaking like ramda and use it sparingly. reply bryancoxwell 3 hours agoparentprevHowâ€™s that saying go? Choose a framework or youâ€™ll invent one? reply danielvaughn 3 hours agorootparentYep, and itâ€™s exactly right. It had been so many years since Iâ€™d worked directly at the DOM layer that Iâ€™d forgotten why I stopped.That being said, it was equally difficult picking a framework that had the least likelihood to handicap me down the road. And I am still using plenty native methods, especially for the CSSOM. That has yet to be sufficiently abstracted into any kind of framework. reply synergy20 3 hours agoparentprevsveltekit is all about SSR these days, svelte itself does not have a default client side router, I was trying svelte for csr spa and gave up reply danielvaughn 3 hours agorootparentIâ€™m using Astro for SSR and only using Svelte for the client-side state. Itâ€™s working out well so far. reply meiraleal 3 hours agorootparentprevWith the view transition API[1], directory&#x2F;index.html is&#x2F;will be the client-side router.1. https:&#x2F;&#x2F;chipcullen.com&#x2F;adding-view-transitions-api&#x2F; reply bakkoting 4 minutes agoprevA number of these are using older, awkward APIs, when there&#x27;s nicer native versions now.For example, \"replace an element\" is ele.parentNode.replaceChild(newEle, ele);but unless you&#x27;re targeting Chrome{ const favicon = document.querySelector(&#x27;link[rel=\"icon\"]&#x27;); favicon.href = (window.matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;).matches) ? &#x27;dark-mode-favicon.png&#x27; : &#x27;light-mode-favicon.png&#x27;; }; setFavicon();And then this:> We can use this function to update the user&#x27;s color scheme preference whenever they make a change. window .matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;) .addEventListener(&#x27;change&#x27;, setFavicon);They are running \"window.matchMedia(&#x27;(prefers-color-scheme: dark)&#x27;)\" twice unnecessarily. If you&#x27;re going to run `setFavicon` as the callback to the event listener, you can get the result from the `event` parameter passed to the callback like this: const setFavicon = (event) => { const favicon = document.querySelector(&#x27;link[rel=\"icon\"]&#x27;); favicon.href = event.matches ? &#x27;dark-mode-favicon.png&#x27; : &#x27;light-mode-favicon.png&#x27;;This would be my preferred way of doing it because it supports users who are visiting with JS disabled.https:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;change-the-favicon-dyna... reply explaininjs 2 hours agoparentAlso, doing it in HTML prevents the flash of alternate-themed content. Doing the same with the `theme-color` meta is also a seldom-used but great feature for detail-minded PWA developers. reply fatnoah 2 hours agoprevWhat&#x27;s old is new again. My first front-end role was building web applications wayyyyy back in 1999. Everything was done with \"Vanilla\" JS, including fancy stuff like drag and drop in a treeview, click to edit, building what effectively was a single page app, etc. etc. The hardest part was negotiating the differences between Netscape and Internet Explorer. reply alin23 2 hours agoprevyeaaaaaaah no. Never going this path again for any interactive page.This summer I built an irrigation system for my father&#x27;s field, with the idea that the scheduling would be done in relative time (water this zone for 2h) instead of absolute (start watering this zone at 9AM and end at 11AM).I kept it as simple as possible: - 16 cheap relays for the 24V AC electric valves - 1 beefy relay for the pump - 1 Pi Pico W for controlling the relaysHere&#x27;s a picture of the \"low-tech\" build: https:&#x2F;&#x2F;files.alinpanaitiu.com&#x2F;lowtech-irigation.jpegThe Pico W would have a code.py with all the relay and schedule logic, and an index.html which would be a simple list of sliders to set how much time each valve should be open. The Pico W would function as a \"hotspot\" which the phone would connect to, and the \"app\" would be available at 192.168.4.1.It can be added to homescreen and it looks just like a native app as far as my father is concerned. It doesn&#x27;t feel like one though, he can tell.The web page looks like this: https:&#x2F;&#x2F;shots.panaitiu.com&#x2F;lHMGplFnThat&#x27;s all there is to it, really. That simple single page with a few sliders and buttons took weeks to build and test thoroughly with only vanilla JS. I was obviously constrained by the flash memory, but there are KB-sized helper frameworks nowadays which I wasn&#x27;t aware of.It&#x27;s also really hard to refactor. I initially stored the schedules at minute-resolution, but then had to store it in second-resolution to add a repeat schedule feature. Oh boy, so much code to parse and change, so many slider.value calculations to redo... yep, never doing this again reply explaininjs 2 hours agoparentThe magic of react is that it handles the entire View part of MVVC and also provides a nice DSL for writing the View Controller. With a lot of architectural planning you can develop as clean of code without it (cleaner, perhaps), but I agree it&#x27;s better to just embrace it. reply tolmasky 1 hour agoprevThese are great. It&#x27;s unfortunate that if there&#x27;s any minor bugs there&#x27;s no way to really keep in sync though, for example the security issue mentioned in another comment in these comments. It would be cool if these examples could be wrapped up into some sort of convenient \"package\" or something like that, such that you could easily know if there are any updates and grab them, as well as just make it easier to grab these functions whenever you need them instead of copy-pasting them... reply blablabla123 1 hour agoprevWhat really blew my mind was when I found out that many of the DOM functions return live HTMLCollections that are automatically updated. I wonder how often that feature is being actively used in production code (before the framework craze or developing with no framework being somewhat more popular again) reply beardyw 3 hours agoprevI am a great fan of vanilla JavaScript so for me it is a great resource. Sadly many js developers nowadays don&#x27;t really understand what the Dom can do for them. A great resource! reply jampekka 2 hours agoprevI appreciate the benefit of not having dependencies but not a fan of this rabid vanilla fixation.Of course you can do anything with Vanilla that you can do with e.g. jquery. Otherwise jquery obviously couldn&#x27;t do it.The problem is that the vanilla DOM API sucks. document.getElementById sucks vs $, document.querySelector sucks vs $. setTimeout and setInterval suck. addEventHandler really sucks. reply mg 3 hours agoprevThese days, I get by with just two dependencies. One is dqs.js:https:&#x2F;&#x2F;github.com&#x2F;no-gravity&#x2F;dqs.jsWhich turns document.querySelector(&#x27;.some .thing&#x27;) into dqs(&#x27;.some .thing&#x27;).The other one is Handlebars:https:&#x2F;&#x2F;github.com&#x2F;handlebars-lang&#x2F;handlebars.js&#x2F;Which I use for all my frontend templating needs. reply bambax 3 hours agoparentInstead of importing dsq you can do this const $ = document.querySelector.bind(document); const $$ = document.querySelectorAll.bind(document); reply gmiller123456 2 hours agorootparentThat&#x27;s pretty much what that include file does, but with \"dqs\" rather than \"$\". I&#x27;m not a fan of using the $ as it might confuse others that you&#x27;re using JQuery. reply bambax 1 hour agorootparentWouldn&#x27;t that be a feature? ;-) reply mg 3 hours agorootparentprevThat&#x27;s what I did before I put those lines (and some more) into dqs.js reply throw555chip 2 hours agorootparentprevThat&#x27;s a neat shortcut, I prefer vanilla JS too. reply xeckr 1 hour agorootparentprevOrconst $ = (query) => document.querySelector(query);const $$ = (query) => document.querySelectorAll(query);(How do I write monospace on HN?) reply nicoburns 2 hours agoparentprevThese days? This is what we were using 10 years ago before React came about! It works fine, but performance can be a problem in large apps as handlebars just naively recreates entire DOM trees rather than diffing. reply dylan604 3 hours agoparentprevare we really concerned about the shortness of the code for the worse tradeoff of being less readable?how much is truly saved by this? these seem like the things of a solo dev vs building something for other people to follow reply mg 3 hours agorootparentWhat&#x27;s wrong with solo devs?You are talking to me on a site done by one.This is the frontend code:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;hn.js reply dylan604 1 hour agorootparentI&#x27;ve been a solo dev as well. But unless you&#x27;re going to live forever, the code should be maintainable by someone else. You can be a solo dev that&#x27;s impossible to ever work with, or you can maintain good practices so that other people can play along as well. reply throw555chip 2 hours agorootparentprevThat&#x27;s wild, never really looked into it, reminds me how a C programmer thinks. I&#x27;m mostly backend but get involved with frontend every now and then. Tried to avoid Angular and React but now trying to teach myself Angular. AngularJS was nice. Angular sucks, very complicated for doing the same things I can do with vanilla HTML&#x2F;CSS&#x2F;JS, but that&#x27;s what companies seem to be hiring for. reply evbogue 3 hours agorootparentprevhn would benefit from the ael library reply von_lohengramm 3 hours agorootparentprevThere&#x27;s a reason why web developers have the reputation they do. reply r-spaghetti 2 hours agoparentprevCan you use Handlebars in Cloudflare workers? EJS was not accepted and string literals are string literals. reply mg 2 hours agorootparenthandlebars.js runs in the browser:https:&#x2F;&#x2F;plnkr.co&#x2F;edit&#x2F;LXb5TOMoyAxdbTxB reply evbogue 3 hours agoparentprevCombine this with https:&#x2F;&#x2F;github.com&#x2F;dominictarr&#x2F;hscrpt and you&#x27;ll be building websites in no time! reply bobmaxup 3 hours agoprevIf I were making a cookbook on HTML&#x2F;JS I would probably leave out this:https:&#x2F;&#x2F;phuoc.ng&#x2F;collection&#x2F;html-dom&#x2F;sanitize-html-strings&#x2F;#... reply panzi 2 hours agoparent`value.startsWith(&#x27;javascript:&#x27;)` and there you have a vulnerability. There can be arbitrary white-space before the URL, so you&#x27;d need to do `value.trim().startsWith(&#x27;javascript:&#x27;)` instead. However, I much prefer white listing instead, i.e. only allowing `http:`, `https:` and maybe `mailto:`, `ftp:`, `sftp:` and such. Maybe allow relative URLs starting with `&#x2F;`. Maybe. Though that would mean to correctly handle all attributes that actually can be URLs. Again, I&#x27;d just white list a few tags plus a few of their attributes. reply zeroCalories 2 hours agoparentprevWhy? This is a common need. Is there an issue with the implementation or another reason why you would want to use a library? reply chrismorgan 1 hour agoparentprevIâ€™m not sure quite why youâ€™re against removing script tags, but honestly that entire article is poor, riddled with disastrously bad advice:â€¢ â€œUsing regular expressionsâ€: it suggests that this approach is acceptable within its limits. Itâ€™s not at all. As a simple example, the expression shown is trivially bypassed by \"â€¦\". This is why, unlike the post claims claims, using regular expressions for cleaning HTML is not a common approach.â€¢ (â€œEliminating the script tagsâ€: I want to grumble about using `[...scriptElements].forEach((s) => s.remove())` instead of `for (const s of scriptElements) { s.remove(); }` or even `Array.prototype.forEach.call(scriptElements, (s) => s.remove())`. Creating an array from that HTMLCollection is just unnecessary and a bad habit.)â€¢ â€œRemoving event handlersâ€: `value.startsWith(&#x27;javascript:&#x27;) || value.startsWith(&#x27;data:text&#x2F;html&#x27;)` is inadequate. Tricks like capitalising and adding whitespace (which the browser will subsequently normalise) in order to bypass such poor checks have been common for decades.â€¢ â€œRetrieving the sanitized HTMLâ€: you are now vulnerable to mXSS attacks, which undo all your effort.â€¢ â€œElements and attributes to remove from the DOM treeâ€: this proposes a blacklist approach and mentions a few examples of things that should be removed. Each example misses adjacent but equally-important things that should be removed. You will not get acceptable filtering if you start from this approach.â€¢ â€œSimplifying HTML sanitization with external librariesâ€: this is pitched merely as easier, faster and cheaper, rather than as the only way to have any confidence in the result.â€¢ â€œConclusionâ€: as I hope Iâ€™ve shown, â€œThe DOMParser API is one tool you can use to get the job done right.â€ is not an acceptable position.Really, the article could be significantly improved by presenting it as what a common developer might think, and then scribbling all over the problematic things with these explanations of why theyâ€™re so bad, and ending with the conclusion â€œso: just use the DOMPurify library; consider nothing else acceptableâ€. (There have at times been a couple of other libraries of acceptable quality, but as far as Iâ€™m concerned, DOMPurify has long been the one that everyone should use. I note also that this article is talking about client-side filtration. Iâ€™m not familiar with the state of the art in server-side HTML sanitisation, where you probably donâ€™t have an actual DOM; this is also a reasonable place to wish to do filtering, but the remaining active mXSS vectors might pose a challenge. Iâ€™d want to research carefully before doing anything.)I look forward to the Sanitizer APIbeing completed and deployed, so that DOMPurify can become just a fallback library for older browsers. reply bobmaxup 1 hour agorootparent> Iâ€™m not sure quite why youâ€™re against removing script tagsMy bad, I left a fragment in the URL. reply sumoboy 1 hour agoprevSame developer who has offers a form validator, very solid and was worth the $ for past projects. Nice list of JS snippets. reply agumonkey 1 hour agoprevVery nice list, it&#x27;s nice to \"down to earth\" after too much react ui libs :). Thanks. reply jeffgreco 3 hours agoprevThis looks like some helpful refreshers as I try and mess with building a browser extension. reply throw555chip 2 hours agoprevAI models will no doubt gobble it up and spit out back out in a unique looking laundered format minus the comments. reply marban 3 hours agoprevAll good except for Create a custom scrollbar reply cmrdporcupine 3 hours agoprevNice. Bookmarked. Another good one in a similar vein is https:&#x2F;&#x2F;youmightnotneedjquery.com&#x2F;I don&#x27;t do web stuff often, but when I do I am completely demoralized by the state of framework-itis there. It&#x27;s gotten completely out of control. A fresh React project is hundreds of dependencies. The weight of that complexity is astounding to me.Meanwhile the core cross-platform browser tech shipped by default has never been more capable and the amount of code to do common things ... isn&#x27;t very much.Front-end devs: are you ok? reply mattlondon 2 hours agoparentI am no fan at all of react or npm, but some frameworks do provide some helpful things that pure DOM APIs do not, so they still have a place I think.So e.g. application state management and routing are two huge things you&#x27;ll still need to implement yourself if you just use pure DOM APIsDepending on the complexity of your app then that might not be such a big deal, but these extras are rapidly worth their weight in gold as soon as you go beyond a basic app. Unless you are using anything that needs NPM in which case you are fucked. reply mablopoule 2 hours agoparentprevAnother excellent resource in the same vein is the marvelous https:&#x2F;&#x2F;javascript.info&#x2F;As a front-end dev who actually like the fundamental platform and the capacities it offers, I&#x27;m both happy to see so much nice things coming our way (nested CSS, container queries, and a generally nicer JS experience since ES6) and at the same time horrified by the amount of people using components for trivial things like buttons or bold text.And I do like framework, but so much framework and library&#x27;s marketing depend on making people believe that CSS and vanilla DOM manipulation is insanely hard (it&#x27;s not once you know the fundamentals), and that pulling a random package on NPM is not just quick and easy, but also the professional thing to do. reply danielvaughn 2 hours agoparentprevAgreed itâ€™s out of control. But in the past year Iâ€™ve transitioned to a platform team so Iâ€™m working with devops-ish tooling for the first time. Holy shitâ€¦the amount of complexity there gives front-end a run for its money. Youâ€™ve got Kubernetes, Helm, Terraform, CircleCI, yadda yadda. The amount of layered terminology in just those technologies alone is absurd. reply throw555chip 2 hours agorootparentAs a mostly backend dev, maybe I&#x27;m biased but all the tech you listed there isn&#x27;t layers of stuff. Each of those do something different. For all but FAANG, kube is overkill. Helm is just templating. The rest existed before DevOps was a term. reply cmrdporcupine 2 hours agorootparentWas replying saying mostly this. I stay away from most of that kind of stuff and work in embedded-type space mostly, but when I have ventured in there, it all kind of makes a certain amount of sense as modules that layer relatively independent of each other. Because, well, most things interoperate via Rest or gRPC or whatever, without a lot of coupling.The problem with the JS&#x2F;TS&#x2F;Node ecosystem is the cultural tendency there to build and proselytize frameworks. Heavily coupling seems to be seen as a positive rather than negative. reply troupo 59 minutes agoparentprev> Meanwhile the core cross-platform browser tech shipped by default has never been more capable and the amount of code to do common things ... isn&#x27;t very much.Have you tried to use this \"capable cross-platform tech\" to do anything non-trivial? And React isn&#x27;t the only thing out there. reply est 2 hours agoprevIs there a CSS version of this? Like Zen-garden but modern reply wangii 1 hour agoprevso, after 10 years of reactjs, we&#x27;ve forgot how to do this in javascript? reply kossTKR 2 hours agoprevGreat resource! I&#x27;ve defaulted to just use vanilla js and petite-vue, it&#x27;s 15 kilobytes and gives a good base.The problem is the build requirements you run into quickly if you want to use plugins these days with vanilla js.Elegance was being able to load actual modules from a CDN. Why did we need these anti-web build steps on top of JS?I feel like there was a sweet spot of complexity around Vue 1 or say 2014.The Vue 2-->3 jump illustrates the welcome but total breakdown of frameworks in my opinion. It constantly gets in your way and moving data on a page with a hierarchy you need some bizarrely complex data flow, so many folders, files, tools, brittle typescript linting, \"auto\" this and that, constant building and so many rules it&#x27;s not fun to engineer anymore.Anyway, i think we are very close to a sweet spot if we just use something like Petite-vue &#x2F; Alpine on top of JS maybe with a tiny router and get creative with JS around that, or just early versions say Vue if we need anything more complex. reply selimnairb 1 hour agoprev [â€“] In Capitalist America, DOM manipulates you! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The web development environment continues to advance with tools like Angular, VueJS, React, Solid and Svelte aiding in managing the Document Object Model (DOM), yet developers still need to interact directly with DOM.",
      "Recognizing this, a collection of resources has been created to help developers master DOM manipulation using straightforward JavaScript. These resources include numerous strategies for tasks such as updating favicons based on user preference dynamically and calculating a webpage's reading time.",
      "The resources offer live demonstrations, advice, best practices, and support for a variety of modern browsers - including Internet Explorer."
    ],
    "commentSummary": [
      "The web development environment is continually evolving, with modern tools like Angular, VueJS, React, Solid, and Svelte aiming to assist in managing the Document Object Model (DOM). However, developers maintain a level of direct interaction with the DOM.",
      "A collection of resources has been developed to aid developers in mastering DOM manipulation using vanilla JavaScript, encompassing dynamic favicon updates based on user preferences, webpage reading time calculation, and HTML insertion at a contentEditable element's current position.",
      "These resources offer live demonstrations, tips, best practices, and support for various modern browsers, including Internet Explorer."
    ],
    "points": 277,
    "commentCount": 82,
    "retryCount": 0,
    "time": 1699278090
  },
  {
    "id": 38156030,
    "title": "Lupus Cerebritis: A Musician's Struggle with Loss and Recovery of Time Perception",
    "originLink": "https://www.salon.com/2023/11/05/a-brain-injury-removed-my-ability-to-perceive-time-heres-what-its-like-in-a-world-without-it/",
    "originBody": "Islumped in a wheelchair in my doctorâ€™s office. The clock above the door ticked erratically, as if someone outside the room was winding the gears forward and then turning them back every few seconds. The words Dr. W spoke seemed to fall from her mouth, then slowly float across the room one by one. To my ears, her speech was devoid of any cadence. Unable to hear the pauses that indicated the ends of her sentences, I kept interrupting her. A month before this doctorâ€™s appointment, lupus, the chronic autoimmune disease I had lived with for the past four years, had spiraled out of control. In rare cases like mine, lupus can cause severe brain inflammation called lupus cerebritis. Iâ€™d first realized I was seriously ill when I stood up after teaching a violin lesson and forgot how to walk. My legs didnâ€™t hurt â€” they simply refused to lift from the floor. Related That familiar unfamiliar feeling: What the opposite of dÃ©jÃ  vu can tell us about cognitive delusions Advertisement: Over the next few weeks, my brain quickly unraveled, despite the high doses of immunosuppressants and IV steroids my doctor prescribed. I lost sensation in my left arm. I forgot that my favorite color was red and even whether or not I liked yogurt. I no longer remembered telling ghost stories around a campfire with my family as a child or the day I left for college. My emotions vacillated from fury to giddiness to crushing depression on an hourly basis. I hallucinated fireworks onto my bedroom ceiling and stared as the air around me appeared to ripple like water. Due to problems with my short-term memory, I repeated myself over and over â€” that is, when I remembered enough of my vocabulary to actually speak. Unable to walk, communicate or think coherently, I lay in bed for months, wondering if my mind would ever be the same. But as a classically-trained string musician, one of the cognitive abilities I grieved the most was my ability to comprehend time. Due to problems with my short-term memory, I repeated myself over and over â€” that is, when I remembered enough of my vocabulary to actually speak. Advertisement: At the core of any orchestra or string quartet is synchrony: a diverse, often eclectic mix of individuals with the finely-tuned ability to play in time with each other, to move together, and even to breathe together. An orchestra of well-trained musicians can accelerate the tempo or slow it down, pause to let a beautiful chord ring throughout a concert hall, then restart exactly together. When each instrument in an orchestra plays precisely in time with each other, the result is a seemingly effortless command of time that can only be achieved through many years of rigorous study. I had fallen in love with the viola as an elementary school student. Over many hours of private lessons, orchestra rehearsals and practice, Iâ€™d built my career as a professional musician. That the many years Iâ€™d spent honing my skill as a musician could vanish in a month terrified me. Whether weâ€™re managing a demanding career, caring for children, or both, most of us have dreamt of not being bound to the metaphorical hourglass through which our day seems to slip. But what we actually want is more time, not the absence of time altogether. Being unaware of the passage of time felt like being trapped in a single chaotic moment that never ends. I had no way of knowing how long Iâ€™d been sick for, when my caretakers would bring me dinner, or how long my recovery might take. Without a sense of time, seconds stretched indefinitely into the future. When I asked my caretakers for food or coffee, they seemed to disappear for hours before they returned. Advertisement: Want more health and science stories in your inbox? Subscribe to Salon's weekly newsletter Lab Notes. In addition to my difficulty perceiving short spans of time, my comprehension of longer periods of time was also affected. I referred to every past event in my life, whether it was my doctorâ€™s appointment the day before or an audition Iâ€™d taken years ago, as having happened â€œyesterday.â€ I couldnâ€™t remember what date, month or even year it was. I forgot what times of day were appropriate to call friends and family on the phone, and I didnâ€™t understand what people meant when they said they were â€œbusy.â€ Bedridden and unable to comprehend time, my illness seemed to drag on for eternity with no end in sight. Even while bedridden, I tried to piece together the meaning behind my brainâ€™s dysfunction. Learning about neuroscience helped me come to terms with my disease. I learned that no one area of the brain is solely responsible for measuring time. Rather, â€œthe entire brain is critically dependent on the timing of neural transmissions throughout,â€ explains Dr. Alan Brown, former chair of psychology and associate dean of the College of Arts and Sciences at Southern Methodist University. â€œThe information that we receive from the outer world is sent through our neurons in waves or pulses, and this is how the brain processes everything (the smell of a rose, the color red, a rough touch). So, literally, we have several trillion small temporal processing units in our brain.â€ Advertisement: \"The entire brain is critically dependent on the timing of neural transmissions throughout.\" Unlike more concrete and specific brain functions with designated areas within the brain, like our sense of smell, sight or touch, the brainâ€™s perception of time is abstract. A combination of external input (like seeing the streetlights outside our house flick on), internal sensory input (like feeling tired), and memory (like groaning when we remember we have an early work meeting ) might lead our brains to determine that itâ€™s time to go to bed. Advertisement: â€œMost recent brain research has found that the brain is a lot less localized than we previously thought. That is, a specific piece of information (i.e., your grandmotherâ€™s face) does not reside in a precise location in the brain, but may involve tens of thousands of different small processing sites throughout the brainstem and cortex,â€ says Dr. Brown. A 2020 report in the Journal of Neuropsychiatry cites meta-analysis of MRIs and PET scans in determining the collaboration between different areas of the brain in processing time. â€œThese studies support the presence of a widespread network of cortical and subcortical areas that are variably recruited based on specific task parameters and demands,â€ the authors write. Neuroscientists have long known that the human brain is capable of measuring units of time under ten seconds more or less accurately due to a group of time-keeping cells in the hippocampus. But in a 2018 study published in the journal Nature, researchers at Norwegian University of Science and Technologyâ€™s Kavli Institute for Systems Neuroscience believe they made a breakthrough discovery in determining exactly how the brain measures longer periods of time. Advertisement: Related Unlocking the brain's spiral symphony: a new path to understanding brain activity â€œOur study reveals how the brain makes sense of time as an event (that) is experienced. The network does not explicitly encode time. What we measure is rather a subjective time derived from the ongoing flow of experience,â€ says the Kavli Instituteâ€™s Dr. Albert Tsao, the studyâ€™s lead author. â€œThe primary function of episodic time is to record the order of events within experience, which does not require a precise representation of metric time,â€ Dr. Tsaoâ€™s and his colleagues explain. A 2021 study in PubMed confirmed that the creation of episodic memories shapes our sense of time, and that the hippocampus â€œbinds features of an event to its context.â€ As we move through our day, our brains react to our environment in the form of thousands of tiny observations. The rumble of the coffee machine, the drop of milk that splashes from our cereal bowl onto the tablecloth and the crunch of cornflakes between our teeth are all observations that our brain makes about our environment without us consciously thinking about it. The observations our brains record occur in a continuous flow. This input from our environment then is encoded, or stored, in our memory. Certain events, particularly those that indicate changes in our environment, serve as boundaries between experiences. These boundaries help our brain organize encoded memories into segments, or episodes. For example, the sensory input above might be grouped into an episode labeled â€œbreakfast.â€ Groupings of memories that were formed in the same environment are referred to as episodic memories. Advertisement: â€œThe primary function of episodic time is to record the order of events within experience, which does not require a precise representation of metric time.â€ The accumulation of episodic memories form the neural clock at the crux of Dr. Tsao and his teamâ€™s discovery, and are responsible for helping humans gauge how much time has passed. The Journal of Neuropsychiatry study reached a similar conclusion: â€œThere is evidence that events that occurred in different episodes are perceived as happening farther apart in time, and events occurring within an episode are perceived as happening closer in time.â€ Advertisement: While no single area of the brain controls humansâ€™ abstract concept of time, particular networks of neurons play a role in our brainâ€™s perception of time by aiding in the formation of episodic memories. Two years after my brain first became inflamed, I stood next to three of my colleagues on a stage holding my viola. After bowing to the audience, we sat facing each other, our musical instruments ready. After a quick cue from the first violinist, we launched into motion, the synchronized voices of our instruments blending precisely together. Recovery from brain trauma is complicated and varies from patient to patient, Dr. Brown explained, adding, â€œMany variables could be involved, with the most important being how the damage occurred.â€ My own recovery had felt like scaling a mountain: exhausting and grueling, but worth it when I finally reached the summit and saw how far Iâ€™d come. For much of the first year I spent recovering, I was too mentally and physically exhausted to practice more than a few minutes. I returned to playing the violin before the viola. Because the violin is much lighter than the viola, my atrophied arm muscles were able to hold it. When I was well enough to begin seriously practicing the viola, I worked extensively with a metronome, a device that musicians use to keep a steady beat. Advertisement: We need your help to stay independent Subscribe today to support Salon's progressive journalism Interestingly, the Journal of Neuropsychiatry study confirms that using a metronome can help brain trauma patients recover their sense of timing. â€œThe therapeutic value of temporally based interventions (e.g., rhythmic cueing, slow rhythmic drumming) has been demonstrated for multiple neuropsychiatric conditions.â€ Six years after my recovery, my memory overall is not as sharp as it was before my illness. I use to-do lists to keep myself on track. I triple-check the rehearsal dates on emails I send my students to make sure I havenâ€™t listed the wrong day or month, although sometimes mistakes still slip through. I also sometimes struggle to remember how far back events in my past happened. Iâ€™ll catch myself wondering if I had the oil in my car changed three months ago or a year ago. But every time I take my viola out of its case, I feel grateful to be able to think like a musician again. Advertisement: While performing in the viola section of an orchestra recently, my mind drifted briefly to the complex method through which the brain comprehends time. Sensory input becomes tiny memories, which then become encoded into episodes that the brain uses to estimate how much time has passed. Then my mind returned to the musicians on stage moving in time: many voices blending together to create a moment, a phrase, then an entire symphony. Read more about neuroscience Your brain is powered by literal emotional energy. An expert explains how to find the right balance Why your brain is hungry for more play, according to a child development expert Dopamine is a brain chemical famously linked to mood and pleasure, but this is often oversimplified",
    "commentLink": "https://news.ycombinator.com/item?id=38156030",
    "commentBody": "A brain injury removed my ability to perceive timeHacker NewspastloginA brain injury removed my ability to perceive time (salon.com) 273 points by mgerdts 16 hours ago| hidepastfavorite154 comments js2 14 hours agoDiane Van Deren is an ultra runner who had part of her brain removed to treat her seizures. It also affected her ability to perceive time and to read maps.She originally started running to self-treat her seizures, but eventually running no longer staved them off.Oddly, not being able to perceive time may have benefitted her running:> Dr. Gerber credits her endurance in part to her brain limitations. He says runners who can better track time and map where they are can be distracted by the details. But Van Deren has a special facility for what he calls â€œflowâ€ that lets her transcend the anguish of running long. â€œItâ€™s a mental state,â€ Gerber says. â€œYou become enmeshed in what youâ€™re doing. Itâ€™s almost Zen. She can run for hours and not know how long sheâ€™s been going.â€https:&#x2F;&#x2F;www.runnersworld.com&#x2F;runners-stories&#x2F;a21763474&#x2F;fixin...I first heard about her on Radiolab:https:&#x2F;&#x2F;radiolab.org&#x2F;podcast&#x2F;122291-in-running reply avisser 2 hours agoparentTowards the end of her life my grandmother had a few small strokes. After one, I clearly remember her loosing her pathfinding ability. She became lost in her 4 room basement, unable to find the stairs.At her worst, she seemingly got lost in her 5&#x27;x8&#x27; bathroom. I remember her just continuing to turn in place and then dismiss each direction as being the wrong way to proceed. reply sizzle 1 hour agorootparentAfter seeing how hard it was to walk after my dadâ€™s stroke Iâ€™ll never buy a 2 story home when Iâ€™m entering old age. Itâ€™s just so dangerous and easy to fall and never recover. reply abrookewood 12 hours agoparentprevThere&#x27;s a book by Oliver Sacks that covers people affected by strange neurological disorders and it is fascinating [0]. I can&#x27;t recall any of them being viewed in a positive light, but it&#x27;s still worth a read.[0]https:&#x2F;&#x2F;www.goodreads.com&#x2F;book&#x2F;show&#x2F;63697.The_Man_Who_Mistoo... reply vjerancrnjak 5 hours agorootparentAnother book that is very illuminating is \"My Stroke of Insight\" by Jill Bolte Taylor. Reading the brain stroke description can give non-experiential understanding of various Zen koans, including the non-verbal one where Buddha simply holds up a flower and it becomes entirely understandable to Mahakasyapa.It also points to the natural tendency of human beings to attribute profoundness or spirituality to various forms of brain malfunction. The author does it in the later chapters of the book. reply electrondood 2 hours agorootparentLSD as well, and simply stopping the mind and experiencing the senses directly, without interpretation. reply ProllyInfamous 3 hours agorootparentprevAs long as we&#x27;re discussing Dr. Sacks, I just read his short story (part of same-titled book) \"An Anthropologist on Mars.\" The titles derive from Dr. Grandin Temple&#x27;s perception of being high-functioning autist in academia&#x2F;husbandry; she is best-known for re-designing slaughterhouse shutes, able to \"see\" as animals do.Her entire story left me, a spectrum-kiddo, grasping for what \"real\" even is.The thing that struck me as her \"most unhuman\" aspect [but with which I can relate, self-inflictedly]: Dr. Grandin doesn&#x27;t understand most human social customs, so from a young age vowed to never enter into a romantic relationship.She considers this entirely a waste of time, and lives to work.Highly recommend Dr. Sacks many books&#x2F;collections, may he RIP:[I&#x27;m just a fleshbag]: Man Who Mistook Wife for Hat[LGB &#x2F; growing up]: Uncle Tungsten[Parkinsonianisms]: Awakenings reply avisser 2 hours agorootparentHer name is Temple Grandin reply dendrite9 9 hours agorootparentprevThat book helped someone I know he was mostly face blind.Recently I was listening to a discussion where someone was trying to explain her synesthesia about time to a bunch of people who didn&#x27;t experience that. I found it really hard to wrap my head around, but there is a picture in this article of how one person visualizes time over the course of a year. The idea is just so far from what I think of as normal that it is difficult to really grasp.\"I thought everyone thought like I did, says Holly Branigan, also a scientist at Edinburgh University, and someone with time-space synaesthesia.\"I found out when I attended a talk in the department that Julia was giving. She said that some synaesthetes can see time. And I thought, &#x27;Oh my god, that means I&#x27;ve got synaesthesia&#x27;.\" http:&#x2F;&#x2F;news.bbc.co.uk&#x2F;2&#x2F;hi&#x2F;8248589.stm reply HenrikB 7 hours agorootparentThat type of synesthesia is referred to as Number Forms (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Number_form).I read a long time ago that it&#x27;s quite common, like 1 in 7 people got it. Some people don&#x27;t even know they have it. I always had it as far as I can recall, but it wasn&#x27;t until my mid 20s, when I stumbled upon a picture, that I reflected on it and the fact that not everyone sees number forms. reply vctrnk 1 hour agorootparentOh wow. Count me in on the I didn&#x27;t know I had that! camp.When younger I struggled horribly with ALL things math, and to this day still do. OTOH I&#x27;ve always had a knack for DIY involving measurements: lenghts, rythms, quantities, sizes, you name it. I just invoke my own \"dynamic mind ruler\" for the task at hand and usually get it right 1st try. Cooking something new? I intuitively know the proper amount of ingredients and spices. Doing work in a friend&#x27;s car? That nut looks like a 3&#x2F;4 and that one a 11&#x2F;16, and who the heck put a 11mm in place of a 7&#x2F;16??Incidentally, the whole concept of Time always flows from right-to-left to me. 1000BC is waaay to the right, and 2030AC is just a stone throw away to the left. Now I wonder if it&#x27;s something only I perceive that way, or everyone does. reply tveita 7 hours agorootparentprevSomewhat related, a survey of how Norwegian people visualize time:https:&#x2F;&#x2F;nrkbeta.no&#x2F;2018&#x2F;01&#x2F;01&#x2F;this-is-what-the-year-actually... reply dendrite9 3 minutes agorootparentOh wow! Thanks for that, I lost a bunch of time with my coffee this morning thinking about time. I feel like I&#x27;ve lost my sense of time and it is all smeared together. It always has been to some extent but the last couple years have been stressful with business health and family health issues.This line made me laugh \"A straight fucking line. Because life moves ahead Right handed man, 20-29, Eastern Norway\" and think about the reporting on the Aymara who describe the past as infront of them and the future behind. That was hard to grasp but then it made sense because one is potentially visible and the other is unknowable until one passes through it. https:&#x2F;&#x2F;www.theguardian.com&#x2F;science&#x2F;2005&#x2F;feb&#x2F;24&#x2F;4 reply Aeolun 6 hours agorootparentprevThis almost caused me an existential time crisis until I drew out my own year and realized itâ€™s still as I remember. It just gets a bit smeared out in my head. reply causi 5 hours agorootparentprevOne of the more terrifying books ever written, if nothing else for the fact it opens one to the possibility we may all have such disorders on a species-wide level. What if there are things none of us allow ourselves to perceive? reply coldtea 5 hours agorootparentHow would that be terrifying?After learning it, it would still be the same exact world as was before we learned about it. reply mistermann 4 hours agorootparentThat would require \"learning it\" to have zero persistence in any form would it not? How might one know such a thing? reply coldtea 3 hours agorootparentNot sure what you&#x27;re asking. What is \"that\" which would require \"&#x27;learning it&#x27; to have zero persistence in any form&#x27;\"?What I meant is, even if we all learn that everybody in the world has \"such disorders\" and that \"there are things none of us allow ourselves to perceive\", what would the objective change be to make this \"teriffying\"?\"There&#x27;s a killer out to get you\" or \"you have X disease which will have those symptoms\" is terrifying.\"There are things that you don&#x27;t know, and that you as a species can&#x27;t even perceive\" is at best amusing.This sudden knowledge doesn&#x27;t change anything that&#x27;s happening or makes anything that happened before more frightening, does it? reply mistermann 4 hours agorootparentprevhttps:&#x2F;&#x2F;slatestarcodex.com&#x2F;2014&#x2F;03&#x2F;17&#x2F;what-universal-human-e...> What if there are things none of us allow ourselves to perceive?What if? There are plenty of topics that people cannot discuss in detail (they&#x27;re typically referred to as \"culture war\" topics) and HN enforces such things (to the degree that it does, to be accurate) like most social media platforms...doing otherwise would be ~\"inappropriate\", etc...culture + the nature of evolved and culturalized consciousness sees to that. reply passion__desire 13 hours agoparentprevMy genius friend in college could naturally focus so intensely that he didn&#x27;t realise that there was electric short circuit near his desk. reply picadores 2 hours agoparentprevI distinctly remember a HN post, were the father of the poster had the same problem. Meaning he would sometimes \"crash\" and just stand there, forgetting about the world everything - often for hours if nobody \"interrupted\". reply mewpmewp2 8 hours agoparentprevFlow happens to at least a lot of runners to be fair though. At certain distance when I&#x27;m running, it becomes very pleasurable, like you are above everything. reply js2 2 hours agorootparentI&#x27;ve run over two dozen marathons, twice run 100 miles, and also run timed races of 24 hours and 12 hours (multiple times). I&#x27;m a decent runner, having run the Boston Marathon 5 times.I&#x27;ve never really experienced anything like what they&#x27;re describing with Diane Van Deren. I&#x27;m not sure I&#x27;ve ever really experienced what folks refer to as the runner&#x27;s high either. At best, I can say that there are times that my running is as effortless as walking, even when running at a fairly decent clip. During those effortless times, I&#x27;ve gotten lost in thought or in conversation with another runner, but no differently than I&#x27;d get lost in thought or conversation during other activities. I&#x27;ve never experienced any sense of euphoria when running. Joy and tears and emotional overload at the end of Boston sure, but not what I&#x27;d call euphoria.That runner&#x27;s high sounds great though, for those who do experience it. :-) reply GlumWoodpecker 7 hours agorootparentprevAlso known as \"runner&#x27;s high\":https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neurobiological_effects_of_phy... reply ekanes 3 hours agoparentprevWow. Interesting connection between \"trouble perceiving time\" (negative) and \"flow state\" (usually positive).People with ADHD also have those two traits, perhaps giving further evidence that they come as a package.Across the broader population, I wonder if one could increase flow state access by removing reminders of time. Casinos do this, I think. reply elAhmo 7 hours agoparentprevThis was such a great listen, I came here to comment this and was glad that it was already here.It is amazing to realise that many of the physical limitations we have are coming from the brain. reply dclowd9901 12 hours agoparentprevI believe this. I never run so well as I do when I just look down about 10 feet in front of me and let my legs move as much on their own as possible. reply grogenaut 13 hours agoprevAfter my mom&#x27;s stroke she basically lost the concept of time. Apparently it&#x27;s relatively common. Kids don&#x27;t get the concept of time until an age, and you can lose it. Amazing how much you need it how things fall apart in this modern life without it. Didn&#x27;t help that she decided to use a 2017 calendar in 2018, I questioned my sanity as I unraveled the mess.It very much impacted her life. About the only good thing was that she&#x27;d just eat at any time we wanted. \"I could eat\" became the answer always, not that it wasn&#x27;t before. Woman loved a good scallop and was quick with a fork if you tried to take hers. reply jen729w 10 hours agoparentHa! What a lovely way with words you have. That made me laugh.Iâ€™ve recently â€˜gone soloâ€™ and I decided to experiment with caring a little less about time. The only starkly visible clock in the house is on the oven, so I reset it. But it still tells a time, and I knew Iâ€™d eventually figure out the delta, so I covered it with tape.Itâ€™s funny how lunch is supposed to be at ~midday, but when you donâ€™t know when midday is, does it really matter when you eat lunch? Itâ€™s a really, really hard habit to kick. Gradually I get more to the stage where I eat â€œwhen I could eatâ€, but still. Lunch is at lunchtime.This experiment did make me realise just how much I looked at that clock. Check the time, check the time. To what end? I canâ€™t tell you. That habit does appear to be on the wane. reply lloeki 7 hours agorootparentI&#x27;ve always had a peculiar, extremely elastic relationship with time, which I do perceive, only it&#x27;s wildly all over the place, day in day out; a \"moment\" could fold hours into an eyeblink, or it could make split seconds last an eternity and a half. A tidy calendar, reminders for basically everything, and setting up \"mental tripwires\"â€  is the only way I found to be any sort of reliable.Finding balance between the constant dread of missing something I have to do that turns me into a slave to the clock and being able to live a relaxed life is an eternal battle of mine.â€  Some kind of autosuggestion trickery, where I lay a callback upon some unrelated physical thing I know will happen in order to trigger another thought. e.g \"when I cross the kitchen door I&#x27;ll think about texting foo about bar\"> Check the time, check the time. To what end? I canâ€™t tell you. It was Grandfatherâ€™s watch and when Father gave it to me he said I give you the mausoleum of all hope and desire; itâ€™s rather excruciatingly apt that you will use it to gain the reducto absurdum of all human experience which can fit your individual needs no better than it fitted his or his fatherâ€™s. I give it to you not that you may remember time, but that you might forget it now and then for a moment and not spend all your breath trying to conquer it. Because no battle is ever won he said. They are not even fought. The field only reveals to man his own folly and despair, and victory is an illusion of philosophers and fools. I donâ€™t suppose anybody ever deliberately listens to a watch or a clock. You donâ€™t have to. You can be oblivious to the sound for a long while, then in a second of ticking it can create in the mind unbroken the long diminishing parade of time you didnâ€™t hear. It&#x27;s always the idle habits you acquire which you will regret. - William Faulkner, The Sound and the Fury> Lunch is at lunchtime. â€œDrink up,â€ said Ford, â€œyouâ€™ve got three pints to get through.â€ â€œThree pints?â€ said Arthur. â€œAt lunchtime?â€ The man next to ford grinned and nodded happily. Ford ignored him. He said, â€œTime is an illusion. Lunchtime doubly so.â€ - Douglas Adams, The Hitchhiker&#x27;s Guide to the Galaxy reply coldtea 5 hours agorootparent>I&#x27;ve always had a peculiar, extremely elastic relationship with time, which I do perceive, only it&#x27;s wildly all over the place, day in day out; a \"moment\" could fold hours into an eyeblink, or it could make split seconds last an eternity and a half. A tidy calendar, reminders for basically everything, and setting up \"mental tripwires\"â€  is the only way I found to be any sort of reliable.All quite common in ADD&#x2F;ADHD. reply vik0 4 hours agorootparentSelf diagnoses (and diagnoses over the internet) are pretty harmful in a society where many people think they have every mental&#x2F;physical disorder just because they have a few (of the many) symptoms associated with one (or many) disordersOne might even argue that the labeling aspect of a certain disorder (particularly a mental one) by a \"professional\" to not be particularly helpful too in addressing ones problems reply coldtea 4 hours agorootparent>Self diagnoses (and diagnoses over the internet) are pretty harmfulSelf-diagnoses can be legitimate or not - depends on the person doing them. They are often a necessity, in an environment where a professional diagnosis takes thousands of dollars or years in waiting (and is often done badly, by ill-informed professionals, like the many-decades prevailing myth that women&#x2F;girls \"can&#x27;t be autistic\", or that \"ADD and autism can&#x27;t coincide\").As (in this case) they are also based not on bloodwork or some physical indicators, but on a subjective assessment of a person&#x27;s way of thinking, the person having the actual experience is often more qualified than the professional. Same to how you don&#x27;t really need a doctor to tell you you&#x27;re gay.>One might even argue that the labeling aspect of a certain disorder (particularly a mental one) by a \"professional\" to not be particularly helpful too in addressing ones problemsOne might argue that the false dichotomy between professionals and laymen, where the former is supposed to hold all the keys to knowledge and the latter to passively consult and follow the advice of the former, is a problem in itself.And a little outdated in modern societies where the \"laymen\" are not some mud dwelling peasants who never went to school and only know farm work, but univercity-educated (even over-educated) in their own right, and libraries are not confined to the rich or the scholars, but every book ever written is a click away.In any case, a self-diagnosis doesn&#x27;t give you the required paperwork to get drugs, or to get benefits, or specific accomondations, or anything like that. So it&#x27;s not like it hurts society by taking resources from \"legitimate\" diagnoses.Last, but not least, pointing that X symptoms is \"quite common to ADD&#x2F;ADHD\" is not self-diagnosis, it&#x27;s not even diagnosing. It&#x27;s a suggestion hinting to a possible condition. It could very well be used for seeking a professional diagnosis.Or do you think people with ADD&#x2F;ADHD just go to the doctor to get diagnosed out of the blue, and not because of some similar suspicion, spotting some unexplained symptoms or themselves, or identification with some symptoms they&#x27;ve read about? reply Kye 3 hours agorootparentprevIf you compare notes and find mitigations&#x2F;strategies used by people with diagnoses helps, then it hardly matters for all but one practical purpose. If you need a diagnosis for disability support, that&#x27;s another matter. It&#x27;s sometimes more trouble than benefit from what I&#x27;ve heard. reply vasco 9 hours agorootparentprevWhen I do this my body adapts to only being hungry after I&#x27;m done with work, making me go about 9 hours without food (plus the sleep time before). I found that without clocks I will just be hungry for a lot of food after work and then I&#x27;m sorted, making it kind of a natural intermittent fasting. reply adiM 9 hours agorootparentprevIt will be fascinating if there were a way for you to record the actual time when you do eat lunch (without the time being revealed to you; say a Rasberry Pi where you press a switch to record current time but it has no display). I am curious how much our body clock synchronizes with real time. I see that in my kids, who don&#x27;t yet know how to read time. But they are hungry right at noon and 7pm, and will get cranky if they don&#x27;t get something to eat within 15 min of that. Do adults retain such strong internal body clocks? reply m-i-l 7 hours agorootparent> \"I am curious how much our body clock synchronizes with real time\"Various experiments have been performed with people in caves, where they don&#x27;t have \"time cues\" such as natural daylight. From the Michel Siffre wikipedia entry[0]: \"He found that without time cues, several people including himself adjusted to a 48-hour rather than a 24-hour cycle ... Several astronauts reported experiences similar to those experienced in underground experiments such as loss of short-term memory to being isolated from external time references.\" And for Stefania Follini[1]: \"her biological clock drifted away from its regular rhythm to following first a 28-hour day, and later on a 48-hour one ... When she finally emerged from the cave at the experiment&#x27;s end ... she estimated that it was ... only two months from the start of the experiment instead of the four that had actually transpired\".[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Michel_Siffre[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Stefania_Follini reply ryanSrich 13 hours agoparentprevHow does that work in practice? She can read a clock right? So would she just not remember to look at the clock to see the time? Genuinely curious. reply haswell 13 hours agorootparentMy brother is dealing with a TBI, and in a nutshell, itâ€™s exactly that.The reason most of us glance at the clock is because we feel the passage of time and wonder what time it must be now.He doesnâ€™t always feel time passing, and he could go hours before it even occurs to him to look at his phone or a clock.He relies on timers and reminders to keep things on track. reply elcritch 10 hours agorootparentThat sounds very familiar. As someone with ADHD when I get hyperfocused I can completely loose track of time. Itâ€™s a large part of why many ADHDers struggle with being on time to things.However it can be great for getting into the flow programming. reply Doxin 10 hours agorootparent> Itâ€™s a large part of why many ADHDers struggle with being on time to things.The group of ADHDers that can be on time on the other hand will often accomplish this by spending the time up to an appointment doing nothing but worrying about being on time. reply Gigachad 13 hours agorootparentprevI think there is a lot of promising tech to help people out with this stuff. Just having your phone prompt you when you need to do stuff. I saw Google maps (perhaps recently) added a feature where you tell it the time you need to be somewhere and it will notify you when it&#x27;s time to leave. reply haswell 12 hours agorootparentHe uses his iPhone extensively to do things like this. I recently learned that the native Calendar app can do the same thing for scheduled appointments that have a location specified. Reminders can be set for things like: (can set multiple) \"30 minutes before travel time\" or \"Time to leave\", etc.One thing he&#x27;s struggled with is establishing the habits to set up these kinds of tools. There are often other issues associated with TBI and memory&#x2F;new habit formation have been a challenge. Needs reminders for the reminders.He&#x27;s determined to find the right combination of tools&#x2F;processes so he can create a guide to help other people use their current tools to manage this aspect of their lives. The potential is definitely there. reply jaggederest 12 hours agorootparentprev> I saw Google maps (perhaps recently) added a feature where you tell it the time you need to be somewhere and it will notify you when it&#x27;s time to leave.Has been doing this for me since 2009, a lifesaver in unexpected traffic or construction. reply JCharante 8 hours agorootparentprevNow I feel bad. I had an ex who would always lose track of time and I wasnâ€™t understanding enough (being late by several hours and forgetting important things like her first chance at a covid shot, or getting lost while driving and having spent an hour driving around the neighborhood without realizing how long sheâ€™d been lost for).Timers make a lot of sense, if you can remember to set them. reply haswell 2 hours agorootparentIâ€™ve gained a lot of empathy for folks dealing with this as Iâ€™ve seen my brotherâ€™s struggles up close. I definitely didnâ€™t understand the dynamics before.One of the things weâ€™ve talked about is how otherwise â€œnormalâ€ he seems to most people, and the problems this causes. The symptoms arenâ€™t as obvious as wearing a cast on your broken leg, or having severe speech issues, etc. and as a result, people are more likely to get frustrated or not understand when the symptoms spill over and affect other people (being late, forgetting things, etc). reply samus 11 hours agorootparentprevI wonder whether my work colleague has that issue. Every 20mins there are timers going off on his phone. On the other hand, he is a smoker, and I imagine that the nicotine dependency ought to act as sort of a replacement for that sense. Not a terribly precise one though. reply henearkr 10 hours agorootparentNo I think it&#x27;s more likely that he is using the Pomodoro method. reply samus 4 hours agorootparentGiven how he often disrespects other people&#x27;s efforts to stay focused, I doubt it. It might be a crude approximation of it though. And I don&#x27;t 100% remember what they interval exactly is.Be as that may, I find the Pomodoro method to be a wonderful tool to structure one&#x27;s time! reply huytersd 12 hours agorootparentprevSeems like forgetting a learned behavior. reply slavik81 11 hours agorootparentprevYou judge the passage of time by remembering the things that have happened since the last time that you checked the time. If you can&#x27;t remember what you&#x27;ve been doing, then it could have been minutes or it could have been hours since you&#x27;ve last checked. reply otikik 11 hours agorootparentprevWhat do you think makes you remember â€œoh, now I have to look at the clock? reply ryanSrich 11 hours agorootparentThe fact that I wear one on my wrist and glance at it by chance without having to remember. The fact that thereâ€™s a clock on every appliance in my house and I look at those without intention to do so. Maybe all of that is subconscious action based on my perception of time. Iâ€™m not sure. reply 512 13 hours agoprevI feel I have a slightly \"time-warped\" memory. I often can&#x27;t tell whether something was two months or over a year ago, for instance.I believe this to be caused by depression, as I&#x27;ve seen many similar reports online. Life assumes a sort of \"flow\" state, as you don&#x27;t care enough to pay attention to things. It affects my memory in general as well. reply devjab 10 hours agoparentWith ADHD this is pretty much a thing. If you also get the â€œfunziesâ€ where you sometimes â€œrelieveâ€ a moment in a way that is so real that you end up speaking out loud in response to the relived memory, you should probably consider getting it looked into.Especially if your depression isnâ€™t a â€œrealâ€ depression. What I mean by that isnâ€™t to minimise itâ€™s impact or how it feels, but I was once in a depression group where I just didnâ€™t fit in. I was in it because I had been suicidal, so, well I obviously wasnâ€™t doing ok, but unlike the other members of the group I could get up in the morning just fine. Long story short, I was diagnosed with ADHD to which depression and anxiety are often â€œfriendsâ€ when it isnâ€™t treated. And by treated I donâ€™t only mean medicated.Anyway, if this isnâ€™t you, then Iâ€™m sorry for derailing you a bit. reply corobo 10 hours agorootparent> I was once in a depression group where I just didnâ€™t fit inOne of my weirdest thoughts to date has to be \"I think this group is for normal suicidal people\" hahaDoing much better now! Similar to you my depression was more of an ADHD symptom than \"real\" depression. Real in terms of end result but treatment for ADHD worked much better than antidepressants.It&#x27;s kinda hard to not be depressed when you&#x27;re not getting any dopamine from doing things I guess reply egeozcan 8 hours agorootparentprev> With ADHD this is pretty much a thing. If you also get the â€œfunziesâ€ where you sometimes â€œrelieveâ€ a moment in a way that is so real that you end up speaking out loud in response to the relived memory, you should probably consider getting it looked into.Is this really a sign of ADHD? I always thought I was just a bit out there. Honestly, I&#x27;d be relieved to attribute it to something relatively common like ADHD rather than some mysterious brain quirk. Just the other day, I caught myself having a full-on conversation with my grandpa, who&#x27;s been gone for over two years now. And it was crystal clear, all the details and everything! reply coldtea 5 hours agorootparentRehearsing and replaying conversations is common with ADHD, as is the \"relieving a moment in a way that is so real that you end up speaking out loud in response to the relived memory\" part. reply lakpan 11 hours agoparentprevIs it possible that you&#x27;ve fallen into a habitual life? I have almost a black hole in my memory from 3-year period that I spent mostly at home. After that period, I can look at a photo in my gallery and know almost the exact date when it happened. Or vice-versa, knowing where I was on a specific period 7 years ago. reply swells34 13 hours agoparentprevI have something very similar. When I get very focused on something, you can ask me what I had for breakfast, and I&#x27;ll vividly remember and recount making an omelette; but that was three weeks ago. This goes for pretty much anything, even very unique events. I can remember it well, but I can only place it within about an 8 month period, even if it was just a week or two ago. I&#x27;m quite nervous about ever having to be in a court proceeding, giving a deposition or anything like that where specific dates are required; it just escapes me how people remember this stuff. reply reitanqild 14 hours agoprevADHD is sometimes wittily described as time dyslexia, and while I haven&#x27;t met anyone who has described symptoms as bad as this and also know friends with ADHD who play music seemingly effortlessly, this article was still very interesting to me.I have understood and it also observed that meditation can help somewhat, and I wonder what else can help for people otherwise function well who struggle with time? reply LoganDark 12 hours agoparent> ADHD is sometimes wittily described as time dyslexiaAs someone with ADHD, this wildly misrepresents the actual disorder. ADHD itself has nothing to do with time, it has to do with executive function. Someone with ADHD can try to will themselves into doing something and simply not be able to. They&#x27;ll try to make a command, and their body or brain will not listen to them. It will completely refuse to do what they tell it to.This isn&#x27;t supposed to happen; you&#x27;re supposed to have control over what you do. You are supposed to be able to decide to do something, get up and simply do it. But with ADHD, it&#x27;s not that simple. Even things that require no physical action are difficult, because it&#x27;s not the actual movement that&#x27;s hard, it&#x27;s the decision-making itself. Hence \"executive dysfunction\".Any \"time dyslexia\" effect, wrt scheduling and deadlines and etcetara, is just a symptom of it. The reason why people with ADHD procrastinate is not:- because they don&#x27;t know what time it is.- because they don&#x27;t know when their deadline is.- because they don&#x27;t know how much time they have.- because they don&#x27;t know how much time they need.- because they don&#x27;t know how easy or hard the task is.It is because their brain wants to do something else more, and it&#x27;s not urgent yet.They absolutely cannot work on the task no matter how hard they try. They have not forgotten. They are not slacking off. They literally just can&#x27;t do it. Their brain refuses to think about it, their body refuses to move for it. They don&#x27;t have the willpower or the motivation for it. They are trapped. They are completely unable to make any progress because their brain will not let them.That&#x27;s what ADHD is.Not everyone has it this bad, but ADHD is typically characterized by this happening for at least some things. It could be \"showering more than once a week\", it could be \"doing the dishes before 20 of them have piled up in a big stack\", it could be \"preparing for a road trip days in advance\". It doesn&#x27;t have to be everything, and it doesn&#x27;t have to be completely insurmountable, but if you have to have a complicated coping mechanism in order to manage to do something that you otherwise can&#x27;t just decide to do, that&#x27;s the disorder. reply reitanqild 12 hours agorootparentI realize my comment above can be read as \"time dyslexia is the whole problem with ADHD\" and that was not my intention. Thanks for your perspective!I also totally agree that ADHD-ers know what time is.That said, ADHD is a thing I have to deal with and for me and a number of those I know who have ADHD, \"time dyslexia\" is a very good explanation for a subset of the problems we observe.> They absolutely cannot work on the task no matter how hard they try. They have not forgotten. They are not slacking off. They literally just can&#x27;t do it. Their brain refuses to think about it, their body refuses to move for it. They don&#x27;t have the willpower or the motivation for it. They are trapped. They are completely unable to make any progress because their brain will not let them.Here it is you who are taking agency away from ADHD-ers.Many can, it just takes a lot more effort than for other students&#x2F;workers.Things I have seen working:- restricting oneself heavily so that the work at hand becomes the only possible thing to do- conjuring up reasons why something is intersting- pair programming- various ways of sneaking up to the subject (start by fixing a few small issues, them improve a unit test, then make a small prototype, then take a look at the actual problem in question)- gamification- etc reply LoganDark 12 hours agorootparent> Here it is you who are taking agency away from ADHD-ers.What do you mean? All the mechanisms you&#x27;ve listed can be ways to help, yes, but you still can&#x27;t just decide to do those things. You can try to reformat it or place it in some other context where it becomes doable, but these are specific coping mechanisms that shouldn&#x27;t always be necessary, like they are for someone with severe enough ADHD.I apologize for implying that all ADHD is that severe, I&#x27;ll see if I can edit it to be more clear there, but I&#x27;m not trying to take agency away; I&#x27;m trying to point out how it results in that \"time dyslexia\", and why it&#x27;s a disorder (rather than just, say, laziness). reply reitanqild 12 hours agorootparent> You can try to reformat it or place it in some other context where it becomes doable, but these are specific coping mechanisms that shouldn&#x27;t always be necessary, like they are for someone with severe enough ADHDOK, I see, I think we agree very much here. reply coldtea 5 hours agorootparentprev>As someone with ADHD, this wildly misrepresents the actual disorder. ADHD itself has nothing to do with time, it has to do with executive function. Someone with ADHD can try to will themselves into doing something and simply not be able to.That&#x27;s misleading. ADHD does involve executive function issues, as it does other issues, including sensory issues like noise and light sensitivity (that have nothing to do with executive function), issues with body balance&#x2F;proprioception (also nothing to do with executive function), issues like rejection sensitivity, as well as time issues (\"time blindness\"). reply otikik 11 hours agorootparentprevADHD presents a variety of symptoms, not the same ones for everyone. The â€œHâ€ (â€œhyperactiveâ€) part in particular often doesnâ€™t manifest (and thatâ€™s how many children become undiagnosed adults- hyperactivity is easier to spot). If you donâ€™t have time dislexia, that doesnâ€™t mean that others donâ€™t. reply LoganDark 11 hours agorootparent> If you donâ€™t have time dislexia, that doesnâ€™t mean that others donâ€™t.In this case it&#x27;s important to define what \"time dyslexia\" even is. I was talking about how the phrase didn&#x27;t accurately describe ADHD symptoms, but you seem to be talking about a third thing that does have a concrete definition. What definition is that? reply throwaway47bs6 12 hours agorootparentprevAhem. That&#x27;s what yours is. Mine is straight-up time defiance. I stopped caring about clocks and deadlines. You speak for yourself: attentional difference is a cluster, not a fixed set of symptoms and behaviours.Another example, I&#x27;m not short of motivation. It&#x27;s just there are so many interesting things to work on. reply LoganDark 12 hours agorootparent> That&#x27;s what yours is.No, it&#x27;s not just what mine is. It agrees with the clinical definition of ADHD, with the experiences of all who I have spoken to about it, including many friends, with multiple[0] articles[1] describing[2] the disorder[3], and so on.Your time defiance is a symptom of a deeper problem. While I can&#x27;t guarantee it&#x27;s the exact same problem that I have, your disorder is certainly not just \"time defiance\", and if it truly is ADHD, the root of it will be executive dysfunction, as that is what defines the disorder.[0]: https:&#x2F;&#x2F;gekk.info&#x2F;articles&#x2F;adhd.html[1]: https:&#x2F;&#x2F;cohost.org&#x2F;cathoderaydude&#x2F;post&#x2F;862603-when-writing-a...[2]: https:&#x2F;&#x2F;invisibleup.com&#x2F;articles&#x2F;27&#x2F;[3]: https:&#x2F;&#x2F;www.autostraddle.com&#x2F;you-need-help-your-adhd-is-fcki...> For example, I&#x27;m not short of motivation. It&#x27;s just there are so many interesting things to work on.I could say the same thing. But I can&#x27;t choose which thing I work on. I want to draw. I want to code. I have hundreds of unfinished projects I could have finished. I have hundreds of abandoned hobbies I could have stayed with. Sure I have \"motivation\", but it&#x27;s motivation to do whatever catches my interest at the moment, not motivation to do any of the things that I actually want to do. reply devbent 12 hours agorootparentprevRandom question,Do all of you have ADHD? If not, that would be... Surprisingly I guess but also kind of hopeful. reply LoganDark 12 hours agorootparentDo you mean all of my dissociative identities? The answer is yes. DID can result in individual identities having different access to certain parts of the brain (for example, certain identities could have aphantasia while others have vivid imaginations), but as far as I&#x27;m aware, ADHD is a problem with the brain&#x27;s reward system itself (the one that has to do with dopamine), so it affects the entire brain function, no matter which part.However, I&#x27;ve been in situations where different identities can do things that I can&#x27;t (i.e. switch in order to get out of bed in the morning), so I don&#x27;t think anything is necessarily set in stone... reply reitanqild 11 hours agorootparentHi again, and thanks for your website about plurality.I am not affected but I am aware of at least one person who seemed to be affected and it was really really interesting to read. reply LoganDark 11 hours agorootparent> thanks for your website about pluralityNot mine :) reply Aerbil313 6 hours agorootparentprevNot to disrespect anyone, but regardless of all the scientific research, the more I hear anecdotes about ADHD, the more it seems like just poor general mental health which would be solved if the person in question didnâ€™t mouthbreathe and eat sugar, and slept well and exercised regularly. Not saying these are easy things to do in this modern world, though.Edit: I recommend everyone read Breath by James Nestor. May be helpful for ADHD, and will be helpful for everything else. It changed my life a lot, and I was not even really unhealthy. From the book:> There are several books that describe the horrendous health effects of snoring and sleep apnea. They explain how these afflictions lead to bed-wetting, attention deficit hyperactivity disorder (ADHD), diabetes, high blood pressure, cancer, and so on. Iâ€™d read a report from the Mayo Clinic which found that chronic insomnia, long assumed to be a psychological problem, is often a breathing problem. reply Karsteski 6 hours agorootparentYou say that, but as another anecdote, I&#x27;m fit and lift weights, and I don&#x27;t eat junk food. I wish my sleep were better but that&#x27;s not for lack of trying.LoganDark&#x27;s comment about some people with ADHD being absolutely unable to will themselves to do certain things describes me so well it&#x27;s scary. It&#x27;s something I&#x27;ve struggled with for years, and has affected me at work lately.I&#x27;ve always known I&#x27;m not lazy or stupid, and in the last few years I discovered programming and taught myself software development. I fixate on things strongly yet I struggle to do mental work if it just so happens that I find it uninteresting or pointless. It&#x27;s fucking me up. My therapist even pointed out a few weeks ago that it&#x27;s a strong sign of ADHD, but I hate the thought of having that... I guess now I really should seriously explore this. I only know that I started feeling like my dev job is pointless even though it&#x27;s a great job, and my coworkers are fantastic. It&#x27;s more deep than just pointless but now I can barely will myself do anything related to work...The brain really is fucked ha reply JackMorgan 5 hours agorootparentprevThis comment is extremely rude.\"Have you just tried not having ADHD?\" is what I&#x27;ve been told thousands of times. Yeah man, I&#x27;ve tried. I am extremely fit, exercise all the time, have a great Vo2 max, lift weights, sleep 9 hours a night, and don&#x27;t snore. I&#x27;m vegetarian and eat very little sugar (I&#x27;ve even done keto for five years with no change in symptoms).If I was neurotypical, sleep and exercise would help a lot more. But I&#x27;m not. I instead have to use dozens of alarms on my phone, pair programming, hyperfocus training, and deadline panic to get through the work week. Home stuff remains in shambles, still haven&#x27;t figured out how to do the dishes or put clothes away.And I&#x27;ve got a pretty mild case.Some of my friends are much much worse off. reply LoganDark 5 hours agorootparentprev> poor general mental health which would be solved if the person in question didnâ€™t mouthbreathe and eat sugar, and slept well and exercised regularly.I cannot do those things.I&#x27;ve been to physical therapy multiple times for things that are caused by a lack of physical activity. Their advice is the same every time: it would take less than 15 seconds every day to do a single stretch. Here are diagrams showing you how to do them. Look how easy it is.But even that is too much. I don&#x27;t want it to be too much, I know it&#x27;s dead simple and any normal person would be able to do it without a second thought, but that is the problem. I&#x27;m not a normal person, and my brain doesn&#x27;t work like a normal person&#x27;s does. It does not let me do that.It does not let me know when I should do that. If I set a timer&#x2F;alarm, then when it goes off, I can&#x27;t even listen to it. Either I just can&#x27;t get up, or I want to finish what I&#x27;m doing first and then I completely forget.This isn&#x27;t something that I control. It&#x27;s not something that I chose. And it&#x27;s not something that I can fix by simply being less pathetic. Because as much as I&#x27;m aware of these behaviors and now ridiculous they are, I cannot help them. There is no way for me to just make myself do something.My brain just only lets me do what it wants. reply coremoff 5 hours agorootparentprevYou fundamentally misunderstand ADHD and your word choices are very antagonistic. reply perilunar 6 hours agoparentprev> time dyslexiaAKA \"time blindness\", though I think \"dyschronia\" would be a better term, to match with dyslexia, dyscalculia, dyspraxia, and dysgraphia.(I think I also have it, though undiagnosed. Three of my last four web projects have been a clock, another clock, and a timeline. I still have a terrible problem with procrastination.) reply corobo 10 hours agoparentprevHaving my watch nudge me on the hour has improved my perception of time immensely reply reitanqild 8 hours agorootparentThanks! Will try something along the lines.When I am in flow state however something more drastic might be needed : ) reply corobo 7 hours agorootparentHah, I also make heavy usage of Todoist and notifications.Got an old tablet on a cheapy plastic arm held above my monitor dedicated to my task list when working. It&#x27;s set up to get my attention as and when tasks need starting.Bit of a balancing act between having it make enough noise&#x2F;vibrate to get my attention and not vibrating itself off its perch and making me jump out of my skin :) reply et1337 12 hours agoprevI think it&#x27;s interesting that HN in particular seems to be fascinated by brain injuries, myself included. Losing brain functionality is a scary thought for people who enjoy using their brains all day.edit: also I find my HN comment history to be a really good check on my perception of time. It&#x27;s weird to see the random thoughts I wrote out in my down time. I ALWAYS think, \"wow that was 3 months ago?\" reply lm28469 7 hours agoparent> Losing brain functionality is a scary thought for people who enjoy using their brains all day.I&#x27;m fairly sure people outside of HN enjoy using their brains too, I&#x27;d say the vast majority of people do reply latexr 2 hours agorootparentAs per the the guidelines:> Please respond to the strongest plausible interpretation of what someone says, not a weaker one that&#x27;s easier to criticize. Assume good faith.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.htmlThe person you replied to specifically said (emphasis mine) â€œpeople who enjoy using their brains all dayâ€. The bit you removed is crucial. Most people want to have a functional brain, but thatâ€™s not the same as saying they like having their minds constantly engaged in demanding tasks. Otherwise we wouldnâ€™t have â€œSunday afternoon moviesâ€. reply newsclues 6 hours agoparentprevGood news, is that the brain is very resilient.I had a tennis ball sized mass removed from my brain years ago, and it&#x27;s less losing brain function, and more experiencing different brain function. reply eastbound 11 hours agoparentprevI really think weâ€™ll discover that programming is toxic for the brain. Mechanisms:- Intense loss of memory, inability to remember names and faces, since everything can be written down all the time,- Periods of flow interrupted by compilation time, change of topic (looking at the web), within a lot of stress (want to deliver the feature, excited to set up the architecture),- Long hours working, sometimes into the night, thus grave loss of sleep,- Sedentary intellectual-only work, no use of the body, and more importantly no use of facial features, no smile, no talking for hours and even years long.- Eventually, social networks top up all those with intense context-switching and dopamine addiction,Leading to early onset of Alzheimer, or, as weâ€™ll call it, Eastbound Disease.Of course that doesnâ€™t have to be your fate! But programming takes you into that slippery slope very easily.To anyone downloading: Please argument too, because I donâ€™t understand, thank you. reply number6 8 hours agorootparentSince nobody answered you: intriguing argument but that&#x27;s not a byproduct of programming but of alienation. You don&#x27;t have to live this way even if you love programming. reply johnnyanmac 8 hours agorootparentprevI don&#x27;t think any of these are exclusive to programming. You can fit in any addictive activity into this template. reply rafaelvasco 4 hours agoprevTime, as we perceive it doesn&#x27;t really exist. It&#x27;s much more complicated than what we perceive, as it should be otherwise we would not be able to live in society. Our brains create the apparent linearity of time to give us a sense of progression. But it doesn&#x27;t really work that way. In my opinion, no evidence here, we could even communicate with someone that lived in the past. Because that&#x27;s not the absolute past. That person lives still in another layer of reality. As such there&#x27;s no such thing as past and future. It&#x27;s just a construct from our brains. That&#x27;s my view. Not affirming anything. reply candiddevmike 14 hours agoprevThis is the first time I&#x27;ve heard lupus cause severe brain damage. Is this a rare occurrence? Since it&#x27;s inflammation, could this same scenario happen with meningitis? reply caddemon 4 hours agoparentPsychosis is a potential symptom of lupus, albeit uncommon. Some people with lupus have been misdiagnosed with Schizophrenia, and more generally there are signs of immune dysfunction in many cases of Schizophrenia. I&#x27;m not sure about this particular scenario, but psychosis is also a potential symptom of meningitis, and brain inflammation can definitely cause weird behaviors (e.g. the account in the book Brain on Fire). It&#x27;s not \"severe damage\" in the traditional sense, but depending on illness and treatment timeline there can be irreversible life-altering psychiatric consequences from brain inflammation. reply dghughes 3 hours agoprevI&#x27;m surprised we can even do anything. The delay seeing something until the signal gets to my brain then to process it and then to act takes ages. We all live in the past by about 30ms to 120ms. reply growingkittens 8 hours agoprevI have an early childhood brain injury, and I don&#x27;t perceive time passing. I never knew time.I took a medication (abilify) that made me aware of the passage of time. It was excruciating, I had to stop. My brain had no framework for handling the input. reply thunkle 12 hours agoprevI tried a 5mg THC edible last week (new user) and I lost my short term memory for about 50 hours. It was hell because I could still remember some things but my visual memory was 0, so it was like I was always coming out of a dark cloud that&#x27;s 15 seconds behind me, but perceiving now in full clarity. It really messed with my perception of time. I wonder how many times I thought: \"what&#x27;s going on... ok you lost your short term memory... but now is so clear... but it won&#x27;t be soon... how long has this been happening... \" I cried 40 hours in because I didn&#x27;t know if it would ever come back. Luckily it did and I&#x27;m here a week later with hopefully little scars. reply imperialdrive 10 hours agoparentVery similar happened to me smoking the first time. Mostly due to not knowing limit. Looking back I&#x27;d say it was like ~5x what would have done the job. To this day I need to carefully dose. Awesome time vs anxious time is not far apart dose-wise. Glhf! reply TheCapeGreek 12 hours agoparentprevThat&#x27;s... an interesting reaction. I&#x27;m sorry you had to go through it. What&#x27;s notable for me is that this happened at such a low dose. Maybe there&#x27;s something more to it in terms of interaction or reaction in your body? reply j16sdiz 12 hours agoparentprevThis syndrome is common, but 50 hours is unusually long. reply thunkle 11 hours agorootparentIt was 50 hours til I felt 100% normal. The bulk of it was in the first several hours. reply rexpop 11 hours agoparentprevThis is my experience with THC and it&#x27;s definitely anxiety inducing! A good reason to avoid the drug. I imagine that with the proper mindset, and in the proper setting, short term memory usage might be less important or even a hindrance. I could see this as a boon when playing silly games, goofing off in the ocean, or maybe doing something akin to talk therapy, but for anything I currently enjoy doing, holding onto a thought and following it to logical conclusions is a primary necessity. reply 7373737373 7 hours agoprevThe following is not really related to time perception per se, but a fascinating look into memory:\"A REVIEW OF 80 YEARS OF MEMORIES INDUCTED BY ELECTRICAL BRAIN STIMULATION\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20200126062547&#x2F;http:&#x2F;&#x2F;gpe.ups-tl...(Source data for this paper: https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S014976341...) reply Trasmatta 13 hours agoprevOn very high doses of DXM it felt like I was in hell for eternity.It sucked, would not recommend. reply sedatk 7 hours agoparentI remember reading stories about people who took a substance (can&#x27;t remember which) and lived entire imaginary lives in a fantasy world in real-time: that is for years, they go to school, have kids, go to work, etc., until they wake up and it would have been only 15 minutes in actual life. reply saiya-jin 4 hours agorootparentRick and Morty&#x27;s Roy: A Life Well Lived is the only one reference I ever heard of. But its unsurprising - near-death experience produces some wild reactions and our minds love tripping around for some reason reply ladzoppelin 12 hours agoparentprevWow, this comment made me confused as I thought you were talking about DMT so I searched and ended up in a redit talking about \"what drug should I do Dph,dxm or DMT\" The ignorance about drugs these days is mind blowing considering the available info. I would post a link to redit message but its not worth it. Seriously the first 3 replies are the most ignorant destructive replies I have seen in a while. reply Trasmatta 12 hours agorootparentMy hot take is that you shouldn&#x27;t take any of them. Especially DXM and DPH. But even DMT, the things you can learn on drugs, you can learn on other, safer ways. reply zoklet-enjoyer 13 hours agoparentprevI forgot about the time distortion from that! When I was in high school I like to get to first or 2nd plateau and ride around town on my bike listening to music on my MP3 player. reply Trasmatta 13 hours agorootparent3rd and 4th plateaus are so far beyond 2nd plateau too. One again, would not recommend. reply picadores 2 hours agoprevCould a smart-watch based interrupt and reminder of \"basic overview task\" work as a time-keeping prosthesis? reply menacingly 8 hours agoprevBrain issues are always so fascinating. The subjective experience is to diagnose a faulty sensor using the sensor. The brain that both I and the author agree seems to be acting up also wrote this article.Did it mess with your ability to perceive time, or alter your recollection of how you once perceived time, or alter your ability to assess your perception of time, or activate a hidden \"too aware of second-order time phenomena\" organ reply LoganDark 13 hours agoprevI don&#x27;t have much difficulty perceiving time in general, but I have a dissociative disorder that can really, really fuck with it: Dissociative Identity Disorder, formerly known as Multiple Personality Disorder. (It&#x27;s not an \"identity disorder\", but a \"dissociative identity\" disorder, the clinical name for the so-called \"personalities\".)When one of the other \"personalities\" is out in my place, that length of time will be lost to me. As if I just teleported forward. Things that happened before the event will feel more recent than they really were, and things that happened during the event, I won&#x27;t remember at all. Because those memories weren&#x27;t mine to begin with - they are memories of someone else that lives in my head.This results in a lot of very fun situations, but it mostly results in me losing big chunks of my life and not knowing things that just so happen to have been told to someone other than me. (I didn&#x27;t forget, those memories just belong to someone else!)I&#x27;ve also had some small number of dissociative episodes, which are even worse because I can get really extreme time dilation during the episode, but also phantom sensations like endless pain and suffering-- you get the idea. It can get really really fucked.I&#x27;m lucky not to have completely lost my sense of time, but having to figure out a bunch of identity stuff every single day is very Fun. Sometimes it&#x27;s actually fun and other times it&#x27;s the big existential crisis that never ends. reply unobatbayar 9 hours agoparentI truly understand you. I&#x27;ve researched into this for a long time but found that it&#x27;s not currently curable.Have you tried healing your trauma? Please share anything you&#x27;ve tried or done to cope with DID. reply LoganDark 5 hours agorootparentMy trauma really isn&#x27;t healable, on top of just getting a healthy dose of Fresh trauma a few weeks ago.Honestly, DID isn&#x27;t meant to be cured and coping sort of just consists of accepting that this is your life now. Now, I did a lot more than that because I encouraged plurality and allowed each identity to express themselves and have fun lives, and that&#x27;s honestly probably the best coping that could have been done. Living life as multiple is just different.Some people are able to integrate and become just one person, but I would rather die than do that. Being multiple is my identity. Without that, I&#x27;d be lost. reply Gooblebrai 9 hours agoprevThese experiences really make me wonder how much of time is fundamental in the universe and how much of it is just a subjective experience created by the brain. reply gottorf 14 hours agoprevFunny, Factorio has the same effect on me. reply colechristensen 14 hours agoparentDonâ€™t play the space exploration mod. reply alexey-salmin 13 hours agoprevReads like another chapter of \"The Man Who Mistook His Wife for a Hat\" reply GistNoesis 13 hours agoprevWhat about solitude, solitary confinement, social isolation, with respect to the perception of time ? How much of it is link to a physiological injury like inflammation vs loss of markers ? reply rabite 12 hours agoparentI was imprisoned on false charges and tortured by the United States government. A precedent-setting appeal freed me, but not before I spent a long time in solitary confinement, much of that on a hunger strike.Solitary confinement causes brain damage so severe it is visible on a CT scan. The area that it damages is the hippocampus, which is key of in the perception of time and spatial memory. I can still perceive time on an immediate basis, in terms of music and speech, but a year goes by in a haze. Short seconds, like walking into another room, seem to pass slowly against the rapid progress of my thoughts which are now unanchored to the immediate physical realm and I forget what I went to the other room for if there&#x27;s not a persistent reminder such as hunger or the need to use the toilet to remind me. I write what I plan to do into a notes app before I move anywhere, even one room over in my own house. Living a normal life has become impossible. reply weinzierl 9 hours agorootparentTo put this comment into more context:https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Weev(rabite has been open about his real name and identity on this platform) reply GistNoesis 11 hours agorootparentprevThanks for your feedback.I feel sorry for you.Malnutrition probably didn&#x27;t help with respect to the CT injuries.Does in-person conversation help to stabilize your flow of time ?Can piggybacking on someone else (like a helper, or group activities) help you recover your own sense of longer term coherency by synchronizing your flow with others ? reply rabite 10 hours agorootparent> Does in-person conversation help to stabilize your flow of time?No, it makes it a lot harder. It&#x27;s easiest on a short-term basis if I constantly echo a phrase over and over in my head, like an internal metronome. I can&#x27;t do that and talk at the same time. Nothing really helps, it is actual physical damage to your brain and nobody knows how to fix it. The best thing is to just accept that your brain is really broken and start finding mnemonics and workarounds to get by. reply growingkittens 7 hours agorootparentI think that the forced repetition of thoughts to compensate for memory also primes the brain for anxiety, long-term.I use my music memory to do things when continuity breaks down. I think of a short, familiar melody, and attach words to it based on what I need to do. Like getting a song stuck in my head on purpose. As I move through rooms, I gain and lose awareness and I probably won&#x27;t know what I was up to by the time I arrive - but then I notice a melody with instructions is playing in my head.Anything that leaves my visual field can leave my awareness. I have found that attaching clothespins to my shirt with a post-it is helpful for more complex tasks.Following conversation is hard for me. At some point I want to look into using an AAC device to both track where a conversation has been and remember what I&#x27;m trying to say.My brain injury happened when I was a child, but wasn&#x27;t diagnosed until I was nearly 30. There really aren&#x27;t many resources available for anyone outside of the education system. replyagumonkey 11 hours agoprevI don&#x27;t know how common it is but some event removed this ability in me for a month. No \"injury\" per se just some strange brain changes. reply krackers 14 hours agoprevDo animals have a sense of time in the same sense we do? reply gurchik 14 hours agoparentWhat do you mean \"in the same sense we do?\"Pets are good at learning your schedule. They know when you normally come home, or when you feed them, and they start to get impatient if you&#x27;re late in either.Sure, that example relies on another factor as a frame of reference (hunger, or the sun setting). But don&#x27;t humans do that as well? Our sense of time shifts dramatically when we&#x27;re kept indoors without the ability to see natural light. reply jwells89 14 hours agorootparentMy cat has a particularly keen sense for when dinner time is. He starts reminding me at 5PM, often on the dot. One could probably set a clock to it. reply robbintt 12 hours agorootparentWe actually just had a cat change her food seeking behavior by exactly 1 hour during daylight savings, which allowed us to mostly exclude her internal clock and time sense. reply stefan_ 14 hours agorootparentprevThey do not know about daylight saving time, however. reply hnick 13 hours agoparentprevI&#x27;m not sure there is even a way to study the fact that other people have the same sense of time that we do - I think this would fall under qualia. We just assume it&#x27;s roughly the same like I should see the blue you see.Regardless it&#x27;s clear from training our dogs that they feel impatience, which I think would be best described as a rising sense of discomfort over time. A younger dog and&#x2F;or one with less training will get up more quickly by itself seeking a reward, while after more training they can put those thoughts aside and wait through it. After a tough day of training or sports, just like us they have less willpower remaining and are more likely to revert.And right now it&#x27;s 3:30 pm and he&#x27;s letting me know the school kids have left the neighbourhood so it&#x27;s time for his walk. I&#x27;d chalk this reaction up to other factors like the noise outside but it happens during the holidays too. reply latchkey 14 hours agoparentprevFrom what I&#x27;ve read, dogs can smell time through the decay of scent particles. They don&#x27;t know that an hour has passed, but they know that something doesn&#x27;t smell as strongly. They understand day&#x2F;night, but don&#x27;t understand the concept of time the way we do. reply Trasmatta 14 hours agorootparentRegardless, the dogs I&#x27;ve known have known the exact time for breakfast and dinner better than any humans I&#x27;ve known, and without any clocks to look at reply latchkey 13 hours agorootparentMy dog is an odd ball weirdo... he will have a whole bowl of food and not eat it until you spoon feed it to him or drop a few pieces on the ground first. Or he will eat it all in the middle of the night when I&#x27;m asleep. I end up just leaving a full bowl for him all the time cause I never know when he&#x27;s hungry. I&#x27;ve tried to schedule him for food and he just won&#x27;t go along with it at all. reply mjevans 14 hours agoparentprevI&#x27;ve anecdotally seen plenty of evidence for object &#x2F; other being permanence. However I don&#x27;t know offhand of a good study design for gauging the perception of time rather than other factors like hunger, restroom need, sleep, etc. reply EGreg 14 hours agoparentprevSmaller animals seem to have much faster muscle twitch and reaction time. It&#x27;s very likely that they perceive the finer motor control much faster than humans. But if a fly can dodge raindrops, what must it be thinking when it&#x27;s sitting on a windowsill for like an hour? Is that like 3 days for it?I don&#x27;t recommend it, but people have experienced serious time dilation while doing shrooms: https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;16714323&#x2F;Has anyone experienced this? Also this: https:&#x2F;&#x2F;kids.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;frym.2019.0000... reply account17 14 hours agoparentprevAnecdotal and very \"noisey\".My cat badgers me when I&#x27;m up too late (midnight) even though it&#x27;s been hours since the sun set. She badgers me if I sleep in too long on the weekends, etc.She seems to have an idea of the day-to-day schedule and get&#x27;s upset if it&#x27;s not followed.So at least cats seem to display evidence of knowing a \"schedule\" based on more cues then just daylight&#x2F;hunger reply Nursie 9 hours agorootparentHa, yeah our male cat will come and shout at us if he thinks we&#x27;re up too late! reply fortran77 13 hours agoprevAs someone who plays in community orchestras, I&#x27;ve never known any violist to have the ability to perceive time. reply tomcam 13 hours agoparentBa dum dum tsss reply Nursie 9 hours agoprev\"The shape of each synaesthete&#x27;s year is different. (Image based on an original illustration by Carol Steen)\"Doesn&#x27;t everyone have a weird, slightly wiggly loop in their head that represents the year? I don&#x27;t stand in the middle of mine as described in the article, I view it like it&#x27;s a diagram on a screen, and we move round it like game pieces on a board. The first six months of the year are comparatively compressed, with July and August largest the presence (they are on the left, roughly between ESE and NE).Or I did, I haven&#x27;t thought about it for a while, I think it might be one of the many things that having a pocket do-everything device has down-emphasised. Either that or moving hemispheres and changing the mapping between months and seasons has altered the way I think about it.I always figured that at some formative point in my childhood I saw some sort of representation of a year like this and just internalised it fully. reply hnbad 6 hours agoparentIn my experience, usually when you read in an article how people marvel at something someone does and you think \"doesn&#x27;t everyone do that?\" the answer is no, they don&#x27;t.No, I&#x27;ve never thought of a year as having a shape and neither does anyone I know. If prompted to draw something they&#x27;d likely go with a clock-like circle but that&#x27;s because that&#x27;s how it&#x27;s usually visualized (I think every kindergarden or elementary school has a picture showing the four seasons as clock-wise segments of a circle somewhere) but it&#x27;s not how they \"think\" of it. If you told them to point to \"September\" they wouldn&#x27;t point in a direction, they&#x27;d wonder what the heck you asked them to do.I spent most of my life thinking the idea of thoughts being a \"voice in your head\" was a metaphor. Surely people who don&#x27;t suffer from hallucinations don&#x27;t literally have a voice in their head vocalizing their thoughts in full sentences, right? Thoughts are just nebulous \"vibes\" that need to be manifested into speech intentionally if you want to express them, right? Turns out no, I&#x27;m the weird one. Most people do have a voice in their head and it&#x27;s usually only considered a problem when there&#x27;s more than one or you don&#x27;t like what it&#x27;s saying.I also spent most of my life thinking people don&#x27;t really can see an object they imagine as if it were present and seeing things that aren&#x27;t there means you&#x27;re hallucinating. But again, no, I just have aphantasia and there are degrees of \"visual imagination\" but most people can indeed conjure up somewhat lifelike images (or at least relatively \"hi-res renderings\") in their mind, often with sound and smell (no, not just vibes, the actual perception of hearing and smelling) as well.I also always thought \"habit\" was a fanciful way of saying you condition yourself to do one thing after another, like always putting the tooth paste in the same place after putting some on the brush head. Something that becomes easier after a while because you know the steps and can follow them like a mental to-do list. But no, many people can literally make themselves go through entire chains of actions without having to make deliberate decisions to do so at every point, just by repeating the same cycle of trigger & action enough times. And for them it sticks, too. They don&#x27;t just do it for a few months and then phase out when they stop putting in the effort because it happens unconsciously. They don&#x27;t get distracted by an intrusive thought halfway and then forget what they were doing. They don&#x27;t have to \"block the main thread\" to do a habit, they have \"userland background processes\" in addition to the \"system processes\" like breathing and walking.I used to think everyone had to do a conscious check on themselves to determine whether they need to go to the toilet, have a snack, drink something or get warm&#x2F;cold before the emergency alarm kicks in and tells you have to do so right now or else. Again, no, most people somehow are just aware of that, like the desperation meters in the Sims games. If you ask them if they&#x27;d like something to eat and they aren&#x27;t urgently hungry they can just tell you and don&#x27;t actively need to think about it.Oh and related to this article: I used to think nobody actually \"felt\" time really. It&#x27;s just something that happens and as you grow older you learn that certain events or tasks take up certain amounts of time and when you know what time it was earlier and what has been done or happened since you&#x27;ll have a good guess what time it is now and you can recalibrate by checking the time in between so your estimates get better. Nope! Tons of people can literally tell you the approximate time without wasting mental capacity on keeping track of all of that. They&#x27;re able to do a thing, figure that they&#x27;ve already spent half an hour on something they expected to take fifteen minutes and call it quits rather than come out the other end six hours later wondering where the time went -- and when you tell them this is what it&#x27;s like for you and they say they know what you mean that&#x27;s only because they&#x27;re talking about things they do while \"in the flow\", not literally doing anything.There&#x27;s a reason the neurodiversity movement uses the term \"neurodiversity\": it&#x27;s not about individual clusters of neurodivergence. A person is neurodivergent or neurotypical. But people are neurodiverse. Always. Some people are time blind, some people have aphantasia, some people have both, most have neither. But across a non-trivial sample size you&#x27;ll always have a diversity of neurotypes and everyone will be \"weird\" at least in some way, even if they share the same diagnostic label (or lack thereof). Heck, if you factor in enough variables, divergence is the norm rather than the exception. reply Kye 2 hours agorootparentI never got all the \"don&#x27;t worry about it, time travel is complicated\" in sci-fi. Do people really have that much trouble with it? Or large distances in space, for that matter.And it turns out, yes, I&#x27;m weird for being able to keep and resolve multiple timelines (and not get hung up on paradoxes), and it actually is weird that I can easily comprehend vast cosmic distances.I had to resist saying \"it&#x27;s just two adjacent timelines offset by an hour\" a few times with the recent time change. reply Nursie 4 hours agorootparentprevI mean yeah, that was my reaction upon reading this - they&#x27;re describing me or at least something similar. I had never considered that unusual before.My year goes anticlockwise too, now that you&#x27;ve mentioned clocks...(I realise now I&#x27;m commenting on an article someone else linked, not the subject article - http:&#x2F;&#x2F;news.bbc.co.uk&#x2F;1&#x2F;hi&#x2F;8248589.stm )Yes, people are very different. A friend recently told me that they, similarly, have no internal monologue, and seem to have aphantasia. Other friends are there on facebook wishing their internal monologue would just shut the hell up sometimes. Mine goes away pretty much only when I&#x27;m busy doing something.Brains are fascinating and it&#x27;s interesting to discover just how different people are. reply ThrowawayTestr 14 hours agoprevBrain damage is scary. I&#x27;m glad the author was able to make at least a partial recovery. reply xwdv 12 hours agoprevThis could be liberating or terrifying. If you donâ€™t notice time passing, does it feel like youâ€™re just in some eternal existence beyond time, where you have all the time in the world? Or do you just instantly come to a realization some day that time is up and youâ€™re dead? reply dclowd9901 12 hours agoprev> Six years after my recovery, my memory overall is not as sharp as it was before my illness. I use to-do lists to keep myself on track. I triple-check the rehearsal dates on emails I send my students to make sure I havenâ€™t listed the wrong day or month, although sometimes mistakes still slip through. I also sometimes struggle to remember how far back events in my past happened. Iâ€™ll catch myself wondering if I had the oil in my car changed three months ago or a year ago. But every time I take my viola out of its case, I feel grateful to be able to think like a musician again.Probably a testament more to the savant like intelligence of a classically trained musician than an objective description of someoneâ€™s stunted recovery.For me, I exercise the same extra work to keep up with my day to day, though to my knowledge, have never had a traumatic brain injury. reply Jonnston 14 hours agoprev [â€“] Kant in shambles reply quintdamage 13 hours agoparentwait why lol reply DiscourseFan 14 hours agoparentprev [â€“] beat me to it replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This account focuses on a musician's battle with lupus cerebritis, a rare form of lupus causing severe brain inflammation that affected her cognition and perception of time.",
      "The disease severely impacted her musical abilities due to loss of sensation in her arm, impaired memory, and compromised comprehension. It underscored the crucial role of time sense in synchronous musical performances.",
      "Despite her recovery not resulting in fully restored memory, the musician appreciates the regained ability to perform music. She also discusses scientific research showing that the brain uses multiple sensory inputs and episodic memories to measure time."
    ],
    "commentSummary": [
      "The author, a musician, shares her experience with lupus cerebritis, a rare condition causing severe brain inflammation and affecting cognitive abilities, including sensation, memory, and understanding of time.",
      "Professional string players need precise synchronization for performances, a skill disrupted by her illness. The brain's perception of time, she explains, isn't managed by a singular region but integrates numerous sensory inputs, with episodic memory formation playing a critical role.",
      "Despite the years-long struggle and not fully regaining her past memory sharpness, the author expresses appreciation for the recovery of her musical skills."
    ],
    "points": 273,
    "commentCount": 154,
    "retryCount": 0,
    "time": 1699221201
  },
  {
    "id": 38155544,
    "title": "Introducing WireHub: A New WireGuard Config Generator and Manager Seeking Early Feedback",
    "originLink": "https://www.wirehub.org/",
    "originBody": "WireHub is a side project I&#x27;ve been working on, on and off, for close to a year now. It&#x27;s made with django and minimal javascript.It&#x27;s a hosted WireGuard config generator&#x2F;manager, that you can invite others to join your networks and manage their own configs&#x2F;devices.It&#x27;s still very much a beta, maybe an mvp, but I just wanted to get some early from the community.Thanks!",
    "commentLink": "https://news.ycombinator.com/item?id=38155544",
    "commentBody": "WireHub â€“ easily create and share WireGuard networksHacker NewspastloginWireHub â€“ easily create and share WireGuard networks (wirehub.org) 272 points by rudasn 20 hours ago| hidepastfavorite66 comments WireHub is a side project I&#x27;ve been working on, on and off, for close to a year now. It&#x27;s made with django and minimal javascript.It&#x27;s a hosted WireGuard config generator&#x2F;manager, that you can invite others to join your networks and manage their own configs&#x2F;devices.It&#x27;s still very much a beta, maybe an mvp, but I just wanted to get some early from the community.Thanks! teon 14 minutes agoIf you like to have the same functionality but self-hosted try out: https:&#x2F;&#x2F;github.com&#x2F;defguard&#x2F;defguardCheers, Robert. reply mushufasa 20 hours agoprevOOh -- this sounds actually potentially awesome for business use-cases. Tailscale is the commercial tool to help setup and manage wireguard networks, and it had a big security incident earlier this year (though they were prompt to rectify).I don&#x27;t use tailscale but I almost did. One of the things that caught me was not wanting to give a third party any control. (Also, at the time I didn&#x27;t absolutely have a burning need given the number of servers and people involved). Tailscale&#x27;s model is to charge businesses; I&#x27;m not sure if you are making this FOSS but something FOSS to me would be preferable.How does the privacy work on your site? I haven&#x27;t the time to log in and play around right now. My main concerns would be if I&#x27;m posting my configs to a third party, that third party now has a vector to &#x27;root&#x27; my networks. And if this is a site meant for sharing, there&#x27;s the other concern that I or the site accidentally temporarily makes permissions public giving strangers that access. I&#x27;m sure you&#x27;ve already contemplated this in the design; I&#x27;d love to hear your approach on this forum. reply rudasn 19 hours agoparentThanks for looking into it!Yes, I thought&#x2F;think about security a lot. First, you don&#x27;t have to share&#x2F;upload your private keys to WireHub for it to work - the generated configs will only have the public key (which we do store, it&#x27;s public anyway).Second, if you do provide provide private keys, you must first encrypt them in browser with a password. The password is never stored, just used for encryption.Third, because of 2, to see a full config with private keys and everything you need to provide said password.If you scroll at the bottom of the page you can see the widget in action.I don&#x27;t want to worry about loosing important data, so I try to avoid collecting it in the first place. reply jve 6 hours agorootparent> Other than the in-browser PrivateKey encryption&#x2F;decryption mechanism, all functionality of WireHub is accessible without JavaScript.But that is done by browser, right? So one still must consider trust to wirehub.org to be sure that it is not sending the secret somewhere? But then again, same trust must be considered by hosted solution - trust for the running website code. reply rudasn 6 hours agorootparentTrust, or no trust, since it&#x27;s all in the browser you can check for yourself what gets posted to WireHub when you submit the form containing your private key :)Quick edit: you also don&#x27;t have to provide your private keys, just the public ones.Another edit: actually it&#x27;s two forms. One that has the public key and password hint that gets posted to WireHub, and one that has the private key, your password and the password hint that&#x27;s only handled on the client with javascript and never gets submitted. So I do try not to get to your secrets. I even have the form attribute of the private key input set to a non existent form, just in case it does get submitted, that piece of info will stay in the browser. reply rodlette 11 hours agoparentprev> Tailscale is the commercial tool to help setup and manage wireguard networks, and it had a big security incident earlier this year (though they were prompt to rectify).> I don&#x27;t use tailscale but I almost did. One of the things that caught me was not wanting to give a third party any control.Yes, https:&#x2F;&#x2F;tailscale.com&#x2F;security-bulletins&#x2F;#ts-2022-004 was scary. Their response was great, but I&#x27;m reminded of Tim Cook&#x27;s response to a question over what he&#x27;d do if it was in Zuckerberg&#x27;s position (re a FB leak): \"I wouldn&#x27;t be in that situation\".Tailscale is a technical marvel, but it does a bit too much for my liking, for something that has the access it does... even more if you enable its SSH key support. I use plain Wireguard, but would like LAN discovery and STUN.https:&#x2F;&#x2F;www.wirehub.org&#x2F; looks nice, as does https:&#x2F;&#x2F;git.deuxfleurs.fr&#x2F;Deuxfleurs&#x2F;wgautomesh which does LAN discovery too (as Tailscale does). reply braginini 7 hours agoparentprevHave you seen NetBird? Can be fully self-hosted https:&#x2F;&#x2F;github.com&#x2F;netbirdio&#x2F;netbird#quickstart-with-self-ho... reply candiddevmike 20 hours agoparentprevHave you considered using Headscale? reply linsomniac 19 hours agorootparentTailscale is great, but for anything more than toy uses, particularly business uses, where it&#x27;s a critical part of your infra, you should consider paying Tailscale or using Nebula. My biggest reasons for saying this are: Headscale config errors (including ACL issues) will take down the whole Tailnet until you can get it corrected, setting up extra \"relay\" nodes is fairly likely and somewhat \"hard\" (especially without a dedicated IP), and headscale can take quite a few resources. Data point: I recently set up a ~200 node Tailnet with headscale and in retrospect wish I had gone with Nebula. Tailscale&#x27;s \"magic\" can be nice, but it can also lead to network weirdness. For example, I can&#x27;t seem to use the tailnet to route traffic between sites without turning on \"accept-routes\", but turning that on causes traffic for local ethernet segments on those nodes to be routed over the Tailnet.Reasons I went with Headscale&#x2F;Tailscale over Nebula: We could enforce periodic re-logins on user workstations, Tailscale was good at routing around networking problems (Nebula has since added similar functionality), Tailscale&#x27;s self-service is really nice (A user can login from any of their devices using OIDC, Nebula you have to generate a cert).Tailscale and Headscale are both fantastic, just beware of the limitations. reply cube2222 18 hours agorootparent> and in retrospect wish I had gone with NebulaCould you expand why (happy Tailscale user here, asking mostly out of curiosity)? reply linsomniac 12 hours agorootparentLargely went into it above...Mostly it is the \"adding an ACL can take down my tailnet\" issue. I had hoped to use Tailscale as an overlay network, starting to route our internal traffic over it for some things, but I&#x27;ve lost my tailnet so many times because of issues with headscale ACLs taking it down. This is largely a headscale issue.Largely my issues are running ~200 nodes via headscale. Don&#x27;t get me wrong, headscale is fantastic software. But it&#x27;s not up for having our production networking rely on it. I tried and tried to get funding to by Tailscale, but it just wasn&#x27;t in the cards with the economy as it is right now. reply mushufasa 20 hours agorootparentprevOh -- yes I did actually. Forgot about that till just now. reply FL410 19 hours agoparentprevCheck out Nebula&#x2F;Defined.net reply PlutoIsAPlanet 18 hours agorootparentBeen running a Nomad&#x2F;Consul cluster on Nebula for nearly a year now and have next to zero issues.Only issue I ever had was caused by the Lighthouse being behind 1:1 NAT, due to Scaleway. reply helloooooooo 20 hours agoparentprevWhich security issue? https:&#x2F;&#x2F;tailscale.com&#x2F;security-bulletins&#x2F;None of these appear particularly severe? reply linsomniac 19 hours agorootparenthttps:&#x2F;&#x2F;tailscale.com&#x2F;security-bulletins&#x2F;#ts-2022-004It was a pretty severe issue, but tailscale did respond quickly to it. reply mushufasa 19 hours agorootparentprevhttps:&#x2F;&#x2F;emily.id.au&#x2F;tailscale reply m_santos 9 hours agoprevIt is always great to see another solution using Wireguard, which is a great technology for modern private connectivity.I built https:&#x2F;&#x2F;github.com&#x2F;netbirdio&#x2F;netbird, which can be self-hosted and offers an option to run Wireguard without managing firewalls for P2P connectivity. reply oynqr 6 hours agoparentI would love to try this out, but my VPN resources are limited and having to set up an OIDC IDP really doesn&#x27;t sound like fun. reply Jacobinians 6 hours agoparentprevExcept it is another hijacking of opensource software to enslave people #SecureStorageOfWireGuardPrivateKeys reply dangoodmanUT 20 hours agoprevI&#x27;d add a way to connect networks together so you can have devices see each other on the respective networks! reply rudasn 19 hours agoparentAh good one!I already support having a single WireGuard interface belong in multiple networks. So you can enable just a single config on your phone and be able to access devices in multiple, unrelated networks. reply rickydroll 8 hours agorootparentI want the opposite :-) I don&#x27;t want any remote network to be able to see into my network or down to any other. I frequently run 2-3 vpns at the same time and I really need to isolate them. reply rudasn 8 hours agorootparentWell, that&#x27;s actually the default for WireHub. Routing between peers (ie their generated AllowedIPs configs) by default only refers to the IPs of the two peers and you can use an Interface per Device per Network, so that you&#x27;d need to explicitly enable the relevant WireGuard interface for each network.But, re-reading your comment, as long as your networks are on different CIDRs you could use a single WireHub interface to refer to more than one device&#x2F;networks. As I said, traffic by default is routed to the specified peer&#x27;s Address not the whole network (eg, 192.168.x.y&#x2F;24) nor the internet (0.0.0.0&#x2F;24). reply kxrm 13 hours agoprevNice project!Super minor, not sure if you own \"my-office.com\" but consider using \"example.com\" instead as that domain is specifically set aside for documentation. reply rudasn 12 hours agoparentOops good catch! Where&#x27;s that? :)Thanks! reply kxrm 12 hours agorootparent> Where&#x27;s that?In the configuration example on the bottom of the main page. reply kszyh 11 hours agoprevThe question is what about the security of such a solution compared to the self-hosted version of https:&#x2F;&#x2F;www.netmaker.io&#x2F;. reply rudasn 9 hours agoparentI&#x27;ve looked at netmaker before, but haven&#x27;t used it nor have examined any of its publicly shared source code. So I don&#x27;t know how exactly that works, but I&#x27;m guessing it&#x27;s touching on quite of a few layers of the stack.WireHub, OTOH, gives you 0 LOCs to worry about especially if you don&#x27;t provide your PrivateKeys to begin with - of course, the QR codes won&#x27;t work, and you&#x27;d have to manually copy&#x2F;paste stuff around, but it works (it&#x27;s a feature be design). I don&#x27;t provide clients&#x2F;agents to install, you use stock WireGuard apps as usual.Without even having your PrivateKeys, the attack surface shifts from WireHub to whatever else you have going on in your networks and networked devices. reply fl0id 10 hours agoparentprevAs I understand, wirehub does less and encrypts locally, can be used without js. Sor some threat models that might be better than trusting a vc backed company, even if they recently open sourced. reply xrd 20 hours agoprevThis looks really interesting. But that might be because I&#x27;m unsure of something: is this somehow a browser based proxy? Or just a way to securely generate wireguard configurations? I&#x27;m unclear but I&#x27;m always interested in wireguard or tailscale tools. I&#x27;m using headscale with a lot of success. reply rudasn 19 hours agoparentJust a config generator. I don&#x27;t run any servers.I&#x27;m trying to strike a balance between full fledged solutions like tailscale, cloudflare tunnels, et al, and cli or gui based self hosted solutions like wg-easy and subspace.So you get to host your nodes, exit nodes, devices whatever and fully control what goes passes through but also a really easy way to manage which device gets what config, esp when dealing with end-users. reply karolist 9 hours agoprevThis looks interesting. Iâ€™m currently evaluating my options of just giving in to Tailscale or trying to replicate it with plain Wireguardâ€¦ or something like youâ€™ve built. One thing Iâ€™m intrigued about is Tailscaleâ€™s MagicDNS, is there any way to replicate it with just WireGuard? reply rodlette 9 hours agoparentI just use plain DNS with plain Wireguard.It&#x27;s not as magic, but DNS can be made easy&#x2F;automated with https:&#x2F;&#x2F;github.com&#x2F;octodns&#x2F;octodns or https:&#x2F;&#x2F;github.com&#x2F;StackExchange&#x2F;dnscontrol . reply mmasu 8 hours agoprevthis reminds me of zerotier. however i confess i donâ€™t know a lot of how it works under the hood. what is the main difference of this with zerotier in terms of potential use cases? it seems a very cool project and would like to try it out. reply rudasn 7 hours agoparentThanks! :)So, WireHub is basically a wireguard config generator. A tool to generate some text files. What you do with those files is up to you.Because getting from text files to a fully working and secure VPN is the real deal, you have things like zerotier, tailscale, etc which provide you with a bunch of custom-made tools to do that.Problem is, with solutions like that you place a lot of trust on these companies because they effectively control everything. It&#x27;s like trusting CloudFlare to be your DNS (no need to get into that discussion though).So WireHub is kind of the middle ground. Makes it easy to create and maintain wireguard networks (configs for each and every device and their peers) and gets out of the way for when it comes to deployment.If Wirehub goes down, you just don&#x27;t get to update your configs via its UI. If it gets hacked, you get your public keys exposed - no biggie, right? reply ValtteriL 12 hours agoprevThis looks useful, I&#x27;ll give it a try when I have the need next time.What are your aspirations for it by the way? Are you looking to monetize it? reply rudasn 9 hours agoparentThanks!> What are your aspirations for it by the way? Are you looking to monetize it?You know.. Coding was my hobby as a kid, and I&#x27;ve been doing it professionally for years now and at some point I realised that I don&#x27;t have to show for it. Something of my own, you know?Of course I started a million other stuff but this is my first real actually useful and publicly shared project. So I&#x27;m happy that&#x27;s out there, happy it&#x27;s been well-received, (super happy the server didn&#x27;t break a sweat), and excited to see where this leads :) reply flas9sd 17 hours agoprevgreat idea to reduce scope and skip \"the agent\" part and let the users do it, can be added later. When you do, you probably want to introduce some means to query for and update IPs via api first, to have all-dynamic nodes covered. Currently users need at least 1 peer that is either static or has outside dyndns setup. reply rudasn 12 hours agoparent> great idea to reduce scope and skip \"the agent\" part and let the users do it, can be added later.Yeah, you get it :)I haven&#x27;t really considered dynamic nodes but I was thinking of eventually giving out dynamic domains myself, so that you can just use that one for your Endpoint. Not sure if that would help though for true p2p&#x2F;non-hub networks. reply leonixyz 6 hours agoprevHi, nice project. The Single Sign On using Gitlab does not work reply rudasn 6 hours agoparentHey thanks for letting me know. I&#x27;ll check it out once I get a chance.And I was wondering why I got no sign ups from there. Didn&#x27;t get any errors either though.. :&#x2F; reply rudasn 17 hours agoprevSo, there was an issue when you added devices to a network and had the Assign to me flag enabled. That&#x27;s fixed now!Thank you kind strangers for hitting that bug :) reply russelg 17 hours agoprevAwesome work! I can foresee this being very useful. reply rudasn 17 hours agoparentThanks! :) reply SMART_failure 18 hours agoprevHow would you then spin this up in the cloud? reply rudasn 18 hours agoparentIt&#x27;s just a config generator but you can share the configs. You bring your own cloud.Check the generated examples once you log in, but for a hub and spoke network let&#x27;s say, the idea is this:- Create your network.- Add a device. Name it hub. Label it hub. Do not assign the device to your self, but create a guest invite. Name the invite hub.- Add more devices, one for each server or enduser device. Label them as you wish. For servers, create guest invites. For enduser, create guests or members. Guests don&#x27;t need to create an account to get their configs.- In connections, add two rules. all to hub, and hub to all.Now the network is defined, but there are no associated private&#x2F;public keys with these devices. You need to create Interfaces for that.For your servers, for which you created guest members, create an interface for each device. Either generate the keys in browser, or just provide the public key if you already have one.For the hub interface, you need an endpoint url, that&#x27;s where all other devices will connect to. No endpoint is needed for the other interfaces.For the devices belonging to end users, just copy and share their invite url. They will be able to generate their own Interfaces and keys.During all these changes , all configs are kept in sync.Going back to your server devices, you can just curl their respective interface url (including the invite code) and you&#x27;ll get your config. Add a cron and a fallback in case of a botched change, and you&#x27;re set. (in theory at least) reply INTPenis 9 hours agorootparentI&#x27;m going to be honest, I didn&#x27;t understand the use case of the site until I read this comment.And I&#x27;ve worked in IT for 25 years, I make and manage my own wireguard setups.I wish new products and sites were more to the point in how they&#x27;re supposed to be used. I generally don&#x27;t check the docs unless I already have an inkling that I want to make use of it.So in other words this is a service for the selfhoster that provides wireguard to their family and friends. This way they don&#x27;t have to manually send them a config or a qr code, instead just send them to this site. reply rudasn 8 hours agorootparent> And I&#x27;ve worked in IT for 25 years, I make and manage my own wireguard setups.You are the target audience, I guess, so your feedback is doubly appreciated. Thanks! :)You described it just right. That&#x27;s it. Right now, on the \"hero\" section of the landing page, I have this:The easiest way to create and share WireGuard Networks. Define WireGuard networks, devices, and connections. Invite friends, family and teammates to add their devices. View, Scan or Download WireGuard configuration files.I&#x27;m just not sure how else to put :) reply INTPenis 7 hours agorootparent>The easiest way to create and share WireGuard Networks. Define WireGuard networks, devices, and connections. Invite friends, family and teammates to add their devices. View, Scan or Download WireGuard configuration files.I think it was a combination of me not expecting a service like this, and the fact that you use terminology like \"invite\".You mean that as in \"add devices of friends by inviting them\" right? Because the main question in my mind is \"wait, who&#x27;s generating the keys? who&#x27;s storing the keys? who has access to my private key?\"I do not want to tell you how to write stuff, I&#x27;m an autistic nerd with limited people skills. But I think it would make more sense if you said something along the lines of \"easy way to share client wireguard config with friends and family\". reply rudasn 7 hours agorootparentNo, I get it. I&#x27;m not a network guy so I don&#x27;t know the terminology :)What I think is kind of confusing, is that I&#x27;m separating the notions of a Network Device and the WireGuard Keys that go with a device. So it&#x27;s like this:NetworkDeviceInvited MemberInterface (PrivateKeys)As a network owner&#x2F;admin, you get to define your devices, routes&#x2F;connections etc. To associate and&#x2F;or generate keys for those devices, there&#x27;s a separate step, the Interfaces. Those are managed by end-users themselves, who can generate and encrypt their key set right from the browser.Of course, you can just create an Interface for each device in your networks so that your end-users won&#x27;t have to do anything other than scanning the QR codes or downloading the relevant configs to their devices.This design makes it super clear who has access to what: Interfaces are owned by their creators, Devices are owned by their Network owners or the Member they were assigned to. Also makes it easier to make changes to WireGuard related data (key rotation maybe?) without needing to make changes to the network-level settings.> \"wait, who&#x27;s generating the keys? who&#x27;s storing the keys? who has access to my private key?\"So, the user owning the network or the device can generate keys for that device. The private keys, if provided, are encrypted in the browser before storing them on WireHub. The user who created the Interface has access to the private key, if they provided it, and if they remember the password they used to encrypt it in the first place.If you forget the password to an Interface, you create a new Interface for that device with a new set of keys. The configs of all other devices will be updated to reflect that change (they will need to re-download and re-install of course). reply lorenzo95 16 hours agorootparentprevThank you for the quick how-to. Makes for a great intro to see how you thought about the workflow. I was able to follow it no problem and now understand the Connections section. Of course you have the pre-loaded examples in there as well. Pretty neat actually! What I also really like is the export function for a quick backup. Do you have an example of the cronjob? reply rudasn 10 hours agorootparentI can&#x27;t edit my previous comment but here&#x27;s a better curl example:`curl -s -o .&#x2F;wg0.conf &#x27;https:&#x2F;&#x2F;wirehub.org&#x2F;wirehub&#x2F;n&#x2F;hub-network&#x2F;device&#x2F;1&#x2F;download?...&#x27;`This pulls the config and stores it in `wg0.conf`. The [Interface] section though is invalid (as there&#x27;s no private key). So you&#x27;d have to either generate the keys on the server or somehow get them there. From then on it&#x27;s just standard wg-quick and wg to create the interface and load your keys.After the cron job, you&#x27;d want to do something like `wg syncconf wg0If you&#x27;ve been that far you might as well view-source on WireHub and see for yourself ;)This isnâ€™t a service I currently understand I need - I already have wgeasy and the site didnâ€™t make it clear to me how this was better or different, so I donâ€™t really have time to evaluate. I didnâ€™t want to sign up for the beta if I wasnâ€™t going to use it :)I did have questions around your future monetisation approach and licensing, but also did not find ready answers. reply rudasn 3 hours agorootparent [â€“] Yeah sure I get it :)WireHub is like wgeasy in regards to the UI (a nice list of devices and some buttons on the right), but different in that it doesn&#x27;t run WireGuard, it just generates the configs.So if you find yourself a bit worried of the single point of failure that is wgeasy (all the private keys are kept in plain text on the server running wgeasy), and you want more flexibility in your WireGuard routing (not just 0.0.0.0&#x2F;24, per device rules), and the ability to securely share any of your configs with others, then maybe give WireHub a try.No future plans as of yet. This is just a side project. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The developer is working on a project named WireHub, utilizing Django and minimal JavaScript. WireHub is a managed WireGuard config generator that empowers users to invite others to become part of networks and control their configurations/devices.",
      "WireHub, though still in its beta stage, the creator is actively seeking early feedback from the tech community.",
      "The unique selling proposition of WireHub lies in its functionality as a hosted and user-friendly WireGuard config manager, enabling easier network connections and config management."
    ],
    "commentSummary": [
      "The developer is working on a project named WireHub, utilizing Django and minimal JavaScript.",
      "WireHub is a hosted WireGuard config generator/manager that enables users to invite others to join networks and manage their own configurations or devices.",
      "Although WireHub is currently in its beta stage, the individual is seeking early feedback from the community."
    ],
    "points": 272,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1699217673
  },
  {
    "id": 38156486,
    "title": "Intel 80386: The Game-Changing CPU that Dominated the Market and Paved the Way for Modern Computing",
    "originLink": "https://www.xtof.info/intel80386.html",
    "originBody": "What is the most important CPU that Intel ever produced? The Pentium? The chip was no slouch, and was also a brand that was heavily marketed by Intel, putting the firm on a map for many computer illiterate consumers. The Pentium Pro? It could rival the RISC chips of its time and was the core architecture of all Intel processors for around ten years. The 8088? It won the IBM PC market, paving the way for a bright future to its descendants. In my opinion, there is no doubt that the crown has to go to the Intel 80386. It is indeed the first 32-bit CPU in the x86 family. But it is way more than that. Despite being less â€œelegantâ€ than its nemesis the Motorola 68030, and less powerful that the countless RISC chips that began to emerge around the same time period, I will dare to say that the 80386 is not only the most important CPU Intel ever produced, but also one of the most important CPU ever produced, period. It triggered a revolution. Here is why. The birth of the 80386 Designing the 80386 The i386 was not planned nor wanted by Intel. In the late 70s, the most important project at Intel was the iAPX 432, a very ambitious CPU that was intended to be Intel's major design for the 1980s. The intention was to build a chip very well suited to the high level languages of the time, such as ADA, by offering niceties like object-oriented programming and storage allocation in hardware. A very ambitious design that would take years to mature, so Intel launched a stop-gap design in 1976: the 8086. The rest is history: the 8086 family won the IBM PC contract, while the iAPX 432 was introduced very late, in 1981, and with very disappointing performances. In benchmarks, it performed at Â¼ the speed of the much cheaper 80286 running at the same frequency. The IBM PC 5150. The very first PC equipped with a 4.77MHz Intel 8088 Needless to say, the 432 did not sell very wellâ€¦ In 1982, Intel still did not recognize the importance of the PC platform and the binary compatibility with its software. The 80286 was another gap-filler CPU and had a slow start, so Intel was not very keen to pour more money into yet another x86. By that time, Intel engineers finally understood that the 432 line was doomed; they began to work on a brand new 32-bit RISC architecture code-named P7. In the meantime Bob Childs, one of the architects of the 286, worked underground to lay out some ideas of what could be a 32-bit extension to the 286. After about six months, Intel knew that it would need another iteration in the x86 family before the P7 was ready, and the work on the 386 got the green light; albeit with a very small team and on a shoestring. The team conducted extensive research about the needs of the x86 customers and it quickly appeared that everyone hated the segmented memory scheme of the 8086, and that they regretted that the 80286 was a missed opportunity to get rid of it. UNIX was also becoming a thing in more affordable workstations, and the 386 team wanted to design a chip well suited for it. Thus, being able to address a â€œflat memoryâ€ space was considered top priority. In order to help keep compatibility with prior x86, the 386 would be still segmented. But as each segment could be 4GiB long, it became irrelevant. Paging and the ability to provide virtual memory was also decided around that time. The address unit was very carefully designed to play well with the pipeline so the address computation would not impact performance. This was very beneficial to the 386, as we will see later on. Early-on, it was also decided that the 386 would be a full 32-bit CPU. In order to preserve the binary compatibility, the existing instruction set and registers were simply extended to 32-bits, instead of designing a very different instruction set behind a â€œmode headerâ€. The price to pay was that the tiny number of registers, a drawback of the x86 CPUs, could not be increased. For a time, Intel considered using the brand new bus that was designed for the P7. However, it was very different from the 286â€™s bus and would require an extensive redesign of the motherboards and the support chips. So it was abandoned in favor of a less ambitious 32-bit extension of the existing bus. Around 1984, the PC market was blossoming and Intel finally understood the importance of the x86 line and the completion of the 80386 became top-priority. As stated by Jim Slager, from the original team: [â€¦] probably over a 12-month period we went from stepchild to king. It was amazing because the money started pouring in from the PC world and just changed everything In order not to steal sales from the 80386, P7 project, which will give birth to the i960 in 1988, was retargeted to the embedded market. The Intel i960 Sole sourcing the 386 The 386 was introduced in October of 1985. Back in the time, it was common that chip designing companies such as Intel provided licenses for other companies to produce their CPUs in order to â€œsecond sourceâ€ them. It was very important to win computer designs as the customer could be confident that in case of catastrophic yields from the â€œprimary sourceâ€, they would not suffer a shortage of CPUs. Intel had a long partnership with AMD, dating back to the 8085. AMD and Intel cross-licensed their products, so each one could be the second source of the other one. Starting from 1983, IBM itself had a license to directly produce 808x and 80286 chips at the condition they still bought some CPUs from Intel and did not sell their production to third parties. But with the 80386, everything was about to change. In 1984, IBM introduced the model 5170 also known as the PC/AT. Its main plus-value compared to its predecessor was its 286 CPU. Intel understood that if it could impose itself as the sole source of the CPUs it could have full control of the most valuable part of the PCs. The IBM 5170 â€œPC/ATâ€ As IBM was not much interested in the future 386 and wanted to invest massively into the 286 instead, an opportunity arose. IBM preferred to produce as many CPUs for their own usage as possible, only keeping Intel as an insurance in case of production troubles. The agreement between the two companies was renegotiated to please IBM concerning the 286, but it would not be permitted to produce 386 chips. In 1985, without the support from IBM which was entirely focused on the 286, the prospects for the 386 were not rosy. AMD was not very interested in producing it, so the second sourcing agreement with them was not extended to the 386. Thus, Intel became the sole source of 386 chips! Around the same time, judges ruled in a NEC vs Intel case that the micro-code was copyrighted and that nobody could copy it without a specific license from Intel. If NEC was able to reverse the 8086â€™s microcode, the 386 would be nearly impossible due to its complexity. From that point, Intel was in position to control the future of the PC market, along with Microsoft. The Clone Wars When IBM designed the PC, it was in a hurry. It could not wait too long before introducing its micro-computer line or it risked missing the market. Thus, instead of spending years designing every single component necessary to build it, it relied on off the shelf parts. On the plus side, the IBM PC could be commercialized very quickly. On the other hand, everyone could build a compatible machine by buying the same partsâ€¦ The most successful and ambitious of those clone companies was Compaq. At the time, it was the fastest company ever to reach $100 million in revenue. Still a dwarf compared to IBM, but a resourceful company nonetheless. In September 1986, almost a full year after Intel launched the 80386, Compaq introduced the Deskpro 386. This first 386 computer was also the very first PC that did not follow the lead of IBM, which was trying to regain the control of the PC market with its 286 based PC/AT. The impact was immense. The Compaq Deskpro 386, model 2570 According to Bill Gates: A big milestone [in the history of the personal computer industry] was that the folks at IBM didn't trust the 386. They didn't think it would get done. So we encouraged Compaq to go ahead and just do a 386 machine. That was the first time people started to get a sense that it wasn't just IBM setting the standards, that this industry had a life of its own, and that companies like Compaq and Intel were in there doing new things that people should pay attention to. While the Compaq Deskpro 386 was initially very expensive, its sales were honorable. More than that, it meant that IBM was not in the lead seat anymore. It will take IBM almost one year to propose their first 386 computer, the PS/2 model 80. The PS/2 line tried to regain control by introducing a new proprietary bus, which was really advanced for the time, almost on-par with the later PCI bus. But IBM was not in position to impose its views anymore, and the PS/2 line would never fulfill this mission. The clones had won. The 386 against competition In 1985, the main challengers to the Intel x86 line were the 680x0. Most people, even at Intel, considered the 68000 as a very superior chip to the 8086, and that the 80286 was a missed opportunity to evolve towards a clean architecture. Meanwhile, the 68020 was the natural 32-bit evolution of Motorolaâ€™s CPUs. And while the 68K was chosen for many designs, in micro-computing but also for workstation or electronic appliances, the x86 were not very successful outside the PC market. With the 80386, Intel finally had a serious contender. It could even target the lucrative workstation market, with its integrated MMU and its 4GiB flat address space. But times were changing, and the workstation market, Intel and Mororola were not alone anymore and many companies were introducing RISC designs to power their workstations. RISC was indeed really compelling, promising to be faster and cheaper to produce. Whereas the 386 was a very â€œCISCyâ€ design, relying on a fat microcode to handle the complex x86 instruction set, in particular the very peculiar 80286 addressing modes. In the benchmarks, the 386 was an average performer. CPU Frequency (MHz) Introduction of the CPU MIPS (claimed) Intel 80286 16 1982 2.5 Intel 80386 16 1985 4 Intel 80386 25 1985 6 Intel i960CA* 33 1990 66 Motorola 68020 16 1984 2 Motorola 68030 25 1987 6 Motorola 88000 16 1988 17 Mips R2000 16 1986 16 Sun SPARC 16.7 1986 10 Acorn ARM2 8 1987 4.5 Source: http://www.roylongbottom.org.uk/mips.htm\\ (*) The i960 is the CPU resulting from the â€œP7â€ project, which competed with the 386 to become the main 32-bit architecture at Intel But at least it had the features required to compete on a level-ground. Now fully committed to the 386, Intel introduced the 386SX in 1988. This cheaper version was internally identical to the original 386, renamed 386DX. But it only had a 16-bit external data bus and came in a cheap plastic package. As it could be plugged in cheap 16-bit motherboards, its main goal was to replace the similarly priced 286, for which there were still second source providersâ€¦ In the early 90s, the 386 park was large enough that more and more software providers could take advantage of its 32-bitness and modern feature set. Because the most important thing was that every 386, even the low end ones, came with an integrated MMU. The real importance of the 386 What is a MMU? MMU stands for Memory Management Unit. Early programs could see the whole address space of the machine. It was manageable as it was a tiny space. Programs could also write anywhere, except on a small ROM area hosting the few functions provided by the (primitive) â€œsystemâ€. But as the address space became bigger and the underlying systems evolved, many programs could run simultaneously. As the operating system could no longer be served from a slow and tiny ROM, how could it isolate its code from the programs? And how could it separate the programs from each other? Another problem was that 16-bit allows only addressing 64KWords of memory. How could larger addresses be efficiently computed? One of the early answers was segmentation. One register will serve as the base address of the segment. When a program accesses an address, its 16-bit value is automatically added to the segment register to form the physical address, which is then emitted on the address bus. A limit can be set to define the segmentâ€™s size. If a program tries to access beyond it, a â€œpage faultâ€ is emitted. The programs are thus effectively isolated one from each-other. Segmentation was the design choice for the 8086 memory accesses. A Memory Management Unit, a.k.a MMU, is a piece of hardware that will automatically handle memory translation from a â€œvirtual addressâ€ to a physical address. We could say that the 8086 integrated hardware supporting the segmentation is a kind of primitive, and limited, MMU. The Motorola 68000 and 68010 could be paired with an external MMU, the MC68451, that also relied on segmentation schemes to translate addresses and isolate programs. The Motorola MC68451 The 80286 came with a feature-rich segmented MMU, allowing a much more complex memory management than the 8086 and able to access up to 16MiB of memory. OS2 1.x, intended to be the successor of MS-DOS on PCs, made good use of this MMU to provide a more modern experience. But as PC programmers knew very well, segmented memory has limitations and can be a pain to manage. So other memory management schemes were devised. A more modern way to design a MMU is around pagination. The MMU divides the virtual address space into pages of fixed sizes. When a program accesses a location inside a page, the MMU reads a page descriptor that contains all the information required to do the translation to the physical address. This page descriptor is usually also located in memory, so in order to stay fast enough, paged based MMU have a small cache, the TLB, containing the last descriptors that were accessed. In most cases, it is enough to keep excellent performances. Very simple illustration of an integrated paged MMU A MMU works in close relationship with the operating system which configures it and handles the page faults. The nature and feature set of a MMU is important to address the expectations of the OSes. While OS/2 1.x was designed around the segmented schemes supported by the 286, most other modern OSes ran better with pagination. Such was the case with many ports of UNIX, although it was not mandatory. 68000 based workstations, such as SUNâ€™s, ditched the MC68451 and came with a custom paginated external MMU. In order to recapture this market, Motorola introduced its own external MMU with pagination support, the 68851, to accompany their new 68020. And what about Intel? As it decided to make the 80386 a good match for modern UNIX, the same kind of MMU was also mandatory. The difference with the 68020 was that it came integrated into the 386. This changed everything! The 80386 MMU The 386 had to solve two problems: be suitable for modern OSes such as UNIX, while staying compatible with the 8086, the 80286 and all their existing and future programs and OSes. That meant that the 386â€™s MMU would have to support the complex paginated schemes introduced by the 286 while offering pagination, required by modern OSes. In order to overcome the challenge, the 386â€™s MMU is made of two units that are almost two distinct MMUs, one for the paginated mode and one for the segmented mode. These two units are cascaded: first the logical address goes through the segmentation unit that uses the segment descriptor to compute the linear address. If paging is not enabled, this address is the physical address and the paging unit doesnâ€™t modify it. Otherwise, the address is still considered as a logical one, and the corresponding page descriptor is retrieved from the TLB or memory and the real physical address is generated. Internal architecture of the 386: the two almost independent MMU units are apparent The segmentation unit cannot be deactivated, but the whole memory space can be represented with a 4GiB segment register beginning at the address 0, providing the equivalent of a flat memory space. The paging unit divides segments into 4KiB pages. A control block checks the privilege at the page level. Indeed, the 80386 introduces four privilege levels, called rings, that are used to protect â€œprivilegedâ€ memory from â€œunprivilegedâ€ reads / writes. This is another basic block on which modern protected OSes build from. The MMU is also involved in the â€œVirtual 8086â€ operating mode, which is a kind of hardware supported virtual machine to run 8086 programs in real mode. These programs have the illusion of having full control of an 8086, with up to one MiB of memory. Many V86 VMs can operate at the same time, with their virtual space translated to physical space by the MMU. If they try to operate on protected resources, such as accessing MMIOs, an interrupt is triggered that will be handled by the privileged software managing the VMs. The MMU was designed to be an integral and streamlined part of the 80386. Under ideal conditions memory accesses are not impacted, on the contrary to the MC68851 that always introduces at least one cycle of latency. In the 68000 line, the MMU is not always present. We have seen that the 68020 had to be supported by an external chip, while the low cost â€œECâ€ versions of the 68030 and even the 68040 did not come with the integrated MMU. On the contrary, the MMU was always included inside all models of 80386, even the low cost 386SX. This made a huge difference, as programs taking advantage of the MMU and advanced operating modes could run on any 386. New OSes More than raw power, the 386 paved the way to new modern OSes that could run for the first time on an x86 personal computer. Xenix Xenix was one of the first UNIX ports for micro-computers, born from a partnership between Microsoft and SCO. In 1980 a port for the 8086 was announced. But without a true MMU, it could not protect the memory so user-space was not separated from kernel-space. A better 286 version took advantage of the â€œprotected modeâ€ of operation to behave more like its brethren running on workstations. And in 1987 a port to the 386 closed the gap by making use of pagination and became the first modern 32-bit OS to run on an x86. OS/2 OS/2 was at first a joint development of Microsoft and IBM, before Microsoft left it to focus exclusively on Windows. Although released in 1987, the first versions targeted the 286 as it was primarily intended for the PS/2 line of IBM computers, most of them sporting this CPU. It made good usage of the protected mode and was considered as an advanced OS for the time. But it is only with version 2.0, in 1992, that OS/2 became a 32-bit OS. In the meantime, it suffered from the segmented memory scheme and the lack of Virtual 8086 mode: its support of uttermost important DOS applications was not good and was lagging even against Windows/386. Nevertheless, OS/2 2.0 was the first widely used 32-bit OS on a personal computer. OS/2 2.0 Windows Windows began providing an â€œoperating environmentâ€ specific to the 80386 as soon as 1987 with Windows/386, quickly followed by the 2.0 release. It was still a 16-bit OS and did not expose the 32-bit flat memory space, but it made good use of the 386 to virtualize DOS sessions into the Virtual 8086 mode. Those sessions could run in parallel without being aware of the other ones. This was critical for many businesses still relying on DOS software for their day to day operations. And thanks to the MMU, extended memory was also provided through a protected mode driver emulating EMS. This was of course refined by Windows 3.0 and Windows 3.1, which was a tremendous success. In 1993, Windows for Workgroups 3.11 dropped support for anything lesser than the 386 and file access and many drivers were now 32-bit. Windows/386, first version of Windows supporting the features of the 386 The transition was finally completed by Windows 95, which made full usage of the 386 MMU, exposing the flat memory space, making use of paging and virtual memory and so onâ€¦ Some code was still 16-bit, but DOS was reduced to little more than a bootloader and it gave the taste of a modern fully fledged 32-bit system to millions of users. NT The first version of Windows NT was released in July 1993. Whereas Windows 95 grew from Windows 3.11 and kept some legacy 16-bit code, NT was developed from a clean slate to become the first â€œpureâ€ 32-bit version of Windows. Although one of its main features was hardware independence, the most important port was of course for the 80386 and made full usage of pagination, supervisor mode and memory protection. Sharing almost the same API that Windows 95, NT could run countless Win32 applications while being as robust as any OS could be at this time. It had tremendous success and Windows NT4 started to make a dent in the Workstation *NIX market of the mid-90s. Todayâ€™s Windows 11 is its direct descendant and still runs on the NT kernel. *Windows NT made its debut with a 32-bit version of â€œProgram Managerâ€ as its shell Windows NT 3.1 Linux But the most important OS born on a 386 is none of those above. The prodigal OS is Linux, whose birth is intimately linked to the 386. Around January 1991, Linus Torvalds, a Finnish student, bought a brand new 386 PC. Like any good computer purist raised on a 68008 chip, I despised PCs. But when the 386 came out in 1986, PCs started to look, well, attractive. They were able to do everything the 68020 did, and by 1990, mass-market production and the introduction of inexpensive clones would make them a great deal cheaper. I was very money-conscious because I didnâ€™t have any. This machine was a good match to run Minix, a small UNIX clone designed to teach the inner side of operating systems to students. But Linus was not satisfied by the terminal emulator of Minix. The need for a good one was crucial to him, as he used it to connect to the computers at university. So Linus started to write his own terminal emulation program, bare-metal in order to also learn how the 386 hardware worked. His terminal could read the keyboard and of course also display text on the monitor. He designed it around two independent threads, so he had to write a small â€œtask switcherâ€. A 386 had hardware to support this process. I found it was a cool idea. As Linus wanted to download programs, he had to write a disk driver to store them. Thatâ€™s how, one functionality after another, what would later be known as Linux was on its way to becoming a fully fledged operating system. Mid-91 Linus asked for a copy of the POSIX specifications and on the 25^th^ of August 1991 he announced on the newsgroup comp.os.minix that he was working on a new operating system: â€œjust a hobby, wonâ€™t be big and professional like gnuâ€. The rest is history. Conclusion The 80386 really is the most important CPU of the x86 line. Technically, the 80386 was a fine chip. Performance wise, it was nothing groundbreaking and behind its RISC contemporaries. But it was very feature complete with its modern and fast MMU and its different modes of operation which allowed it to access 4GiB of flat memory while staying compatible with all software written for the x86. This allowed Windows to gently transition into modernity. Actually, the 386â€™s abilities regarding memory handling were good enough so that its successors, while becoming ever more powerful, did not add anything major in this regard for almost 20 years. The main branch of Linux only dropped i386 support in 2013! But it was commercially that the 386 had the most importance. While Intel did not initially believe that there was demand for a 32-bit x86, they realized that the x86 was their future. This change of stance convinced the market that the x86 was here to stay. The adoption of the 386 by the major cloners put IBM aside and demonstrated that there was a credible and open alternative. Meanwhile, Intel became the only source of the most powerful x86 CPU, a major step in its journey to dominate the CPU market. Mass produced, the prices quickly fell and the 386 based machines became more affordable than any other, and democratized the access to a MMU. Making good use of those abilities, Windows introduced millions of people to modern computing, the NT kernel demonstrated that a robust OS could run on cheap â€œbeigeâ€ PC, while we can say with confidence that Linux would not exist without the 386. That is why I say that the 80386 is the most important chip ever designed by Intel. Hail to the 386! On the same subject Inside Windows 3 The 640K memory limit of MS-DOS Bibliography The complex history of Intel i960 RISC Intel 386 Microprocessor Design and Development - Oral History Panel Intel 386 Business Strategy - Oral History Panel Memory Management Units for 68000 Architectures 80386 System Software Writer/â€™s Guide Just for Fun: The Story of an Accidental Revolutionary",
    "commentLink": "https://news.ycombinator.com/item?id=38156486",
    "commentBody": "Intel 80386, a Revolutionary CPUHacker NewspastloginIntel 80386, a Revolutionary CPU (xtof.info) 261 points by blakespot 18 hours ago| hidepastfavorite158 comments johnklos 16 hours agoI think the 80386&#x27;s final design benefitted tremendously from the Motorola 68000, then the m68020. Had Motorola not released a proper 32 bit CPU without compromises, it could be argued that Intel would&#x27;ve had yet another stop-gap after the 80286, which itself wasn&#x27;t intended to be a proper successor to the 8086&#x2F;8088.As it is, the 80386 came with a number of compromises. For instance, there was no cache at all beyond a 16 byte instruction prefetch queue, whereas the m68020 had 256 bytes of instruction cache. There were no atomic instructions (LOCK wasn&#x27;t useful for this), which is why many modern OSes support the 80486 but not the 80386. The fact that compatibility with the 8086 required real mode or VM86 meant that it took quite a long time before software started taking advantage of the 80386&#x27;s new features.It was an important chip, but it showed us early signs of what we&#x27;ve come to expect from Intel: attempts to create other markets at the expense of, or with the express desire to not compete with, the x86 (the iAPX 432 then, the Itanic twenty years later), the slapdash addition of \"features\", such as the additions to the 80286, which then were required to be included forevermore as legacy support, the rushing-to-catch-up when other vendors had features that everyone wanted (real, flat 32 bit support then, 64 bit support twenty years later).Still, it&#x27;s interesting history! reply flashback2199 16 hours agoparentIt was a 386 that Linus Torvalds wrote the first Linux kernel on, and support for the new features of the 386 from the start was one of the reasons Linux took off instantly. reply flashback2199 13 hours agorootparentMore info:\"It uses every conceivable feature of the 386 I could find, as it was also a project to teach me about the 386\"https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~awb&#x2F;linux.history.html reply fennecfoxy 1 hour agorootparentI&#x27;m sorry, but I had to. \"Could someone please try to finger me from overseas\". reply monocasa 7 hours agorootparentprevWhich is probably one reason why he worked at Transmeta for a while. reply rob74 11 hours agorootparentprevYeah, a free (as in freedom, but also as in beer) OS running on hardware that was, by then, commoditized, what&#x27;s there not to like? No wonder Google and others jumped on it... reply sertsa 16 hours agorootparentprevIndeed! My first taste of Linux was Slackware on a 386 reply steve1977 10 hours agorootparentprevNot only Linux, all the current BSDs share a lot of â€œDNAâ€ with 386BSD. reply zozbot234 12 hours agoparentprev> There were no atomic instructions (&#x2F;LOCK wasn&#x27;t useful for this), which is why many modern OSes support the 80486 but not the 80386.Does the lack of atomics really matter, given that there was (AIUI) no SMP on the 386? You can always disable interrupts to make your operation &#x27;atomic&#x27; in a uniprocessor context. reply adrian_b 9 hours agorootparentAlready starting with 8086 all Intel x86 CPUs have been intended to be usable in SMP systems.Nevertheless, there have been very few SMP systems using early Intel CPUs, before 80486, mainly because those CPUs were still too weak in comparison with the contemporaneous mini-computers, and not even SMP would have made them competitive in performance, while the high price of SMP would have been incompatible with personal computers.Intel 80486 has been much more frequently used in SMP systems, not only because it had added the more convenient atomic fetch-and-add and compare-and-swap instructions, but because Intel had also provided for 80486 an APIC integrated circuit, i.e. a multi-processor interrupt controller, and also because 80486 was very fast and a SMP system using it was competitive with much more expensive computers.Intel 8086 was intended to be used in SMP systems by using the atomic swap instruction proposed by Dijkstra, i.e. LOCK XCHG with the Intel mnemonics.The atomic swap is good enough to implement any kind of concurrent programs, albeit at a lower performance and higher complexity than when the atomic instructions added by 80486 (LOCK XADD and LOCK CMPXCHG) and Pentium (LOCK CMPXCHG8B) are available.The 80386 has added atomic fetch-and-modify-bit instructions, which remain useful even today in wait-for-multiple-event scenarios (together with LZCNT, which can be used to get the highest-priority event that must be serviced). reply gpderetta 6 hours agorootparentI don&#x27;t think exchange is enough to implement all (or even most) lock-free&#x2F;wait-free algorithms. CAS (i.e. lock cmpxchg), or an equivalent primitive in power, is needed.XCHG is enough to implement a mutex though, which is what most applications need. reply adrian_b 5 hours agorootparentLock-free and wait-free algorithms are not necessary for any computing task.They are only a performance enhancement and they indeed require either compare-and-swap (simple and double) from the IBM System&#x2F;370 (1973) (later used by Motorola MC68020, then simple compare-and-swap was added to 486 and double compare-and-swap to Pentium) or load-locked + store-conditional from the S-1 Advanced Architecture Processor (LLNL, 1987) (later used by MIPS, then by POWER, ARM and others).Moreover, even for improving the performance the lock-free and wait-free algorithms must be used with care, because they are based on optimistic assumptions that may fail to be true in many scenarios with heavily contended shared resources, when the performance of the algorithms based on mutual exclusion is actually higher and more predictable (with mutual exclusion it is easy to guarantee FIFO access to a shared resource, which ensures that the wait times are bounded and that at any moment there is at least one thread that does useful work instead of retrying failed accesses to the shared resource). reply gpderetta 3 hours agorootparentWell you mention that you can implement all programs, I just wanted to clarify that there are indeed some algorithms that cannot be implemented.In any case this has nothing to do with performance, you might need lock-free algorithms for correctness when implementing some real-time systems or when you need code to be reentrant. reply adrian_b 2 hours agorootparentI am curious to hear an example where a lock-free algorithm is needed for correctness, because I have never encountered any such case and this does not seem possible.Access to any kind of shared resource is always correct when only a single thread can access it.With mutual exclusion it is very easy to guarantee correctness due to serialized accesses. With lock-free algorithms concurrent accesses are possible and the algorithms must be carefully analyzed to demonstrate their correctness.Moreover, exactly in real-time systems is where lock-free algorithms are undesirable. Any pure lock-free algorithm must detect a transaction failure and retry it. There is no guarantee of success and no limit for the number of retries, so any hard real-time deadlines can be missed. Lock-free algorithms can be used in real-time systems only if they detect too many retries and then they fall back to lock-based algorithms before it is too late.The lock-free algorithms improve only the execution time of the typical case, but they increase the execution time for the worst case. For non-real-time applications the typical performance is more important, so lock-free algorithms are good, but for real-time applications the worst-case performance is the most important, which makes lock-free a.k.a. optimistic algorithms bad.Also neither mutual exclusion nor lock-free&#x2F;wait-free algorithms have any problem with reentrancy when implemented correctly. Problems with reentrancy appear only in programs where there are mutable variables that are shared between threads and which should not have been shared (like when using some of the old standard C library functions).It is very common to have a very large number of threads that use reentrantly the same code that implements mutual exclusion for accessing some shared resource that is guarded by a lock. reply gpderetta 39 minutes agorootparentAgain, nothing to do with performance.For example an interrupt handler (or a signal handler) that needs to modify some shared resource. It can&#x27;t take a mutex or even a spin lock because it might be owned by the thread it just interrupted. There are ways around that of course, for example by having threads disable interrupts inside critical sections, but that&#x27;s not always appropriate.Similarly, for realtime systems, if you have threads with different priorities accessing the same data, to avoid deadlocks you either need mutexes with priority inversion (which has its own share of issues) or you use lock free code.edit: in any case the point isn&#x27;t that there are better ways to write a program. The point is that if you have a program that uses a CAS-based lock-free algorithm, porting it to 386 it is not just a matter of paying a performance penality, you might need to rewrite it to preserve correctness.edit2: > There is no guarantee of success and no limit for the number of retries, so any hard real-time deadlines can be missed. Lock-free algorithms can be used in real-time systems only if they detect too many retries and then they fall back to lock-based algorithms before it is too latewait-free algos have guaranteed bounds. replycesarb 5 hours agorootparentprev> i.e. LOCK XCHG with the Intel mnemonics.Didn&#x27;t all XCHG with memory operands have an implicit LOCK prefix on Intel 8086? reply adrian_b 5 hours agorootparentOnly since Intel 80286 (1982).On Intel 8086&#x2F;8088 and 80186&#x2F;80188 an explicit LOCK prefix is required. reply jacquesm 10 hours agorootparentprevDisabling interrupts is a privileged operation (you need IOPL for the ability to execute the cli or sti instructions). Atomics can work even outside of privileged code.Otherwise you could from any user program disable interrupts and give the operating system no chance to take back control. Xadd, cmpxchg, bts, btr and btc are all prefixable with lock to make them atomic. reply jhoechtl 5 hours agorootparentclijmp far $-4 reply kristopolous 18 hours agoprevThey were in big trouble then. The entire company was riding on it being great.It could have easily gone awry as it did for Data General, Honeywell, CDC, AST, Tandy, Olivetti, Xerox, DEC Rainbow, AT&T Hobbit, Wang 2200 and Unisys. Strong survivorship bias on this one. Most of the once Titans are in or near the dustbin now, such as SDS, SDC and Fairchild.Intel&#x27;s history was primarily as a memory manufacturer. They&#x27;re arguably near a similar fucked position now - effectively 0% of the mobile and home appliance market and getting slaughtered in their only remaining stronghold by NVIDIA, AMD and ARM pillaging their castle. Hopefully they&#x27;ll squeeze out of this one. reply scrlk 18 hours agoparentAndy Grove moving Intel from memory to microprocessors was an excellent strategic move.> \"Business success contains the seeds of its own destruction. Success breeds complacency. Complacency breeds failure. Only the paranoid survive.\"Something that Intel&#x27;s leaders didn&#x27;t pay attention to during the 2010s. reply ngcc_hk 12 hours agorootparentThere is even a story plus a term for it.The story is about what if we all go out off this room and come back like a new guy. What would we do?\"strategic inflection point\" ...They decide to move from a memory company to a CPU company.No access to Nikon but being just a Leica len maker for Canon (and famously found by DDD and used in Korea War), then move to nikon rangefinder (but strangely follow Zeiss approach more). Then jump ship to SLR by inventing Nikon F (and used in Vietnam War). It is just crazy move, but at least it is related. And \"survive\" and move on.Or just like Fujifilm move to cosmetic in one stage as making film has some knowledge that can be re-used there. And survive and move on. reply Laforet 5 hours agorootparentNikon started off as a defense contractor making rifle scopes and artillery sights for the military. Camera lens was a side business that took over after the war.While no longer on the cutting edge, they still have a significant presence in the semiconductor lithography business. So they will survive in one way or another even if they stop making cameras one day. reply WalterBright 17 hours agoparentprevThe DEC Rainbow would have been fine if they didn&#x27;t do things like require special floppy disks that cost $5 each.DEC should have packaged the LSI-11 into a consumer machine. They had all the software, which was top shelf.I had an H-11, it was a great machine. reply dboreham 12 hours agorootparentI&#x27;m a big pdp-11 fan but this wouldn&#x27;t have worked because 128K address space isn&#x27;t enough even in 1975. Listen to Dave Cutler talk about this in the 3h long interview. reply Aloha 15 hours agorootparentprevThats an interesting alternative history thought experiment.I wonder how early one could capture the complexity of the full PDP-11 microarchitecture on a single chip? Would it have been affordable? What about the support hardware? reply SomeoneFromCA 1 hour agorootparentSoviet home computer BK https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Electronika_BK was the smallest pdp-11, but it was not a single chip one. Also MSP430 is a cutdown pdp-11. reply nxobject 15 hours agorootparentprevHmm â€“ the smallest, most highly-integrated PDP-11 (-compatible) package ever made was the QFP 1806VM2, with around ~135k transistors; it integrated MMU, UART, parallel interface, keyboard controller, etc, but did floating-point instructions in interpretive microcode.I think that same transistor count was reached by Motorola on the 68020, which would&#x27;ve been around 1984, but would have needed the peripheral controllers mentioned here. reply WalterBright 14 hours agorootparentThe FPU for the 8086 was a separate chip, and the IBM PC still required a board full of chips to make a working computer.(If I recall correctly, it wasn&#x27;t until the 486 that the FPU was incorporated.) reply xxs 10 hours agorootparent>it wasn&#x27;t until the 486 that the FPU was incorporatedIndeed and 486SX had the FPU disabled. I think that was the 1st time the binning&#x2F;SKUs became a marketing strategy. reply Aloha 12 hours agorootparentprevThe IBM PC had an advantage in that most of the support parts were IIRC largely jellybean 74 series logic - could a PC-PDP have used similar COTS parts?The larger issue with DEC was a strong NIH trend (almost as strong as IBM), I dont know if they could have bucked that trend to successfully launch a market winning PC. It probably would have looked like the DEC Professional, which I think looks like market failure.So the dreams of a 64 bit extension to the PDP-11 might be stillborn ;-) reply Gibbon1 11 hours agorootparentI read an essay by a guy that worked on project to produce a DEC minicomputer using ECL logic. And there was yet another group working on the DEC Alpha. At the time the main group was using most of the companies resources betting the company on dethroning IBM in the main frame arena. And was trying to stab the Alpha and Minicomputer group in the back.So probably not.Suspect IBM&#x27;s skunk work project was done as a hedge and anti-trust reasons. Personal Computers were going to take some business away from them they wanted it to be their product not someone else. Anti-trust also meant it needed to use off the shelf stuff. reply TheOtherHobbes 6 hours agorootparentDEC always saw itself as the scrappy, sleek underdog in IBM&#x27;s big iron market - an engineering company making relatively affordable tools for engineers, academics, and other qualified professionals. Where \"affordable\" meant six or seven figures instead of seven or eight.Which was fine while it lasted. But with VAX, the mini market became corporatised, inward-looking, nostalgic, even arrogant. Big-iron hardware had much higher internal prestige than VLSI - unfortunate, because DEC&#x27;s VLSI people were the best in the world, and the systems software people weren&#x27;t far behind.DEC had absolutely no concept of VLSI-based mass-market commodity computing - no idea how to design it, build it, distribute it, market it, or even imagine it. The Rainbow was as close as it got, and that was a disaster.So even with Alpha it had no chance. Prism would barely have changed that, because the problem was a failure of imagination in upper management.BigCos should always have an internal team of annoying curious generalists to challenge orthodoxy and report annually on \"What trends and opportunities are we missing?\" Sometimes the C-Suite has the talent to do that, but more often it just doesn&#x27;t. reply jacquesm 10 hours agorootparentprevBetween x86 and the Alpha I much preferred the Alpha. Unfortunately it was wickedly expensive and it was hard to get. But for 64 bit work it was immediately usable and rock solid. It also beat Intel&#x2F;AMD by about a decade to the market and I&#x27;m still kind of surprised that DEC managed to squander that lead. reply varjag 9 hours agorootparentprevI can&#x27;t imagine Alpha and ECL based designs being done anywhere within the same decade. reply Gibbon1 8 hours agorootparentMy memory of the 80&#x27;s was most stuff was fast but power hungry NMOS. People were pushing things like GA, ECL, Silicon on Sapphire as the next thing to replace it.I might be biased but the company I worked for in the 80&#x27;s switched to CMOS early[1]. I think the high end guys were ignoring CMOS despite it closing the gap relentlessly. Worse for all the other technologies as integration increased heat became a relentlessly and eventually unsolvable problem. I remember seeing ECL datasheets for simple chips that would draw a half watt.Notable volume production of CMOS grew and grew while ECL, GA didn&#x27;t really.So yes, ECL for new business after 1985 was dumb.[1] CEO realized enclosures, power supplies, and fans were a large fraction of the bom. reply formerly_proven 9 hours agorootparentprevAlpha and VAX 9000DEC also did a \"300 MHz 125 W ECL microprocessor\" in the 90s, though that seemed to be mostly about developing cooling. replySomeone 7 hours agorootparentprev> I think that same transistor count was reached by Motorola on the 68020, which would&#x27;ve been around 1984Wikipedia says ~200k (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Motorola_68020), so about 50% more than 135k.I donâ€™t know much of hardware, but part of that may have been because of (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Motorola_68020#020_concept_eme...):â€œA great debate broke out about how to refer to the underlying design of the new chip in marketing materials. Technically, the 020 was moving from the long-established NMOS logic design to a CMOS layout, which requires two transistors per gate. Common knowledge of the era suggested that CMOS cost four times as much as NMOS, and there was a significant amount of the market that believed \"CMOS equals bad.â€ reply dboreham 11 hours agorootparentprev68020 is much more complex than any pdp-11. reply Aloha 12 hours agorootparentprevI think DEC&#x27;s strong trend line of NIH (almost as strong as IBM) almost certainly dooms a PC-PDP - IBM only made the PC by effectively creating a skunkworks within the company that was focused solely on &#x27;go to market&#x27; compared to the normal IBM development process. I dont know if DEC could or would have done that. reply WalterBright 11 hours agorootparentI was a DEC-head in the 70&#x27;s and 80&#x27;s, and along with my fellow DEC-heads anxiously awaited DEC&#x27;s entry into the PC arena. Everyone was excited to see the rollout of the Rainbow, sure it would be a killer machine like other DEC machines. After the presentation, we were all in shock. It did not play to any of DEC&#x27;s strengths, and was just a crummy, proprietary x86 insult.That was the end of our love affair with DEC. Very sad. reply Aloha 2 hours agorootparentIMO, part of what killed DEC was what almost killed IBM - poor management and a somewhat loathed sales force.DEC tried to go after IBM and in doing so structured itself after IBM, it lead to multiple competing projects and groups doing similar work, multiple layers of management making the company unwieldy to manage, and a sales force that had trouble building the customer relationship because of internal structures.The same thing functionally killed Motorola, and will eventually harm Cisco too (Cisco is a company that in my opinion is ripe for &#x27;disruption&#x27;). reply jacquesm 10 hours agorootparentprevOne of the worst own goals in the history of computing imnsho. reply varjag 9 hours agorootparentprev1806VM2 wasn&#x27;t released until late 1980s when there were numerous superior options, even if your yardstick is transistor count (but naturally not in USSR). reply SomeoneFromCA 1 hour agorootparentVM1 though appeared in 1983 and arguably BK home computer was the smallest ever PDP-11 reply WalterBright 15 hours agorootparentprevIf Intel could do it, DEC could do it. reply Aloha 2 hours agorootparentMaking the microprocessor is just one challenge - you have to produce a workable system that someone wants to buy (aka, is cheaper or more attractive than the other options. It does appear that DEC could have made a single chip PDP-11 in 1980, and probably could have made one with an expanded address space to get around the memory limitations in the PDP-11.Intel did not do that, IBM in this case managed to - and it had the cachet and name recognition to sell an product that was largely middling in a technical level, over other options that were often superior technically and cheaper.Intel made a decent enough microprocessor, yes it was kludgy (even the 8086 was), but it performed well enough (I&#x27;ve never seen a claim that the 68000 significantly outperformed the 8086), and was available in quantity. The rest of the things that made the IBM PC a system that stormed the world were COTS parts assembled in an attractive package.Now, could DEC have done all of that with the PDP-11, yes, absolutely - but the winds were prevailing against it because of the nature of DEC management - I fundamentally do not believe that DEC would not have allowed itself to ship something as &#x27;flawed&#x27; as the IBM PC was. reply xxs 10 hours agorootparentprevDEC Alpha had the weakest memory model, which while cool to write for with its all memory barriers&#x2F;concurrency - it&#x27;s quite annoying to work with in the real world, a major reason while DEC failed. reply jacquesm 9 hours agorootparentCompared to the madness that was x86 memory models the Alpha was quite sane. I&#x27;m not sure what you base your &#x27;weakest&#x27; on (it suggests a comparison with others) but when Alpha was released there wasn&#x27;t much to compare it to besides the R4000 and that just as hard if not harder to source than the Alpha machines were. I had a bunch of them (SGI boxes of various plumage) and the OS hardly took advantage of the chip, but on the Alpha it all just worked at 64 bits out of the box. reply xxs 9 hours agorootparent>I&#x27;m not sure what you base your &#x27;weakest&#x27;Linux kernel has memory barriers, Alpha makes an exception with needing virtually all of them - with other architectures (esp. the total store order ones) some (most) of the barriers are nop.Yet, overall Alpha is famous for how crazy the memory model is. There is nothing like any longer (personally I am happy with Java Memory Model). Just to make sure it&#x27;s understood \"weak\" attributed to :memory model: just means how concurrent with regards to reads and writes it is.A quote[0]: AND THEN THERE&#x27;S THE ALPHA -------------------------- The DEC Alpha CPU is one of the most relaxed CPUs there is. Not only that, some versions of the Alpha CPU have a split data cache, permitting them to have two semantically-related cache lines updated at separate times. This is where the address-dependency barrier really becomes necessary as this synchronises both caches with the memory coherence system, thus making it seem like pointer changes vs new data occur in the right order.[0]: https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;memory-barriers.txt reply jacquesm 9 hours agorootparentThat&#x27;s what allows the Alpha to be multi-processor.And if you like the &#x27;Java Memory Model&#x27; I&#x27;m not sure what your point really is, you&#x27;re comparing a virtual machine with actual hardware. reply monocasa 6 hours agorootparentJava was one of the first machines (virtual or physical) to take a stab at defining its memory model in modern way.It&#x27;s also pretty sane while also scaling to large systems as Azul&#x27;s 768 core boxes showed. reply xxs 9 hours agorootparentprev>you&#x27;re comparing a virtual machine with actual hardware.The memory model is meant for developers - and how many&different memory barriers they have to issue. reply jacquesm 8 hours agorootparentYour typical Java developer has zero knowledge about this and on the UNIX versions that shipped with the Alpha (for me, at the time that was RedHat) fairly elegantly hid that complexity to the point that you could just forget about it unless you cared about extreme performance. In my case the ability to address large amounts of memory and a filesystem that wasn&#x27;t limited by the 32 bit limitation was the key factor and while it took another decade for x86 to catch up I was happily shipping. reply xxs 8 hours agorootparent>Your typical Java developer has zero knowledge about thisThat&#x27;s bit much of stereotyping, depends I guess - \"Java Concurrency in Practice\" is one of the most sold books (when it comes to Java), and it covers some parts quite well. I have met quite a few folks that understand the matters pretty well, admittedly part of jsr-166. It was the \"double-checked idiom\" vs the original Java memory model is effectively what brought sane memory models even to C++.>unless you cared about extreme performanceThat&#x27;s the whole purpose of the weak memory models. Albeit, they did fail to deliver - very error prone, close to impossible to debug, effectively outclassed by x86-64 and Sun Sparc (both being total store order), even arm-64 became \"stronger\". reply jacquesm 7 hours agorootparent> That&#x27;s bit much of stereotyping, depends I guess - \"Java Concurrency in Practice\" is one of the most sold books (when it comes to Java), and it covers some parts quite well.That&#x27;s because Java concurrency in practice is harder than it should be. I&#x27;ve seen plenty of teams struggling with what should be trivial problems trying to work their way around the various limitations and&#x2F;or bugs in the JVM.> Albeit, they did fail to deliver - very error prone, close to impossible to debug, effectively outclassed by x86-64 and Sun Sparc (both being total store order), even arm-64 became \"stronger\".That&#x27;s true, but at that time those weren&#x27;t shipping 64 bit systems. It was pretty much R4000, Alpha or bust and the Alpha - even taking into account those limitations - worked surprisingly well for real world problems. In fact the systems that I&#x27;m talking about worked non-stop for more than a decade until they got de-commissioned and I never heard a single peep from those that took over the project that the memory model of the Alpha was somehow either a problem in practice or a concern.Could it have been done better: sure, but with the knowledge of the time these seemed to be pretty reasonable choices and compared to what the alternatives were they were doing great. If anything the failure of the Alpha architecture is one of marketing more than anything. Pjlmp has some good information elsewhere in this thread, which pretty much corresponds with my experience of the time. There were some systems on the drawing board that were better in theory and they were still better in theory 10 years later, meanwhile DEC was shipping.That they squandered their head start has nothing to do with the memory model, but everything to do with how they ran their business. reply formerly_proven 9 hours agorootparentprevThe \"oh we can&#x27;t make the cache as big as we&#x27;d like so we just make it two fully independent banks and bake the lack of synchronization of them into the ISA\" in DEC Alpha very clearly falls into the \"baking restrictions of today into the ISA\", which virtually always turns out to be a bad idea. reply jacquesm 8 hours agorootparentThat I&#x27;ll be happy to agree with but compared to the 640 k limit it&#x27;s clearly a manageable one, and one that end users of the system normally do not have to concern themselves with.I&#x27;ve written a (small) OS for x86&#x2F;32 just prior to getting my hands on an Alpha and compared to that the Alpha looked (and still looks) like a model of sanity to me (as does 68xxx). Just the number of tricks required to get an x86 system properly booted up is off the scale, you have to deal with a whole pile of memory insanity including repeated switches between modes (and memory models) in order to get your stuff even loaded. I spent weeks debugging that loader, to the point that I had a reset switch connected to a musical instrument foot pedal so I didn&#x27;t have dive under the desk 10 times per hour to reset the box I was writing this on. If not for DJGPP to validate the 32 bit code ahead of time I doubt I would have been able to bring it up at all (note this was well before VMs became a thing on consumer hardware). replyxxs 10 hours agorootparentprevDEC Alpha had the weakest memory model, which while cool to write for with its all memory barriers&#x2F;concurrency - it&#x27;s quite annoying in the real world, a major reason while DEC failed. reply mschuster91 17 hours agoparentprev> and getting slaughtered in their only remaining stronghold by NVIDIA, AMD and ARM pillaging their castleNVIDIA doesn&#x27;t make good CPUs, even their SoCs are ... not exactly state of the art. No mass market adoption besides automotive who don&#x27;t care about anything but long availability of (spare) parts and the Nintendo Switch which likely only still uses the same 2015-era Tegra chipset because even someone as big as Nintendo couldn&#x27;t kick enough arses at NVIDIA to bring up a new design. ARM doesn&#x27;t make generally available server CPUs (the ones that do exist all get gobbled up by cloud providers), and there are outside of the Mac world no viable ARM desktop or laptop CPUs because Qualcomm completely fucked up that market for likely years to come - I don&#x27;t see any way of ARM adoption in that market unless ARM comes up with a competing solution to Rosetta and Qualcomm comes out of the mindset \"if it works just barely, ship it\" that may be acceptable to smartphone vendors but not the PC&#x2F;desktop market.That leaves AMD as the sole remaining threat to Intel, and AMD doesn&#x27;t have the fab space to be enough of a threat to Intel&#x27;s moat.Yes, I may or may not be extremely frustrated at the state of competition in general computing. reply kristopolous 17 hours agorootparentArm snatched Intel&#x27;s hold on the Apple market and we can invoke any Clayton Christensen book on the Raspberry PI series. Maybe not the Raspberry PI 5, but what about say, the imaginary 8 or the 9 a few years hence?The Pi 400 isn&#x27;t their final attempt into the PC market, only their first. Surely a slew of decent ARM based laptops from one of these SBC manufacturers is coming along eventually - and I don&#x27;t mean chromebooks. reply jacquesm 9 hours agorootparentI have a Pi 400 here and for the money it&#x27;s most impressive. It runs pretty much the whole house in terms of heating and power management, with full autonomy using HA and a very limited bit of custom stuff. You can stick in anything at all and it &#x27;just works&#x27; the only things that have given me a headache are Zigbee dongles, everything else worked without issue. reply mschuster91 17 hours agorootparentprevYeah but that was only made possible by Apple&#x27;s unique circumstances: their close control over the entire tech stack, their experience with architecture transitions (PPC->x86-32->x86-64->ARM) and the required tooling (Rosetta, fat binaries, compilers), their relatively small market size, their expertise in developing with&#x2F;for ARM from iOS, and enough cash in hand to buy out the entire fab capacity of TSMC.In the Windows world, no one holds even closely to the capabilities required for a transition to ARM: Microsoft doesn&#x27;t have a fat binary standard or the toolchains needed (that all went down the drain with the end of Microsoft Windows CE &#x2F; Windows Phone and even then, it was a nightmare to develop for these), the third party developers - especially the enterprise tailor-made application market - have zero experience with ARM and a lot of stuff used in enterprise was made by companies that went defunct long ago, Microsoft doesn&#x27;t have any hold over what the device vendors do in terms of drivers (unlike Apple, who famously cut ties with NVIDIA because NV didn&#x27;t want Apple to write drivers for their GPUs), and neither Microsoft itself nor the conglomerate of hardware OEMs has the cash in hand to take all the stuff people use on x86 Windows and make it work on ARM Windows (as they infamously discovered with the early Windows Qualcomm stuff). Oh, and the ARM vendors can&#x27;t be bothered to get something as basic as PCIe working on a fundamental level beyond \"if it works in my very specific use case, ship it\" - just look at the RPi 4&#x27;s issues where people developed breakout boards for PCIe only to discover that crucial functionality was flat out broken [1]. And even with people complaining for years about PCIe issues on the Pi 4, turns out the Pi 5 still managed to fuck things up [2].The only player in town able to make ARM work on anything but smartphones and servers is Apple, and they don&#x27;t (and never will, assuming regulators don&#x27;t finally wake up and force them) sell to third parties.> The Pi 400 isn&#x27;t their final attempt into the PC market, only their first. Surely a slew of decent ARM based laptops from one of these SBC manufacturers is coming along eventually - and I don&#x27;t mean chromebooks.These things are and will be toys. The money is in getting corporate to switch over to ARM and until the problems above (especially backwards compatibility and standards conformance) are worked out, which I don&#x27;t see happen any time soon because it&#x27;s so hard to break through the chicken-egg scenario, there will be no threat to Intel. Especially not if even many years of development and complaining are not enough to arse Broadcom into fixing PCIe.[1] https:&#x2F;&#x2F;www.jeffgeerling.com&#x2F;blog&#x2F;2023&#x2F;i-built-special-pcie-...[2] https:&#x2F;&#x2F;www.jeffgeerling.com&#x2F;blog&#x2F;2023&#x2F;testing-pcie-on-raspb... reply FirmwareBurner 17 hours agorootparentYou&#x27;re wrong about Microsoft here. They too have loads of experience developing their SW for multiple architectures, even moreso than Apple.Windows NT shipped on about 7 or so architectures including PowerPC for the Xbox360. And they also have experience with emulation, that&#x27;s how they got Xbox 360 emulation on the newer X86 models. Just read their papers on arch emulation.What they don&#x27;t have and Apple has is 10+ experience in shipping tailor made ARM chips fit to their needs, because they always left chip design to their partners, as they were always a SW company first not a HW product company like Apple, and this isn&#x27;t something they can start and catch up with in the snap of a finger. reply hakfoo 16 hours agorootparentThe problem isn&#x27;t even building NT for ARM&#x2F;RISC-V&#x2F;SH-4 or whatever, it&#x27;s being able to reproduce enough of the surrounding universe that Windows on x86-64 has.Apple has more leverage over devs; they can say \"No more x86-64 in N years\" and the developers basically have to move to ARM or abandon MacOS. This bootstraps the market; people who want MacOS have to suck it in and buy ARM because it&#x27;s the only new hardware we&#x27;ll be seeing in the future.Microsoft doesn&#x27;t control the hardware sector to put a hard deadline on new x86-64 products, and it would be suicidal to cut off the x86-64 software support at any time in the near future.This means we&#x27;ll see new x86-64 Windows machines in the store, and all the third-party apps supporting x86-64 Windows, for years to come. So as a consumer, why would I want an ARM-Windows machine? It has little exclusive software, is likely buggier and less mature, and probably runs the vast majority of x86-64 software in an emulation penalty box. reply dboreham 10 hours agorootparentI&#x27;ve been using an ARM Windows machine as a daily driver (when I&#x27;m traveling) for a year. I do that because it has much better battery life than any similar Intel machine, and it has integrated 4G. reply jacquesm 9 hours agorootparentHow is the software support for that? What kind of machine do you have? reply dzonga 16 hours agorootparentprevyeah, I wouldn&#x27;t count microsoft or intel out.arm cpu performance advantages at low power are great. i&#x27;m typing this on an m2.but i&#x27;m sure intel will figure out how to have great cpu&#x27;s at low power draws that perform well. amd already did. so i&#x27;m sure intel will. reply bigstrat2003 15 hours agorootparentprev> Apple has more leverage over devs; they can say \"No more x86-64 in N years\" and the developers basically have to move to ARM or abandon MacOS.I think Microsoft could do the same thing as far as devs go. If Apple can exert that influence over devs with their extreme minority market share, I think MS could too. The problem for MS is their customers. Apple customers will buy whatever Apple puts out, because they&#x27;re extremely loyal to the brand. The same isn&#x27;t true for Microsoft, and I imagine that pressure would cause them to fold on any major changes. reply xxs 10 hours agorootparent>I think Microsoft could do the same thing as far as devs goThat&#x27;s absolutely unrealistic. The main Windows part is the backward compatibility + the corporates. Nowadays Microsoft develops stuff written in Javascript, not their own frameworks, even. reply pjmlp 10 hours agorootparentprevThat is not the issue, rather convincing the Windows developer community to actually care about ARM.Traditionally Microsoft isn&#x27;t like the others (Apple&#x2F;Google), \"take this or go away\", which is why they became so big for enterprises in first place. reply ack_complete 14 hours agorootparentprevThey do have lots of experience porting Windows to multiple platforms. They don&#x27;t have very good experience managing the user experience of Windows transitioning between platforms.I was one of the early adopters of Windows on ARM, the Windows 10 native port to ARM64 (ARMv8). At release, practically the only native development tool was WinDbg -- neither Visual Studio nor Windows Performance Analyzer had been ported. You could install Visual Studio in x86 emulation mode but it wouldn&#x27;t run reliably as the toolchain would keep throwing heap errors, so cross-compilation was required and debugging was harder. There was basically zero information about what was and wasn&#x27;t supported -- you&#x27;d just start porting and run into something like there being no OpenGL acceleration support. Or even more fun, that there was no ARM64 version of the Visual C++ Redistributable published, so you couldn&#x27;t distribute a program that was dynamically linked to the CRT -- and the Visual Studio support staff didn&#x27;t even know what ARM64 was and pointed to the x64 redist. This didn&#x27;t start getting ironed out until around three months after Windows on ARM machines had started shipping.And assuming you got past these problems, Windows has no universal binary system, so it&#x27;s your job to figure out how to properly get the right platform executable installed and launched without any support from the OS. This was really bad in the early days of x64, where XP would just say \"invalid executable\" when trying to launch a x64 program on x86; these days it displays a slightly less cryptic \"Machine Type Mismatch\" error dialog with no further help.Microsoft is trying to fix these problems now, but they&#x27;re years late and the amount of software available as native ARM64 is still very low. Oh, and they already dropped support for the early gen Snapdragon 835 and 850 devices in Windows 11, which means no x64 emulation or ARM64EC support, and an even tinier effective market. In contrast, Apple managed the ARM transition much, much better -- they had native tooling, documentation, and development systems lined up in advance and a much more polished user experience on day one. reply pjmlp 10 hours agorootparentAre you sure regarding ARM64EC? I doubt that they are dropping it any time soon, specially when they just announced the Arm Advisory Service. reply mschuster91 16 hours agorootparentprev> Windows NT shipped on about 7 or so architectures including PowerPC for the Xbox360. And they also have experience with emulation, that&#x27;s how they got Xbox 360 emulation on the newer X86 models.The entire NT stuff has gone down the drain. The last non-x86 platforms were dropped around 2000 [1] until ARM entered the picture in 2012, but the latter was mostly used for Windows Phone for many years which itself got discontinued around 2017.All these many thousand human-years of experience have long ago retired or went to other companies, their institutional knowledge is effectively lost for Microsoft. And that is the problem.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Windows_NT#Supported_platforms reply Kwpolska 10 hours agorootparentWindows 2000 shipped with x86 support only. But just one year later, Windows XP had an Itanic port at launch. reply dboreham 10 hours agorootparentprevCutler is still there :) reply pjmlp 10 hours agorootparentAs per his last interview, nowadays busy porting GNU&#x2F;Linux into XBox running on Azure, for AI workloads when they are idle.Somehow there is a certain irony on that. reply pjmlp 10 hours agorootparentprevMicrosoft has a now an hybrid format for ARM&#x2F;x64, Arm64EC.https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;arm&#x2F;arm64ecBesides Windows NT has been designed and used for multiple architectures for years, Windows CE and Pocket PC were not the only ones with ARM support, Windows IoT and the original Windows 8 WinRT tablets did as well.The biggest issue is lack of incentives, for most business there is no ROI to install ARM compilers alongside x64 for .NET and C++ toolchains, and yet another set of architectures to debug on, and take into consideration.It is as you say, unless they behave like Apple or Google, imposing a transition, most of those companies won&#x27;t care.They have recently putted out a kind of ARM porting help center, but I doubt it will make any impact.https:&#x2F;&#x2F;blogs.windows.com&#x2F;windowsdeveloper&#x2F;2023&#x2F;10&#x2F;16&#x2F;window... reply Someone 6 hours agorootparentprev> The only player in town able to make ARM work on anything but smartphones and servers is Apple, and they don&#x27;t (and never will, assuming regulators don&#x27;t finally wake up and force them) sell to third parties.I think that would be new territory for regulators. Have there ever been examples where they forced a vertical integrator to sell individual parts to other parties (either at mass to other manufacturers or to consumers?) reply fanf2 16 hours agorootparentprev> These things are and will be toys.Heh, https:&#x2F;&#x2F;cdixon.org&#x2F;2010&#x2F;01&#x2F;03&#x2F;the-next-big-thing-will-start-...Â«The reason big new things sneak by incumbents is that the next big thing always starts out being dismissed as a â€œtoy.â€ This is one of the main insights of Clay Christensenâ€™s â€œdisruptive technologyâ€ theory.Â»Tho Raspberry Pi computers are designed to a low price point, so they are aiming for the low end not the high end. But the 5 is about as powerful as the last-generation Intel MacBooks, despite using fairly old Arm cores. I donâ€™t think they can be casually dismissed. reply KerrAvon 14 hours agorootparentI still have an original RPi model B serving as a PiHole for my home network. It never was a toy. reply flykespice 16 hours agorootparentprev> there are outside of the Mac world no viable ARM desktop or laptop CPUs because Qualcomm completely fucked up that market for likely years to comeHuh citation needed? I would like to know more context on that reply mschuster91 6 hours agorootparentQualcomm had an exclusivity deal with Microsoft for years [1], which they then used to deliver absolute crap to customers, which in combination with almost zero software being available on Windows for ARM [2] (even popular software such as Chrome...) led to these things being nice paperweights.[1] https:&#x2F;&#x2F;www.xda-developers.com&#x2F;qualcomm-exclusivity-deal-mic...[2] https:&#x2F;&#x2F;www.digitaltrends.com&#x2F;computing&#x2F;why-windows-on-arm-c... reply awiesenhofer 6 hours agorootparentprev> no viable ARM desktop or laptop CPUs because Qualcomm completely fucked up that market for likely years to comenot a huge fan of Qualcomm but you might be interested in this:\"Qualcomm Snapdragon X Elite Performance Preview: A First Look at Whatâ€™s to Come\" https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;21112&#x2F;qualcomm-snapdragon-x-e... reply mschuster91 6 hours agorootparentI&#x27;ll believe it when I see it having been taken apart by actually independent nerds who look beyond benchmarks and actually take care if the fundamentals work properly.The state of Qualcomm products over the last years has left me with absolutely zero confidence. reply xattt 18 hours agoparentprevWhatâ€™s their hope right now? Some sort of Quark-derived desktop CPU Ã  la Dothan? reply kristopolous 17 hours agorootparentComing out with better chips at lower prices. I know how obvious that sounds but it&#x27;s true.The difficulty is they need to make nearly decade-long bets that are the size of small countries economies due to the complexity of manufacturing and Intel&#x27;s made a few bad ones recently.I don&#x27;t know who to listen to on what chip design will be a market win in 2030 either. AI applications are extremely resource intensive so that will be driving things for a while but how to solve that in an affordable chip created by a reliable efficient manufacturing process is beyond me. This stuff is phenomenally hard.I&#x27;d say the winner is something like an NVIDIA graphics pipeline that is separated into a pile called \"graphics\" and \"ai\" and then has the graphics part gutted for a cheaper AI pipeline which can use system memory as opposed to preciously expensive graphics memory and then gets integrated into their next gen CPUs taking Nvidia out of the loop and dealing a blow to AMD at the same time. They&#x27;d mop the floors with something like that especially if you could just drop it into pytorch and have it work automagically. They could probably then just turn around and license it to ARM.AMD and Nvidia wouldn&#x27;t work together to mount a unified defense because of ATI and this would allow Intel to weasel their way back into the Apple money stream.But I&#x27;m just some unemployed dude typing this on a 4 year old android. Don&#x27;t listen to me. reply comex 16 hours agorootparentIsnâ€™t AI usually highly memory-bandwidth constrained? Designing a new powerful AI chip to rely on slow memory probably isnâ€™t the best strategy. reply kristopolous 15 hours agorootparentI&#x27;d imagine when the AI dust clears there&#x27;s going to be consumer and producer sides of AI and the consumer requirements will be a carveout of the producer.This just appears to be the case with everything else. Making a video game, movie, song, computer program, etc, requires more resources than using one and there was a significant price delta between them for a long time. reply tomcam 17 hours agorootparentprevwouldnâ€™t graphics and AI both pretty much reduce to mad matrix operations? reply kristopolous 17 hours agorootparentThings like hardware raytracing probably aren&#x27;t needed and there&#x27;s shading units, texture mapping, and ROPs. I&#x27;m not a hardware engineer but there&#x27;s probably some way Intel can rejigger its IGT (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Intel_Graphics_Technology) to have the equivalent of Nvidia&#x27;s \"tensor cores\".They have an NVIDIA mainline competitor in their A770 but I think they should be exiting the direct nvidia assault strategy. I really don&#x27;t know how that&#x27;s going to work. They&#x27;re basically a nonplayer https:&#x2F;&#x2F;www.videocardbenchmark.net&#x2F;high_end_gpus.htmlWhat&#x27;s their winning move in that approach? It&#x27;s just money in a volcano. reply monocasa 17 hours agorootparentprevGraphics has more than that between texture pipelines, ROPs, more complex compute dispatch than needed for AI, etc. All of that comes with area and power costs even if you clock gate the blocks you can. reply scrlk 18 hours agorootparentprevDoubling down on becoming a competitive foundry and becoming the western equivalent to TSMC. If their 18A process ships on time (2025), there&#x27;s a chance that they could regain process leadership. reply sweetjuly 8 hours agorootparentI can&#x27;t imagine the US government letting Intel Foundry Services fail. Not so much in the banking \"too big to fail\" sense but a \"national security interest\" sense: not only are semiconductors critical to the domestic economy but having the latest nodes available for military application gives a major leg up. We see this elsewhere too; SkyWater is a kinda terrible fab (their yield for even very old processes is incredibly bad lol) but the DoD is still throwing gobs of money at them to developed their radhard process because they need some domestic, trusted vendor to turn to. IFS might not be competitive but they probably aren&#x27;t going anywhere, even if they fail to deliver anywhere near on time. reply baq 10 hours agorootparentprevFingers crossed but they donâ€™t have a good track record in the last 10 years. The world needs at least two good processes. reply cmrdporcupine 15 hours agorootparentprevImagine they got into making an ARM CPU competitive with Apple&#x27;s? Or a server chip that is better than Ampera&#x27;s, and sold at scale? I think they have the expertise and ability to do it. reply userbinator 15 hours agorootparentA dual-mode ARM-x86 core would be an interesting CPU that they certainly have the skill to create. Thanks to the protected mode descriptor model, imagine having ARM64, ARM32, x86-32, and x86-64 code segments all coexisting in the same system, with no emulation nor virtualisation. There wouldn&#x27;t be any overhead, because all instructions regardless of ISA get translated to uops anyway. They could add a RISC-V front-end too, just for completeness sake. reply mratsim 10 hours agorootparentThat sounds like dev and user experience nightmare. replysombragris 14 hours agoprevI want to stress how important the 386SX was. My dad wanted a PC for me and asked a friend to build a 286 clone. My friend gave us instead a 386SX. \"It&#x27;s about the same price as an 286 but what you&#x27;re getting now is a 32-bit CPU, make no mistake about it\", he said, and he was right. I was able to run Win 3.11 with it. A 32-bit CPU for the price of a 286 and thus quite affordable? That was genius. reply StillBored 14 hours agoprevIts nice that the 286 gets some love here too, that chip was really underrated considering how much better its IPC was vs its predecessors, which is largely the main ding against the 386. Running existing 16 bit code its IPC was basically the same as the 286, and given its initial 12Mhz clock rate, was pretty underwhelming. It wasn&#x27;t until the clock really started to scale and people started using the 32-bit capabilities that it was anything more than an expensive dos&#x2F;286 competitor. reply StillBored 14 hours agoprevI really wish people would stop applying the \"flat 32-bit\" revisionist history to the 386. That wasn&#x27;t its obvious target, but rather picking up the important \"capability\" arch features which were seen as the future before unix&#x2F;c&#x2F;risc&#x2F;single supervisor&#x2F;ideas destroyed the previous 30 years of mainframe&#x2F;minicomputer OS research in things like security.So, what this article fails to really clarify is that the segment registers were now basically \"selector\" indexes into tables with base+length (in either pages or bytes) fields, execution permission controls. And these selectors and the GDT&#x2F;LDT&#x2F;IDT&#x2F;TSS&#x2F;call gates&#x2F;task gates&#x2F;etc were all designed to support OSs with a 4 level permissions hierarchy, user&#x2F;library&#x2F;driver&#x2F;kernel (or similar), passing around access selectors which could do things like enforce the size of data structures, etc. And to support this, they added FS&#x2F;GS so that all the general purpose registers could have their own permissions masks.Pause for a moment and consider that again, Pointers (capabilities, aka selectors) can have not only a base address, but a hardware enforced limit, along with a permissions model that means a function like strcpy() would be incapable of writing to any memory that wasn&#x27;t the target buffer or part of its own scratch space. Languages&#x2F;os&#x27;s could have enforced that called functions were unable to write to the callers stack, or even possibly run in their own completely separate stack. And that is just the beginning.So, here nearly 40 years later the industry is still trying to recover from the mistakes of designing OS&#x27;s and programming languages around flat memory models and simplistic user&#x2F;supervisor permissions models. The 386 provided hardware assistance for writing OS&#x27;s features that to this day aren&#x27;t common.ex: see CHERI. reply bonzini 11 hours agoparent> Pointers (capabilities, aka selectors) can have not only a base address, but a hardware enforced limitThere are only 8k possible pointers in the LDT, plus 8k in the GDT. The x86 segmented model isn&#x27;t really suitable for implementing capabilities. reply StillBored 8 hours agorootparentSure, 40 years later, but for comparison my computer in 1990 had 1MB ram. Its replacement had 8M iirc a year or two later. I remember in the later 1990&#x27;s having a problem with my socket7 computer because the caches couldn&#x27;t physically tag more than 64M of ram, so everything above that was uncached. Linux of the mid 1990&#x27;s would print a half dozen lines when one typed &#x27;ps&#x27;.A limitation of 8 thousand different protection ranges would have been a lot for a program utilizing a few hundred KB of actual data and coming from a system were it was a PITA to access a data structure > 64K. It might not have been enough to do a super fine grained implementation, but it was more than enough for the time period, and had any significant OS&#x27;s used it in a meaningful way I&#x27;m sure it would have been extended when limitations here hit, as was everything else in the following products.Oh, and also one could have reloaded the GDT, or swapped some number of LDTs at some boundary if needed. It wouldn&#x27;t have really been much more disruptive in the 1980s than switching the page tables on task switch, like every modern OS. reply aforwardslash 7 hours agorootparentprev8k LDT descriptors per task. Each LDT table also needed an entry in GDT, limiting the amount of tasks to also another 8k. So 64k descriptors total. Thing is, most ia32 operating systems dont actually use the existing hardware model for multitasking, and instead use simpler approaches that cater to the flat memory model. reply nezirus 10 hours agoparentprevIt been long since I have stopped following grsecurity, but I would not be surprised if segment registers are still used (e.g. Pax UDEREF)https:&#x2F;&#x2F;forums.grsecurity.net&#x2F;viewtopic.php?f=7&t=3046https:&#x2F;&#x2F;pax.grsecurity.net&#x2F;docs&#x2F;PaXTeam-H2HC12-PaX-kernel-se... reply sweetjuly 8 hours agorootparentI would be amazed of PaX still used segment registers seeing as they don&#x27;t really do much in long mode. In fact, they specifically call this out in the description of UDEREF in your second link. reply cturner 7 hours agoprevIntel evolved the x86 line beyond 386, into Pentium Pro and then to amd64. Why did Motorolla not do the same with the 68k?I have seen discussion that treats it as assumed knowledge that 68k was obsolete and needed to be replaced by powerpc. But it seems like conjecture - I have not seem technical arguments, and 68k seems like a cleaner architecture to ride forward than post-286 x86. reply ndsipa_pomu 7 hours agoparentNot enough computers were sold using 68k to provide enough demand for the chips and so it couldn&#x27;t compete with Intel on price and couldn&#x27;t justify spending lots on research and development. Intel had the PC-compatible market and thus a huge demand for its chips. reply xtracto 5 hours agorootparentHere&#x27;s my 3rd world country perspective, from someone growing in the 80s:We never had intel chips. They were expensive. I had a Texas Instruments 286 and a cyrix 486 years later. The computer in my Dad&#x27;s office were all \"pc compatible\" . This was a university (biology department in a God forsaken poor small city in Mexico). Someone got it bought because it was going to \"change the world\" (oh boy). We played TDCGA.EXE and PRINCE.EXE from its 2 51&#x2F;4 floppy drives. No HDD.We heard about apple&#x27;s, commodore&#x27;s and other crazy computers, but they were for \"gringos\" or rich people.At the same time, we shared (pirated) software as if there was no tomorrow. Spread like wildfire. As and played the heck out of ID shareware.Then you had the Mexican middle upper class: whose dad bought Macintosh one of those others. They NOW had to spend all this money to get it to do something (buy software). Nobody was pirating&#x2F;sharing programs, and the PC ones just didn&#x27;t work.So a virtuous cycle continued and we kept buying x86.Great memories! I&#x27;m glad I was part of that dawn of PC. reply tyingq 6 hours agoparentprev>Were there any commercial efforts to build IDE, VESA or PCI systems around a 68k processor?Sort of. There was the VME bus. An attempt to have a standards based bus that would work across vendors, and also for the the 88k cpu. It wasn&#x27;t wildly successful, but was mildly successful. reply hulitu 5 hours agorootparentThere was also Apollo. They had the Domain 3000 series pretty close to PC architecture (ISA bus). Unfortunately, HP ate them. reply commandlinefan 2 hours agoprev> Bob Childs, one of the architects of the 286, worked underground to lay out some ideas of what could be a 32-bit extension to the 286. After about six monthsHow the hell does _that_ happen? I&#x27;ve never had a job in 30 years where I didn&#x27;t have somebody breathing down my neck to produce something tangible every couple of _days_. reply nu11ptr 17 hours agoprevThe 386 was released shortly before I bought my first computer as a teen. At the time, I saw both 286 and 386 PC&#x27;s on the market, but the latter with a large price premium. I wasn&#x27;t sure what the difference was at the time, so I bought the 286 system. Within a couple of years I had learned the difference and had large regret I didn&#x27;t save my money and buy a 386. Shortly after that, I started writing low level assembler including the system bootstrap for a toy OS, which now could not use 32-bit protected mode. In addition, 386-only games started to be released at some point, and so I very much felt left out until I bought my first 486, but that wasn&#x27;t until years later. reply HankB99 16 hours agoprevThe &#x27;386 was the first in that line to support demand paged virtual memory which opened up a lot of things an OS could do. IMO that&#x27;s the most important thing the &#x27;386 provided. My second PC was a &#x27;386 that ran SCO UNIX. (First was an 8080 based Heathkit H-8 that ran CP&#x2F;M.) reply hulitu 9 hours agoprevI&#x27;m shocked nobody mentions it: 386 added the turbo button. reply mkl 5 hours agoparentBecause it&#x27;s not true. I had a 286 with a turbo button. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Turbo_button reply M95D 6 hours agoparentprevI saw a 8088 that had a frequency switch, but it was on the back and not labeled as \"Turbo\". reply mrnage 7 hours agoprevWebsite is down, got a 502; see mirror: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231106023033&#x2F;https:&#x2F;&#x2F;www.xtof.... reply blakespot 16 hours agoprevInterestingly, I&#x27;ve never owned a 386. I was not much of a DOS&#x2F;Win PC person until 1994, when I got a 486 to run NEXTSTEP. I&#x27;ve had an i8088, NEC V20, i80286, i80486, AMD 5x86, P4, and then on with the Mac starting in 2006 with (well, just before) Intel Core microarch. In those early days I was more Amiga, ST, etc. ( https:&#x2F;&#x2F;bytecellar.com&#x2F;the-list&#x2F; )I felt more spiritually connected to the MC68K line back when, for lack of a better term.Amusingly the i386 system I spent the most time with was in college in 1993&#x2F;4 on a Sun 386 tower running SunOS or Solaris in the APCS lab. reply TristanBall 16 hours agoparentWhile I got the 80386 programmers manual as one of my teen birthday presents.. I only ever actually did a little ASM programming but I loved that book anyway and read it a lot.Really annoyed at myself that I got rid of in some fit of \"well, I&#x27;ll never use that again\" cleaning some time.. especially given somehow still have \"Sendmail, edition 2\".I might read the 386 book for nostalgia.. the Sendmail one..well, PTSD isn&#x27;t something you get nostalgic about! reply steve1977 11 hours agorootparentIâ€™ll dream in M4 tonight, thank youâ€¦ reply aap_ 5 hours agoprevI always found it interesting how the 286 and 386 seem to have been designed with Multics in mind. The hardware is not a perfect match, but it&#x27;s weirdly close. reply phendrenad2 17 hours agoprevThe 386 (and actually, the 286) were great designs because they kept backward compatibility. 386 assembly isn&#x27;t fun to write, and memory management with segment registers is gross. But backward compat is worth it. reply doubloon 13 hours agoprevSo many times it is some small team working on a unknown disregarded project, from fallout to the mac intel transition, that a company is saved by. The best companies surely must know this and knowing it, allow these tiny disregarded projects to exist on purpose. reply selimnairb 16 hours agoprevNice write-up. Re-reading about the evolution and complexity of x86 makes me wonder about attempts to modernize x86. Does anyone know how Intelâ€™s x86-S proposal to do a cleaned-up 64-bit architecture has been received? I looked for updates in the media but havenâ€™t been able to find anything. reply lloydatkinson 13 hours agoparentI seem to remember I didnâ€™t think it was radical enough but a good start. reply weinzierl 17 hours agoprevThe 80386 DX was a revolutionary CPU. It certainly foreshadowed the 486 and ultimately the Pentium. Most people I know only had a 80386 SX which was still revolutionary but it hid it well by being essentially a glorified (but slower) 80286 on the outside. reply nu11ptr 17 hours agoparentThe SX may have been slower, but it could still run all 386 software which was a huge advantage over the 286. I had a 16MHz 286 and I so badly wanted a 386SX 16Mhz so I could run 32-bit software. reply MichaelRo 5 hours agorootparentSuperseding a ZX-Spectrum, luckily my first PC was a Siemens-Nixdorf 386-SX @ 16 Mhz, no FPU, with 2 Mb of RAM and a 40 Mb hard drive: https:&#x2F;&#x2F;www.ebay.com&#x2F;itm&#x2F;172038842293I did install Windows on it briefly from what I recall but wasn&#x27;t impressed, there wasn&#x27;t much to do with it. Games would be pure DOS and for programming I&#x27;d use Borland Pascal so again DOS.But as a gaming machine it ran anything I could throw at it at the time, which was 286-games actually. Without realizing I had the absolute best \"286\" machine I could have, for DOS gaming it is apparently much better to play them on a 386: (Why you don&#x27;t want a vintage 286 PC -- but I like mine anyway): https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Htbvm5_NZHc reply icedchai 15 hours agoparentprevThe 386SX could do everything the 386DX did, just slower. My first Linux box was a 386SX machine. Before Linux, I ran Coherent on it: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coherent_(operating_system) reply ianmcgowan 10 hours agorootparentThere are dozens of us! Dozens!I paid $99 for Coherent because Linux didn&#x27;t support the fancy RLL hard drive in my work computer. Eventually Linux caught up (or I got a new computer) and Slackware replaced Coherent (for little things like X11, better networking etc.). Those were the days :) reply weinzierl 9 hours agorootparentprevCoherent sounds really cool, I&#x27;ve never heard of it before. I wish I had known about it back in the day.I had a 386DX, 25 MHz with 4MB and ran Slackware 3.0 with kernel 1.2.13 on it. It worked pretty nicely for me, but I have to say that I spent most of my time on the console. X11 did run but it was too slow to be fun. reply icedchai 3 hours agorootparentCoherent was quite impressive for the early 90&#x27;s! It had some limitations though, the major one being no networking built in. I remember using a 3rd party app (KA9Q, I think?) to give me \"user level\" networking so I could connect to another system over SLIP. I had an early home network built out of serial cables.Once Linux stabilized, the writing was on the wall... reply jacquesm 9 hours agorootparentprevAnother Coherent fan here. That was an awesome system. And for once: awesome documentation. The Coherent book long outlived the Coherent system for me. reply icedchai 3 hours agorootparentI totally agree! I still remember the cover of that book, with the picture of the shell on it. I learned most of the \"POSIX\" APIs from it before I moved to Linux. reply lakkal 2 hours agorootparentI still have all my Coherent manuals and disks. I still referenced the manual for years after I moved on to Linux. replykjs3 17 hours agoprevMaybe just me, but this sounds pretty revisionist wrt &#x27;most important&#x27;. If the 8086&#x2F;8088 hadn&#x27;t stumbled into ubiquity via the IBM PC, there probably never would have been an 80286 much less an 80386. YMMV.That said...the 386 was a world-changing engineering achievement, and as much as I think in a just and fair timeline the 68030 would have taken over the world ( :-) ), you can&#x27;t discount what Intel did. reply atan2 12 hours agoprevI just found some other great articles on this website as well. reply drooopy 4 hours agoparentI came here to say that. Lots of good articles in there on retro systems. reply TMWNN 18 hours agoprevHow different are the instruction sets of the 80486 and Pentium from the 386? Put another way, had the instruction set been frozen as of the 386 (barring any required changes for 64-bit), would we notice any difference in performance today? reply duskwuff 18 hours agoparent80486 and Pentium added relatively few instructions to the core instruction set, but there are a couple of pretty important tools which got added. The ones you&#x27;d miss the most would probably be:* CMPXCHG (486). Central to multiprocessor synchronization and locking.* CPUID (P6). Admittedly, if the instruction set were frozen you wouldn&#x27;t need this... but if not, it&#x27;s how you detect what CPU you&#x27;re running on and what it supports.* RDMSR&#x2F;WRMSR (P6, kernel only). A general-purpose mechanism for adding extra special-purpose registers to the CPU without having to allocate an instruction to each one.* INVD&#x2F;WBINVD&#x2F;INVLPG (486, kernel only). This was the first Intel CPU to support cache; these instructions were used to manage it. reply userbinator 18 hours agorootparentCPUID was available on late-model 486s too. reply epcoa 17 hours agoparentprevYes.For one no one has mentioned that the 386 itself had no on die hardware floating point. That seems like a huge one.Even if only a few instructions were added to the â€œcoreâ€ some are huge like CMPXCHG, CMOV and although not an instruction itself the LOCK prefix.But the extensions are huge. We donâ€™t even still use the floating point instructions of the 386&#x2F;387 era. MMX was pretty lame but SSE and AVX are critical. AES-NI is now necessary for most people with FDE commonplace. reply toast0 17 hours agorootparent> although not an instruction itself the LOCK prefix.From what I can tell, the 386 had the LOCK prefix. Pin 26 (bottom left) is LOCK# driven by the LOCK prefix [1]. But CMPXCHG is very useful and wasn&#x27;t available until 486, and Pentium added some other stuff that&#x27;s important.[1] https:&#x2F;&#x2F;www.eeeguide.com&#x2F;intel-80386-pin-diagram-description... reply userbinator 14 hours agorootparentThe 8086 had the LOCK prefix. Intel was thinking of multiprocessing from the beginning. reply epcoa 17 hours agorootparentprevMy mistake. reply tssva 18 hours agoparentprevThe 486 added XADD, BSWAP, CMPXCHG, INVD, WBINVD, INVLPG to the instruction set.The original Pentiums added CPUID, CMPXCHG8B, RDTSC, RDMSR, WRMSR, RSM to the instruction set.Later Pentiums added the MMX instruction set. reply pkaye 18 hours agoparentprevI think the early Pentiums are pretty close to the instruction set of the 80386. However there were many iterations to the Pentium that added new instruction like the MMX. reply jstanley 18 hours agoparentprevIn specialised applications, you&#x27;d definitely miss AVX and SSE. Beyond that, I&#x27;m not sure. reply giantrobot 17 hours agoparentprevEven besides new instructions the 486 and then Pentium ran existing x86 code faster than the 386 clock for clock. Various new instructions did add capability but just running existing code faster was a huge win on subsequent chips. reply TacticalCoder 16 hours agorootparentYeah. Not many new instructions but many instructions required less cycles per instruction and the 486 also got way bigger caches. And the integrated FPU (IIRC the FPU was an add-on for the 386).Switching from a 386 to a 486 was bringing a huge speedup back then. reply api 18 hours agoparentprevVector instructions are the obvious thing and being massive gains to media, cryptography, math, AI, graphics, and signal processing.Beyond that there have been a few additions like CMOV (conditional move) that would be missed, though instruction fusion in pipelines can sometimes achieve the same speed up.Lastly you would have to add some atomic instructions to support SMP. reply clausecker 17 hours agorootparentCMOV is very important for high-performance programming as it greatly simplifies the design of branchless code. There are workarounds, but they either involve conditional branches (you don&#x27;t want these) or increase the critical path latency significantly (the simplest workaround is to materialise the carry flag using SBB, then use that as a mask). reply outside1234 17 hours agoprevI forgot how powerful the i960 was - and how this demonstrated that despite that - that compatibility was king. reply nine_k 17 hours agoparentThe i960 apparently had enough embedded use, e.g. in printers, switches, terminals. That is, where binary compatibility did not matter much.We under-appreciate how little binary compatibility matters now, so that you can even develop something on an ARM-based machine and then rebuild and safely deploy to an x64-based machine (usually because it&#x27;s Node, JVM, Python, etc). reply shrubble 14 hours agoparentprevThe problem with the i960 was that (according to the last comment on https:&#x2F;&#x2F;www.righto.com&#x2F;2023&#x2F;07&#x2F;the-complex-history-of-intel-... ) the 386 team got more resources and a better process node than the i960, which was produced on a 1.0-micron process, which was already old at that time. reply kjs3 17 hours agoparentprevSee also: Itanium. Volume customers care about software, not hardware. reply exstential 17 hours agoprev [â€“] crazy stuff replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Intel 80386, also known as the 386, used a flat memory space and provided virtual memory, marking it as a significant innovator in the CPU industry.",
      "The 386's launch altered Intel's sales strategy, enabling them to become the exclusive supplier of the CPU and dominate the x86 market.",
      "The 386's Memory Management Unit allowed efficient memory access and modern computing expansion, and its adoption paved the way for advanced developments like Windows and Linux."
    ],
    "commentSummary": [
      "The Intel 80386, or 386, was a major tech milestone due to its transformative influence on the PC market, mainly because of its use of a flat memory space and the provision of virtual memory.",
      "The 386 marked a shift in Intel's sales approach, allowing it to monopolize the CPU supplier market as competitors like IBM and AMD focused on other areas.",
      "The 386's inclusion of a Memory Management Unit (MMU) revolutionized memory access and paved the way for modern computing. Its adoption led to Intel's domination of the x86 market and the subsequent development of platforms like Windows and Linux."
    ],
    "points": 261,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1699224482
  },
  {
    "id": 38161016,
    "title": "Prossimo Unveils First Stable Release of sudo-rs, A More Secure version of Linuxâ€™s sudo Utility, Written in Rust",
    "originLink": "https://www.memorysafety.org/blog/sudo-first-stable-release/",
    "originBody": "Prossimo is pleased to announce the first stable release of sudo-rs, our Rust rewrite of the critical sudo utility. The sudo utility is one of the most common ways for engineers to cross the privacy boundary between user and administrative accounts in the ubiquitous Linux operating system. As such, its security is of the utmost importance. The sudo-rs project improves on the security of the original sudo by: Using a memory safe language (Rust), as it's estimated that one out of three security bugs in the original sudo have been memory management issues Leaving out less commonly used features so as to reduce attack surface Developing an extensive test suite which even managed to find bugs in the original sudo The Wolfi Linux OS already includes sudo-rs and we hope that others will follow their lead. \"When we first set out to build Wolfi, making sure it was memory safe was always a top priority,\" said Dan Lorenc, CEO and Co-founder at Chainguard. \"The sudo utility is a perfect example of a security-critical tool that's both pervasive and under-appreciated. Security improvements to tools like this will have an outsized impact on the entire industry. The work that went into building the first sudo-rs release is a great step forward in eliminating potential security issues by adopting memory safe languages like Rust. This is critical for upholding and maintaining Wolfi as the secure-by-default foundation for developers who want to address most modern supply chain threats.\" A joint team from Tweede Golf and Ferrous Systems built sudo-rs under contract with Prossimo. We're pleased with how much progress they've made since starting this project in December, 2022. An external security audit of the sudo-rs code is scheduled to start in September 2023. After that, the team will start on Milestone 4 of our work plan, which focuses on enterprise features. The original C-based sudo utility has been maintained by Todd C. Miller for many years now, and we're grateful to him for taking on this huge and important task. We're also grateful that Todd has made time to offer us excellent advice on implementing sudo-rs. Prossimo is able to take on the challenging work of rewriting critical components of the Internet thanks to our community of funders from around the world. Weâ€™d like to thank the NLnet Foundation for their funding of the audit of Sudo-rs. We'd also like to thank Amazon Web Services for supporting this work and supporting the transition to memory safe software. ISRG is a 501(c)(3) nonprofit organization that is 100% supported through the generosity of those who share our vision for ubiquitous, open Internet security. If you'd like to support our work, please consider getting involved, donating, or encouraging your company to become a sponsor.",
    "commentLink": "https://news.ycombinator.com/item?id=38161016",
    "commentBody": "The first stable release of a memory safe sudo implementationHacker NewspastloginThe first stable release of a memory safe sudo implementation (memorysafety.org) 235 points by goranmoomin 6 hours ago| hidepastfavorite158 comments coggs 3 hours agoAs one of the original creators of sudo (https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sudo) I&#x27;ve witnessed it getting nearly totally rewritten and then incrementally bug-fixed over the last 43 years. It must take the prize for the UNIX command most highly-scrutinized for security flaws. Flaws which have been identified and fixed.Thousands of developers and security experts have gone over it. So part of me wonders - how is it possible for a single dev team to totally reimplement it without unknowingly introducing at least a bug or two? Is there something to this Rust language which magically eliminates all chances of any bug being introduced? reply jchw 3 hours agoparentAdvanced type systems and borrow checking&#x2F;memory safe languages DO go a long way, but obviously, No. The best developers can do pulling a RiiR is try to follow best practices and learn from past mistakes. We&#x27;ve certainly come a long way in 43 years. Ditching C string handling eliminates a ton of bugs before you factor in the memory safety. Heck, you have to admit: someone setting out to make a secure sudo replacement could do a lot better nowadays even using just C. The OpenBSD project does a pretty good job demonstrating this imo. If you make a programming language that doesn&#x27;t have many of the sharp edges OpenBSD code avoids, you could probably get yourself a head start, but clearly it also is going to take plenty of care and experience too, and a programming language can&#x27;t really grant you that.I think it&#x27;s at least worth humoring. It probably shouldn&#x27;t be shipping as a default any time soon, though... reply sgerenser 1 hour agoparentprevIt can eliminate many bugs, but it certainly wouldnâ€™t eliminate all bugs. During implementation they realized they were not implementing sudoâ€™s (undocumented) feature of failing to run if the sudoers file is world-writable: https:&#x2F;&#x2F;ferrous-systems.com&#x2F;blog&#x2F;testing-sudo-rs&#x2F;.Of course they did find and fix the bug, but in general Rust isnâ€™t going to protect you from bugs like this that are essentially logic errors. reply Calzifer 13 minutes agorootparentThat is documented. Since the mercurial web interface isn&#x27;t very nice to use I picked a random version. sudo 1.8.6 from 2012 writes in the man page \"The sudoers file must not be world-writable,\".https:&#x2F;&#x2F;www.sudo.ws&#x2F;repos&#x2F;sudo&#x2F;file&#x2F;SUDO_1_8_6&#x2F;doc&#x2F;sudoers.m...This is also a very common behaviour for security sensitive applications to check config file permissions. Another example I remember are ssh private keys.I might be to harsh but it is not so trustworthy they still made this error and still miss the documentation. reply jrmg 5 minutes agorootparentIâ€™m not sure why people are downvoting you. I suspect they may be clicking the link and thinking â€˜thatâ€™s not documentation itâ€™s source codeâ€™, not realizing it actually _is_ documentation.The language itâ€™s in is â€˜mdocâ€™ - a markup format for man pages: https:&#x2F;&#x2F;man.freebsd.org&#x2F;cgi&#x2F;man.cgi?mdocItâ€™s the source code for the man page, which is about as documentationey as you can get. reply hoherd 3 hours agoparentprevRust aside, one thing to consider is that a reimplementation of an existing piece of software does offer the benefit of being able to test the old version and the new version side by side for consistent behavior. You could have an entire class of test cases that is just \"do X with the old version, and then do X with the new version, and just make sure the result is the same.\" There is also the entire bug history of the old version that can be investigated during reimplementation. If the old version has specific tests for each resolved bug, those can also be run against the new version to ensure it has consistent behavior.In this case though, it&#x27;s only a partial reimplementation: \"Leaving out less commonly used features so as to reduce attack surface\", which would complicate that approach. reply tptacek 2 hours agoparentprevWhatever else happened in those 43 years, we had a widely-exploitable memory corruption vulnerability (Baron Samedit) as recently as 2021. reply ndr 2 hours agorootparentAnd it looks like it was a buffer overflow:https:&#x2F;&#x2F;blog.qualys.com&#x2F;vulnerabilities-threat-research&#x2F;2021...Would Rust prevent this? reply steveklabnik 1 hour agorootparent> Would Rust prevent this?This is often hard to say.In a very literal sense, you could write this same code, in unsafe Rust, so one could argue that Rust does not prevent it.Some may argue that if this program was written in Rust in the first place, \"concatenate all command line arguments into one big string for processing\" wouldn&#x27;t be the way you&#x27;d go about escaping command line arguments. The issue here is about misplacing a null terminator, Rust strongly prefers a \"start + length\" style of representing strings instead of null terminators, so you&#x27;d never really end up in this situation in Rust in the first place.I&#x27;m sure there&#x27;s other ways to evaluate the situation as well. Which one you find compelling is up to you. reply jvanderbot 1 hour agorootparentprevYes buffer overflows are one of the explicitly addressed vulnerabilities of Rust&#x27;s bounds checker, which is always on, if memory serves. I haven&#x27;t touched Rust in a year. reply slashdev 26 minutes agorootparentprevYes, unless you use unsafe code. reply Hedepig 1 hour agorootparentprevYes it would be prevented by the borrow checker. reply SpaghettiCthulu 39 minutes agorootparentNot by the borrow checker. It would be prevented by bounds checking on slices, arrays, strings, etc. reply jwilk 1 hour agorootparentprevDiscussed on HN:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25919235 (321 comments) reply nolok 3 hours agoparentprevIn memory safety ? Yes, the language is much better at being safe by default. But it does nothing for logics bugs.The thing is, replacing from C (sudo or anything else), the number of exploit due to null pointer or buffer abuse or ... represent easily 50% of it. reply jerf 3 hours agorootparent\"But it does nothing for logics bugs.\"\"Nothing\" is too strong. It does not solve logic bugs, but type systems stronger than C can solve some logic bugs too.Even something as simple as having some concept of \"private\" and \"public\" and some boundaries between them can help. I&#x27;m writing some code right now in Go, hardly a super strong type system, but I&#x27;ve still put some basic barriers in place like, you can have a read-only view of the global state, but the only way to write to it is to a per-user view of that state, and the only way to witness the changes to the underlying value is through one of those per-user write handles. This eliminates a large class of logic errors in which one accidentally reads the original global state when you should be using the per-user modified state or vice versa. This is a rewrite of some older code, and this error is so rampant in that code as to be almost invisible and probably in practice unfixable in the original code. (Which was solved in practice by only every dealing with one user at a time, and if there was multiple users, it simply ran the process completely from scratch once per user. It tried to cache its way out of repetition of the most expensive stuff, but, the cache keys had some of the same conceptual underlying problems, so it just wasn&#x27;t as good as it should be.)You can&#x27;t solve everything this way. Rust&#x27;s stronger type system offers more options, but you can&#x27;t solve everything with that either. But with good use of types, there are still classes of mistakes you can eliminate, and classes of other mistakes you can inhibit.(There are some tradeoffs, though; with bad types you can alse mandate incorrect usage. But I think in the case of something like a sudo replacement we can reasonably assume fairly high skill developers and that there will be a lot of high skill oversight, as evidenced by the fact they&#x27;ve already sought out a third-party security review.) reply timschmidt 3 hours agorootparentprevEnums, Option and Result types, absence of null, not to mention that the type system, borrow checker, and static everything by default, rewards encoding application state and state transitions using all these mechanics, such that they can be verified at compile time. I&#x27;d say the language does quite a lot to address logic bugs as well as memory safety. It can&#x27;t protect a determined developer from themselves, but it provides incredibly useful tools to anyone who can work out how to use them. reply cyber_kinetist 2 hours agorootparentEven if I don&#x27;t like the design of Rust&#x27;s borrow checker I still do appreciate how Enums&#x2F;Option&#x2F;Result types and pattern matching can make your code more robust. Really wish I can bring some of them to C++... I frequently use a poor-man&#x27;s version of Result types with a `TRY()` preprocessor macro, but I&#x27;m often jealous of what Rust has in its toolbelt. reply antoinealb 2 hours agorootparentIsn&#x27;t Rust&#x27;s result type basically the same as Abseil&#x27;s Status, or am I missing something ? https:&#x2F;&#x2F;abseil.io&#x2F;docs&#x2F;cpp&#x2F;guides&#x2F;status reply steveklabnik 2 hours agorootparentGenerally the same idea, yes. Your parent mentioned a key difference though: \"and pattern matching.\" enums in Rust have much stronger language support.But there are also differences, for example, errors must be absl::StatusCode, whereas enums in Rust allow for arbitrary error payloads.Also don&#x27;t discount ecosystem usage: everyone uses Result in Rust, abeseil isn&#x27;t used by most things, and std::expected has its own issues (though I can appreciate how tough making those calls is) and only landed in C++23, so it&#x27;s not as widely used as Result either. reply ajuc 3 hours agorootparentprevStatic-everything is such a gimmick in my opinion. It sounds great until you try to do something useful with your code. It&#x27;s almost never the case that people actually want to hard-code stuff in the source code.Almost always you read configuration files at run-time (like sudo does) and change your behavior depending on run-time information - so you will have run-time errors. reply kstrauser 2 hours agorootparentâ€œStaticâ€ here means that variables are const by default, and you canâ€™t modify one without explicitly marking it as mutable.In your case, a config object would be mutable inside the function that loads it from disk into memory, then read-only everywhere else by default. reply ajuc 39 minutes agorootparentI use rust, and it does have static by default in many places (for example it&#x27;s hard to do the traditional OOP virtual polymorphism or to keep objects of various types in one container) and it makes it pretty hard for me to write \"nice\" looking code.It usually devolves into a lot of nested if-else and switch (match) instructions. reply kstrauser 0 minutes agorootparentI haven&#x27;t run into that so much myself. What I have run into is trying to write C-but-in-Rust, for which the compiler yells at me to please knock it off. It got way easier when I gave up and committed to doing things the Rust way.Not saying you haven&#x27;t done that, just sharing my personal experience with it. GTP 27 minutes agorootparentprevIf your program doesn&#x27;t have a way of reloading its configuration at runtime, then even that first object created by reading the configuration from file can be immutable. reply kstrauser 2 minutes agorootparentYep! What I mean, though, is that the loading function itself will need to mutate the object as it reads settings from disk and updates the in-memory data structure. Once that&#x27;s done, you can pass that around as a read-only object.bunderbunder 2 hours agorootparentprevThis gets said a lot, but I am coming to believe that the case is overstated. For two reasons:1. Valgrind exists. It&#x27;s not perfect, but it does arguably do a pretty good job as long as you&#x27;re writing modern C. The biggest gap I&#x27;m aware of is that it can&#x27;t really help you with global pre-allocated buffers. But I don&#x27;t think that any language or tool can effectively protect you from information leakage if you&#x27;re doing that sort of thing, not even Rust.2. Memory-safe is not the same thing as secure. Programs written in memory-safe languages are rotten with security vulnerabilities, too. Rust&#x27;s happening to be a memory-safe language that doesn&#x27;t use garbage collection does not render it immune to this situation. It has some protections around concurrent usage of data that do add additional safety under certain circumstances (assuming you don&#x27;t switch them off), but I doubt it&#x27;s a panacea. I worry, though, that the Rust community&#x27;s tendency to pitch this stuff as a security panacea could breed a culture of complacency that negates the advantages that Rust does bring to the table for systems programming languages. People tend to take unnecessary risks when they believe they&#x27;re invincible. reply shakow 2 hours agorootparent> Valgrind existsYou may be right on an infinite frictionless plane, but unfortunately that does not work in real life, cf. e.g. https:&#x2F;&#x2F;msrc.microsoft.com&#x2F;blog&#x2F;2019&#x2F;07&#x2F;why-rust-for-safe-sy...> Memory-safe is not the same thing as secure.And safety belts do not help you if your car is on fire, still it&#x27;s better to wear it. reply adgjlsfhk1 47 minutes agorootparentprevThe fundamental problem with valgrind is it only looks at what happened, not what could happen. Valgrind is great at making sure you don&#x27;t have memory safety issues for \"normal\" inputs, but is basically useless at making sure your code doesn&#x27;t have memory safety vulnerabilities when fed atypical inputs. reply lionkor 3 hours agorootparentprevIs that because theyre easy to find, or because theyre the worst? reply tialaramex 2 hours agorootparentA lot of the most serious security vulnerabilities are memory safety because e.g. remote code execution is very often along the lines of \"LOL, I smash buffer with machine code, it gets executed\" and that&#x27;s a memory safety problem.For sudo you have potential for some very serious logic bugs, where the program does exactly what the programmer wrote, but what they wrote was not what they intended.Rust&#x27;s type safety makes it less vulnerable to these mistakes than some languages, but there is no magic. In C obviously a UID, a PID, a duration, an inode number, a file descriptor, a counter are all just integers. In Rust you could make all those distinct types (the \"New type idiom\"), and out of the box the Duration and the File Descriptor are in fact provided as distinct types. So, some improvement. reply Someone 2 hours agorootparent> In C obviously a UID, a PID, a duration, an inode number, a file descriptor, a counter are all just integers. In Rust you could make all those distinct typesFor various kinds of IDs you can do that in C, too: struct UID { int value; };A C compiler can pass these in registers to functions (https:&#x2F;&#x2F;wintermade.it&#x2F;blog&#x2F;posts&#x2F;value-struct.html). So, performance impact should be zero.It may be not as nice as other languages, but it isnâ€™t bad, either. If you use C++, it can be made a bit nicer, and you could also have such structs that you can calculate with. reply yakubin 3 hours agorootparentprevTheyâ€™re easy to make. reply godelski 16 minutes agoparentprevYou gotta start somewhere right? I mean its not like you got it right the first time. Don&#x27;t everyone go switching over just yet, but people can&#x27;t scrutinize something that doesn&#x27;t exist. reply noahjk 3 hours agoparentprevOn the surface, sudo seems fairly straightforward, so itâ€™s interesting to hear how much work has gone into it! Do you have any interesting facts or anecdotes youâ€™d care to share? reply coggs 3 hours agorootparentHackaday interviewed me about the origin story - https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LaAwl3HN5ds&ab_channel=HACKA... reply ralgozino 1 hour agorootparentgreat story! also, TIL that I&#x27;ve been pronouncing `sudo` wrong, I was 100% sure that it was supposed to be like pseudo, but I guess that is a myth :)It&#x27;s so great to be able to listen and learn from the people that invented these important building blocks themselves, I feel lucky. Thanks for sharing. reply nolok 3 hours agorootparentprevThe key is in \"on the surface\". While the common usage of sudo is fairly straightforward, you me and most people use like 5% of it. The trick is in all the side shows. reply yesimahuman 3 hours agorootparentMakes you wonder then why it does so much, if those rarely used features increase the surface area of possible exploits? This is just a question Iâ€™ve had about *nix utilities in general, since sudo is hardly the only tool with obscure flags and features reply yjftsjthsd-h 3 hours agorootparentBecause the long tail of features is useful to someone. Mind, I like doas for this reason, but having the more feature rich option available makes sense. reply __turbobrew__ 1 hour agorootparentprevThis is part of what the openbsd â€˜doasâ€™ was trying to solve. They drastically reduced the functionality to reduce the attack surface. reply folmar 3 hours agorootparentprev> sudo seems fairly straightforward`su` is straightforward, `sudo` is a very powerful piece of software and the configuration has a lot of edge cases. reply SoftTalker 2 hours agorootparentYes, have a read of the sudoers man page and marvel at the complexity of the configuration, and wonder about your chances of getting it right if you are not well-experienced. This is the config file with the infamous paragraph:The sudoers grammar will be described below in Extended Backus-Naur Form (EBNF). Donâ€™t despair if you are unfamiliar with EBNF; it is fairly simple, and the definitions below are annotated.OpenBSD replaced sudo with their own \"doas\" command a few years ago; the doas.conf manual page is about 100 lines; sudoers is over 2,000. reply pohl 3 hours agoparentprevI think a better question might be whether it prevents categories of bugs that are more likely to be exploitable than, say, the logic errors that no language could ever prevent?Also, it sounds like your seasoned eyes would be valuable in reviewing this code. reply 0xbadcafebee 2 hours agoparentprevRecently a Rust sudo replacement (maybe this one?) got a security audit. It not only had the same security vuln that hit the non-Rust version, but the severity was worse in the Rust version. But it wasn&#x27;t a memory safety bug, so... mission accomplished?> Is there something to this Rust language which magically eliminates all chances of any bug being introduced?Nope. It&#x27;s just hipsters who want a new toy to play with. The \"memory safety\" is a total red herring. They probably aren&#x27;t even familiar with writing secure code. Might not even use fuzzing.It&#x27;s ridiculous that it&#x27;s gotten this popular. Like building an entirely new car company around only making side-impact collisions safer. The rest of the car they just... left the same. reply empath-nirvana 3 minutes agorootparent> It not only had the same security vuln that hit the non-Rust versionThe audit of the rust version is how they discovered the vulnerability in the C version. reply steveklabnik 2 hours agorootparentprev> Recently a Rust sudo replacement (maybe this one?) got a security audit.It is the same one. It&#x27;s weird because, this article is from August. But the one you&#x27;re referencing is from three days ago: https:&#x2F;&#x2F;ferrous-systems.com&#x2F;blog&#x2F;sudo-rs-audit&#x2F;> the severity was worse in the Rust version.I am unsure where you got this. It&#x27;s the same vulnerability. reply db-interface 1 hour agorootparentThe link you cite says it was worse in the Rust version:> During the audit, it came to light that the original sudo implementation was also affected by [CLN-001: relative path traversal vulnerability], although with a lower security severity due to their use of the openat function. reply steveklabnik 1 hour agorootparentThank you. I literally re-read it to try and find this, and missed it somehow. Guess I need to drink even more coffee. reply jwilk 1 hour agorootparentprevI don&#x27;t see how openat() would help. reply das_keyboard 1 hour agorootparentprevRe: I am unsure where you got this. It&#x27;s the same vulnerability.> During the audit, it came to light that the original sudo implementation was also affected by this issue, although with a lower security severity due to their use of the openat function. reply jwilk 1 hour agorootparentprevThe audit discussed on HN:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38131442 reply stefs 3 hours agoparentprev> Is there something to this Rust language which magically eliminates all chances of any bug being introduced?no, altough it has features that prevent or reduce the probability of some types of bugs - one example of this being memory safety bugs. rust can&#x27;t prevent logic bugs.the rust reimplementation probably has more bugs than the original, but a theoretically better chance to achieve fewer bugs in the long run.is rewriting mature linux infrastructure in rust a good idea? many people agree that no, it&#x27;s probably not a good idea outside of special use cases. reply grayhatter 49 minutes agoparentprevThank you for working to create one of the tools which is obviously on the list of the most valuable and beneficial to computer security. Perhaps only second to netfilter.And I&#x27;m really sorry so many people have decided they&#x27;re going to imply something is wrong or broken with it for their own clout. Or because they&#x27;ve bought into the lie that no code written in C can be safe or correct.For what it&#x27;s worth, I and all the engineers I willingly associate with (read: the ones who I respect) all have said the exact same thing. Switching to rust here, just &#x27;cause, isn&#x27;t going to meaningfully increase anyone&#x27;s security. But what are you gonna do. Other than ask people to be honest?Annoying fanboys aside... again, *thank you*! The computer security world is meaningfully better because of your work, and that&#x27;s something the RIIR fad will never be able to replace :) reply narinxas 3 hours agoparentprev> Is there something to this Rust language which magically eliminates all chances of any bug being introduced?apperently, yes... and its the type system, but grantedit&#x27;s only &#x27;memory safety&#x27; bugs... the kind of error that C languages are really suceptible to. reply WesolyKubeczek 3 hours agoparentprevWhile I don&#x27;t negate your experience and I genuinely anticipate that this project is going to rediscover some pain, there&#x27;s something to be said about the fact that we don&#x27;t have to replicate the life work of Newton, Leibniz, Maxwell, etc to really \"get\" classical physics. It fits now into the high school curriculum, and if you pass it, you can be fairly decent at it; with a little additional effort, you can get real freaking good at what took those people their whole freaking lifetimes.This is because we can stand on those giants&#x27; shoulders and have the benefit of hindsight and not have to also repeat each and every of their blunders, and have better technology and learning methodology to boot.So I presume if you yourself wanted to rewrite sudo from the first principles, you, with all your experience and knowledge already there, would spend a lot less time doing it, and it would be way cleaner and simpler.So while I&#x27;m not dunking on your effort and experience, I&#x27;m just pointing out that it&#x27;s not impossible to take your experience and turn it into something better over a smaller timespan. reply khimaros 2 hours agoparentprevi wonder what fraction of these fixes came with automated texts to prevent regressions (and to aid new implementations from making the same mistakes). reply lionkor 3 hours agoparentprevNo, but maybe it feels better to know memory bugs aren&#x27;t there (but all others are, and worse than in sudo) reply cyber_kinetist 2 hours agoprevI think the two bullet points they listed for their project (other than using Rust) are often overlooked:- Leaving out less commonly used features so as to reduce attack surface- Developing an extensive test suite which even managed to find bugs in the original sudoWhich are the most important aspects when writing any safety-critical code, even moreso than rewriting in Rust! reply garblegarble 5 hours agoprevI&#x27;m assuming this project&#x27;s aim is to replace sudo, in which case hand-waving away \"Leaving out less commonly used features\" is a bit worrying. What are these features? How uncommonly are they used? In which way will it fail if a configuration uses those features?Edit: Looks like their github readme outlines some of these limitations, https:&#x2F;&#x2F;github.com&#x2F;memorysafety&#x2F;sudo-rs#differences-from-ori... reply hpb42 5 hours agoparentOne of those left out features is `sudoedit` or `sudo -e`. I use this a lot when editing files in &#x2F;etc or any file that my user does not have permissions. The flag first copies the file to a temporary location with permissions for my user to edit, then opens my text editor (defined via $SUDO_EDITOR env var) as _my user_, without any sudo permissions. After I close the editor, the file is copied back with the original permissions only if there were any changes.The cool thing is running the editor via my user, which loads my user&#x27;s configuration&#x2F;plugins, instead of the root user&#x27;s. reply lucideer 4 hours agorootparent> One of those left out featuresThey indicate that many omittted features are by design, but this particular one is implied to be planned:> Some functionality is not yet supported; in particular sudoedit reply josephg 4 hours agorootparentprevThis would be a great use case for a capability security model. Essentially what you really want is the sudo command to acquire a temporary capability token to edit that specific file. Then run your editor and pass it the capability. (And revoke the capability when the editor process closes).Itâ€™s a pity this isnâ€™t more straight forward to implement on Linux. reply quotemstr 4 hours agorootparent> Essentially what you really want is the sudo command to acquire a temporary capability token to edit that specific file.This should be doable with an XDG portal model, right? reply yakubin 3 hours agorootparentIt&#x27;s doable by opening the file in a privileged process (sudo) and passing the file descriptor to a non-privileged process.Maybe one could make a sudoedit that opens a file in sudo process and then spawns a non-privileged editor process which inherits the file descriptor and is given the &#x2F;dev&#x2F;fd&#x2F; path on the command line, so it stays none the wiser about the whole process. reply vlovich123 1 hour agorootparentSounds like a bit of recipe for accidentally handing access to an unintended privileged fd through inheritance (ignoring the &#x2F;dev&#x2F;fd one) such that a compromised unprivileged SUDO_EDITOR value gives you sudo access. Maybe not likely, but Iâ€™d really be hesitant about any feature that relies on implicit fd inheritanceâ€¦ reply aumerle 54 minutes agorootparentprevSo now any program that&#x27;s running as your user, even your browser, can edit any file you edit with sudo. It just has to watch for your editor to quit and win a race with sudo to modify the file before sudo reads it. reply tambourine_man 4 hours agorootparentprevIn Vim::w !sudo tee %Which I map to :w!!Of course, if youâ€™re not using Vim, youâ€™re doing it wrong :) reply phanimahesh 4 hours agorootparentIt is a little less useful if the file is not readable by your user, and once you authenticate anything within your vim can also silently run other sudo commands since on most distros sudo remembers the autnentication for a while.Now that I think of it, not sure how sudoedit behaves wrt this cached auth. reply KMnO4 3 hours agorootparentI think you can use the sudo -k flag to clear the cached auth reply Zardoz84 4 hours agorootparentprevI usully use \"sudo -E EDITOR_OF_MY_CHOICE\" reply liftm 4 hours agorootparentMany editors can execute shell commands, so this isn&#x27;t the same at all. reply Denvercoder9 3 hours agorootparentprevThat makes your editor run as root, which is a bad idea for many reasons (aside from security, any mistake now has the potential mess with the whole system). reply gary_0 5 hours agoparentprevOne feature they didn&#x27;t mention they left out was the ability to run `make me a sandwich` (https:&#x2F;&#x2F;github.com&#x2F;sudo-project&#x2F;sudo&#x2F;blob&#x2F;main&#x2F;Makefile.in#L...) reply queuebert 4 hours agorootparentIs that a Slashdot reference? reply tecleandor 4 hours agorootparentOld XKCD joke (maybe coming from an older joke)...https:&#x2F;&#x2F;xkcd.com&#x2F;149&#x2F; reply gary_0 4 hours agorootparentThat XKCD joke was all over the Internet in 2006. Now get off my lawn. reply tecleandor 3 hours agorootparentWell, that comic is from 2006 so not that far away :P ;) replysigio 4 hours agoparentprevOne of the features I use in some (larger) environments, which isn&#x27;t on the roadmap or implemented is LDAP support in sudo-rs. Using the regular sudo, this allows you to manage the sudo permissions for the entire network from the central LDAP configuration, and even make rules that are time&#x2F;host&#x2F;user&#x2F;command limited in a central location with no chance of simple syntax-errors wiping out your entire configuration, just that single rule is being ignored in this case. reply SonOfLilit 5 hours agoparentprevI&#x27;ve lived through the transition to systemd. I&#x27;m sure sysadmins will be able to manage this one. And I&#x27;m sure great technical documentation exists, this was a PR release, not where I&#x27;d look for a list of missing features. reply jacquesm 5 hours agorootparentI still run into trouble on a fairly regular basis on account of systemd. Especially the log files continue to cause all kinds of issues. reply happymellon 5 hours agorootparentWhat issues do you run into with the log files?I have plenty of ideological problems with the design of the logs, but not actually ran into problems in the real world. reply djbusby 5 hours agorootparentI&#x27;m still mad I can&#x27;t just `tail -f` reply eptcyka 4 hours agorootparentjournalctl -f? reply abofh 4 hours agorootparentprevHave you tried reading them? They&#x27;re often helpful reply fermuch 5 hours agoparentprevIt seems like those changes are noted here: https:&#x2F;&#x2F;github.com&#x2F;memorysafety&#x2F;sudo-rs#differences-from-ori... reply garblegarble 5 hours agorootparentThe text seems to imply that&#x27;s not a list of features not implemented, it&#x27;s a list of features not implemented that don&#x27;t output a clear error reply dtx1 5 hours agoparentprevHaving so many different feeatures in one of the most basic unix tools is much more of a red flag. reply garblegarble 5 hours agorootparentFor sure! But that mistake has already been made, and has been in the wild for years, so removing those features (and proposing yourself as a replacement for the original) is now a breaking change reply mprovost 5 hours agorootparentOpenBSD replaced sudo with doas (with a vastly reduced feature set) several years ago, and without breaking everything. Sure there are use cases where you absolutely need some feature of sudo, but you can always install it. reply garblegarble 5 hours agorootparentThat seems like the Right Way to do it...As annoying as it is to have to update every sudo reference -> doas, it forces you to think about everywhere you&#x27;re using it, rather than waiting to see what breaks and then trying to fix it. reply codetrotter 3 hours agorootparent> As annoying as it is to have to update every sudo reference -> doas, it forces you to think about everywhere you&#x27;re using it, rather than waiting to see what breaks and then trying to fix it.In my scripts I never call sudo or doas. Instead, if the script needs to do something as root, I write the whole script so that it expects to itself be run as root.And then when I want to run my script, I run it as root doas .&#x2F;somescript.zsh reply PrimeMcFly 3 hours agorootparentThat&#x27;s a much worse approach from a security pov. reply codetrotter 3 hours agorootparentNo. Thatâ€™s a blanket statement on your part that you cannot make because you donâ€™t know what my scripts look like, or what commands they call. reply xyzzy_plugh 3 hours agorootparentNo, it&#x27;s never better to run whole scripts as root when root is only required for part of it. Unless every expression in your script requires root, the blanket statement holds.In my experience, and in my own scripts, it is better to explicitly check if you are being run as root, advise against it and exit (with maybe some break glass flags) and invoke sudo when escalated privileges are required. reply PrimeMcFly 3 hours agorootparentprevYes, it&#x27;s a blanket statement, better it&#x27;s an absolute statement because it&#x27;s absolutely true.You&#x27;re taking a shortcut due to convenience and it&#x27;s bad security practice.It&#x27;s that simple. reply josefx 2 hours agorootparentprevI just constantly run as root since there is always a chance that I might need root permissions for something. &#x2F;s reply samus 4 hours agorootparentprevOpenBSD is much more open (pun not intended!) about breaking parts of userspace to push through beneficial changes. After all, they control their own userspace and can fix up most things before they even become an issue. Linux is only the kernel. reply cpach 3 hours agorootparentIn that case, shouldnâ€™t Linux distros be even more free to break thingsâ€¦? In theory they can bundle any userland tools they want.AFAIK sudo isnâ€™t really tightly coupled to the kernel itself. reply samus 3 hours agorootparentThey could, but their users really won&#x27;t like that. They have their workflows that they got used to. In practice it&#x27;s gonna be GNU Coreutils and Glibc and the other usual suspects. If they bundle something more exotic, it better be for a very good reason. For example musl on Alpine or what Android does. reply xorcist 3 hours agorootparentprev> without breaking everythingExcept all exiting use of sudo ...It&#x27;s such an entrenched tool that I&#x27;m sure there a compatible replacement could be useful.Personally I would appreciate someone to take on the mess that is PAM. It was much too complex from the start and it hasn&#x27;t become better over the years. reply Fnoord 4 hours agorootparentprevOpenBSD uses BSD_Auth instead of PAM. So you cannot use your YubiKey with doas via PAM on the Linux ports. At least not in the same way, as they do not support caching it seems. reply PrimeMcFly 3 hours agorootparentprevOpenBSD is mainly used by hobbyists and not sysadmins, which is why there are not complaints about the missing functionality. reply kristjank 2 hours agorootparentOpenBSD is mainly used where other Unices can be used, and provides widely used software like OpenSSH, OpenBGPD and OpenSMTPD. To say that it&#x27;s a hobby project strikes me as very ignorant. That said, it is not very easy to convince the developers that a function is missing because it&#x27;s a pretty opinionated project, and they might not share the user&#x27;s definition of needed functionality. Thankfully, they&#x27;re nowhere near ebassi levels of functionality deletion disorder. reply PrimeMcFly 1 hour agorootparent> OpenBSD is mainly used where other Unices can be used,That&#x27;s a pretty general statement, and I&#x27;d say to that not really. It&#x27;s very much a hobbyist OS. A few people use it at home as firewalls, a few small businesses maybe, but it&#x27;s mostly hobbyists and developers.> To say that it&#x27;s a hobby project strikes me as very ignorant.I mean, I&#x27;ve been familiar with the project for over 20 years, so I don&#x27;t think I&#x27;m ignorant at all. The developers primarily make the OS for themselves and people with the same ideas and priorities.> That said, it is not very easy to convince the developers that a function is missing because it&#x27;s a pretty opinionated project, and they might not share the user&#x27;s definition of needed functionality.Right, the devs prioritize their own needs, and can do so because it&#x27;s a hobbyist OS. reply speed_spread 5 hours agorootparentprevPriorities. If a fundamental security tool&#x27;s design limits it&#x27;s trustworthiness, it greatly reduces it&#x27;s usefulness and \"breaking\" it&#x27;s interface is thus warranted. reply quotemstr 4 hours agoparentprevThere&#x27;s a gulf between being 99% compatible with something and being a 100% drop-in. If the author of program B wants to replace A, he should go the extra mile and implement every feature of A so as to erase technical excuses for stasis. B needs to put aside his ego, swallow his pride, and implement all the features of A, even the ones her personally dislikes, because the effect of doing otherwise will be that B doesn&#x27;t replace A. We have to work backwards from out desired outcomes. reply marcus0x62 3 hours agorootparentPerhaps, but if the goal is security of a critical tool, losing some attack surface (features) if they arenâ€™t widely used is a win. Other projects, like ntpsec[0] have taken this approach with good results. Although, I agree with another commenter in this thread[1] that this effort would have been better directed at something with an inherently small attack surface like doas.0 - https:&#x2F;&#x2F;www.ntpsec.org&#x2F;accomplishments.html1 - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38161016#38162736 reply godshatter 2 hours agorootparentThe features they are leaving out were presumably added for a reason. If someone is on a system that is using sudo-rs as a drop-in replacement (not under their control) and they need to use one of those less widely-used features, how secure is the work-around they have to use instead? I&#x27;m hoping this factored in to their analysis.Sometimes reimplementing something and leaving out lesser-used features to \"reduce the attack surface\" can sound an awful lot like \"let them pound sand\". reply db48x 2 hours agorootparentprevItâ€™s actually mostly just a matter of time and money. They implemented the most commonlyâ€“used features first, because those are the most important ones. The majority of people could probably swap sudo-rs in for their existing sudo implementation today with nothing lost. Others will need one of those â€œenterprise featuresâ€ and so they should either wait or pitch in. reply fdsfsdfdsffgg 5 hours agoprev> Apache-2.0+MIT vs GPL-2.0So, you may get a memory-safe su&#x2F;sudo-rs, but those who distribute it in a binary form won&#x27;t be obliged to show you the source code it was built from (potentially including some modifications). reply alwayslikethis 5 hours agoparentI wanted to write this comment too, as I have a serious concern that the effort to displace GPL tools with Rust rewrites in MIT&#x2F;Apache will one day lead to a proprietary Linux, but the original sudo is not in GPL. It&#x27;s in an ISC&#x2F;MIT style license [1].1. https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply pjmlp 4 hours agorootparentLook at Android, the only GPL thing left is exactly the Linux kernel, and it hasn&#x27;t been yet replaced by Zirkon because reasons.Look at the FOSS UNIX like RTOS, none of them is GPL. reply fdsfsdfdsffgg 5 hours agorootparentprevThere are su and runuser in util-linux (GPL-2.0) [1].[1]: https:&#x2F;&#x2F;github.com&#x2F;util-linux&#x2F;util-linux&#x2F;tree&#x2F;master&#x2F;login-u... reply wiz21c 5 hours agoparentprevMoving away from the GPL is a very bad idea for such critical component. Exactly for the reason you give.(this post is just here to insist on the issue) reply mhh__ 5 hours agorootparentIn general a lot of \"[GNU tool] but in rust\" projects do seem to come very close to violating the GPL, especially in spirit. reply tsimionescu 4 hours agorootparentIn what sense? It is completely in the spirit of the GPL to reimplement a GPL tool from scratch with the same behavior and a different license. After all, that&#x27;s how the free Unixes came about (though admittedly those were BSD licensed typically). reply zorked 3 hours agorootparentKinda? Historically there were indeed concerns about reimplementation and copyright. One of the ways that the GNU Project tried to fight claims was to reimplement the tools using dynamically-allocated memory (instead of Unix&#x27;s traditional fixed-size buffers) to make sure the implementation was sufficiently different. Other ways were making the implementation Posixly correct, adding internationalization or trying to pick different approaches (like using more modern algorithms for sorting). The GNU tools were better, not just direct ports, and that&#x27;s why it was common to install them in systems like Solaris or HP-UX. reply mustache_kimono 31 minutes agorootparent> Kinda? Historically there were indeed concerns about reimplementation and copyright.Free software projects should welcome multiple implementations and interoperability, because these are the mother&#x27;s milk of free software. It&#x27;s frankly incoherent, given values of free software, that a reimplementation of, for example, Unix coreutils (GNU) would find fault with a reimplementation of itself (uutils).Notwithstanding how philosophically incoherent it is, a desire, now that Linux and free software have some market power, to be a bully back, to grasp for monopoly power, to play AT&T, is really distasteful. What&#x27;s exciting about free software is not the artifact, Linux or coreutils or sudo, but that anyone can create new and interesting alternatives. That users get to make choices about which implementation to use. The \"solution\" to an MIT licensed coreutils is a GNU licensed fork which is 10x better. reply iudqnolq 2 hours agorootparentprevArguably writing in Rust will force a similar magnitude difference to those examples. For example, you&#x27;ll probably replace that modern sorting algorithm with a call to .sort reply oynqr 3 hours agorootparentprevPretty sure you&#x27;d have to clean room the whole thing. Which these Rust implementation might not have. This is not legal advice. reply mhh__ 2 hours agorootparentI can&#x27;t remember enough to name names but one obviously didn&#x27;t to the point that it mentioned what the GNU code does in comments. reply mustache_kimono 59 minutes agorootparentI&#x27;m really not certain this is enough to be a copyright problem.For instance, GNU and POSIX both publish their specs for the coreutils. If a coder were to take a look at the actual GNU code (which BTW is published for everyone to see), copyright law has a well trodden distinction between the idea and the expression -- that is, ideas are not copyrightable. If the \"idea\" simply amounts to what would be a more a detailed specification, I&#x27;m not sure there is a problem, like ... GNU uses this kernel facility for X. The problem would be vast amounts of \"expression\", especially \"creative expression\", directly copied and reimplemented in Rust. If the code is meat and potatoes, not 10xer galaxy brain fare (\"I wrote a custom allocator which is suspiciously like the custom allocator implemented by GNU\"), there shouldn&#x27;t be an issue.Think about what copyright to a play, or a novel, or a screenplay is. Now imagine a comment in the text&#x2F;source: \"This is how Toni Morrison did her characterizations in Beloved\". This obviously isn&#x27;t a copyright violation, unless you&#x27;re copying the actual expression or a translation of the actual expression found in Beloved. replynindalf 2 hours agorootparentprevBut it wasnâ€™t GPL to begin with - https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply Nullabillity 5 hours agoparentprevSudo is ISC-ish[0] (very permissive), not GPL. Su does indeed seem to be GPL though.[0]: https:&#x2F;&#x2F;www.sudo.ws&#x2F;about&#x2F;license&#x2F; reply ksherlock 4 hours agorootparentThe GNU ~~coreutils~~ util-linux version of su would be GPL but su is from Unix V1 (1971) so there are AT&T implementations, BSD implementiations, etc. reply fdsfsdfdsffgg 5 hours agorootparentprevThere are su and runuser in util-linux (GPL-2.0) [1].[1]: https:&#x2F;&#x2F;github.com&#x2F;util-linux&#x2F;util-linux&#x2F;tree&#x2F;master&#x2F;login-u... reply zigzag312 5 hours agoparentprevIs that really bad? You are free to not use such distributions.Regarding security, malicious actor could show you a different source code from what he distributes in a binary form. GPL or no GPL. reply pbmonster 4 hours agorootparent> Regarding security, malicious actor could show you a different source code from what he distributes in a binary form.That&#x27;s why hashes are published by distributors and checked by package managers, right? reply zigzag312 2 hours agorootparentThat only checks that binaries are the same as what is published by distributors.It doesn&#x27;t help against a malicious distributor, unless package managers also do a deterministic build themselves and verify that checksum from self-build binary matches the checksum published by a distributor. reply EspressoGPT 5 hours agorootparentprev> Is that really bad? You are free to not use such distributions.It&#x27;s not really bad but it kinda defeats the purpose. reply pjmlp 4 hours agoparentprevFor better or worse the use of GPL is going away, even the future of Linux kernel is not guaranteed.In the realm of IoT FOSS UNIX like operating systems, all the contendants are using a mix of Apache, MIT and BSD licenses, including the ZephyrOS sponsored by the Linux Foundation.When the GPL generation is gone from the face of the Earth, it won&#x27;t last long that UNIX-like OSes get another steward alternative to the Linux kernel, with a more appealing license to big corps. reply NegativeK 1 hour agorootparentHow would they be able to drop the GPL from the Linux kernel? reply pjmlp 31 minutes agorootparentBy replacing it with something else, duh.Linux kernel isn&#x27;t the first nor the last UNIX clone. reply markhahn 14 minutes agoprevPolishing a turd?Seriously, better reimplementations are great. But weren&#x27;t you shocked to read about all those weird sudo features? I mean, the normal stuff is very weird, subtle, and therefore fragile.Anyone who uses sudo \"deeply\" should probably think about whether there are other ways. reply slacka 4 hours agoprevWould be interesting to see a a Debian derivative that combines this with the Rust Implementation Of GNU Coreutils.[1] Could be a big win for memory safety and performance.[1] https:&#x2F;&#x2F;github.com&#x2F;uutils&#x2F;coreutils reply guerrilla 3 hours agoparentI wonder how we are from a Rust UNIX userland. At least we wouldn&#x27;t have to implement a C compiler! reply yjftsjthsd-h 2 hours agorootparentA Linux distro is going to need to see compiler to self-host regardless of the user land. If you can live without Linux, there&#x27;s redox ( https:&#x2F;&#x2F;redox-os.org&#x2F; ) reply guerrilla 44 minutes agorootparentWell, I guess if the build system uses that distro or if it&#x27;s a sourve distro. reply nonameiguess 1 hour agorootparentprevI looked into this a few years back when I was making my own toy Linux distro, and this is the list of packages provided by a typical GNU system that meet POSIX requirements for a userspace:* `bash`* `bc`* `binutils`* `bison`* `Coreutils`* `Diffutils`* `file`* `Findutils`* `flex`* `gawk`* `glibc`* `grep`* `tar`* `gzip`* `M4`* `make`* `man-db`* `man-pages`* `procps-ng`* `psmisc`* `sed`That&#x27;s a reasonable start, but you also need, minimally, something to replace `pciutils`, `IPRoute2`, a bootloader, and an init system. For a close to expected experience, add in `TexInfo`, `XZ`, `ZStd`, and `bzip2`, plus `shadow` if you don&#x27;t want passwords stored in plaintext.POSIX doesn&#x27;t dictate an editor, but you probably want something that can run in a terminal. Usually `cURL` and either `openssl` or `GnuTLS`, plus `bind-utils`, `ldns`, or something equivalent are there for actually using the network, something to replicate `gpg` functionality if you&#x27;re going to install signed packages, and of course the package manager itself. Cargo is fine for Rust app developers, but can&#x27;t replace an installer of system packages. You likely need an `ssh` implementation to replace `OpenSSH`.I&#x27;m sure there&#x27;s more I&#x27;m missing, but this is pretty close to what you&#x27;d get in a minimal server image.If you&#x27;re looking to fully get rid of C and not need a C compiler, though, Linux itself is a hurdle. You don&#x27;t necessarily need a kernel quite as fully-featured, but you need something that at least implements the POSIX system calls. Just about every Linux distro I&#x27;m aware of seems to also provide Python and Perl these days as a whole lot of system utilities and build scripts use them. Presumably, rewriting all of Perl and Python in Rust is not feasible, so you either need some other interpreted scripting language good for system scripting that is written in Rust, or somehow make your shell a superset of POSIX but also much closer to a real programming language.Don&#x27;t underestimate the lift of replacing `libc`, either. It&#x27;s not just the C standard library and interface to system calls. It also provides the linking loader that makes it possible to even run other programs, all of the locales and time zones, the system&#x27;s name server, profiler, memory dumper. A whole lot of stuff. reply brundolf 2 hours agoprevBy now we&#x27;ve gotten several projects that re-implement a core unix util in a safe language, often also with better performance due to concurrency, better standard primitives, etcHave any of these ever been adopted by a distro as the default implementation? Is that something that might happen? I.e. I would never bother to upgrade my sudo command or my grep command, but getting better defaults would be betterObviously they would have to be perfect drop-in equivalents, which some of these projects don&#x27;t try to be, but others of them do reply steveklabnik 2 hours agoparentIn 2021, uutils was far enough along and compatible enough to boot debian. I haven&#x27;t heard of anyone actually moving to them fully yet. reply feldrim 3 hours agoprevI&#x27;d love to see a verification &#x2F; validation parameter&#x2F;flag&#x2F;tool that allows the user to dry-run the current sudo configuration and print out the parts unsupported by sudo-rs.Portability helpers for projects like these enable a frictionless change. reply lambdaone 2 hours agoprevThis is good work, and I&#x27;m not quite sure why people are complaining about it; there clearly won&#x27;t be any replacement of the traditional C sudo by this unless it&#x27;s driven by distros and the community making it happen.Multiple implementations make it much easier to do fuzzing and generate automatic test suites that may be used to improve all the versions of this critical utility. reply thiht 1 hour agoparentAs they say in the article, their test suite was even able to uncover 2 bugs in the original sudo. That&#x27;s definitely a win. reply charcircuit 1 hour agoprevHaving sudo itself makes an OS less secure since malware can use it to easily get root. Sure a rust version may be more secure, but even better would be deleting it entirely. reply lynx23 5 hours agoprevdoas is 43184 bytes, and does everything I need. That sudo exploit was a wakeup call. If you haven&#x27;t moved away from it yet, give opendoas (Debian package) a try. reply hannob 4 hours agoparentDo you use doas on Linux? It is not protected against tty pushback attack: https:&#x2F;&#x2F;github.com&#x2F;Duncaen&#x2F;OpenDoas&#x2F;issues&#x2F;106That&#x27;s a pretty severe unsolved security issue. reply zokier 1 hour agorootparentIt&#x27;s solved; TIOCSTI is disabled by default since Linux 6.2 https:&#x2F;&#x2F;git.kernel.org&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;torvalds&#x2F;lin... reply badrabbit 4 hours agoprevIIRC, all the recent sudo vulns are logic errors, not memory safety. I mean, rewrite away but let&#x27;s not pretend that there couldn&#x27;t be some new bug introduced due to a misunderstanding of how something works or just a plain old mistake. reply alpaca128 3 hours agoparentIn the same way a new memory bug could be introduced to the original sudo. Shrinking the attack surface with static checks seems like a better deal in the long run. reply latexr 3 hours agoparentprev> let&#x27;s not pretend that there couldn&#x27;t be some new bug introduced due to a misunderstanding of how something works or just a plain old mistake.Is anyone doing that? I see a lot of claims of memory safety, but as far as I can see the project isnâ€™t saying other types of bugs are for sure eliminated. reply jackmott 5 hours agoprev [â€“] I remember a couple of years ago a root exploit in Sudo that was the result of failing to check for a sentinel value, thinking â€œthat is a bug that wouldnâ€™t happen in Rust, even though it isnâ€™t related to memory safety!â€Rust enums are sum types, and imho are one of the few unambiguously good language feature ideas. I miss them any time I use a language where they are not built in. F# is another nice language where they are first class and where I first got familiar with them reply Karellen 4 hours agoparent [â€“] > Rust enums are sum types,I wouldn&#x27;t mind so much if they just called them \"sum types\" or \"tagged unions\", or even some other new name. Reusing the existing name \"enum\" from other languages, but differently from the way all those other languages have used it for 45 gorram years, is freaking maddening. reply n_plus_1_acc 4 hours agorootparentThat&#x27;s inherited from OCaml I think.https:&#x2F;&#x2F;www.ocamlwiki.com&#x2F;wiki&#x2F;Enum reply ode 3 hours agorootparentNobody in the OCaml community commonly refers to them as Enum&#x27;s today or any time recently (maybe they were at the time Rust was created though? IDK).They&#x27;re usually called &#x27;variants&#x27;. reply debugnik 2 hours agorootparentprevI don&#x27;t think so, OCaml consistently calls them \"variant types\". I don&#x27;t know who wrote that page, but that wiki didn&#x27;t even exist before September and it isn&#x27;t endorsed by ocaml.org, so I suggest you don&#x27;t consider it authoritative. reply cyber_kinetist 4 hours agorootparentprev [â€“] Swift and Scala also uses the enum keyword to define sum types, and their history goes earlier than Rust, so now you have multiple languages to yell at! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Software company Prossimo has launched the first stable release of sudo-rs, a redrafted version of the common Linux sudo utility, written in the Rust programming language.",
      "The sudo-rs project concentrates on elevating the security of sudo, aiming to mitigate memory-management-related bugs, coupled with a comprehensive test suite for bug detection.",
      "The new sudo-rs is incorporated into the Wolfi Linux OS, with an exterior security audit slated for September 2023. The NLnet Foundation and Amazon Web Services financially backed the project."
    ],
    "commentSummary": [
      "Prossimo, a software company, has rolled out the first stable version of sudo-rs, a revised model of the broadly used Linux sudo tool, implemented in Rust, a programming language.",
      "The sudo-rs project aims to enhance sudo's security, addressing its numerous memory-management related bugs through simplification and a comprehensive test suite that found bugs in the original sudo.",
      "sudo-rs is now part of the Wolfi Linux OS and will undergo an external security review in September 2023. The NLnet Foundation and Amazon Web Services financially backed the project."
    ],
    "points": 235,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1699268304
  },
  {
    "id": 38161452,
    "title": "Introducing Ladder: A New Open-Source and Customizable Alternative to 12ft.io and 1ft.io",
    "originLink": "https://github.com/kubero-dev/ladder",
    "originBody": "Hey thereI made a opensource alternative for these services. Although these workedd very well, I was not so confident what they do. So I made my own and opensourced it.It is written in Golang and is fully customizable.",
    "commentLink": "https://news.ycombinator.com/item?id=38161452",
    "commentBody": "Ladder, open source alternative to 12ft.io and 1ft.ioHacker NewspastloginLadder, open source alternative to 12ft.io and 1ft.io (github.com/kubero-dev) 224 points by 2cpu1container 5 hours ago| hidepastfavorite98 comments Hey thereI made a opensource alternative for these services. Although these workedd very well, I was not so confident what they do. So I made my own and opensourced it.It is written in Golang and is fully customizable. some1else 4 hours agoRelevant: 12ft.io was banned by Vercel, taking down the developer&#x27;s entire account with multiple other hosted projects & domains: https:&#x2F;&#x2F;twitter.com&#x2F;thmsmlr&#x2F;status&#x2F;1718663563353755982Edit: Access to other projects & domains was apparently restored some time after: https:&#x2F;&#x2F;twitter.com&#x2F;thmsmlr&#x2F;status&#x2F;1719480558932148272 reply abofh 4 hours agoparentLovely, the Google classic \"ban the world\" approach -- I&#x27;ve been desperately trying to move my client off of vercel, this might just be the gasoline. reply benjaminwootton 2 hours agorootparentI followed this drama on Twitter. The author was breaking the terms of service and creating DMCA support burden for Vercel. They had proactively been in touch with him a few times to reach a solution.I think itâ€™s quite reasonable that they blocked the account rather than the project. You wouldnâ€™t have got that level of service from big tech. reply ComputerGuru 7 minutes agorootparent> I think itâ€™s quite reasonable that they blocked the account rather than the project.Iâ€™m just responding to your last sentence: why would you go out of your way to say it is reasonable to block the account rather than the project?I can understand locking the account just as the â€œlazy defaultâ€ but I would not call it in any way reasonable - but you did, so Iâ€™m curious.If that is reasonable, what would you consider unreasonable?(Because to me, the obviously reasonable thing to do would be to block the project and not his entire account.) reply threatofrain 3 hours agorootparentprevOne might consider Cloudflare as a very nice competitor to Vercel in terms of DX, although I suspect all companies use a ban the world approach, even banks. reply MaKey 3 hours agorootparentNot the best time to recommend Cloudflare. reply judge2020 2 hours agorootparentMaybe, but they&#x27;ve kept customers informed throughout the entire outage. https:&#x2F;&#x2F;www.cloudflarestatus.com&#x2F;incidents&#x2F;hm7491k53ppg reply JCharante 2 hours agorootparentprevThey just had a big outage. Whatâ€™s the probability of having another so soon? reply orphea 2 hours agorootparentIf outages don&#x27;t depend on each other, the probability is the same. reply explaininjs 1 hour agorootparentWhy would you assume independence? I&#x27;d expect an outage to put people \"on edge\" for a period of time following the outage, during which changes are scrutinized to a higher degree, and&#x2F;or a greater engineering focus&#x2F;budget is dedicated to reliability to reflect the changed business&#x2F;image requirements. replyrgrieselhuber 4 hours agorootparentprevAside from this (which is already very shitty and would cause the same response in me) what are the issues youâ€™re running into with Vercel? reply abofh 3 hours agorootparent- Support is failing us - I want my team to use you for vercel support, but it isn&#x27;t there. - Support is failing our customers - when you fail, I end up reverse-depending your repo to tell us why it&#x27;s failing -- just give us a clear answer, we all move away happy, bullshit and I go to lambda where I just accept it. - EOD: Vercel makes engineers happy to bullshit, but gives operations teams nothing acceptable - I want a deliverable product. reply judge2020 2 hours agorootparentFYI you have to use two line breaks to start a new line with (HN&#x27;s) Markdown. reply Rauchg 1 hour agorootparentprevWould love to dig into your support issues. Let me know: rauchg@vercel.com reply abofh 6 minutes agorootparentAnd if the support team had done so, I&#x27;d have nothing to converse about :)After digging upwards, additional support seems like an option delivered too late, and too outside of &#x27;proper&#x27; channels - if you want a sanitized rant I can probably deliver it tomorrow, but too-little too-late is where vercel has landed in the operations team. reply lxe 1 hour agorootparentprevVery disappointing that this was the path Vercel chose to take. This is something I would expect from Google or Amazon, but not a developer darling like Vercel. Seems like all companies shed their values is service of growth and capitalism at some point or another. A shame. reply canadianfella 3 hours agorootparentprevWhy desperately? reply treyd 2 hours agoparentprevI don&#x27;t know why anyone trusted Vercel in the first place. The vibes of VC money funding an unsustainable offering for a relatively niche market are so strong, it doesn&#x27;t make any sense. reply paulgb 3 hours agoparentprevTaking down all his projects (not just 12ft) is heavy-handed, but otherwise Guillermoâ€™s response in that thread seems pretty reasonable to me:> Hey Thomas. Your paywall-bypassing site broke our ToS and created hundreds of hours of support time spent on all the outreach from the impacted businesses.> Our support team reached out to you on Oct 14th to let you know this was unsustainable and to try to work with you. reply hombre_fatal 3 hours agorootparentThe 12ft guy doesnâ€™t look so great in that thread. He admits to ignoring the email (gosh I was busy mmkay?) and then argues that Vercel is lying about the extra work they had to do. reply flutas 2 hours agorootparent> gosh I was busy mmkay?Mischaracterization much? He was on Vacation.How many of us read every email for personal projects that comes in when you&#x27;re half way across the world and supposed to be relaxing. reply jdminhbg 5 minutes agorootparentI might ignore personal project emails while I&#x27;m on vacation, but I also won&#x27;t complain if one of those emails says my billing method is out of date and I come home and it&#x27;s been turned off. reply paulgb 1 hour agorootparentprevSure, but if you go on vacation and don&#x27;t check your email for two weeks, you can&#x27;t really claim â€œno warningâ€. If two weeks isnâ€™t sufficient notice because of vacation itâ€™s fine to say so, but itâ€™s not the same thing as â€œno warningâ€ just because youâ€™re not checking email. reply stronglikedan 51 minutes agorootparentTo take it even further, if you&#x27;re a one man operation, not checking emails regarding your operation for two weeks is pure negligence, vacation or not. replyalwayslikethis 3 hours agoparentprevShows the importance of controlling your own critical infrastructure, or at least not being dependent for critical functions. Other examples include Github and Discord, both having shown the tendency do arbitarily ban users with little recourse. reply ktpsns 4 hours agoprevI got the feeling that these features should be part of a browser extension the same way as there are AdBlock extensions. I guess the reason it is not is \"personal preference\" of the author, or is there some technical reason? reply sva_ 4 hours agoparent> these features should be part of a browser extensionYou mean like Bypass Paywall Clean?https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-chrome-clean reply Beijinger 1 hour agorootparentDoes not work so well anymore. Better use a bookmarkletjavascript:location.href=&#x27;https:&#x2F;&#x2F;archive.is&#x2F;?run=1&url=%27+encodeURIComponent(documen... reply NelsonMinar 50 minutes agorootparentprevThis works quite well and probably covers 90% of my needs. For the other 10% I still use archive.today or 12ft (RIP).It&#x27;s a shame Google won&#x27;t let this addon be in the store. reply johnmaguire 3 hours agorootparentprevIs there a Firefox version? reply xipho 3 hours agorootparentYou don&#x27;t even need to install it, just add it as an import line to UO, Google how. Game changing. reply xaellison 3 hours agorootparentWhat&#x27;s UO? This would&#x27;ve been a great comment with a little more info :) reply rustyminnow 3 hours agorootparenthttps:&#x2F;&#x2F;ublockorigin.com&#x2F;After install go to \"Filter Lists\" > Import ... > and add the url of the \"list\"... which is actually from a different repo: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-clean-filter...Note: this apparently works for fewer sites than the linked extension. reply sva_ 1 hour agorootparent> Note: this apparently works for fewer sites than the linked extension.Still, this is great to know because it can then be used on Firefox mobile. reply gzer0 2 hours agorootparentprevThank you for the insight. I usually hesitate to install add-ons, but now I can avoid that step entirely based on your advice. reply sva_ 3 hours agorootparentprevhttps:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea...It used to be in mozillas addon store, but they removed it, so have to install via dev mode reply m-p-3 1 hour agorootparentOr you can load this uBO filterlist, which should basically do the same thing as the extension https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-clean-filters&#x2F;-&#x2F;raw&#x2F;main&#x2F;bpc-paywall-filter.txt reply bluish29 1 hour agorootparentThat&#x27;s why I like reading HN comments. Thanks for that filter link reply penguin_booze 1 hour agorootparentprevNo need for dev mode - signed XPIs are avaiable from releases: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea.... reply sva_ 1 hour agorootparentAh you&#x27;re right, I confused it with Chrome replybilekas 4 hours agoparentprevI don&#x27;t know for sure, but I would imagine there are more severe actions taken against circumventing paid material (content behind a paywall) than there is for free content supplemented by advertisements..Edit : The Digital Millennium Copyright Act (DMCA) prohibits circumventing an effective technological means of control that restricts access to a copyrighted work. I guess that would apply here. reply mckirk 4 hours agorootparentGiven how liberally the DMCA is applied, you definitely don&#x27;t want to be on the wrong side of that.I remember some guy that wrote a WoW bot and got sued using the DMCA, with the argument that his bot was circumventing the anti-cheat and the anti-cheat could be seen as a &#x27;mechanism protecting copyrighted material&#x27;, because it was safeguarding access to the game servers, the servers were generating parts of the game world (such as sounds) dynamically, and those were under copyright... Wild stuff. reply kkzz99 38 minutes agorootparentIt happened to Honorbuddy, a very advanced bot for World Of Warcraft made by a German company. The argument in relation to DMCA was that the bot was circumventing warden, the games anti-cheat system. The legal battle was long and they ultimately had to strip many features of the bot, until the company went under. reply judge2020 2 hours agorootparentprevAs far a I know section 1201 has never been prosecuted. Distribution of the copyrighted material is what&#x27;s focused on. reply mckirk 1 hour agorootparentThis seems a good summary of the case I was talking about:https:&#x2F;&#x2F;massivelyop.com&#x2F;2020&#x2F;02&#x2F;28&#x2F;lawful-neutral-cheating-c... reply Aaargh20318 2 hours agorootparentprev> The Digital Millennium Copyright Act (DMCA) prohibits circumventing an effective technological means of control that restricts access to a copyrighted work. I guess that would apply here.It doesn&#x27;t if you&#x27;re not in the US. reply zeusk 1 hour agorootparentKim Dotcom believed so too, didn&#x27;t fare too well. reply nerdbert 2 hours agorootparentprevIsn&#x27;t anything that can be circumvented ineffective?Or, looking at it the other way, if you put a small sticker that says \"do not do X\" and even one person follows that, isn&#x27;t that therefore an \"effective\" method? reply nottheengineer 4 hours agorootparentprevGood old section 1201. The EFF has been fighting it for a while, but hasn&#x27;t had much success unfortunately. reply overtomanu 4 hours agoparentprevthere is below extension for this purpose which I know of, I think there can be many more if we search for themchrome and firefox extension for removing paywall: https:&#x2F;&#x2F;github.com&#x2F;iamadamdev&#x2F;bypass-paywalls-chrome reply user764743 2 hours agorootparentThis extension is asking for a lot of permissions it shouldn&#x27;t ask forIf you want an alternative that only requests permissions for sites with paywalls, this one is better: https:&#x2F;&#x2F;gitlab.com&#x2F;magnolia1234&#x2F;bypass-paywalls-firefox-clea... reply fyzix 3 hours agoprevI&#x27;m very new to this kind of service, but do you have to write your own rulesets for each site you want to bypass? The repo doesn&#x27;t seem to include much... reply 2cpu1container 2 hours agoparentYes, the one i provide is still pretty empty yet. I plan to build one that can be used as a starting point or as a default. reply fader 4 hours agoprevFor folks like me who have no idea what 12ft.io or 1ft.io are, they appear to be services for bypassing paywalls on websites. reply alberto_ol 2 hours agoparentPrevious dicussions of the service on HN:https:&#x2F;&#x2F;hn.algolia.com&#x2F;?q=12ft.io reply 2cpu1container 2 hours agoparentprevThose were Paywall bypassing tools. 12ft.io was shut down one week ago and 1ft.io still works.But I feel a bit unconfident to let someone inject code to sites i view. reply donohoe 2 hours agoprevI use services like this as I often skip news site paywalls because I just can&#x27;t afford, nor is it practical, to have so many subscriptions.That said, I work in news media (and have been involved in building paywalls at different orgs - NYT and New Yorker). I know how money for these directly support journalism - salaries and the costs with associated with any story.If you are skipping paywalls a lot, I would encourage you to pay for a subscription to at least one or two news sites you respect - bonus points if its a small or medium local newsroom that benefits!For me that has been; NYTimes, New Yorker, Wired, Teen Vogue, and my wife&#x27;s hometown paper in Illinois. reply mejthemage 1 hour agoparentThere&#x27;s a huge need for subscription bundles. I&#x27;d gladly pay $20&#x2F;mo for access to a bunch of big names, even if I&#x27;m limited to like 60 articles per month combined across those sources.Instead I just don&#x27;t pay anyone, turn back when I encounter a paywall and look for someone&#x27;s summary if I&#x27;m really interested. reply orpheansodality 28 minutes agorootparentIsnâ€™t that the value-prop of Apple News? reply j-a-a-p 1 hour agoprevIn the README there is a WHY paragraph:> Freedom of information is an essential pillar of democracy and informed decision-making. While media organizations have legitimate financial interests, it is crucial to strike a balance between profitability and the public&#x27;s right to access information. The proliferation of paywalls raises concerns about the erosion of this fundamental freedom, and it is imperative for society to find innovative ways to preserve access to vital information without compromising the sustainability of journalism. reply j-a-a-p 1 hour agoparentFor me this is grotesque. Democracy is in dispair so is journalism. What exactly is this software doing to support journalism or democracy? reply 2cpu1container 35 minutes agorootparentWe live in a world, where we have more misinformation and poor journalism every day, and less money in the pockets of the people to afford paying for good journalism. So this might start a more open discussion on how to finance journalism. And while discussions are still going on, people can inform themselves with good journalism, which supports the democracy. reply pacifika 3 hours agoprevOpen source makes it easy for the cat in the cat mouse game, right? reply lucideer 3 hours agoparentThere&#x27;s no real cat & mouse game here (yet*) - sites don&#x27;t do anything to mitigate this. Sites deliberately make their content available to robots to gain SEO traction: they&#x27;re left with the choice of allowing this kind of bypass or hurting their own SEO.* I say \"yet\" because there could conceivably be ways to mitigate this, but afaik most would involve individual deals&#x2F;contracts between every search engine & every subscription website - Google&#x27;s monopoly simplifies this somewhat, but there&#x27;s not much of an incentive from Google&#x27;s perpsective to facilitate this at any scale. reply tiagod 3 hours agorootparentGoogle publishes IP ranges for GoogleBot. You can also reverse-lookup the request IP address - the resolved domain should in turn resolve to the original address. reply ForkMeOnTinder 2 hours agorootparentDoes anyone else remember 10 years ago when Google would penalize sites for serving different content to GoogleBot than to normal users? Those were the days. reply SigmundurM 4 hours agoprevYou mention 13ft as another open source inspiration. How is Ladder improving on what 13ft does? reply 2cpu1container 4 hours agoparentI did try 13ft. But it misses several points.The ladder applies custom rules to inject code. It basically modifies the origin website to remove the Paywall. It rewrites (most of) the links and assets in the origins HTML to avoid CORS Errors by routing thru the local proxy.The ladder uses Golangs fiber&#x2F;fasthttp, which is significantly faster than Python (biased opinion) .Several small features like basic auth ... reply withinboredom 3 hours agorootparent> The ladder uses Golangs fiber&#x2F;fasthttp, which is significantly faster than PythonI have a feeling that this performance difference is practically imperceptible to regular humans. It&#x27;s like optimizing CPU performance when the bottleneck is the database. reply ComputerGuru 3 minutes agorootparentNot for any publicly hosted instance, itâ€™s not. Weâ€™re not talking about the time it takes to perform one request but the scalability it affords a small vm to handle so many requests in parallel when it is being used by the general public. reply roydivision 4 hours agoprevIs it just me or has 12ft become less and less effective? I rarely get through with it these days. reply user_7832 4 hours agoparentTheir policies have apparentlyâ€¦ changed. They accept donations to not have your website bypassed. Archive.org is much better.Edit: apparently it is down now.402: PAYMENT_REQUIRED Code: DEPLOYMENT_DISABLED ID: fra1::8wkv2-1699275385535-39dedae23d6a reply jdiff 4 hours agorootparentIs it donations they accept or legal threats? reply ProllyInfamous 3 hours agorootparentYes. reply i67vw3 4 hours agorootparentprevArchive.today never fails compared to Archive.org or various browser extensionsTo remove paywalls 12ft settings > privacy and security > DNS over HTTPS > Manage exceptions > Add \"archive.is\", \"archive.ph\", and \"archive.today\" reply snarkyturtle 4 hours agoparentprevBefore they went down it seemed that there were many big publishers who got the owner to disable it for their sites. Either that or the sites learned to actually not send their articles unless the user is logged in (and didn&#x27;t care about googlebot not scanning it).It was just an effective way to get through substack&#x2F;medium in my experience. reply ams92 4 hours agoparentprevIâ€™ve rarely found it to be able to skip a paywall, I gave up after trying a few times. reply gumby 2 hours agoprevThe README says \"The author does not endorse or encourage any unethical or illegal activity.\"Is it actually illegal anywhere to bypass a paywall? reply 2cpu1container 53 minutes agoparentNot sure about the paywalls. But it might be used for \"drive by attacks\" or phishing. reply szaboat 3 hours agoprevNot relevant to the project but I usually check for earlier versions of the paywalled pages in the wayback machine (~75% success). I felt bad using these services (paywall removers), and just feeling a bit better checking in archive.org. reply JustinGoldberg9 2 hours agoprevI still miss outline.comI use txtify.it reply jwmoz 2 hours agoprev12ft was really good! reply 2cpu1container 2 hours agoparentIn deed it was. Sad it&#x27;s gone.One single downside was the intransparency. It was not clear which code was added or removed on the site you where looking at. reply rounakdatta 2 hours agoprevGiven a very different paywall model for Substack, what exactly would work for bypassing their paywalls?Wouldn&#x27;t we always require a paid account to cache the HTML through (the SciHub model)? reply arendtio 1 hour agoprevSounds great, not just for paywalls, but for removing CORS as well:> Remove CORS headers from responses, assets, and images ... reply KoftaBob 2 hours agoprevCreate a browser book mark and set this as the URL of the bookmark:javascript:window.location.href=\"https:&#x2F;&#x2F;archive.is&#x2F;latest&#x2F;\"+location.hrefIt will usually open up the archived version of article without the paywall. reply janejeon 3 hours agoprev [â€“] Really dummy question: how do services like this work? As in, how do they bypass these paywalls?The obvious thing is to mock Googlebot, but site owners can check that the request isn&#x27;t coming from a Google-published IP and see that it&#x27;s a fake, right? reply Fnoord 2 hours agoparentSome possible clues:> https:&#x2F;&#x2F;github.com&#x2F;kubero-dev&#x2F;ladder#environment-variables> USER_AGENT User agent to emulate Mozilla&#x2F;5.0 (compatible; Googlebot&#x2F;2.1; +http:&#x2F;&#x2F;www.google.com&#x2F;bot.html)> X_FORWARDED_FOR IP forwarder address 66.249.66.1> RULESET URL to a ruleset file https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubero-dev&#x2F;ladder&#x2F;main&#x2F;rul... or &#x2F;path&#x2F;to&#x2F;my&#x2F;rules.yaml reply ComputerGuru 1 minute agorootparentI donâ€™t know of any off-the-shelf product that respects X_FORWARDED_FOR unless the current request ip originates from a whitelisted (or lan) address. reply janejeon 1 hour agorootparentprevOh wow... I&#x27;m surprised that&#x27;s enough. When I was researching scraping protection bypass, you had to do some real crazy stuff with the browser instance + using residential IPs at a minimum... reply 2cpu1container 50 minutes agorootparentThats not the full story. It works on many sites, but some (ft.com as an example) have more severe countermeasures to bypass the paywall. Therefore the ladders modifies the served HTML from origin to remove such.Those rules still need to be build up. (by me or the OS-community) reply narinxas 3 hours agoparentprev> site owners can check that the request isn&#x27;t coming from a Google-published IP and see that it&#x27;s a fake, right?just because they can doesn&#x27;t mean they will... also most \"site owners\" are (by this point) a completely different people than \"site operators\" (who I take to be the &#x27;engineers&#x27; who indeed can check this IP things) reply calflegal 3 hours agoparentprev [â€“] related: If this is how they work, why doesn&#x27;t google offer a private service to allow publishers to have content indexed while still protected? reply matsemann 3 hours agorootparent [â€“] It used to be against guidelines to serve different content to google vs what users would see. Not sure if still the case, but I don&#x27;t think it&#x27;s in google&#x27;s interest to give a result that the user actually can&#x27;t access. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has developed an open-source alternative to some services they found lacking in functionality.",
      "The alternative solution is written in Golang, a statically-typed compiled language originally developed at Google.",
      "This new software is fully customizable, allowing users to tailor its behavior to their specific needs."
    ],
    "commentSummary": [
      "The author has created an open-source alternative to certain services due to dissatisfaction with their functionality.",
      "The alternative software was developed using Golang, a statically typed, compiled programming language.",
      "This new alternative is fully customizable, allowing users to modify it to fit their specific needs."
    ],
    "points": 224,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1699272284
  },
  {
    "id": 38160703,
    "title": "FFmpeg Enhances Video Transcoding with Introduction of Multithreaded Pipelines",
    "originLink": "https://twitter.com/FFmpeg/status/1721275669336707152",
    "originBody": "The most powerful video transcoder in the world is getting better with multithreaded transcoding pipelines. Find out more in the patchset below:https://t.co/oiuy2GJ7wqâ€” FFmpeg (@FFmpeg) November 5, 2023",
    "commentLink": "https://news.ycombinator.com/item?id=38160703",
    "commentBody": "FFmpeg is getting better with multithreaded transcoding pipelinesHacker NewspastloginFFmpeg is getting better with multithreaded transcoding pipelines (twitter.com/ffmpeg) 221 points by raybb 7 hours ago| hidepastfavorite56 comments jokoon 8 minutes agoAre there any video editing software that take advantage of ffmpeg? I once thought about making something to draw geometry through SVG and use ffmpeg then, or maybe add some UI or whatever, or just to add text, but I never started.Avidemux feels like it&#x27;s a bit that.Since ffmpeg internals are quite raw and not written to be accessed through a GUI, any video editor based on it would probably be quite clunky and weird and hard to maintain.Maybe an editor that use modules that just build some kind of preview with an command explainer, or some pipeline viewer.ffmpeg is quite powerful, but it&#x27;s a bit stuck because it only works with a command line, which is fine, but I guess it somehow prevents it from being used by some people.I&#x27;ve already written a python script to take a random amount of clips, and build a mosaic with the xstack filter. It was not easy. reply raphaelj 5 hours agoprevAbout 2x faster on my 4-cores ARM server, without any significant parallelism overhead: $ time ffmpeg_threading&#x2F;ffmpeg -i input.mp4 -ar 1000 -vn -acodec flac -f flac -y &#x2F;dev&#x2F;null -hide_banner -loglevel quiet 14.90s user 2.08s system 218% cpu 7.771 total $ time ffmpeg -i input.mp4 -ar 1000 -vn -acodec flac -f flac -y &#x2F;dev&#x2F;null -hide_banner -loglevel quiet 14.05s user 1.80s system 114% cpu 13.841 total reply CrendKing 1 hour agoparentYou are not using hardware acceleration on the decoding side, and removing video output here. I wonder what happens if we use both hardware acceleration on video decoding and encoding, i.e. something like this on NVIDIA card ffmpeg -hwaccel cuda -i $inputFile -codec:a copy -codec:v hevc_nvenc $output reply cm2187 5 hours agoparentprevBut what part gets multi threading? Because the video compression is already multithreaded. Video decompression I am not sure. And I think anything else is fairly small in comparison in term of performance cost. All improvements are welcome but I would expect the impact to be fairly immaterial in practice. reply raphaelj 5 hours agorootparentWell, that&#x27;s the very specific command I&#x27;m using in one of my webapps (https:&#x2F;&#x2F;datethis.app), and it&#x27;s one of the main performance hotspots, so it&#x27;s very *not* immaterial. reply j1elo 4 hours agorootparentVery interesting! I had seen the \"learn more\" video already, but it stayed in a corner of my mind.To compare any given piece of sound with reference sounds for ENF analysis, the references must have been recorded to start with.The fact that a webapp like yours can exist... does it mean that we, indeed, have recordings of electrical hum spanning years and years? Are they freely available, or are they commercial products?It seems so crazy to me that someone decided to put a recorder next to a humming line just to be able to later in the future match the sound with some other recordings... reply raphaelj 4 hours agorootparentFor Europe, there are academic and public organizations that publish these ENF backlog since about 2017.For US, I couldn&#x27;t find any open dataset. For these regions, I&#x27;m basically recording the sound of an A&#x2F;C motor to get the reference data, but I only have a few months of backlog.See here for the coverage of the webapp: https:&#x2F;&#x2F;datethis.app&#x2F;coverage reply garblegarble 2 hours agorootparentI notice in your coverage plot, the UK National Grid data appears to end mid-2023... have they stopped providing this data? reply raphaelj 2 hours agorootparentNo, but they do not provide the data in real time. replytimvdalen 4 hours agorootparentprevWow, I learned something today, did not know this was a thing! reply drewtato 5 hours agorootparentprevThis is removing the video stream (-vn) so that&#x27;s not involved. Not sure which parts are in parallel here, but I&#x27;m guessing decoding and encoding the audio. reply izacus 4 hours agorootparentprevThreading depends on implementation of each encoder&#x2F;decoder - most video encoders and decoders are multithreaded, audio ones not so much. At least that was the state of the world the last time I&#x27;ve looked into ffmpeg internals. reply pjc50 4 hours agorootparentprevMultithreading the filter graph itself at the top level, so \"decode\", \"sample rate convert\", and \"encode\" can be in separate threads. reply jbk 6 hours agoprevIt&#x27;s difficult to understand what this is about without the presentation from Anton, at VideoLAN Dev Days 2023, that you can watch here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Z4DS3jiZhfo&t=1221 reply dylan604 1 hour agoparentMan, i really want to watch this presentation, but the piss poor audio just causes my brain to have a fit. How in today&#x27;s time is this still possible to screw up so badly? reply aidenn0 59 minutes agoparentprevOh, I&#x27;ve run into so many issues related to the \"Extras\" listed on the slide at ~33m into that video. reply gooseus 4 hours agoparentprevNice, great presentation! Curious what he has in mind for the \"dynamic pipelines\" and \"scripting (Lua?)\" he mentions in the \"Future directions\" section. I&#x27;m imagining something more powerful for animating properties? reply gbersac 6 hours agoparentprevI love this guy! He&#x27;s a very talented guy who devoted his life to open source. reply lofaszvanitt 6 hours agorootparentHope he doesn&#x27;t have to beg for donations. reply bobsmooth 6 hours agoparentprevWhat is difficult to understand? Does multithreaded mean something different in the realm of video transcoding? reply t43562 6 hours agorootparentHis work is not primarily about multithreading but about cleaning up ffmpeg to be true to its own architecture so that normal human beings have a chance of being able to maintain it. Things like making data flow one way in a pipeline, separating public and private state and having clearly defined interfaces.Things had got so bad that every change was super difficult to make.Multithreading comes out as a natural benefit of the cleanup. reply pjc50 5 hours agorootparent^ This: I just spent the ~20 minutes necessary to watch that part of the talk at a reasonable 1.5x speed, and that&#x27;s the summary. Ffmpeg was suffering from 20 years of incremental change and lack of cleanup&#x2F;refactoring, and that&#x27;s what he&#x27;s spent two years doing.A couple of great lines including \"my test for deprecating an option is if it&#x27;s been broken for years and nobody is complaining, then definitely nobody is using it\". reply andrewstuart 5 hours agorootparentI often look for obscure options to get specialists tasks done in all sorts of software.Better to fix the option.Just because I didnâ€™t go to the very significant effort of complaining doesnâ€™t mean I didnâ€™t want and burn hours trying to use that option. reply pjc50 5 hours agorootparentThe verdict of the presentation was that many options are (bad) duplicates of the filter graph, and you should configure the software through the filter graph.We saw in openssl what the consequences of never removing any code for decades were. It has a real cost.Always a case-by-case decision, though. reply Zagitta 5 hours agorootparentprevYou can&#x27;t expect open source developers to be omniscient and know you want to use a specific feature if you don&#x27;t communicate that to them. Would you rather have them add telemetry? reply bootloop 6 hours agorootparentprev> \"multi-threading it&#x27;s not really all about just multi-threading - I will say more about that later - but that is the marketable term\"That&#x27;s whats said in the video at least in the first 10 seconds so it might be that multi-threading is just a too trivial term for the work here. (But haven&#x27;t watched the video yet so just an observation.) reply defrost 6 hours agorootparentprevThere are many different types of pipelined processing tasks with many differing kinds of threading approaches, and I guess the video clears up what kinds of approaches work best with transcoding .. reply pjc50 6 hours agoprevTweet just links to http:&#x2F;&#x2F;ffmpeg.org&#x2F;pipermail&#x2F;ffmpeg-devel&#x2F;2023-November&#x2F;31655..... which in turn references code at https:&#x2F;&#x2F;git.khirnov.net&#x2F;libav.git&#x2F;log&#x2F;?h=ffmpeg_threading reply kierank 6 hours agoparentThe tweet links to the mailing list which is the official source of the patchset (you can choose \"Next in Thread\" to continue). reply m3kw9 2 hours agoprevIs it automatic or does one need to do command line parameter gymnastics? reply sharkski 1 hour agoprevAwesome to see improvements to FFmpeg! I&#x27;m hoping to see Dolby AC4 support soon. reply kjuulh 5 hours agoprevVery nice.Hopefully this will make my small transcoding needs faster for plex (as I don&#x27;t have hardware transcoding support on my graphics card) =D reply amelius 4 hours agoprevIsn&#x27;t video transcoding easily parallelizable? I mean just split the video into N equal parts (at keyframes) and divide the work. reply xuhu 2 hours agoparentNot a video encoding expert, but for live streams you can&#x27;t merge the output until you process all the N parts, so you introduce delays. And if any part of the input pipeline, like an overlay containing a logo or text, is generated dynamically i.e. not a static mp4, it basically counts as a live stream. reply bambax 2 hours agorootparentWhy not cut the image in rectangles and process those simultaneously? Wouldn&#x27;t that work for live streams? (There may be artefacts at the seams though?) reply slimscsi 1 hour agorootparentYes, and we do, but that is not the slow part: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Motion_compensationAnd as for seams: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Deblocking_filter reply amelius 2 hours agorootparentprevYes, good point about live streams. reply MaxikCZ 2 hours agoparentprevYou are right, but first you ideally should perform scene change detection (to put keyframes at propper positions), and that alone takes quite some processing. reply angrais 37 minutes agorootparentExactly as some encoders (H264) have keyframes at intervals (e.g., every 30 frames) rather than where the action occurs.As such, they are suboptimal by default if a lot of motion occurs. reply Alifatisk 6 hours agoprevIâ€™ve been looking for more ways to speedup the transcoding process, one solution I found was using gpu acceleration, another was using more threads but its hard to find the optimal amount I should provide. reply marcyb5st 5 hours agoparentCan&#x27;t you just use Hyperparameter Optimization to find the best value? Tools like Sherpa or Scikit-optimize can be used to explore a search space of n-threads&#x2F;types of input&#x2F;CPU type (which might be fixed on your machine). reply corndoge 4 hours agorootparentI don&#x27;t think \"just\" is appropriate here, that makes it sound like this should be a trivial task for anyone while it is not. Using \"just\" like this minimizes work and makes people feel stupid which leads to various negative outcomes.Sorry for lecturing reply pjc50 5 hours agoparentprevFor most workloads, setting the number of threads to the number of vCPUs (i.e. count each hyperthreaded core as 2) works. But GPU acceleration is much better if it&#x27;s available to you. reply angrais 35 minutes agorootparentGPU acceleration may produce worse quality and slightly larger files. So there&#x27;s a trade-off to be had. reply cm2187 4 hours agorootparentprevThough in my tests I found that gpu acceleration of video decoding actually hurts performance. It seems software decoding is faster than hardware for some codecs. Of course not the case for encoding. reply themoonisachees 3 hours agorootparentThat heavily depends on the GPU being used and whether or not it has hardware support for your codec. Maybe your GPU is just old&#x2F;weak compared to your CPU? reply cm2187 1 hour agorootparentThat&#x27;s possible, but if you look at nvidia, the whole range uses the same hardware accelerator, so at most it is a difference in term of chip generation, not so much GPU model.I am not saying GPU hw decoding isn&#x27;t useful, it certainly is in term of power consumption, and the CPU might be better used for something else happening at the same time. But in term of raw throughput it&#x27;s not clear that a GPU beats a recent CPU. replykeepamovin 5 hours agoprev [â€“] Can anyone clarify the licensing requirements of large scale ffmpeg deployments? In what cases are fees required? reply hutzlibu 5 hours agoparent\"FFmpeg is licensed under the GNU Lesser General Public License (LGPL) version 2.1 or later. However, FFmpeg incorporates several optional parts and optimizations that are covered by the GNU General Public License (GPL) version 2 or later. If those parts get used the GPL applies to all of FFmpeg. \"http:&#x2F;&#x2F;ffmpeg.org&#x2F;legal.htmlMeaning it is free, but if you use some modules, you might have problems mixing it with proprietary code. reply bsenftner 5 hours agoparentprev [â€“] The general use case for ffmpeg inside proprietary software is the version of ffmpeg used needs to be statically compiled and linked into the software&#x27;s executable, or it needs to be a separate executable called by the proprietary software. reply Daemon404 4 hours agorootparentYou have that backwards - it must be dynamically linked. Static linking without providing your source would violate the LGPL. reply keepamovin 3 hours agorootparentCan you drill down a bit more into this? I would consider static linking to be including unmodified ffmpeg with my application bundle and calling it from my code (either as a pre-built binary from ffmpeg official or compiled by us for whatever reason, and called either via a code interface or from a child process using a command line interface). Seems bsenftner&#x27;s comment roughly confirms this, tho their original comment does make the distinction between the two modes.What&#x27;s someone to do? reply Daemon404 2 hours agorootparentIt is widely known and accepted that you need to dynamically link to satisfy the LGPL (you can static link if you are willing to provide your object files on request). There is a tl;dr here that isn&#x27;t bad: https:&#x2F;&#x2F;fossa.com&#x2F;blog&#x2F;open-source-software-licenses-101-lgp...But, speciically the bit in the LGPL that matters, is secton 5: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;old-licenses&#x2F;lgpl-2.1.en.html#S... - particularily paragraph 2.As always, IANAL, but I also have worked with a lot of FOSS via lawyers.Also, this is and always has been the view of upstream FFmpeg. (Source: I work on upstream FFmpeg.) reply ta1243 2 hours agorootparentprev> What&#x27;s someone to do?Release your code as GPL reply keepamovin 4 hours agorootparentprev [â€“] Thanks. We&#x27;re doing the second but I heard that at a certain scale you might need to pay fees anyway? reply bsenftner 4 hours agorootparent [â€“] I used to work at an FR video security company, where our product was in a significant percentage of the world&#x27;s airports and high traffic hubs. Statically linked ffmpeg for the win. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FFmpeg, a significant video transcoder globally, has announced enhancements to its services, notably multithreaded transcoding pipelines.",
      "The update was officially announced on November 5, 2023."
    ],
    "commentSummary": [
      "FFmpeg, a leading video transcoder globally, is enhancing its features by introducing multithreaded transcoding pipelines.",
      "This upgrade, officially announced on November 5, 2023, aims to improve its service quality.",
      "A multithreaded transcoding pipeline allows the software to perform multiple transcodings simultaneously, increasing efficiency."
    ],
    "points": 221,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1699265554
  },
  {
    "id": 38155541,
    "title": "Insightful Beginner's Guide to the GNU Debugger Command Line Interface",
    "originLink": "https://beej.us/guide/bggdb/",
    "originBody": "Beej's Quick Guide to GDB Release 2 (2009 Jun 14) Translations: Russian This is a very quick-and-dirty guide meant to get you started with the GNU Debugger, gdb, from the command line in a terminal. Often times gdb is run via an IDE, but many people out there shun IDEs for a variety of reasons, and this tutorial is for you! Again, this is only a getting-started guide. There's much much MUCH more to learn about what the debugger does than is written in these few short paragraphs. Check out your \"man\" pages or the online resources listed below for more info. This tutorial is meant to be read in order, up to, but not including, the \"Misc\" section. Contents Compiling to use a debugger More Information License Starting gdb and getting to main() Breakpoints Stepping Around Examining Variables Misc Stuff Stack Manipulation Additional Stepping Methods Jumping to an Arbitrary Section of Code Changing Variables and Values at Runtime Hardware Watchpoints Attach to a Running Process Using Coredumps for Postmortem Analysis Window Functions Display Registers and Assembly Writing a Front-End Quick Reference Cheat Sheet Compiling You have to tell your compiler to compile your code with symbolic debugging information included. Here's how to do it with gcc, with the -g switch: $ gcc -g hello.c -o hello $ g++ -g hello.cpp -o hello Once you've done that, you should be able to view program listings in the debugger. More Information Check out the Official GDB Documentation for more information than you can shake a stick at! Also, a good GNU GDB front-end is DDD, the DataDisplayDebugger. License Beej's Quick Guide to GDB by Brian \"Beej Jorgensen\" Hall is licensed under a Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License. Starting The Debugger First things first: you can enter help at any gdb prompt and get more information. Also, you can enter quit to quit the debugger. Finally, just hitting RETURN will repeat the last command entered. Now let's fire it up! There are several ways to start the debugger (e.g. if you were an IDE you might start it with a particular mode that's not so human-friendly) but I'll mention two of them here: vanilla console mode and curses GUI mode. The GUI is better, but let's quickly cover the simple one, and launch a program called hello in the debugger: $ gdb hello GNU gdb 6.8 Copyright (C) 2008 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or laterThis is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"i486-slackware-linux\"... (gdb) run Starting program: /home/beej/hello Hello, world! Program exited normally. (gdb) The last line is the gdb prompt, waiting for you to tell it what to do. Type r or run to run the program. (gdb allows you to abbreviate commands until they become ambiguous.) To start in neato and highly-recommended GUI mode, start the debugger with gdb -tui. (For many of the examples, below, I show the output of gdb's dumb terminal mode, but in real life I use TUI mode exclusively.) And here is a screenshot of what you'll see, approximately: All the normal gdb commands will work in GUI mode, and additionally the arrow keys and pgup/pgdown keys will scroll the source window (when it has focus, which it does by default). Also, you can change which file or function is displayed in the source window by giving the command list with a location as an argument, for example, \"list hello.c:5 to bring up the file hello.c on line 5. (See \"Breakpoints\", below, for sample locationsâ€”the same locations that work with breakpoints will work with the list command.) As a side note, list also works in dumb terminal mode. Now, notice that we passed the name of the executable on the command line. Another option you have is to just start gdb with nothing else on the command line, then give it the command file hello, and that will cause the executable \"hello\" to be loaded up. Command line arguments! What if you have to get something into argv in your program? Pass them as arguments to the run command when you start execution: $ gdb hello GNU gdb 6.8 Copyright (C) 2008 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or laterThis is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"i486-slackware-linux\"... (gdb) run arg1 arg2 Starting program: /home/beej/hello arg1 arg2 Hello, world! Program exited normally. (gdb) Notice where it says \"Starting Program\", above, it shows the arguments \"arg1\" and \"arg2\" being passed to \"hello\". Breakpoints Just starting the debugger to run the program straight through isn't very usefulâ€”we need to stop execution and get into stepping mode. First, before you issue the run command, you need to set a breakpoint someplace you'd like to stop. You use the break or b command, and specify a location, which can be a function name, a line number, or a source file and line number. These are examples of locations, which are used by various other commands as well as break: break main Break at the beginning of the main() function break 5 Break at line 5 of the current file break hello.c:5 Break at line 5 of hello.c So for this test, let's set a breakpoint at main(), and start the program: $ gdb hello GNU gdb 6.8 Copyright (C) 2008 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or laterThis is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"i486-slackware-linux\"... (gdb) b main Breakpoint 1 at 0x8048395: file hello.c, line 5. (gdb) r Starting program: /home/beej/hello Breakpoint 1, main () at hello.c:5 5printf(\"Hello, world!\\n\"); (gdb) As you see, we've arrived at main() and execution has stopped at the breakpoint we set there. If you're running in dumb terminal mode, gdb will print the line it will execute next. If you're running in cool GUI mode, the line it will execute next will be highlighted in the source window. To list the current breakpoints, use the info command, like so: \"info breakpoints\" (or the shorter \"i b\"): (gdb) b main Breakpoint 1 at 0x8048395: file hello.c, line 5. (gdb) i b Num Type Disp Enb Address What 1 breakpoint keep y 0x08048395 in main at hello.c:5 To clear a breakpoint, use the clear command with the breakpoint location. You can also clear a breakpoint by number with the delete command. Additionally, you can enable or disable breakpoints, though these two commands take a breakpoint number as an argument, not a location! The enabled/disabled status of a breakpoint is visible under the \"Enb\" column in the breakpoint listing. (gdb) i b Num Type Disp Enb Address What 1 breakpoint keep y 0x08048395 in main at hello.c:5 (gdb) disable 1 (gdb) i b Num Type Disp Enb Address What 1 breakpoint keep n 0x08048395 in main at hello.c:5 (gdb) clear main Deleted breakpoint 1 (gdb) i b No breakpoints or watchpoints. Stepping Around Once execution stops at a breakpoint, you can tell the debugger to do a few things. Let's start with the next command (or n). This command moves you to the next statement in the current function (or returns to the function's caller if you've stepped off the end of the function.) Here's a sample run; remember that gdb is printing the line it will execute next just before the \"(gdb)\" prompt. Also notice that when we run next on the printf() line, we see the output appear. (gdb) b main Breakpoint 1 at 0x8048395: file hello.c, line 5. (gdb) r Starting program: /home/beej/hello Breakpoint 1, main () at hello.c:5 5printf(\"Hello, world!\\n\"); (gdb) next Hello, world! 7return 0; (gdb) next 8 } (gdb) next 0xb7d6c6a5 in __libc_start_main () from /lib/libc.so.6 (gdb) next Single stepping until exit from function __libc_start_main, which has no line number information. Program exited normally. (gdb) (That weird stuff at the end about __libc_start_main() shows you that there was another function that called your main() function! It wasn't compiled with debugging information so we can't see the source, but we can still step through itâ€”which we doâ€”and the program exits normally.) Now, notice that next steps over function calls. This doesn't mean that function doesn't get called; it means that next will execute the function until it's done, and then return you to the next line in your current function. What if you have a function you want to step into from your current function, and trace through that function line-by-line? Use the step (or s) command to do this. It works just like next, except it steps into functions. Let's say you're tired of single stepping, and just want the program to run again. Use the continue (or c) command to continue execution. What if the program is running but you forgot to set breakpoints? You can hit CTRL-C and that'll stop the program wherever it happens to be and return you to a \"(gdb)\" prompt. At that point, you could set up a proper breakpoint somewhere and continue to that breakpoint. One final shortcut is that just hitting RETURN will repeat the last command entered; this will save you typing next over and over again. Examining Variables If you have some variables you wish to inspect over the course of the run, you can display them, but only if the variable is currently in scope. Each time you step the code, the value of the variable will be displayed (if it's in scope). (The following output is missing source code output between lines for clarityâ€”it's what you'd see in GUI mode. Imagine you're seeing the highlight bar bouncing around the source code while you're running this:) (gdb) b main Breakpoint 1 at 0x8048365: file hello.c, line 5. (gdb) r Starting program: /home/beej/hello Breakpoint 1, main () at hello.c:5 (gdb) disp i 1: i = -1207447872 (gdb) next 1: i = 1 (gdb) next 1: i = 1 (gdb) next 1: i = 2 (gdb) next 1: i = 2 (gdb) next 1: i = 4 (gdb) next 1: i = 4 (gdb) next 1: i = 4 (gdb) The number to the left of \"i\", above, is the display number of the variable. Use this number to undisplay the variable. If you forget the display numbers, you can type info display to get them: (gdb) b main Breakpoint 1 at 0x8048365: file hello.c, line 5. (gdb) r Starting program: /home/beej/hello Breakpoint 1, main () at hello.c:5 (gdb) display i 1: i = -1207447872 (gdb) info display Auto-display expressions now in effect: Num Enb Expression 1: y i (gdb) undisplay 1 (gdb) If you just want to one-off know the value of a variable, you can print it. Here we see the value of \"i\" is 40: (gdb) print i $1 = 40 (gdb) (The \"$\" with the number after it means something, but it's not important for beginners.) There's also a handy printf command that you can use to better format your output if you want to: (gdb) printf \"%d\\n\", i 40 (gdb) printf \"%08X\\n\", i 00000028 (gdb) Misc Stuff This is stuff that doesn't really fit in the earlier sections, but it fun enough to list somewhere. Stack Manipulation The command backtrace (or bt) will show you the current function call stack, with the current function at the top, and the callers in order beneath it: (gdb) backtrace #0 subsubfunction () at hello.c:5 #1 0x080483a7 in subfunction () at hello.c:10 #2 0x080483cf in main () at hello.c:16 (gdb) Type help stack for more info on what you can do with this. Additional Stepping Methods To exit the current function and return to the calling function, use the finish command. To step for a single assembly instruction, use the stepi command. To continue to a specific location, use the advance command, specifying a location like those shown in the \"Breakpoints\" section, above. Here's an example which advances from the current location until the function subsubfunction() is called: Breakpoint 1, main () at hello.c:15 15printf(\"Hello, world!\\n\"); (gdb) advance subsubfunction Hello, world! subsubfunction () at hello.c:5 5printf(\"Deepest!\\n\"); (gdb) advance is just shorthand for \"continue to this temporary breakpoint.\" Jumping to an Arbitrary Section of Code The jump command works exactly like continue, except it takes a location to jump to as an argument. (See the the \"Breakpoints\" section, above, for more information on locations.) If you need to stop at the jump destination, set a breakpoint there first. Changing Variables and Values at Runtime You can use the set variable command with an expression to evaluate, and this allows you to change the value of a variable during the run. You can also shorthand this by just using set with a parenthesized expression after it: Breakpoint 1, main () at hello.c:15 15int i = 10; (gdb) print i $1 = -1208234304 (gdb) set (i = 20) (gdb) print i $2 = 20 (gdb) set variable i = 40 (gdb) print i $3 = 40 (gdb) This, along with the jump command, can help you repeat sections of code without restarting the program. Hardware Watchpoints Hardware watchpoints are special breakpoints that will trigger whenever an expression changes. Often you just want to know when a variable changes (is written to), and for that you can use the watch command: Breakpoint 1, main () at hello.c:5 5int i = 1; (gdb) watch i Hardware watchpoint 2: i (gdb) continue Continuing. Hardware watchpoint 2: i Old value = -1208361280 New value = 2 main () at hello.c:7 7while (i10, but have had mixed results. You can get a list of watch points with info break or info watch, and you can delete them by number with the delete command. Finally, you can use rwatch to detect when a variable is read, and you can use awatch to detect when a variable is either read or written. Attach to a Running Process If your program is already going and you want to stop it and debug, first you'll need the process ID (PID), which will be a number. (Get it from Unix's ps command.) Then you'll use the attach command with the PID to attach to (and break) the running program. For this, you can just start gdb with no arguments. In the following complete run, you'll notice a few things. First I attach to the running process, and it tells me it's in some function deep down called __nanosleep_nocancel(), which isn't too surprising since I called sleep() in my code. Indeed, asking for a backtrace shows exactly this call stack. So I say finish a couple times to get back up to main(). $ gdb GNU gdb 6.8 Copyright (C) 2008 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or laterThis is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"i486-slackware-linux\". (gdb) attach 3490 Attaching to process 3490 Reading symbols from /home/beej/hello...done. Reading symbols from /lib/libsafe.so.2...done. Loaded symbols for /lib/libsafe.so.2 Reading symbols from /lib/libc.so.6...done. Loaded symbols for /lib/libc.so.6 Reading symbols from /lib/libdl.so.2...done. Loaded symbols for /lib/libdl.so.2 Reading symbols from /lib/ld-linux.so.2...done. Loaded symbols for /lib/ld-linux.so.2 0xb7eab21b in __nanosleep_nocancel () from /lib/libc.so.6 (gdb) backtrace #0 0xb7eab21b in __nanosleep_nocancel () from /lib/libc.so.6 #1 0xb7eab05f in sleep () from /lib/libc.so.6 #2 0x080483ab in main () at hello.c:10 (gdb) finish Run till exit from #0 0xb7eab21b in __nanosleep_nocancel () from /lib/libc.so.6 0xb7eab05f in sleep () from /lib/libc.so.6 (gdb) finish Run till exit from #0 0xb7eab05f in sleep () from /lib/libc.so.6 0x080483ab in main () at hello.c:10 10 sleep(1); (gdb) list 5 { 6int i = 1; 78while (i .) (gdb) info win SRC (36 lines)CMD (18 lines) (gdb) fs next Focus set to CMD window. (gdb) info win SRC (36 lines) CMD (18 lines)(gdb) fs SRC Focus set to SRC window. (gdb) (Window names are case in-sensitive.) The winheight (or wh) command sets the height of a particular window, but I've had bad luck with this working well. Display Registers and Assembly In TUI mode, the layout command controls which windows you see. Additionally, the tui reg allows control of the register window, and will open it if it's not already open. The commands are: layout src Standard layoutâ€”source on top, command window on the bottom layout asm Just like the \"src\" layout, except it's an assembly window on top layout split Three windows: source on top, assembly in the middle, and command at the bottom layout reg Opens the register window on top of either source or assembly, whichever was opened last tui reg general Show the general registers tui reg float Show the floating point registers tui reg system Show the \"system\" registers tui reg next Show the next page of registersâ€”this is important because there might be pages of registers that aren't in the \"general\", \"float\", or \"system\" sets Here's a nifty screenshot to whet your appetite, showing source and assembly in \"split\" mode: Assembly code comes in two flavors on Intel machines: Intel and AT&T. You can set which one appears in the disassembly window with set disassembly-flavor. Valid values are \"intel\" and \"att\". If you already have the assembly window open, you'll have to close it and reopen it (layout src followed by layout split, for example.) To display registers in dumb terminal mode, type info registers for the integer registers, or info all-registers for everything. Writing a Front-End You're thinking, \"Wow, this is pretty cool, but I could write a killer front-end for this thing that worked so much better! How do I do it?\" GDB supports what it calls the \"machine interface interpreter\", or GDB/MI. The interpreter is selected on the gdb command line with the --interpreter switch. Basically you'll launch gdb and read commands and results to and from it (probably using pipes). Pretty straightforward. See the GDB documentation for all the details. Quick Reference Command parameters are in italics. Optional parameters are in square brackets. All commands can be abbreviated until they become ambiguous. This list is very very incomplete, and only shows things talked about in this tutorial! Help Commands help command Get help on a certain command apropos keyword Search help for a particular keyword Starting and Quitting gdb [-tui] [-c core] [exename] (Unix Command) Start gdb on an executable or standalone; specify \"-tui\" to start the TUI GUI; specify \"-c\" with a corefile name to see where a crash occurred run [arg1] [arg2] [...] Run the currently loaded program with the given command line arguments quit Exit the debugger file exename Load an executable file by name Breakpoints and Watchpoints break location Set a breakpoint at a location, line number, or file (e.g. \"main\", \"5\", or \"hello.c:23\") watch expression Break when a variable is written to rwatch expression Break when a variable is read from awatch expression Break when a variable is written to or read from info break Display breakpoint and watchpoint information and numbers info watch Same as info break clear location Clear a breakpoint from a location delete num Delete a breakpoint or watchpoint by number Stepping and Running next Run to the next line of this function step Step into the function on this line, if possible stepi Step a single assembly instruction continue Keep running from here CTRL-C Stop running, wherever you are finish Run until the end of the current function advance location Advance to a location, line number, or file (e.g. \"somefunction\", \"5\", or \"hello.c:23\") jump location Just like continue, except jump to a particular location first. Examining and Modifying Variables display expression Display the value of a variable or expression every step of the programâ€”the expression must make sense in the current scope info display Show a list of expressions currently being displayed and their numbers undisplay num Stop showing an expression identified by its number (see info display) print expression Print the value of a variable or expression printf formatstr expressionlist Do some formatted output with printf() e.g. printf \"i = %d, p = %s\\n\", i, p set variable expression Set a variable to value, e.g. set variable x=20 set (expression) Works like set variable Window Commands info win Shows current window info focus winname Set focus to a particular window bby name (\"SRC\", \"CMD\", \"ASM\", or \"REG\") or by position (\"next\" or \"prev\") fs Alias for focus layout type Set the window layout (\"src\", \"asm\", \"split\", or \"reg\") tui reg type Set the register window layout (\"general\", \"float\", \"system\", or \"next\") winheight val Set the window height (either an absolute value, or a relative value prefaced with \"+\" or \"-\") wh Alias for winheight set disassembly-flavor flavor Set the look-and-feel of the disassembly. On Intel machines, valid flavors are intel and att Misc Commands RETURN Hit RETURN to repeat the last command backtrace Show the current stack bt Alias for backtrace attach pid Attach to an already-running process by its PID info registers Dump integer registers to screen info all-registers Dump all registers to screen Copyright 2009 Brian \"Beej Jorgensen\" Hall",
    "commentLink": "https://news.ycombinator.com/item?id=38155541",
    "commentBody": "Beej&#x27;s Quick Guide to GDB (2009)Hacker NewspastloginBeej&#x27;s Quick Guide to GDB (2009) (beej.us) 205 points by mooreds 20 hours ago| hidepastfavorite29 comments matheusmoreira 17 hours agoThank you so much. I didn&#x27;t know about layouts at all! The source code and disassembly layout is amazing and it&#x27;s exactly what I want to see when I launch a debugger. I wonder why it&#x27;s not the default. GDB has a huge number of features but it&#x27;s quite hard to use. That&#x27;d go a long way towards making it easier.Is there&#x27;s a data structure explorer panel? I went to rather insane lengths to develop pretty printing scripts for my data structures. It&#x27;s got support for 3 scripting languages: Python, Guile Scheme and its very own GDB command file language. The first two require libc. My project was freestanding so I had to parse bits and bytes with GDB&#x27;s arcane syntax. A proper structure visualizer would be so helpful... reply b5n 11 hours agoparent> I wonder why it&#x27;s not the default.You can toss something like this in ~&#x2F;.config&#x2F;gdb&#x2F;gdbinit: tui new-layout default regs 1 {-horizontal src 1 asm 1} 2 status 0 cmd 1 tui layout default tui enable reply arun-mani-j 15 hours agoprevAwesome guide :)I these days use coredumpctl with gdb to debug seg faults.So if a program crashes, I simply do:$ coredumpctl debugNow it launches gdb (or your default debugger) on the latest core dump. Then I use `bt` and others to debug the issue.https:&#x2F;&#x2F;www.freedesktop.org&#x2F;software&#x2F;systemd&#x2F;man&#x2F;latest&#x2F;core... reply 01100011 17 hours agoprevOne thing I think is sorely lacking is approachable documentation for writing python extensions to GDB.I&#x27;ve managed to cobble together a collection of data structure explorers written in Python but it is barely an improvement over straight gdb scripting due to the way all structs end up as dictionaries. Some higher level libraries for inspection might be nice. Automatic integration with something like graphviz would be amazing. For instance, it would be cool to just describe my data structure(specifying &#x27;next&#x27; pointers or how to access edge lists for a graph) and just have a library dump a visual representation for me. That said, I wonder if half of that logic is best left in a debug module inside the application or library where it remains more tightly coupled to the actual layout. reply jaredsohn 16 hours agoparentChatGPT might work well for this. reply 01100011 10 hours agorootparentFunny you say that. Literally the first thing I tried to get ChatGPT to do for me was to give me an example gdb extension written in Python. It hallucinated badly and as I pushed it, it produced increasingly erroneous errors. That was about 6-8 months ago though. It might be better now. reply jaredsohn 9 hours agorootparentMake sure you try gpt4 if you didn&#x27;t.I&#x27;ve had partial success in getting gpt4 to write rubocop cops (i.e. custom linting rules for ruby). I had to iterate a bit but eventually got something working and wouldn&#x27;t have tried if I didn&#x27;t have gpt helping me. reply 01100011 8 hours agorootparentI think it was gpt3 at the time. I just tried 4 and it got it right! reply literalAardvark 7 hours agorootparent3 is unusable and mostly just a curiosity. 4 is scary smart.There&#x27;s 3 years between them. Bit worried about 2026. replyaoetalks 2 hours agoprevBeejâ€™s guide for networking really saved me:https:&#x2F;&#x2F;beej.us&#x2F;guide&#x2F;bgnet&#x2F;I continue to find that knowledge useful in my day job working on distributed systems. reply the-smug-one 17 hours agoprevI wish that there was a performant and useful front end to GDB for Linux. There isn&#x27;t, so I&#x27;m stuck using GDB manually. Maybe CLion is good? reply dannymi 17 hours agoparentI wrote https:&#x2F;&#x2F;github.com&#x2F;daym&#x2F;idea-native2-debugger as a stop-gap. It uses gdb and works in IntelliJ IDEA Community edition. Setting it up the first time is kinda weird since you need to add a new run&#x2F;debug configuration \"Native2Debugger\". I could not figure out how to hook this directly into the existing run configuration that you use to run your program to begin with. Otherwise, I like how it turned out.If you want a standalone frontend instead, https:&#x2F;&#x2F;github.com&#x2F;epasveer&#x2F;seer is extremely good.And emacs has gdb integration. By now I tried it, and... I guess it&#x27;s better than nothing. reply monocasa 17 hours agoparentprevI&#x27;ll preface this with &#x27;it might not be what you&#x27;re looking for, but I&#x27;ve found useful in this space&#x27;, but gdb has multiple interfaces built in to the normal binary. It&#x27;s not just the raw gdb command prompt, but it has a pretty nice curses mode that&#x27;ll display registers, asm, source with breakpoints, etc. that you expect from a debugger interface. It&#x27;s very much a hidden gem of gdb.https:&#x2F;&#x2F;sourceware.org&#x2F;gdb&#x2F;current&#x2F;onlinedocs&#x2F;gdb.html&#x2F;TUI.h... reply i15e 14 hours agoparentprevgf2 is worth looking at: https:&#x2F;&#x2F;github.com&#x2F;nakst&#x2F;gf reply the-smug-one 6 hours agorootparentWow! I just tried it out, and that was like a dream come true. Thank you for linking this. reply amadvance 11 hours agoparentprevIf you need something text based, cgdb is nice: http:&#x2F;&#x2F;cgdb.github.io&#x2F; reply hegzploit 5 hours agoparentprevThere is also GEF, which is widely used by the reverse engineering and CTF community.https:&#x2F;&#x2F;github.com&#x2F;hugsy&#x2F;gef reply GuB-42 16 hours agoparentprevQt Creator is not terrible. I&#x27;ve yet to come across a GDB front-end I would qualify as \"good\".It is a full IDE for writing Qt apps, but it also works for non-Qt applications in C or C++ (cmake recommended), and it has some of the best GDB integration. It is also free (GPLv3). reply spicynoodles 16 hours agoparentprevI think gdbgui [0] is useful. I&#x27;m not sure about its performance though.[0] https:&#x2F;&#x2F;www.gdbgui.com&#x2F; reply NextHendrix 17 hours agoparentprevHave you tried the Emacs gdb mode? reply BaculumMeumEst 17 hours agoparentprevVS Code has been sufficient for me, at least for hobby stuff. I used it solely as a debugger before I switched over to using it as an editor as well. reply fsociety 15 hours agoparentprevSounds like you want a full UI, but GEF is quite good for making GDB pleasant to use. reply tcoff91 17 hours agoparentprevCLion is definitely good. reply zzixp 15 hours agoprevThis guide is the only reason I passed my school&#x27;s version of CMU&#x27;s CS:APP (malloc)Huge shoutout to Beej! reply blux 10 hours agoprevI didn&#x27;t know about `jump`, that is super useful. reply BirAdam 15 hours agoprevFor some reason, I initially read the title as â€œâ€¦Guide to PBJâ€ and I got really excited about a potential guide to better indulgent sandwichesâ€¦ but nope. reply andrewflnr 15 hours agoparentVery lightly toasting the bread makes them tastier. That&#x27;s just a general sandwich tip, really. But with a PBJ you want it to stay soft enough that the filling doesn&#x27;t squeeze out the side. reply monlockandkey 10 hours agoprevAnyone know which is \"better\", GDB or LDB? reply alberth 16 hours agoprev [â€“] Beej has lots of great guides, check them out:https:&#x2F;&#x2F;beej.us&#x2F;guide&#x2F;Everyone knows the networking guide, but Beej also has: C, UNIX, Python, and many more. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Beej's Quick Guide to GDB\" is a beginner-friendly guide that provides a detailed introduction to the GNU Debugger (gdb), a tool used for debugging programs through the terminal command line.",
      "The guide covers various aspects of gdb usage, including starting the debugger, managing breakpoints, monitoring variables, handling core dumps, and more. It is particularly useful for coders who choose not to use an Integrated Development Environment (IDE).",
      "It also equips users with knowledge of creating a personalized front-end interface using gdb's \"machine interface interpreter\", GDB/MI, and guidance on window layouts management in text user interface (TUI) mode. This guide is licensed under a Creative Commons license."
    ],
    "commentSummary": [
      "\"Beej's Quick Guide to GDB\" is a beginner-friendly instruction manual for using the GNU Debugger (gdb) through the terminal command line, aimed at those who forego an Integrated Development Environment (IDE) for programming.",
      "The guide provides an in-depth overview of gdb, encapsulating various commands like initiating the debugger, configuring breakpoints, examining variables, and handling core dumps.",
      "It additionally elucidates how to generate a personalized front-end interface using gdb's \"machine interface interpreter\", GDB/MI, and the management of window layouts in text user interface (TUI) mode."
    ],
    "points": 205,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1699217638
  },
  {
    "id": 38160161,
    "title": "Mozilla Transitioning Firefox Development from Mercurial to Git: A Two-Phase Approach",
    "originLink": "https://groups.google.com/a/mozilla.org/g/firefox-dev/c/QnfydsDj48o/m/8WadV0_dBQAJ",
    "originBody": "Groups Conversations All groups and messages Sign in î—„ î¢™ îˆ î‰ Firefox Development Is Moving From Mercurial To Git 9336 views Skip to first unread message glob unread, Nov 5, 2023, 11:59:13 PM (10 hours ago) î º î…Ÿ î—” to dev-pl...@mozilla.org, firef...@mozilla.org FIREFOX DEVELOPMENT IS MOVING FROM MERCURIAL TO GIT For a long time Firefox Desktop development has supported both Mercurial and Git users. This dual SCM requirement places a significant burden on teams which are already stretched thin in parts. We have made the decision to move Firefox development to Git. - We will continue to use Bugzilla, moz-phab, Phabricator, and Lando - Although we'll be hosting the repository on GitHub, our contribution workflow will remain unchanged and we will not be accepting Pull Requests at this time - We're still working through the planning stages, but we're expecting at least six months before the migration begins APPROACH In order to deliver gains into the hands of our engineers as early as possible, the work will be split into two components: developer-facing first, followed by piecemeal migration of backend infrastructure. Phase One - Developer Facing We'll switch the primary repository from Mercurial to Git, at the same time removing support for Mercurial on developers' workstations. At this point you'll need to use Git locally, and will continue to use moz-phab to submit patches for review. All changes will land on the Git repository, which will be unidirectionally synchronised into our existing Mercurial infrastructure. Phase Two - Infrastructure Respective teams will work on migrating infrastructure that sits atop Mercurial to Git. This will happen in an incremental manner rather than all at once. By the end of this phase we will have completely removed support of Mercurial from our infrastructure. -- glob âˆ™ moz://a Senior Engineering Manager âˆ™ Engineering Workflow & Release Management î…Ÿ Reply all î…ž Reply to author î…” Forward",
    "commentLink": "https://news.ycombinator.com/item?id=38160161",
    "commentBody": "Firefox Development Is Moving from Mercurial to GitHacker NewspastloginFirefox Development Is Moving from Mercurial to Git (groups.google.com) 201 points by l0b0 8 hours ago| hidepastfavorite165 comments mark_undoio 7 hours agoSad to see another Mercurial holdout switch to Git.The latter is ahead in network effects in a massive way - but I always preferred Mercurial. I&#x27;d have used it more but - network effects! - used git because of the projects I was working on.The main thing was that the mental model of Mercurial fit in my head and the CLI was predictable and regular, whilst the tool still scaled to large codebases.I find Git wants me to think about its implementation details and internal terminology at unpredictable moments during use. It still gets the job done but it feels like doing random CAPTCHAs in the midst of my work. reply hannob 4 hours agoparentLast time I checked every alternative to Git was lacking some major feature that would immediately make it a non-alternative to me. (And I wouldn&#x27;t even remotely call myself a Git power user.)For Mercurial, that was shallow clones. I haven&#x27;t looked at it in a while, but a quick googling shows me a wiki page where they discuss how an implementation could look like, and it hasn&#x27;t been updated since 2015.Network effects are certainly a reason git is popular. But also: If you want to challenge git, you should be able to compete with its basic features. reply klodolph 1 hour agorootparentYeah.My workflow in Git centers on the index, and making lots of commits on local branches that will never get pushed. I spend a day working, rewrite history, stash a bunch of unrelated changes on temporary branches, and then send a couple commits on the main branch for code review.When I used Mercurial for work, it felt surreal to have all of those weird little workflows get locked away behind plugins. I get how Mercurial is way easier for people starting out, and I also get that I could just use the plugins, but when I use Git, all those history-rewriting tools are right there where I want them. reply failingslowly 3 hours agorootparentprevAny of the non-distributed version control systems? &#x2F;only ever so slightly s reply giraffe_lady 4 hours agorootparentprevFossil? reply PH95VuimJjqBqy 1 hour agorootparentno rebase, which for me is perfectly ok as I agree with the creator of Fossil wrt rebase.But it&#x27;s definitely missing git&#x27;s rebase :) reply ocharles 7 hours agoparentprevYou might like https:&#x2F;&#x2F;github.com&#x2F;martinvonz&#x2F;jj, which gives an alternative frontend to Git. reply retpoline__ 5 hours agoparentprev>The latter is ahead in network effects in a massive way - but I always preferred Mercurial. I&#x27;d have used it more but - network effects! - used git because of the projects I was working on.I&#x27;ve recently wrote a post about it - how we&#x27;re kinda locked with git and how other solutions have it hard to get adoption.git was good, but github cemented git&#x27;s position. So unless GitHub allows other solutions, then we&#x27;re kinda locked.>So what&#x27;s the issue here? I&#x27;m worried that just because GitHub is so good, then unless they decouple from git as letters management engine and allow any&#x2F;other, then we will be locked with git.https:&#x2F;&#x2F;trolololo.xyz&#x2F;github reply naasking 4 hours agorootparent> So unless GitHub allows other solutionsI just use Mercurial with the git extension. Works fine. reply Shorel 2 hours agoparentprev> Sad to see another Mercurial holdout switch to Git.I am very happy when something like this happens. It means performance matters and a performant implementation is rewarded with market share.This performance is what made git win, even if the â€œporcelainâ€, the CLI commands we use, are not as well-designed as the ones in Mercurial.Hopefully one day we will no longer use any chat client written in Electron. =) reply rickydroll 56 minutes agorootparent> It means performance matters and a performant implementation is rewarded with market share.by this you mean technical group think or monoculture? reply pjmlp 7 hours agoparentprevYeah pour another glass for Mercurial, it is a shame that git ended up winning, but then again what to expect when it was a child of Linux kernel project. reply bayindirh 6 hours agorootparentSome arguably* technically superior designs end up being obsoleted over time, because the arguably* inferior design ends up being more lined up with how people think and machines work.This is what we&#x27;re seeing here, IMHO.*: There&#x27;s nothing \"objectively something\" in computer science, because it&#x27;s all is a pile trade-offs to get the desired result. reply The_Colonel 6 hours agorootparentWell, can you explain why it applies here?Mercurial vs. Git is IMHO a textbook example of \"good enough with large user base\" beats \"subtly superior with smaller user base\" over the long term. reply lovepronmostly 6 hours agorootparenthg was not superior to git at v1. I have no idea if it is now. hg was horrible at git style branches wich are arguably a killer feature of git. I&#x27;ve heard they were grafted in to hg but you can go look up old tutorials on the net from say 2008 and see they were clearly not the norm for hg---looked it up, the feature is called \"bookmarks\" and was added in 2010. And even after added, all tbe tutorials ignored them. no idea if they are the recommended workflow now but if not then you probably don&#x27;t get why git is superior to hg reply btreecat 5 hours agorootparentBookmarks were a wonderful feature and I used them all the time for release workflow. It was basically like tags in docker.Hg was indeed better than git, especially in the early days. But as git added features, made sane default choices more common, and cleaned up the CLI a bit, it doesn&#x27;t matter now. They all do roughly the same thing, but the Hg workflow was still better imo because they just worked more intuitively coming from never using any DVCS before. reply klodolph 1 hour agorootparentprevI think â€œsubtly superiorâ€ is not a given, here.Git has better performance for a lot of users in a lot of use cases. Git is certainly not always better-performing than Mercurial, but it is often better performing, and the performance differences have varied at different points in time.Some parts of Mercurial struck me as obviously better than Git, like the overall CLI UX, but some of the differences just seem like differences that come with tradeoffs. Mercurial is somewhat more opinionated about branches and allows some weird scenarios like â€œbranches with multiple headsâ€, whereas in Git, a branch is just a label for one of the heads in the first place. Gitâ€™s index seems to trip up a lot of beginners, but as a long-time Git user, I donâ€™t know how I could live without it.I can see why people say â€œsubtly superiorâ€, but I think the superiority is overstated. The CLI UX is the only part where it seems like Mercurial has a clear advantage (and maybe better support for large files). reply Aeolun 4 hours agorootparentprevHmm, I held onto mercurial for years, feeling that Git was kind of incomprehensible. I have never really lost that feeling, but I can do a lot of things with Git that I never could with Mercurial, and short of anyone solving the â€˜merge one patch to multiple different branches without a bunch of conflict resolutionâ€™ I donâ€™t think Iâ€™m likely to switch. reply pas 1 hour agorootparenthttps:&#x2F;&#x2F;pijul.org&#x2F; commutative as it&#x27;s based on \"theory of patches\" not diffs. reply andrewaylett 4 hours agorootparentprevI think you&#x27;re agreeing?The superiority of Mercurial works against it, because while https:&#x2F;&#x2F;xkcd.com&#x2F;1597&#x2F; is entirely relatable, people are used to Git&#x27;s specific flavour of incomprehensibility. It&#x27;s normal, and the lore helps folk avoid the sharp edges most of the time. reply The_Colonel 4 hours agorootparentNo, the op made a much stronger case - basically that git is naturally more aligned with how people think and machines operate.Meanwhile, you&#x27;re saying git is aligned with how people think merely because it got more mindshare early on. reply Ygg2 2 hours agorootparentIf that was the case, you wouldn&#x27;t see thousands of git tutorials on the web. You wouldn&#x27;t get pages like https:&#x2F;&#x2F;jvns.ca&#x2F;blog&#x2F;2023&#x2F;11&#x2F;01&#x2F;confusing-git-terminology&#x2F; getting on HN top page.And no, it&#x27;s not how people think, nor how git shows it. Git shows changes as diffs, while it operates on compressed snapshots. replysedatk 7 hours agoparentprevI totally agree about Mercurial&#x27;s superior ease of use and simpler mental model. However, it feels quite abandoned. I don&#x27;t know what caused it though. It&#x27;s a perfectly viable alternative to Git in my opinion. reply madeofpalk 7 hours agorootparentI tried contributing to firefox, but figuring out mercurial was a huge discourger. reply fhd2 7 hours agorootparentI never understood how people can happily figure out a gigantic, complicated code base like Firefox and are taken aback by learning a little bit about some tool. In your case, was it a thing where working with an unfamiliar tool was simply demotivating? Cause I think if you can write a Firefox patch, you should be able to learn Mercurial during breakfast. reply bdd8f1df777b 6 hours agorootparentI could resonate with him&#x2F;her if I were not familiar with Mercurial. Because he&#x2F;she is aiming to fix or improve Firefox, figuring out a gigantic, complicated code base of Firefox is directly towards his&#x2F;her end goal. Meanwhile, figuring out hg feels a waste of time. reply madeofpalk 4 hours agorootparentprevI didn&#x27;t need to figure out a gigantic, complicated codebase. I only needed to figure out devtools frontend which is a significantly smaller portion of it and relatively self-contained. reply 4death4 6 hours agorootparentprevIs not wanting to do extra work really that hard to understand? reply sedatk 6 hours agorootparentprevWhich problems did you encounter while figuring out Mercurial? reply radarsat1 6 hours agorootparentSpeaking for myself, coming from git, I found the branching model of Mercurial hard to understand. Then it turns out that instead of branches I should have been using \"bookmarks\"? But which to use when?I tried to work on a Mercurial-based project (not firefox) but eventually gave up and just used git to track my changes and then prepare, very carefully, specific big merged commits to contribute back. It was easier, because I could never figure out how to clean up properly if I made a mistake -- so this way, I could just wipe the local copy and start over instead. But it was annoying, and I was glad to stop using it.Completely bewildered by every thread about mercurial full of people proclaiming its advantages and \"simpler mental model\". Simpler than a DAG with pointers? Sure.If I remember correctly you even needed to install some kind of external plugin just to do simple rebasing &#x2F; branch cleanup. Now&#x27;s the part where you tell me I&#x27;m doing it wrong, shouldn&#x27;t be rebasing etc etc, but I&#x27;m just saying, it didn&#x27;t work for me; and the constant claims that it&#x27;s somehow objectively \"better\" but misunderstood get really boring. reply adhesive_wombat 6 hours agorootparentThe thing about git is that while it does leak the implementation a bit and some of the CLI commands show their history of its emergency cobbling-together, it supports many many workflows equally well. At the expense of not having one completely gleamingly-simple happy path as a first-class citizen. It&#x27;s a bit like C in that respect, which isn&#x27;t especially surprising considering the inventor. reply btreecat 5 hours agorootparentprevThat&#x27;s one diff between git and Hg. Hg likes to consider the history \"sacred\" so there&#x27;s no \"cleaning up your commit\" via squashing&#x2F;cherry picking.This is actually superior in most cases. Being able to force rewrite history so easily in git was a design mistake imo. reply dahart 1 hour agorootparentWhy? What need is there to preserve the commit history, and why is it superior? Are you very familiar with git and understand what workflows the git history rewriting tools were intended for, and how theyâ€™re typically used and supposed to be used?Git only ever advocated for rebase, squash, and cherry pick for local branches that havenâ€™t been pushed or shared yet. It is arguably not even â€œhistoryâ€ until you publish it. There have always been strong warnings about doing those things to published histories. Itâ€™s bad form generally speaking to do it (rebase or change history) to a published branch that more than 1 person is using for any other reason than extreme necessity, such as cleaning up security accidents where someone pushed keys or info they werenâ€™t supposed to. And for exactly that reason, all the opinionated DVCSs that claim to consider history â€œsacredâ€ actually offer the ability to easily rewrite published history, including Hg and Fossil!Gitâ€™s ability to rewrite local history is pretty important for (1) encouraging a commit-early commit-often workflow where the local repo is a safe space, (2) cleaning up your work to make it presentable and semantically organized before sharing&#x2F;publishing, and (3) allowing real-world workflows where people didnâ€™t plan every single feature branch in advance but happened to end up working on multiple things and committing them first.A version control system should be, first and foremost, a safety net that simply backs up any work you do without judgement. Personally, I consider the idea that commit history is sacred to be dogmatic and unnecessary. Iâ€™m open to the idea that it might help prevent a few specific types of accidents here and there, and it might be fair to argue that offering tools to groom the historyâ€™s presentation, separately from the log of commit activity, is on average a slightly better idea than having to actually reorganize commits before pushing them. Maybe that didnâ€™t occur to git devs before it was released, or maybe it did and they decided it wasnâ€™t going to help anyone and just add confusion to have two separate timelines. I donâ€™t see any reasons justifying calling gitâ€™s design a â€˜mistakeâ€™, and there seems to be evidence that most people are fine with git, and that for whatever reason the DVCSs that reacted to git in 2005 by immediately trying to fix some of gitâ€™s perceived issues didnâ€™t ultimately fix the problem well enough to gain traction. reply aayjaychan 4 hours agorootparentprevPublished history in Mercurial is sacred [1]. Modern Mercurial fully embraces history editing, and provides (IMO) better tools than git to facilitate (safe, collaborative) history editing.[1] You can still change published history if you try hard enough with a lot of co-ordination.If Mercurial doesn&#x27;t support cleaning up of commits, then the commit history of Mercurial itself [2] wouldn&#x27;t look so clean.[2]: https:&#x2F;&#x2F;foss.heptapod.net&#x2F;mercurial&#x2F;mercurial-devel&#x2F;-&#x2F;commit... reply naasking 4 hours agorootparentprev> It was easier, because I could never figure out how to clean up properly if I made a mistakeMade a mistake doing what? A mistake in your code or a mistake using Mercurial?> If I remember correctly you even needed to install some kind of external plugin just to do simple rebasing &#x2F; branch cleanup. Now&#x27;s the part where you tell me I&#x27;m doing it wrong, shouldn&#x27;t be rebasing etc etc, but I&#x27;m just saying, it didn&#x27;t work for meI&#x27;ve never understood this impulse to do \"branch cleanup\". Why are you adding unnecessary work? Is the source repository for tracking a history of changes or is it an art exhibition? reply jorams 2 hours agorootparent> I&#x27;ve never understood this impulse to do \"branch cleanup\". Why are you adding unnecessary work? Is the source repository for tracking a history of changes or is it an art exhibition?The history is an end product intended to help others (often future me) figure out when and why things changed, preferably in self-contained, well organized steps. Often my first attempt at a change does something in an illogical order, or I make related changes that later end up in a separate merge. In the short term that change in thinking is somewhat interesting. In the long term it only confuses whoever is looking through the history to figure out why something is the way it is.The stronger version of this completely discards any history when merging a feature, but I often find that to get rid of useful information. reply prirun 1 hour agorootparentSmall \"cleanup\" changes required after merging a larger change are often very educational. They highlight subtle nuances in the code that were not thought of when the large change was merged. If the smaller changes were rebased to make one large commit, the edge-case history is also lost. reply naasking 2 hours agorootparentprev> The history is an end product intended to help others (often future me) figure out when and why things changed, preferably in self-contained, well organized steps.Development is often not self-contained or perfectly organized, so we shouldn&#x27;t expect this of the change history either.> In the long term it only confuses whoever is looking through the history to figure out why something is the way it is.Source control is the wrong tool for this job, that&#x27;s what comments and tickets are for. Source control can help you retroactively figure out how and sometimes why something evolved in a certain way, like a good archaeological tool, but we don&#x27;t use crude tools when there are better tools.I&#x27;ve taken to literally embedding ticket numbers along with comments directly in the code and in commit comments to document exactly why code does what it does. All functional changes are tied to tickets [1], and the tickets contain the full context, and the code comments contain a summary of that rationale. Use the right tool for the job.[1] where \"functional changes\" means changes that can affect behaviour, rather than stylistic changes on variable naming or revising comments that have no visible effects. reply dreamcompiler 2 hours agorootparentprevI&#x27;be never understood this either--this idea of revising history to make the log look better.The proper solution is to build a better log display tool, not change the log itself. reply naasking 1 hour agorootparentA better log display tool could help depending on why they want the log to look better. I often find that the reasons they want a better log is because they&#x27;re not documenting&#x2F;commenting their code sufficiently to figure out the code currently looks the way it does, so they want their source control to make up for poor coding practices. reply nonbirithm 4 hours agorootparentprevMy brief experience was largely the same. A disproportionate amount of time trying not to lose all my work and having to checkout again and then trying to use Git on top of it which failed. I only went back to trying to contribute patches when BitBucket sunset Mercurial and the maintainers had to switch to Git. reply atq2119 6 hours agorootparentprevThank you for putting my own (~10 years ago) experience with Mercurial into words. reply bdd8f1df777b 6 hours agorootparentprevNot abandoned. Both Facebook and Google internally use Mercurial as one of the version control systems. They will keep the Mercurial project alive. reply progbits 4 hours agorootparentSee sibling comment linking to https:&#x2F;&#x2F;github.com&#x2F;martinvonz&#x2F;jjI expect google to switch to that. reply godzillabrennus 5 hours agorootparentprevExplaining in a nutshell why mainframes are still in use and COBOL has market demand. reply anon23432343 5 hours agoparentprevSay you dislike one peace of software because it does not have your used to mental model? okay sounds reasonable.Mercuiral booksmarks are strange to me and dont fit my mental model. Its like finding a bookmark for an article in my bookmark manger that I stored in some folder. So Random reply mark_undoio 3 hours agorootparentI think one thing that helped git early on was that its concept of branches were just really convenient.For me I&#x27;d say it&#x27;s that git doesn&#x27;t fit my mental model and Mercurial does - it&#x27;s that mercurial gave me a smaller mental model that (at least back when I used it heavily) allowed me to do what I needed.Even as a fairly experienced and advanced Git user, Git requires me to keep more state in the head and at, from my point of view, unpredictable times needs me to extend the model. The strange thing is that this is despite having a very simple model at the lowest levels of the system. reply cryptos 8 hours agoprevnext [â€“]- We will continue to use Bugzilla, moz-phab, Phabricator, and Lando - Although we&#x27;ll be hosting the repository on GitHub, our contribution workflow will remain unchanged and we will not be accepting Pull Requests at this time - We&#x27;re still working through the planning stages, but we&#x27;re expecting at least six months before the migration beginsI understand that this could make sense as a first step, but I guess they could profit if they would go all-in with GitHub and use the tools it offers. Phabricator (https:&#x2F;&#x2F;www.phacility.com&#x2F;phabricator&#x2F;) is even deprecated! And who likes to file bugs in Bugzilla?As sad as it is, I see Firefox losing ground even on the developer side. reply southerntofu 7 hours agoparentBugzilla and Phabricator don&#x27;t have the most user-friendly interface, but at least they&#x27;re not vendor lock-in for proprietary software making money from open source software like Github is.Gitea, Gitlab and Sourcehut are in my opinion very decent alternatives to Github providing the same functionality (if not more) without any ethical downsides. reply bayindirh 6 hours agorootparentI can second Sourcehut. It looks clunky, but works fast, works well and is easy to interface with.I moved completely to Sourcehut, and I&#x27;m not coming back. reply mrweasel 6 hours agorootparentThe Sourcehut build system is also really good. It looks clunky and feels like it should be lacking in features, but realistically most of my build pipelines always ends up as a Makefile or a shell script anyway, so it fits my mental model pretty well. reply drstewart 6 hours agorootparentprevHow is Github making money off of open source software in a way that Gitea, Gitlab, and Sourcehut don&#x27;t? reply southerntofu 5 hours agorootparentGitea, Gitlab and Sourcehut are all making money on open source. However, they themselves are open source projects and contribute to a wider ecosystem.Github makes money without producing open source software. Well, that is if you don&#x27;t count a sluggish electron-based text editor. Github is not open source, Github Pages is not open source, Github Actions is not open source. Sure, they offer free hosting for open source projects, but that kind of charity creates a hard dependency on Github for many projects, which is not healthy for the ecosystem.What happens if Github closes? Or stops free offers for open source projects? To make an analogy from another ecosystem, if Mac Donalds started offering free meals to homeless people, that would not solve the problems of poverty and would not contribute to healthier food for the people or a more sane&#x2F;ecological agriculture. That&#x27;d just be charity for their own gain. reply MRtecno98 2 hours agorootparentProducing OSS is not the only way to contribute to the OSS ecosystem, github offers free services for OSS developers who otherwise would have to host them themselves, that&#x27;s the reason why they got so popular and i think we can all agree on the fact that it&#x27;s incredibly useful. reply cfiggers 4 hours agorootparentprevI think you&#x27;re selling GitHub&#x27;s positive impact on open source really short. reply southerntofu 3 hours agorootparentIt&#x27;s important to see both sides to the benefits&#x2F;risks analysis. Let that be a counterpoint to the people overselling the positive impact without ever mentioning the immediate or potential downsides. Nothing is ever truly binary outside of computing. replycardanome 7 hours agoparentprevGoing \"all-in\" on GitHub means making yourself dependent on a Microsoft service that could change its conditions every day.It is bad enough that Firefox depends on Google money for survival, I don&#x27;t see why they should give even more of their freedom away to rely on Microsoft.The kind of low-effort contributions that you would get from people that will only contribute because you are on Github are not the ones that you need or want anyway.Plus if they wanted a alternative, Gitlab is so much more pleasant to work with anyway. reply xtracto 6 hours agorootparentFirefox source being hosted by Microsoft us blowing my mind as a child of the 80s.If someone had told my 80s&#x2F;90s Libre&#x2F;OSS advocate rebel mind how things would pan out, I&#x27;d laughed. reply PedroBatista 5 hours agorootparentI think it&#x27;s more or less similar to the cultural shift that by the 80&#x27;s most hippies turned into yuppies working at big corporations and one of the nastiest materially and status obsessed people.Even the most ardent woke&#x2F;eco&#x2F;[insert flavor]ists\" need daddy to pay their rent, vacation and iPhone bills. reply goalieca 5 hours agorootparentIâ€™m a child of the era. It surprises me Microsoft embraced open source but it also doesnâ€™t surprise me they bought the largest service for it. While git doesnâ€™t have lock-in GitHub actions and the rest totally do. reply PedroBatista 4 hours agorootparentMy view about this: Don&#x27;t be fooled! Microsoft embraced money, opensource is a tool and strategy in order to get more clients&#x2F;money by being relevant and don&#x27;t have all the costs of developing everything.Azure accounts for something like 35-40% of all their revenue, and those pesky tenants insisted on installing Linux and running things on their other than Exchange or Microsoft Dynamics.With Microsoft there&#x27;s always a catch. ( they&#x27;re are not alone of course ). reply philistine 4 hours agorootparentYeah, people think Microsoft somehow saw the light and completely changed. They didnâ€™t. Microsoft will follow market trends to make money on those. Thatâ€™s what GitHub and Linux on Azure are. The marketâ€™s there, letâ€™s go there.They also try to influence the market. To build lock-in. Theyâ€™ve never stopped doing that. reply MRtecno98 2 hours agorootparentprevwell, if we want to get philosophical, that&#x27;s kinda the point of free market. Not saying that it always works out well of course, but in this case it did so i don&#x27;t see anything to criticize to Microsoft per se replyxorcist 7 hours agoparentprevPhabricator is so much nicer to use for code review than Github. The ticket system is also much more powerful. I understand why people can dislike Bugzilla, it is not really meant for modern workflows and looks a bit dated. But Phrabricator is a modern tool which is easy to use, yet so many seem to abandon it for objectively inferior and dated tools. It is a mystery to me. reply thiht 4 hours agorootparentI hated Phabricator for all the years I had to use it. The expected workflow was completely impossible to understand to me. And the name of the tools completely opaque (\"Herald\"? \"Maniphest\"? \"Phriction\"??)We ended up switching to Gitlab because Github-style code reviews are just so much easier to deal with. They hit the spot between \"good enough\" and \"easy to use\". Phabricator&#x27;s bloat made it unbearable reply xorcist 3 hours agorootparentTotally agree on the \"cute\" naming (and the cute strings too, turning them off on new installations is probably what most people do). If you can look past that, it is a well integrated and modern tool. Or was.Don&#x27;t understand moving to Gitlab because of the bloat though. There are lots of lighter weight tools, but Gitlab is the whole kitchen sink and then some. The integrated-ness is part of the package with these tools. reply thiht 1 hour agorootparent> Don&#x27;t understand moving to Gitlab because of the bloat thoughYou&#x27;re totally right: it was years ago, and Gitlab used to not be as bloated at the time. I guess today we&#x27;d migrate to Gitea instead. reply gray_-_wolf 7 hours agorootparentprevIt phabricator still alive? Or rather, phorge? I somewhat stopped paying attention, so I wonder if the fork is alive and well. reply djbusby 5 hours agorootparentStill alive. reply inferiorhuman 6 hours agorootparentprevI don&#x27;t remember Bugzilla ever being particularly usable. In fact it&#x27;s always stuck out as one of the things I detest about contributing to FreeBSD. reply LeanderK 8 hours agoparentprev> And who likes to file bugs in Bugzilla?Not only has it been years since I last have heard of bugzilla, I also really resented it! Moving to a modern infrastructure really improves accessibility for developers. For example, GHC (Haskell Compiler) moved to gitlab a few years ago and while previously it was really hard for me to understand the development process nowdays I can follow pull requests and tickets of features I am interested in without any problem. Previously the system was really complicated and, if I remember correctly, Phabricator, mailing list etc. based. Off putting as a newcomer. reply pointlessone 7 hours agorootparentBugzilla is fine. It looks OK. You can subscribe to individual tickets. It doesnâ€™t support emoji reactions or Markdown but it does the job just fine. One thing it does better than GH issues is ticket dependencies even with tree&#x2F;graph visualization. GH just has no blockers&#x2F;blocked issues in any form or shape. New ticket form has a few more fields than the default GH issue form but itâ€™s much better than it used to be back in the days and comparable to custom form on GH. You also can log it with your GH account so not much of a barrier there, too. Seriously, try Mozillaâ€™s bugzilla now and see for yourself. Itâ€™s probably not as bad as you remember it.Also consider that Mozillaâ€™s bugzilla contains quarter of a century worth of tickets. Thereâ€™s no way it can be moved to GitHub (or wherever) without loss of data. reply pascalchevrel 7 hours agorootparentFYI, Bugzilla supports markdown just fine :) reply ChrisSD 7 hours agorootparentprevIIRC, Python managed to move to github issues. reply sydbarrett74 7 hours agorootparentprevAgreed. Bugzilla just needs to go away. There are so many, far superior issue trackers. Plus Mozilla could allocate resources to Firefox and Thunderbird. reply mrweasel 7 hours agoparentprevMost of the organizations still on Phabricator all seems to be switching to Phorge (https:&#x2F;&#x2F;phorge.it&#x2F;).I understand why Mozilla can&#x27;t&#x2F;won&#x27;t go that route, but seeing organizations attempt to duct-tape solution together with Bugzilla, Phabricator, Jenkins and whatnot makes you appreciate what Atlassian has done with Jira, Bamboo and Bitbucket (even if Bamboo and Bitbucket aren&#x27;t exactly industry leading). I suppose that Phabricator was intended to be a Jira&#x2F;Bitbucket&#x2F;Gerrit alternative of sorts, but just failed to get the required traction to survive.It&#x27;s not just open source projects, way to many companies are attempting to knit something together and they end up with this unholy mess of systems that sort of communicate, but not really. The saving grace is actually Git, because everything works with Git. reply pjc50 7 hours agoparentprevBugzilla was built for mozilla. They have a quarter century of tickets in there. It&#x27;s not going anywhere soon. reply sydbarrett74 8 hours agoparentprevPhorge has seized the torch, and it looks promising as a Phab successor. reply CalRobert 7 hours agoparentprevYou can go all-in on git and not with github. reply otabdeveloper4 6 hours agorootparentWell, yes. Despite both having the three letters \"git\" in the name, Github works against the features of Git and there&#x27;s a huge impedance mismatch.Git was suposed to be a decentralized, peer-to-peer way of managing software using standard open-source tools, and Github is the exact opposite. reply gilcot 6 hours agoparentprevIt&#x27;s like they don&#x27;t know that they can have a clone on Github and still continue using Mercurial.On the other hand, they&#x27;re moving from open source tool to a closed ecosystem and depends on microsoft. Very sad. reply liampulles 8 hours agoparentprevAs you say it is a first step - let&#x27;s wait and see reply lifeisstillgood 7 hours agoprevThe exact Bugzilla &#x2F; phab &#x2F; Haskel style text files shoukd not matter - i mean it&#x27;s all just tickets. what matters is the level of integration code written by devs to make their lives easier. That&#x27;s what means it&#x27;s hard to move.it almost feels like there should be some intermediate target language - like the syntax server that syntax highlighters target (forget the name)not aure how but it feels doable reply mgd 8 hours agoprevIs there a reason why they used Mercurial as opposed to git? Did it provide some benefit? reply gtsteve 8 hours agoparentThe two came out at roughly the same time and for the first few years it wasn&#x27;t really obvious which was better.Personally, I chose Mercurial to start with, because I liked the Windows tooling available and it felt a lot more like Subversion, which is what I used previously.However, Git won the mindshare war in the end, so I moved over to that. reply santiagobasulto 8 hours agorootparentHappened the SAME thing to me. Mercurial felt more natural than git. Back then, Github allowed you to choose the VC and Mercurial and Git were available, I&#x27;d always choose Mercurial. I was also using Google Code that allowed for mercurial as well.Ahhhh, thanks for brining back those memories. reply antod 8 hours agorootparent> Github allowed you to choose the VC and Mercurial and Git were availableYou&#x27;re not thinking about BitBucket are you? I thought Github was always git only? reply pseudalopex 7 hours agorootparentGitHub started with Git and added Subversion. Never Mercurial. reply antod 7 hours agorootparentOh that&#x27;s right - I&#x27;d forgotten about that april fools joke that wasn&#x27;t. It was just client compatibility right - the backend was still git from memory?What I mean was that it wasn&#x27;t like Bitbucket where you chose whether you wanted git or hg for starting your project. reply innocenat 7 hours agorootparentBitBucket was Mercurial-only for quite some times before they added Git (when it was obvious that Git was winning).I think Google Code provide both from the early day (along with Subversion), and maybe also SourceForge. reply pseudalopex 7 hours agorootparentSourceForge added Git, Mercurial, and Bazaar in 2009. Google Code added Mercurial in 2009. Bitbucket and Google Code added Git in 2011. reply jamesdwilson 17 minutes agorootparentI&#x27;ve used them all and Bazaar is my fav by far. replyMikusR 4 hours agorootparentprev12 years ago I decided to try using Mercurial for tracking history of my plain text notes. I chose Mercurial for it&#x27;s supposed better Windows support. Turned out that it couldn&#x27;t work with unicode filenames. While git could. reply gwd 8 hours agorootparentprevThe Xen Project chose Mercurial to replace Bitkeeper (the proprietary but free-as-in-beer-to-open-source-projects-for-a-while VCS that Linux used before it became no-longer-free-as-in-beer) as well. There were things that were nicer about it; but in the end it just made more sense to move over to git.That was over a decade ago, however; I&#x27;m actually pretty surprised it took Mozilla so long to switch. reply agumonkey 7 hours agorootparentprevAfter all the years, do you think mercurial still offers tangible benefits in terms of versionning workflow ? faster operations, more extensibility, things that are somehow only possible in hg and not git (or way too hard in git)I&#x27;m a happy git user but I&#x27;m really curious about losing good ideas from other tools due to mindshare. reply fer 8 hours agoparentprevDisclaimer: I haven&#x27;t touched hg in nearly a decade.hg has (had?) a sane CLI that blows (blew?) git out of the water.I like the power of git, but it either needs external tooling to prevent people from shooting the team in the foot, or relatively thorough study by everyone using it.In practice, for git, most of the time you need some software to protect people from messing up repositories and history (gitlab, github, etc, which honestly you&#x27;re gonna use anyway), while hg on its own protects you out of the box from most mistakes.All FAANGs I&#x27;m familiar with use git, but there are some serious handrails and straps on what you can do. On smaller companies I had so many arguments about rebasing public branches that it&#x27;s not even funny.For a while I had in the back of my head to create a hg clone that was a wrapper around git, in a way that it&#x27;d allow hg-style workflows and protections, while having a faster and more well-known \"backend\". reply Pesthuf 7 hours agorootparentWhen using hg, I often accidentally created multiple heads for a branch and I remember that being really annoying and unintuitive to undo. reply fer 6 hours agorootparentI haven&#x27;t touched it in a while, but AFAIK multiple heads in a branch only happens when you commit twice against the same changeset in the branch. I don&#x27;t see how this can happen accidentally. Either way you want to merge them back under a single head (a normal &#x27;hg merge&#x27;) or remove the spurious head (&#x27;hg strip&#x27;). Maybe it was more complicated at the beginning? reply naasking 4 hours agorootparentprevWhy undo it? Just merge them. Undoing isn&#x27;t difficult either, but I&#x27;ve never understood this obsession with clean repository history. reply Krutonium 2 hours agorootparentYour comment describes the obsession perfectly. A clean repository history. It helps with things like... History. reply naasking 2 hours agorootparentExcept then it&#x27;s not a history of what actually happened, and revising it to make it look \"clean\" doesn&#x27;t actually help with anything. reply dahart 43 minutes agorootparent> Except then itâ€™s not a history of what actually happened?What do you mean? Hg and Fossil donâ€™t capture what â€œactually happenedâ€ either, they donâ€™t show you stuff you typed and deleted before committing. They donâ€™t save the compile errors you had or what you tested and changed. They donâ€™t save what you said to your coworkers about their code because they were afraid to set their changes in stone or make a mess. They donâ€™t prevent anyone from putting things into the commit log in a different order than they â€œactually happenedâ€, and they canâ€™t: these are systems that record only what you tell them. What problem, exactly, does trying to capture â€˜what actually happenedâ€™ help solve?I donâ€™t understand this obsession with what â€œactually happenedâ€, as if thereâ€™s something critical about auditing keystrokes, as if the working code and the state of the tree are secondary. No DVCS can prevent someone from designing what their commit history looks like. In the mean time, the idea that you shouldnâ€™t be able to edit or change anything locally before showing it to other people seems like a negative force against the basic safety net that a version control system needs to provide. (And no existing DVCS actually prevents rewriting history anyway, they just claim to have preferences and they offer commands to rewrite history that have other names and say honor system, you shouldnâ€™t use them very often.)> revising it to make it look â€œcleanâ€ doesnâ€™t actually help with anything.Hehehe youâ€™ve never had to look at the history then, I assume? Youâ€™ve never had to bisect? You donâ€™t care if people interleave different topics in their branches, or make lots of 1-line fixup commits all over the place? You donâ€™t care about how long it takes to find out who changed something, or whether you have to sift through a mountain of garbage to find it?Donâ€™t the DVCSs that have the â€˜history is sacredâ€™ dogma also have tools to make presenting and viewing history less noisy than the so-called â€˜what actually happenedâ€™ commit log? Doesnâ€™t that prove that a clean view of history does help, and is something a lot of people want? reply naasking 6 minutes agorootparent> What do you mean? Hg and Fossil donâ€™t capture what â€œactually happenedâ€ either, they donâ€™t show you stuff you typed and deleted before committing.They show you what you actually committed and don&#x27;t allow you to do stupid things that might corrupt your repository, which is what matters.> I donâ€™t understand this obsession with what â€œactually happenedâ€, as if thereâ€™s something critical about auditing keystrokes, as if the working code and the state of the tree are secondary.\"Working code\" and important milestones in the state of the tree are tracked by tags, bookmarks and other features depending on what source control system you use. The state of the tree at every single commit point is an irrelevant detail.> No DVCS can prevent someone from designing what their commit history looks like.Correct, and they shouldn&#x27;t encourage you to waste your time trying to do so, and so shouldn&#x27;t make it easy either.> Hehehe youâ€™ve never had to look at the history then, I assume? Youâ€™ve never had to bisect?I&#x27;ve been doing it for over 20 years, first with subversion, then Mercurial. Never had any issues figuring out what was going on. Taking a few extra minutes here and there to scan through such changes is nothing compared to trying to fixing a repository corruption, which is something I recently had to deal with.Everything you describe can be handled by 1) a better commit log history tool and more importantly, 2) better development practices that requires adding proper comments and tickets to codes and commits that describe context and rationale for changes. Opening up history to revisions is a sledgehammer trying to solve a bunch of distinct problems that require better solutions.> Doesnâ€™t that prove that a clean view of history does help, and is something a lot of people want?Of course it helps, sometimes. As I&#x27;ve described above, that doesn&#x27;t mean you should introduce a sledgehammer for the occasional pain point that can be addressed by other, arguably better means.Ygg2 8 hours agorootparentprevIf hg cli was butt raving mad, it would still be considered \"saner\" than git.If git was an equivalent modern appliance, it would be against the Geneva convention.I give Git this, it&#x27;s a bit faster. reply otabdeveloper4 6 hours agorootparentGit is not opinionated and doesn&#x27;t care about your workflow, which is why it won in the end. (The downside is that, yes, it&#x27;s ultimately just a collection of tools, not a \"framework\" for doing software.) reply Ygg2 3 hours agorootparentNon-opinionated is not an excuse. It&#x27;s CLI is an unlearnable mess ( ours vs theirs). I&#x27;ve got 10 years in it and it still bites me in the ass. reply hiddencost 8 hours agoparentprevhttps:&#x2F;&#x2F;wiki.mozilla.org&#x2F;VersionControlSummit2006 reply Xenoamorphous 8 hours agoparentprevI wonder how much the â€œvictoryâ€ of git vs mercurial was influenced by having a big name like Linus Torvalds behind it. reply flohofwoe 8 hours agorootparentIMHO it was Github which made git popular. GH filled a gap left when Sourceforge went down the shitter, and Github was there at the right time to host open source projects for free. The actual version control system used (svn vs git) wasn&#x27;t all that important. The important feature was free hosting. reply antod 7 hours agorootparentYeah, hardly anyone cared what Linux used (a few years earlier before Bitkeeper it was mostly just patches).But Rails was a very early Github user, and the big Rails usage upswing coincided with that and helped drive it. I was a Python and PHP dev at the time, and that other stuff mostly predated Github and was scattered across all kinds of places. Whereas Rails and Github both kicked off about the same time, and practically no Rails devs or libraries used anything else.It was that fast growth and hype combined with the network effects of Github that drove git usage rather than what Linux used. The Linux contribution workflow was also pretty alien to Github focused webdevs. reply xorcist 7 hours agorootparentprevThis. Free hosting has mass market appeal. It is a so much bigger market the product itself isn&#x27;t terribly important. Not until the later pivot to a paid product.That&#x27;s how Sourceforge rose to become so dominant it appeared unstoppable, and how Docker became immensely popular so quickly. reply jasode 7 hours agorootparentprev>IMHO it was Github which made git popular. [...] The important feature was free hosting.Bitbucket also had free Mercurial hosting in 2009. I think it was 1 private repo in the free plan and it was later expanded to unlimited free repos with max 5 users around 2011. reply Cthulhu_ 6 hours agorootparentprevFor sure; Github was also (IMO) responsible for the rise and success of NodeJS and its dependency ecosystem, and consequently the web based front-end ecosystem we have today (angular, react, vite, etc). reply agumonkey 6 hours agorootparentprevwas there ever a github for mercurial ? reply gwd 8 hours agorootparentprevThe main reason Linus didn&#x27;t go with Mercurial was that Mercurial just wasn&#x27;t efficient enough to do operations on the Linux repo with what Linus considered a reasonable speed. (I think it was maybe written in Python at the time?) Linus knew what the kernel was doing, so he could write a core engine that was a lot faster, and that mattered to him.So Linux was always going to be git; and I think things flowed from there. reply dahart 19 minutes agorootparentWasnâ€™t git written by Linus and chosen for Linux development before Mercurial was announced? Is the main reason Linus didnâ€™t choose Mercurial because Mercurial didnâ€™t exist when the decision was made? reply goku12 7 hours agorootparentprevMercurial is still written in Python. There seems to be a bit of Rust in there somewhere. reply magicalhippo 7 hours agorootparentprevHaving used Mercurial for years in OSS projects before switching, my view is that pull requests on Github was a major factor.There wasn&#x27;t a similar offering for Mercurial, in part I think because of the fundamentally different way Git and Mercurial treated (treats?) branches. In Mercurial branches were permanent, so a PR-like flow would leave a lot of stale branches around.Mercurial also didn&#x27;t support rewriting history as a core philosophical choice.It was introduced via a plugin but that meant each dev had to enable it to use it. AFAIK it has been moved into the core, but I think too late.At least as a primarily Mercurial user at the time, I feel Git didn&#x27;t really outcompete Mercurial until Github and PRs came along. It just allowed for much smoother cooperation. reply pseudalopex 7 hours agorootparentBitbucket had Mercurial pull requests. Later maybe. reply liotier 8 hours agorootparentprevNot just Torvalds and hosting Linux development: Git solves \"big code\" problems and large organizations pull the market - including tiny projects for which Git&#x27;s trade-offs are not optimal, where Mercurial tended to make users happier. reply VMG 8 hours agorootparentprevbig name and big speed - I distinctly remember how slow \"mercurial\" was when git started dominating reply ygra 8 hours agorootparentSince then both had lots of optimizations put into it to work outside their original intended domains. Facebook did a lot to make Mercurial performance on very large repositories tenable. Microsoft did the same for Git to allow their Windows developers to work with Git, which apparently was initially not able to do common operations at all at acceptable speed.But those are big company problems and I suspect the caveats, tradeoffs and solutions matter a lot less for the 99 % of other VCS users. reply bvrmn 8 hours agorootparentprevFor me it was a quite important factor TBH in 2007 when I wanted to try something new outside of SVN. And it was my big argue point to force my company to switch to Git. reply joewalker 7 hours agoparentprevWhen the decision was made to use Mercurial, Git for Windows wasn&#x27;t a viable option. If you wanted to develop on Windows, the choices were Mercurial or SVN.Edit: I work for Mozilla, although I didn&#x27;t in 2006 when I think the decision was made. reply dale_glass 8 hours agoparentprevMercurial came out about a week after Git did. I&#x27;ve not used it in a long time but I recall that it was a lot less confusing than Git to use.But Git definitely won the war, and Mercurial has been disappearing little by little. reply vardump 8 hours agoparentprevProbably tooling and developer mindshare.Might not have much to do with git vs Mercurial technical merits. reply Espressosaurus 8 hours agorootparentYeah. From a technical standpoint Git is a lot worse in a lot of ways. From an ease-of-use standpoint it&#x27;s about as much worse as it&#x27;s possible to be.Still, with Linux behind it, Git won the DVCS wars. Mercurial became an also-ran and now almost nobody uses it.Too bad.We&#x27;ll need something new and innovative to come out and rid us of constant blogs explaining how Git works because it&#x27;s too hard to understand how to make it do the things you want it to do.I&#x27;ve been using Git for about 7 years now (5 exclusively) and I know how to use it, but I still have to look things up occasionally because its UI is trash and non-obvious. I didn&#x27;t have that problem with Mercurial. There, I knew the verb and the built-in help was sufficient to get me where I wanted to go.Worse is better wins again. reply hexo 7 hours agorootparentReally? I don&#x27;t think so.Just try to clone Mercurial repo (Firefox) and roll back in history to specific commit. Let me tell you beforehand - it is (at least used to be) impossible. What you could do is to \"clone it again\" up to specified point. You have to do it somewhere else on a filesystem just to needlessly spoil disk space and you end up having the same sources at least twice. When I asked why I got handwaving arguments about code history being sacred, which is of course nonsense. If it was so... in Git I&#x27;d make another branch and rewind to commit I care about without wasting another 25gigs of disk. How can be this achieved in mercurial these days (without waste)? Back in days I wanted to do some work specifically on Firefox, and those steps were the only I could come up with after a lot of searching and asking questions, and because of Mercurial I gave up, to me it felt like it was a high entry barrier on purpose.So, no my friend, a lot better wins time. reply thfuran 7 hours agorootparent```hg clone ssh:&#x2F;&#x2F;example.com&#x2F;repocd repohg update -r $SOME_COMMIT_HASH_OR_ID```Unless you mean you want to delete history back to a certain point, in which case it&#x27;s only slightly more complicated. reply TerrifiedMouse 8 hours agorootparentprev> We&#x27;ll need something new and innovative to come out and rid us of constant blogs explaining how Git works because it&#x27;s too hard to understand how to make it do the things you want it to do.Read about Jujutsu awhile back.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30398662 reply pptr 7 hours agorootparentprevI really enjoy the simplicity of Googles internal repo (piper).I am quite surprised that no open source variant has been created yet (that I&#x27;m aware of). reply Ygg2 7 hours agorootparentHow does piper work? reply WesolyKubeczek 7 hours agorootparentprev> Yeah. From a technical standpoint Git is a lot worse in a lot of ways. From an ease-of-use standpoint it&#x27;s about as much worse as it&#x27;s possible to be.Ten years ago, when I had started to use git after having used both Subversion and Mercurial, I noticed one thing.Git was FAST.On repositories of the same size, git was blowing Mercurial out of the water and running circles around it, that&#x27;s how fast it was.It was FAST and it fucking got out of my way and let me work.I don&#x27;t know the inner workings of either and academic advantages of Mercurial, but I know for a fact it had been a dog and its move to Python 3 had been disastrous. So whichever technical or UX advantages Mercurial might have, I simply don&#x27;t care anymore. reply geraldwhen 6 hours agorootparentYounguns may forget or never have experienced branching an svn repo. It used to literally take hours.Then git-svn was born, and you could branch instantly. This was such an obvious improvement that all future repos moved to git. reply zilti 8 hours agorootparentprev\"developer mindshare\" has become one of the most bullshit reasons to use something. reply j1elo 8 hours agorootparent\"developer mindshare\" usually equals better or at least more support. Which is not at all a bullshit reason to use something. reply sshine 8 hours agorootparentprevFor that reason alone, yes.But not for the reasons that correlate, such as education, tooling. reply woadwarrior01 8 hours agoparentprevIn the early days, the git UX was quite rough around the edges and Mercurial was a lot more idiot proof than git was. Git has evolved significantly since then. reply bsder 8 hours agoparentprevYeah, the mental model of Mercurial is way better.Unfortunately, GitHub got VC funding and caused network effects to kick in.Git came along with GitHub as a parasite much like C came along as a parasite with Unix. reply adastra22 8 hours agorootparentI&#x27;m a huge critic of git, but I think it won out on merits. Git was used by the Linux kernel, and therefore from the beginning scaled better to much larger projects than other DVCS systems. reply goalieca 5 hours agorootparentGot scales very well with source code but does terrible with binary resources like graphics. The only time Iâ€™ve seen got really fail (but still work at slowly as other systems) is when some stubborn tech lead decides to put 3rd party source deps as tarballs in git. reply Ygg2 8 hours agorootparentprevExcept it doesn&#x27;t scale to codebases the size of Facebook and Google. reply dig1 5 hours agorootparent> Except it doesn&#x27;t scale to codebases the size of Facebook and Google.Actually, it scales well with large codebases, but the monorepo mental model pushed by FB&#x2F;Google doesn&#x27;t match the git workflow model. Git repo design is for a concrete entity: a single tool, binary, service, kernel, whatever. When you start bashing in multiple unrelated projects where you try to have various per-project versioning schemas, it becomes a mess. reply jahav 7 hours agorootparentprevMS stores windows codebase in a single repo (https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;bharry&#x2F;the-largest-git-repo-o...). 300GB.I don&#x27;t really see a benefit of having all code of an org in a single repo. Single product, sure. But whole company? Why. Not to mention, these are serious outliers.From security standpoint, it doesn&#x27;t seem great, from practical side, it&#x27;s not great (bandwidth cost ect). reply Ygg2 3 hours agorootparentThere was similar push by Facebook to improve Mercurial perf and they showed some impressive advantage over Git. It seems it was abandoned eventually for Git. reply bvrmn 8 hours agorootparentprev> Yeah, the mental model of Mercurial is way better.It&#x27;s more complex comparing with git naked graph. Contains more domain entities. But it could be more convenient for starters and SVN users. reply kelnos 8 hours agoprevShame they&#x27;re going to host it on GitHub, though, rather than self-hosting. I get that the network effects of GitHub will make it easier for potential contributors to get started, but... ugh, just sucks to see a successful open source project moving to a proprietary platform for their hosting. reply jiripospisil 7 hours agoparentI&#x27;m surprised they didn&#x27;t go with a self-hosted GitLab instance. I&#x27;ve always found GL to be better at handling large projects (projects, groups, sub-groups etc.), not to mention they could easily modify their instance to better integrate with the rest of their services. reply pointlessone 7 hours agorootparentThey donâ€™t seem to plan to use any of the GitHub features. They only use it as a â€œcloud git hostingâ€. My guess is if there was as reliable git-web hosting they might as well go with it. By only using GH for repo hosting theyâ€™re not locking in themselves and also outsource the tedious parts (infra, backups, etc.). reply severino 6 hours agoparentprevToo bad they&#x27;re going Github, indeed. Keeping away those users that won&#x27;t be willing (or able) to contribute to the project unless it&#x27;s hosted on GitHub is something desirable, in my opinion. reply akmittal 8 hours agoprevGit has become de facto standard and with github&#x27;s popularity most open source projects are loosing potential contributors. reply Banditoz 8 hours agoparentAre they losing contributors, though? I like GitHub&#x2F;Gitlab&#x2F;BitBucket as much as most people but choice of version control is going to be secondary to getting your patch into a major web browser in my opinion.Also looks like they&#x27;re still planning on using existing review tools:> We will continue to use Bugzilla, moz-phab, Phabricator, and Lando... Although we&#x27;ll be hosting the repository on GitHub, our contribution workflow will remain unchanged and we will not be accepting Pull Requests at this time reply lloydatkinson 8 hours agorootparentIâ€™ve not met anyone that claimed to like BitBucket! reply Banditoz 8 hours agorootparentLol, only reason I don&#x27;t mind it is my company&#x27;s self-hosted instance is extremely snappy compared to SaaS GitHub&#x2F;Gitlab. But it&#x27;s getting behind in features. Wish it had multiline comments&#x2F;suggestions. reply mapasj 8 hours agorootparentprevI have been quite happy with BitBucket. This is mostly from a UX perspective. The design is clean and simple. Iâ€™m using GitLab lately too, and my experience is the opposite. I canâ€™t think of any webapp with a worse UX. reply hahnchen 8 hours agoparentprevwhy would projects be losing potential contributors because github is getting more popular? reply Jochim 7 hours agorootparentNot using it increases the barrier to entry. Potential contributors are likely to know git and be relatively familiar with the contribution process on Github.Hosting it elsewhere means people are less likely to find it in the first place.Using a different VCS means people are less likely to pick it up. Instead of starting on their contribution they have to install and learn new tooling that is irrelevant to the project itself.Emacs is a pretty good example of this. There are plenty of people who would be interested in making improvements to the project but a much smaller proportion are willing to engage with the email-based process that&#x27;s required to do so.The crux of the issue is that the social element of an approach is just as important as the technical merit. A technically superior solution will quickly languish if no one can bear to work on it. reply severino 6 hours agorootparent> Hosting it elsewhere means people are less likely to find it in the first place.But Firefox is a well-known software and, at the same time, a very complex one. I don&#x27;t think that somebody who struggles figuring out how to contribute via, e.g., Gitlab, will be able to contribute anything useful to the project. reply WesolyKubeczek 7 hours agoprev> a significant burden on teams which are already stretched thin in partsJust saying that an insignificant reduction of the C-suite&#x27;s cushy bonuses and perks could very likely enable hiring quite a few smart people to reduce the burden on teams.If you try to bring the Foundation vs Corporation argument, I&#x27;ll say outright that this division is pretty much an exercise in creative bookkeeping invented not in the least to circumvent tax agencies and enable the top people to harvest top money while it lasts, while the line workers are being stretched thin. reply snvzz 8 hours agoprevAbout time. reply Dudester230602 6 hours agoparentLuckily, by now there are enough tools to hide the insanity of git behind the scenes. Just make sure to avoid the 1980-s way of using it (console). reply SuperNinKenDo 8 hours agoprev [â€“] Anybody got a link that explains why? reply lutrinus 8 hours agoparent [â€“] It&#x27;s right there in the first paragraph:> For a long time Firefox Desktop development has supported both Mercurial and Git users. This dual SCM requirement places a significant burden on teams which are already stretched thin in parts. We have made the decision to move Firefox development to Git. reply sideshowb 8 hours agorootparent [â€“] Nitpick, but that quote doesn&#x27;t explain why not move to mercurial (although I think we all know why) reply pointlessone 7 hours agorootparent [â€“] It was Mercurial-only for a long time. Then people got used to git and asked for a mirror to stop dealing with whatever was the layer providing mercurial access for git locally. Apparently, now there are more git people out there than Mercurial. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla has disclosed plans to shift the development of Firefox from Mercurial to Git, prompted by the struggles in managing both tools.",
      "Although the repository will be moved to GitHub, Mozilla will continue its current contribution process and will not accept Pull Requests at this time.",
      "The transition will be achieved in two stages: the first involves changes primarily affecting developers, and the second will transfer the back-end infrastructure from Mercurial to Git. The initiation of the process will follow a planning period of at least six months."
    ],
    "commentSummary": [
      "Mozilla has decided to change Firefox's development process from Mercurial to Git due to the challenges of maintaining both systems.",
      "Despite shifting the repository to GitHub, Mozilla plans to keep its current contribution workflow and will not entertain Pull Requests for now.",
      "The transition will happen in two stages: initially focusing on changes that impact developers, and later migrating the backend infrastructure. The process, set to kick off after a minimum six-month planning period, signifies a substantial shift in Mozilla's approach to code management."
    ],
    "points": 201,
    "commentCount": 165,
    "retryCount": 0,
    "time": 1699260604
  },
  {
    "id": 38158529,
    "title": "Chez Scheme to Merge Branches: Prepares for v10.0 Release",
    "originLink": "https://groups.google.com/g/chez-scheme/c/D7g6mIcYLNU",
    "originBody": "Groups Conversations All groups and messages Sign in î—„ î¢™ îˆ î‰ upcoming merge to create Chez Scheme v9.9.9 8393 views Skip to first unread message Matthew Flatt unread, Oct 16, 2023, 8:53:12 AM î º î…Ÿ î—” to chez-scheme Hi everyone, I have been working with long-term Chez Scheme maintainers to bring the https://github.com/cisco/ChezScheme and https://github.com/racket/ChezScheme branches together. We are just about ready to merge! Merging will change the code in cisco/ChezScheme to v9.9.9-pre-release.20, which is a step toward a v10.0 release. After the merge, the content of the main branch at cisco/ChezScheme will be almost the same as the current content of the racket/ChezScheme branch. In other words, we're merging all of the racket/ChezScheme functionality to cisco/ChezScheme. We've worked to ensure that the changes in racket/ChezScheme are not Racket-specific and that the changes preserve Chez Scheme's coherence, character, and quality. In case you're not already familiar with changes in the racket/ChezScheme branch, I've put a temporary copy of the release notes here: https://users.cs.utah.edu/~mflatt/tmp/release_notes_v999.html By \"content\" of the branch being the same as racket/ChezScheme, I mean the content of a checkout using the most recent commit of cisco/ChezScheme after the merge, but the history from both branches will be preserved. Tip: Clone the cisco/ChezScheme repo with `git clone --filter=blob:none` to avoid eagerly downloading large boot files for older versions. (That's a useful approach already, even before the merge, and this tip will appear at the top of the new \"README\".) The only substantial difference from the current racket/ChezScheme content is that the merged cisco/ChezScheme will not refer to a Git submodule for pb (portable bytecode) boot files. Instead, pb boot files will be part of the cisco/ChezScheme repo, continuing the way some platform-specific boot files have historically been checked into the cisco/ChezScheme. Matthew Amirouche BOUBEKKI unread, Oct 18, 2023, 11:49:31 AM î º î…Ÿ î—” to chez-scheme Thanks Matthew Flatt, and teams ! î—“ î…Ÿ Reply all î…ž Reply to author î…” Forward",
    "commentLink": "https://news.ycombinator.com/item?id=38158529",
    "commentBody": "Racket branch of Chez Scheme merging with mainline Chez SchemeHacker NewspastloginRacket branch of Chez Scheme merging with mainline Chez Scheme (groups.google.com) 200 points by Decabytes 14 hours ago| hidepastfavorite61 comments aragonite 11 hours agoI&#x27;ll always have a soft spot for Racket, because of how the IDE literally draws arrows for you, overlaid on top of the code, linking the various occurrences of a (hovered) variable together.[1][2] Seriously, how cool is that? Alas, it&#x27;s unclear how to begin to implement such a feature in a performant way in a DOM-based editor like vscode (I&#x27;m not a web developer, so maybe I&#x27;m wrong about this.)[1] Example gif: https:&#x2F;&#x2F;docs.racket-lang.org&#x2F;define-attributes&#x2F;examplecodear...[2] > Lexical Structure: The lexical structure is shown with arrows overlaid on the program text. When the mouse cursor passes over a variable, DrRacket draws an arrow from the binding location to the variable, or from the binding location to every bound occurrence of the variable. reply mark_l_watson 6 hours agoparentI have also have a soft spot for Racket, although professionally I lean on Common Lisp to get stuff done.The once a year RacketCon conference was two weekends ago, and in addition to the technical talks, I enjoyed hearing the 4 academics who are primary implementers and maintainers taking questions from the audience about why Racket is not targeted at industry. I am just finishing up writing a Racket book, that really just consists of my own little code experiments (no big deal). You can read it free online https:&#x2F;&#x2F;leanpub.com&#x2F;racket-ai&#x2F;read reply samth 3 hours agorootparentI don&#x27;t have notes of what I said (recording will be up soon) but that isn&#x27;t what I was trying to say at all. We value use in industry and have made significant effort to enable it (like relicensing everything). It is true, of course, that we&#x27;re academics, and so we need to learn from others about how to better enable industrial use of Racket, but that is not because we don&#x27;t care about it. reply mark_l_watson 4 minutes agorootparentI enjoyed the panel discussion - I was not complaining. reply davidrupp 1 hour agorootparentprevI didn&#x27;t read \"not targeted at industry\" as equivalent to \"doesn&#x27;t care about industry\". reply velcrovan 1 hour agorootparentIndeed. Racket maintainers do consider input from industry users and sometimes make changes based on that input, but it is not targeted at industry. reply bmitc 4 hours agorootparentprev> I enjoyed hearing the 4 academics who are primary implementers and maintainers taking questions from the audience about why Racket is not targeted at industry.That&#x27;s interesting. That seems like a flip from a few years ago. What was the summary regarding this? reply mark_l_watson 4 hours agorootparentMy understanding of what the 4 academic devs were saying: Racket is a rich environment for language experiments, academic work, and is attracting some interest in wider commercial use. I think they hope that more 3rd parties do some of the work for supporting industrial applications. I didnâ€™t take notes of the panel discussion, but this is how I remember the dialog. reply soegaard 3 hours agorootparentThe question came from a young member of the audience. It was phrased something like:\"Why isn&#x27;t Racket used in industry like OCaml is used by Jane Street.\"And what can you answer to that? Of course the Racket would like for some both commercial users as well as sponsors.In fact [1] Racket changed its license from LGPL to Apache 2.0 or the MIT license to make sure it can be used commercially.The fact is that language popularity isn&#x27;t determined by quality of language&#x2F;implementation alone. Having companies like Apple (Swift) and Google (Go) backing a language helps tremendously.[1] https:&#x2F;&#x2F;blog.racket-lang.org&#x2F;2019&#x2F;11&#x2F;completing-racket-s-rel... reply Galanwe 9 hours agoparentprevDid not try Racket specifically, but I did quite a lot of work with Dr. Scheme 15&#x2F;20 years ago, which I think is Racket&#x27;s ancestor?It would allow you to define variables which were actual images. The image was a thumbnail in the code, which you could assign and print. It was mind blowing.Also your Lambda keywords could be replaced with an actual Lambda symbol, which was the pinnacle of coolness.This, as well as the super snappy and responsive split REPL, made the overall development experience with Scheme feel super futuristic and just a pure enjoyment all around. reply Y_Y 8 hours agorootparentIt&#x27;s got myriad other issues, but Mathematica is a lot like this. You can have \"rich\" literals like a sound file or and image or a 3d model and see and manipulate along with the source, which has a couple of different levels of syntactic sugar, from being an ugly text-only lisp up to stuff that looks like it comes from a well-illustrated text book.What&#x27;s missing from my life is a good FOSS equivalent. There are several wonderful tools which do similar things, e.g. Jupyter, org-babel, TeXmacs, Sage, mathics. None of them feel quite right though. reply mark_l_watson 5 hours agorootparentI once had fantasies about starting an open source project for Common Lisp that would attempt to implement a little bit of the Wolfram data display stuff. I stopped paying for LispWorks, so I no longer have CAPI for GUIs, so that is not going to happen. Racketâ€™s UI support would make Racket a good platform for an open source project like this. (I wonâ€™t work on it, LLMs are so much more fascinating than any other tech, and use up my free time).I did recently pay for an entire year of Wolfram Desktop in the hope of really getting into it. I have code experiments for all the stuff I love (machine learning, deep learning, LLM, semantic web, etc.) but to be honest using the Wolfram Language does not give me the joy that I get using Common Lisp,and various Schemes, and, for some things Python is the most practical language. reply bitwize 5 hours agorootparentprevHolyC is like this as well. reply tkzed49 9 hours agoparentprevNo idea how to do this in a VSCode extension, but you could place a full-screenover the whole editor with pointer-events: none. Then, you could use offsetLeft&#x2F;Top or getBoundingClientRect() on relevant text elements to figure out where to draw lines on the canvas. reply ReleaseCandidat 10 hours agoparentprevI don&#x27;t think you can do that in a \"normal\" VS Code editor window, but it is possible in a custom one (where you have to do \"everything\" yourself): https:&#x2F;&#x2F;code.visualstudio.com&#x2F;api&#x2F;extension-guides&#x2F;custom-ed...(so, not much chance of adding that to my Chez extension ;) reply pjmlp 10 hours agoparentprevThe coolness of Lisp Machines and Interlisp-D, which is where Raket took its inspiration from. reply mark_l_watson 5 hours agorootparentI agree. In 1982 I got a Lisp Machine running InterLisp-D, and it was â€œbatteries includedâ€ like Racket.If you donâ€™t mind Smalltalk, the modern Pharo ecosystem is also amazing. reply sitkack 9 hours agorootparentprevThe Racket community is just full of wholesome folks and it is contagious. Racket makes more awesomeness in the world. reply jboynyc 8 hours agorootparentYes, there are many lovely people, but it is best not to romanticize the community as a whole -- it&#x27;s got some issues, too: https:&#x2F;&#x2F;beautifulracket.com&#x2F;appendix&#x2F;why-i-no-longer-contrib... reply BaculumMeumEst 8 hours agoparentprevracket is also one of the very few free lisps with a visual debugger and stepper, which is crazy when you realize how widely available that is in modern languages reply pjmlp 4 hours agorootparentMost likely because they got students to implement that for their thesis work, and free Common Lisp are seldom used for thesis stuff.Like in many free tooling most people rather do with what they have instead of improving them, hence why all great developer experiences are either commercial or corporate sponsored. reply mbork_pl 3 hours agorootparent> all great developer experiences are either commercial or corporate sponsoredGNU Emacs disagrees. reply pjmlp 2 hours agorootparentVSCode adoption in detriment of Emacs disagrees. reply mbork_pl 2 hours agorootparentGP talked about \"great developer experiences\", not adoption. There are lots of reasons inferior tools, standards etc. may be more widely adopted. reply pjmlp 1 hour agorootparentGreat development experiences drive adoption.I know Eight Megabytes of Continuous Memory Swap since those 8MB actually mattered. replyspdegabrielle 7 hours agoparentprevI donâ€™t think the arrows are possible with VScode?Someone said they might be possible with CodeMirror https:&#x2F;&#x2F;codemirror.net&#x2F;(Just in general - not specifically for racket- Iâ€™d love to see this for rust and elixir) reply ReleaseCandidat 7 hours agorootparent> I donâ€™t think the arrows are possible with VScode?They are possible, but not using the \"normal\" editor window, you need a \"custom\" one: https:&#x2F;&#x2F;code.visualstudio.com&#x2F;api&#x2F;extension-guides&#x2F;custom-ed... reply spdegabrielle 5 hours agorootparentSo not completely impossible, just effectively impossible. reply cfiggers 4 hours agorootparentWhy would doing it in VS Code be any more or less impossible than implementing it from scratch anywhere else? replyashton314 12 hours agoprevVery exciting stuff! I submitted a link to the actual MR when this happened; wasnâ€™t discussed much at the time: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37932016(This current submission is really nice; you get to hear more from Matthew about whatâ€™s going on.)Chez Scheme (pronounced â€œshay-schemeâ€) is the most performant and compliant Scheme implementation out there. [1] For a long time it was closed-source, but was recently open-sourced.It now forms the foundation of the Racket language. (So, I guess that means HN relies on it, provided Arc is using a recent version of Racket.) I asked Matthew why he picked Chez to base Racket off of, and I want to say that he picked it because it implemented continuations really really well and had a good garbage collector too. I could be wrong.[1]: https:&#x2F;&#x2F;ecraven.github.io&#x2F;r7rs-benchmarks&#x2F; reply ReleaseCandidat 11 hours agoparentIdris has changed from compiling to C to Chez (or Racket) with Version 2 https:&#x2F;&#x2F;youtu.be&#x2F;h9YAOaBWuIk?si=gkm2d_KLa3d9eoy5 reply LAC-Tech 36 minutes agoprevWasn&#x27;t chez scheme the dialect that let you compile scheme code at runtime as a first class thing? I remember looking into this stuff when I had it in my idea to make a configurable text editor.I know racket is very static in this regard. reply samth 32 minutes agoparentBoth Chez and Racket allow you to compile new code at runtime. reply neilv 12 hours agoprevIt&#x27;s encouraging to see all this solid (if unglamorous) software engineering work happening, for a powerful platform that also has active research going on.(Maybe I&#x27;ll find a way to resume using Scheme&#x2F;Racket. The last few years, I&#x27;ve been using Python, JS, and Rust, partly for employability reasons. They have their merits, but I&#x27;m aware of what I&#x27;m missing.) reply giancarlostoro 4 hours agoprevDoes this mean that the Racket and Chez teams will merge as well? I always liked Racket, and use it on spare time once in a blue moon (not enough hours in the day), but I am out of the loop on Chez vs Racket. reply samth 3 hours agoparentIt means that Matthew Flatt will become a committer to Chez Scheme, and they plan to include Racket-driven changes. Of course, we plan to maintain Chez Scheme&#x27;s high standards for quality and preserve all the things about it that are good outside Racket as well. reply cellularmitosis 9 hours agoprevOne of the things which the racket fork added was resurrecting support for OS X on PowerPC. Very cool to be running the fastest scheme on my old iMac! reply HwyarkGnuor 10 hours agoprevIs any software of note built on top of either software? All i can think of are these forums which, while enormously culturally influential, seem unremarkable from a technical perspective. reply Bogdanp 9 hours agoparentI donâ€™t know about â€œof noteâ€, but here are a couple pieces of software Iâ€™ve built with Racket:* https:&#x2F;&#x2F;franz.defn.io&#x2F;* https:&#x2F;&#x2F;remember.defn.io&#x2F;* https:&#x2F;&#x2F;defn.io&#x2F;2023&#x2F;08&#x2F;10&#x2F;ann-franz-source-available&#x2F;HN itself runs on Racket. reply mark_l_watson 5 hours agorootparentWonderful projects! I wish I could give you more than 1 upvote. reply jjtheblunt 1 hour agorootparentYou just make me realize, you effectively can.In the way those Dyson bladeless fans pull more air through. In your case, many folks know you (some from UIUC too like me) and know your upvote is worth reading, ending up upvoting too. reply Bogdanp 1 hour agorootparentprevThank you! reply bmitc 4 hours agorootparentprev> HN itself runs on Racket.Isn&#x27;t it an ancient version of Racket, though? reply velcrovan 1 hour agorootparentBut doesnâ€™t it also work fine, whatever version it uses? reply bmitc 6 minutes agorootparentMy point, based on the assumption that indeed Arc runs on an old Racket version, is that it doesn&#x27;t serve as a good example, in my mind, in terms of what you could build with Racket today.This is a quite common pattern in the Lisp and Scheme world where examples of real world usage are given, but they&#x27;re effectively outdated. reply sidkshatriya 9 hours agoparentprevEven though your question is a bit pointed, I think it is worth understanding what software is built upon Chez and Racket. The answers would be instructional to everyone. Here is some stuff, that I _do_ know -- there is probably a lot more usage in the wild:Racket is used extensively in education and research relating to Scheme and programming languages in general. Lots of work on gradual typing, programming language semantics comes out of the Racket community. Many colleges around the world use Racket. Scheme&#x2F;Racket is very pared down language and lends itself to this kind of work -- the principles of whatever you are studying shine through quite easily in a way that it may not if you were using C, Rust, Python etc. in the problem domain.As noted elsewhere in the comments, Idris 2, an important dependently typed language outputs to Chez&#x2F;Racket Chez. Previously it output C code which was then compiled.See: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Racket_(programming_language)#...In general, Chez is probably a great language to use as a \"base\". It lends itself to embedding and is performant. Lua and some Javascript implementations come to mind as comparables. In general, we might not know much about Chez being used a lot in the wild because it could be tucked deep into various proprietary company products. reply tmtvl 1 hour agoparentprevI believe mediKanren () runs on Racket. reply sillysaurusx 11 hours agoprevApropos nothing, can we get SICP added to SCP? Iâ€™ve always thought it would be a fantastic entry. \"Artifact SCP-31415, code name SICP, is a book on an obscure programming language. Students have been observed to die of fright when confronted with it, and those that escape have visible scars from what appear to be tiny blade-like curved objects. Some, however, end up becoming superstar programmers; therefore itâ€™s been an object of fascination for the agesâ€¦\" reply KingMob 7 hours agoparenthttps:&#x2F;&#x2F;scp-wiki.wikidot.com&#x2F;scp-061https:&#x2F;&#x2F;github.com&#x2F;KingMob&#x2F;SCP-061 reply devnull3 8 hours agoprevI had a look at https:&#x2F;&#x2F;racket-lang.org. Where we can download this build? [1][1] Assuming that the merge has happened since the announcement was on Oct 16 reply aseipp 3 hours agoparentRacket today and for several years now has already been using Chez Scheme, so any of the latest versions you download will be built that way. On all platforms. So, just download away, and you&#x27;re good to go.Rather, this announcement is about going the other way around, and about merging changes back into Chez: when Racket was migrated to Chez several years ago, as a matter of practicality they had to fork it and significantly modify it to maintain feature parity with the existing implementation. The changes were very large; multiple new supported ISAs and ABIs, new compiler optimizations, an entirely new build system, etc. For years, nobody was totally clear about what would happen with this fork, but the hope was that it would all go back to the upstream Chez codebase. That is finally happening, and so this post is not about merging code from Chez into upstream Racket, but rather merging code from Racket back into upstream Chez.The hope is changes will soon be available as a new major Chez Scheme release, version 10.0, and so the fork and the upstream version will finally be unified. (Racket will likely continue to use its own fork of Chez scheme as a practical matter of engineering, but presumably it will only need very minor tweaks and small patches, if any, for it to work.) reply soegaard 5 hours agoparentprevThe main line of Chez Scheme is here:https:&#x2F;&#x2F;github.com&#x2F;cisco&#x2F;ChezSchemeThere is more work to be done before release 10.0. reply wrx100 11 hours agoprevDoes this mean I can now use all&#x2F;lots of Racket libs in Chez? reply ReleaseCandidat 10 hours agoparentNo. But you (may?) no longer need the special Racket Chez to build and run Racket. reply rurban 8 hours agoprevTitle. Already merged about two weeks ago reply ltac 11 hours agoprevIs it a good idea to use Chez Scheme to go over SICP? Meaning, is it still standard to enter into compatibility mode to do so? Thanks. reply neilv 11 hours agoparentWe made Racket support for SICP, which is the next best thing to using MIT Scheme for SICP:https:&#x2F;&#x2F;docs.racket-lang.org&#x2F;sicp-manual&#x2F;(SICP Scheme isn&#x27;t quite standard, and there&#x27;s also some SICP-specific libraries.) reply kkylin 11 hours agoparentprevIt&#x27;s been a long time since I went through SICP, so someone with more recent knowledge should correct me. But I think you can get through most (almost all?) of SICP with what&#x27;s in the Scheme standard, which is a pretty small subset of almost any Scheme implementation. In particular, I don&#x27;t recall much that uses anything specific to MIT Scheme. reply dark-star 7 hours agoprevTIL that ChezScheme seems to be a Cisco project. Interesting... reply kryptiskt 5 hours agoparentCisco bought Kent Dybvig&#x27;s company some years back and open sourced Chez Scheme (they likely have more profitable products than a performant Scheme compiler and don&#x27;t need the thousands they could make on such a product). reply randomascoward 4 hours agoprev [â€“] Isn&#x27;t the title \"upcoming merge to create Chez Scheme v9.9.9\"? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Matthew Flatt and Chez Scheme's maintainers plan to merge Chez Scheme's different versions to streamline its code towards a release of v10.0.",
      "Post-merge, the code in racket/ChezScheme will be almost identical to the main branch at cisco/ChezScheme, with all changes made cautiously to uphold Chez Scheme's essence and quality.",
      "Contrary to current practice, the portable bytecode (pb) boot files will no longer refer to a Git submodule but will be incorporated directly into the cisco/ChezScheme repository."
    ],
    "commentSummary": [
      "Matthew Flatt and the Chez Scheme maintainers plan to merge the existing branches of Chez Scheme found on two GitHub links, aiming to transition the code in cisco/ChezScheme to v9.9.9-pre-release.20 and progress towards a v10.0 release.",
      "Post-merge, the content of the racket/ChezScheme branch will mirror the main branch at cisco/ChezScheme closely, with changes carefully checked to maintain Chez Scheme's character and quality.",
      "Notably, the merged cisco/ChezScheme will forgo referencing a Git submodule for portable bytecode (pb) boot files, incorporating them into the cisco/ChezScheme repository instead."
    ],
    "points": 200,
    "commentCount": 61,
    "retryCount": 0,
    "time": 1699241700
  },
  {
    "id": 38154045,
    "title": "New Tutorial Explains How to Host a Website on ESP32",
    "originLink": "https://esp.khalsalabs.com",
    "originBody": "This WebPage is hosted on ESP32 This website is up from November 1st, 2023. Tutorial about hosting website on esp32",
    "commentLink": "https://news.ycombinator.com/item?id=38154045",
    "commentBody": "Website hosted on ESP32Hacker NewspastloginWebsite hosted on ESP32 (khalsalabs.com) 196 points by harry247 23 hours ago| hidepastfavorite160 comments chefandy 22 hours agoHmmm... given the nature of the site, it probably would have been better to post an archive link:https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231105185258&#x2F;https:&#x2F;&#x2F;esp.khals... reply nativeit 22 hours agoprevHeadline should read Website was hosted on ESP32 ($3 chip) reply codetrotter 22 hours agoparentLoaded fine for me now :D reply nativeit 20 hours agorootparentThey must have spun up another cluster, maybe with some Teensyâ€™s running Redis on SD cards. ;-) reply harry247 19 hours agorootparentIts still a single esp32 s2 mini. Its Nginx that is saving this little mcu !! reply mikewarot 21 hours agoprevI wondered why in the heck a static web site running in a resource limited environment would even attempt to run https. Based on the HN hug of death, it appears the reverse proxy adds HTTPS&#x2F;TLS&#x2F;whatever support, and it wasn&#x27;t able to cope.I wish that the demand for security theater in web browsers wasn&#x27;t so high that they effectively prohibit plain old text transport. reply bradly 19 hours agoparentI would blame ISP&#x27;s and public WIFI providers before blaming browsers for preventing ISPs and&#x2F;or anyone else from MitM&#x2F;malware&#x2F;injection attacks. reply rstuart4133 14 hours agoparentprev> I wondered why in the heck a static web site running in a resource limited environment would even attempt to run https.Err, I don&#x27;t think it is. This is in the response headers: Server: nginx&#x2F;1.18.0 (Ubuntu)Getting nginx (let alone Ubuntu) running on a ESP32 would be a seriously impressive achievement.The web site also says: Once it is running, you can access your website by entering your ESP32&#x27;s IP address in a web browser. For example, if your ESP32&#x27;s IP address is 192.168.1.100, input http:&#x2F;&#x2F;192.168.1.100 in your browser.The site works for me now, and I&#x27;d suspect it to be able to support a minimal HTTP (not HTTPS) server if it ran natively, but then we also have this: You need to download micropython.py file and place it in the root directory of esp32.So it&#x27;s written in Python and a chip with the compute power of a potato. It would be interesting to compare it to the same thing done in C or Rust. reply pjmlp 8 hours agorootparentThe potato chip is more powerful than a Amstrad PC1512.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;PC1512While any serious game development was done in Assembly, languages like Python were a plenty to chose from.So instead of looking down on the project for having chosen Python, as someone that used to play Defender of the Crown at the high school computing club, I marvel to what an ESP32 is capable of. reply oytis 8 hours agoparentprevThere are multiple TLS stacks for microcontrollers, that are like decade old and running on devices much less performant than ESP32. reply harry247 20 hours agoparentprevyes, you got it right its due to that odd message web browsers show for non-https websites. This HTTPS&#x2F;TLS has been added using nginx on different server, also to support some load. The website is still under lot of traffic, however nginx is doing good on its part. reply 1vuio0pswjnm7 19 hours agoparentprevMaybe it&#x27;s more than security theater. With mandatory TLS, i.e., encryption plus third party-mediated authentication, the ability to publish a website comes under the control of third parties, so-called \"Certificate Authorities\" (CAs).The source of this third party \"authority\" is unclear. If a CA uses DNS for verification, then the \"authority\" is ultimately ICANN. And we know that ICANN&#x27;s \"authority\" is completely faked up. It has no legal basis. These pseudo regulatory bodies have no democratic process to ensure they represent www users paying for internet subscriptions. As it happens, these organisations generally answer only to so-called \"tech\" companies.Effectively, CAs and browser vendors end up gatekeeping who can have a website on the public internet and who cannot. Not to mention who can be an \"authority\" for what is a trustowrthy website and what is not (CA&#x2F;Browser Forum).The hoops that the browser vendors make a www user jump through in order to trust a website without the assistance of a third party are substantial and unreasonable. It seems that no www user can be expected to make their own decisions about what they trust and what they don&#x27;t. The decision is pre-made, certificates are pre-installed, autonomy is pre-sacrificed, delegated to so-called \"tech\" companies.Meanwhile these so-called \"tech\" companies, who are also the browser vendors, are commercial entities engaged in data collection for online advertising purposes. For more informed users, these actors are perhaps the greatest eavesdropping threat that they face. The largest and most influential of them has been sued for wiretapping www users on multiple occassions.There are conflict of interest issues all over the place.tl;dr Even if the contents of the transmission are not sensitive and perfectly suited to plain text, the system put in place by so-called \"tech\" companies, to benefit themselves at the expense of every www users&#x27; privacy, ensures that TLS must be used as a means of identifying what is an \"acceptable\" website and what is not. Absence of a certificate from a self-appointed, third party certificate authority means \"not acceptable\". Presence of certificates from first party authorities, i.e., ordinary www users, means \"not acceptable\". reply jampekka 7 hours agorootparentLet&#x27;s encrypt is doing god&#x27;s[1] work to work around the CA scam. And they&#x27;ve made it extremely easy to use. Literally just run one command and you have SSL on your website. May take a few more commands if you&#x27;re not using one of the more standard HTTP servers.You have to have a domain to use it obviously. Lucky there are other god&#x27;s workers like duckdns to work around the domain scam too.https:&#x2F;&#x2F;letsencrypt.org&#x2F; https:&#x2F;&#x2F;www.duckdns.org&#x2F;[1] Obviously not referring to any theistic entity here, but more to something like the spirit of FSF or whathaveyou. reply nanis 19 hours agorootparentprevAlso note https:&#x2F;&#x2F;www.nu42.com&#x2F;2014&#x2F;12&#x2F;https-everywhere-and-hr4681.htm... reply xvector 18 hours agorootparentprevabsolutely nothing prevents you from either:- adding another root CA, or- bypassing HTTPS warnings, or- taking 10 minutes to set up LetsEncryptand for obvious reasons, neither of the first two should be easy reply groestl 19 hours agoparentprevBecause sniffing? And tampering? reply tinus_hn 8 hours agoparentprevSo here is nginx running on a real computer and a homebrew webserver running on a potato, and your theory is nginx is the limiting factor. And itâ€™s all the fault of encryption and the browser cartel. reply IshKebab 20 hours agoparentprevIn what way is requiring encryption security theatre? reply anigbrowl 20 hours agorootparentNot everything needs to be encrypted. If I&#x27;m serving static webpages the only thing I might want to log is the which IPs visited at what time of day. reply shepherdjerred 13 hours agorootparent> Not everything needs to be encrypted. If I&#x27;m serving static webpages the only thing I might want to log is the which IPs visited at what time of day.As a frequent user of public WiFi (mostly at coffee shops, airports, etc.), I prefer that every page is encrypted so that nobody can MITM me&#x2F;tamper with what I see in my browser, even on plain text pages. reply averageRoyalty 53 minutes agorootparentIf you are frequently using networks you suspect to be hostile, wouldn&#x27;t you L2VPN your traffic back to a trusted exit point regardless? HTTP&#x2F;HTTPS is likely only part of the information your computer is providing to said hostile networks. Worrying about the encryption of plain text pages seems to be like worrying about a stain on the couch whilst the house is on fire. reply shepherdjerred 15 minutes agorootparentI think there are two discussions here:* Is using HTTPS enough on an insecure network? Should one also be using a VPN?* Would end-users see a benefit from HTTPS on simple&#x2F;plaintext sites?> HTTP&#x2F;HTTPS is likely only part of the information your computer is providing to said hostile networks.What other non-encrypted information might a normal person&#x27;s computer be communicating?I understand that VPNs do improve privacy. Privacy is moderately important to me, but I don&#x27;t think it&#x27;s important enough for me to use a VPN.There are also occasional vulnerabilities in TLS&#x2F;SSL&#x2F;HTTPS but... what can I really do about that? Even a VPN might establish its session with those technologies.> wouldn&#x27;t you L2VPN your traffic back to a trusted exit point regardless?It&#x27;s reasonable to expect someone technical like myself to do this, and maybe I am really just playing loose with my security. But, nobody outside of the tech community is even thinking about this. 99% of people are happy using whatever WiFi is free and aren&#x27;t going to question its security.So, using HTTPS for \"simple\" sites is still beneficial since you will be making your content more secure for less technical users who might be on insecure networks. reply woodruffw 19 hours agorootparentprevWhere does logging enter into it? To my understanding, serving traffic over HTTPS doesn&#x27;t require you to do any additional logging (or any logging at all).The point about static webpages would be a potentially good one in a world where ISPs and other Internet middlemen are honest and transparent actors, but this has so far proven not to be the case. I think it&#x27;s in everyone&#x27;s interest for your static content to reach my browser without ads or tracking mechanisms being injected by a third party. reply ZeroSolstice 15 hours agorootparentWhat example would you have of an ISP or third-party injecting an ad or tracker within the HTTP response? I&#x27;ve certainly seen the DNS query hijacking and while HTTPS will encrypt the transmission, at the ISP level they already have your DNS query and src&#x2F;dst IP address. Even with HTTPS based on session data it wouldn&#x27;t be difficult label Netflix&#x2F;Youtube traffic patterns.Do you also have any reference to what exactly the collected data is useful for? I could see an ISP selling traffic data for a zip or area but they would already have that based on your billing address. reply woodruffw 14 hours agorootparentAt the registrar level: the .tk registrar was (in)famous for injecting both ads and random JS into websites that were hosted on domains registered against it.At the ISP level: I had a Spanish ISP attempt SSL stripping on me a few weeks ago.> Do you also have any reference to what exactly the collected data is useful for? I could see an ISP selling traffic data for a zip or area but they would already have that based on your billing address.The goal is always more (and more precise) data points. Being able to run JS on the same origin as the request is more valuable than just the rough GeoIP data the ISP already has. reply anigbrowl 19 hours agorootparentprevAs in, if someone hacks into there wouldn&#x27;t be anything much to grab. For any sort of commercial service I would use as regular computer or on S3. reply woodruffw 19 hours agorootparentUnless I&#x27;m misunderstanding, I think that&#x27;s kind of orthogonal to the question of encrypted transit. Plenty of services that expose only HTTPS don&#x27;t encrypt at rest (and vice versa). reply esrauch 17 hours agorootparentprevHttps in this case prevents your users from being hacked, it does nothing to prevent your serverside from being hacked regardless. reply kxrm 19 hours agorootparentprevI agree that not everything needs to be encrypted, but unfortunately a lot of people who browse the web are concerned when the browser complains that something is not secure.From the browser maker&#x27;s side, how does a browser know whether something should or should not be secured? They have clearly taken a more aggressive approach to inform users what is going on within the underlying protocol. While I do agree that not everything needs to be encrypted, I also agree that the user should know what is or is not happening under the hood. reply zakki 18 hours agorootparentI guess the warning should be â€œnot encrypted â€œ. Being encrypted doesnâ€™t mean being secure (as there is a way to overcome the encryption). reply sodality2 17 hours agorootparentExplain how this encryption can be â€œovercomeâ€..? reply bmicraft 17 hours agorootparentprevIf everything that didn&#x27;t _need_ encryption wasn&#x27;t, then the use of encryption could be suspicious in itself. For my part I&#x27;m glad that we&#x27;ve moved to a mostly working system of encryption most things reply bradly 19 hours agorootparentprevIsn&#x27;t HTTPS needed to stop injection by ISPs? reply serf 19 hours agorootparentcustomer migration and fiscal loss is what should happen. reply ceejayoz 19 hours agorootparentBut it wonâ€™t, soâ€¦ https everywhere. reply IshKebab 8 hours agorootparentprevThis is like saying police aren&#x27;t needed because people should just move away from dangerous neighbourhoods. reply arthurcolle 19 hours agorootparentprevHow can you reliably know that you&#x27;re not being MITM without HSTS? reply mikewarot 19 hours agorootparentHow can you know you&#x27;re not with HSTS? The whole centralized security system is suspicious in terms of failure points. reply woodruffw 19 hours agorootparentThe Web PKI is hierarchical, but it isn&#x27;t particularly centralized (other than Let&#x27;s Encrypt increasingly eating everyone else&#x27;s lunch, which is probably a good thing).But in terms of actual failure points: if you&#x27;re initiating a connection over HTTPS, then the only way an attacker can MITM you is by convincing a CA to incorrectly issue them a certificate for that domain. That&#x27;s why Chrome and Safari monitor certificate transparency logs, and why website operations should also generally monitor the logs (to look for evidence of misissuance on their own domains). reply anigbrowl 19 hours agorootparentprevNot my problem if I am just serving a static page.For a commercial service or if I was handling people&#x27;s credentials I&#x27;d use something more robust. reply xboxnolifes 18 hours agorootparentIt is your problem if you&#x27;re interested in making sure people are actually getting your static page. reply ZeroSolstice 14 hours agorootparentI&#x27;m sure there is some nuance to what someones static site is serving but someones blog doesn&#x27;t need to be HTTPS. If they are offering downloads you can provide checksums or verify their data through other sources or contacting them out-of-band.Anything that needs some form of validation from any site should be verifiable in multiple ways. Just because they have HTTPS doesn&#x27;t mean the provided information or data is automatically correct. replyrany_ 22 hours agoprevHow are they doing SSL certificate management on an ESP32? Their article at https:&#x2F;&#x2F;khalsalabs.com&#x2F;hosting-a-website-on-esp32-webserver-... makes no mention of how that would work, only really basic code for a static cleartext HTTP server. Is it even capable of such a thing?Edit: I got a default nginx&#x2F;1.18.0 (Ubuntu) gateway timeout message after a few minutes trying to load this page, this is reverse proxied. reply jcalvinowens 21 hours agoparentAn HTTPS server with esp-idf is absolutely trivial: https:&#x2F;&#x2F;github.com&#x2F;espressif&#x2F;esp-idf&#x2F;blob&#x2F;b4268c874a4cf8fcf7...The software support is incredible IMHO, it&#x27;s a huge reason to use these chips. I made some toy temperature sensors with an esp32 last year, they make it so easy: https:&#x2F;&#x2F;github.com&#x2F;jcalvinowens&#x2F;tempsensor reply _Microft 21 hours agorootparentWhatâ€™s the project behind this? 30 assembled pieces are more than I had expected. reply jcalvinowens 20 hours agorootparentThere was no real goal beyond the experience of building the thing and making it work. I use them to monitor stuff like fridge&#x2F;freezer and HVAC intake&#x2F;output, and as leak detectors in my crawlspace.As you&#x27;d probably guess, the fixed cost of the manufacturing was extremely high. Unfortunately I didn&#x27;t write the numbers down... but going from memory, ordering 5 instead of 30 would have only reduced the total cost by ~20%. I remember a weird valley in cost-per-unit at a quantity of 30: my understanding is that JLC combines small orders, so my guess is that 30 of that board was the largest order they were willing to squeeze onto the same panel as another one. reply barbazoo 20 hours agorootparentprevEven a 8266 would probably work for most people. reply constantly 21 hours agorootparentprevVery curious about the scaling process. I&#x27;ve been building something on a breadboard with an esp32 and I&#x27;m pretty happy with it. Now I want it to be a lot smaller, and in one piece rather than with a bunch of wires and components on a breadboard.How do you make the step from breadboard dev to something manufacturable? reply jcalvinowens 18 hours agorootparentI didn&#x27;t do any breadboarding at all, I just jumped off the cliff with this. I started by designing a 1\"x1\" PCB in EasyEDA with just the MCU and pin headers, and had five manufactured&#x2F;assembled by JLCPCB to test the core of it. The first time I&#x27;d ever touched an ESP32 was when I got those PCBs in the mail and started trying to program them! It was really fun.Once I&#x27;d proved it worked, I pasted that 1\"x1\" layout into a larger footprint, and added the sensor, power supplies, and batteries. Again, I had no real way to test any of the new stuff: I just iterated until I stopped finding problems to fix, then had them manufactured. A big part of the fun of this has been having to commit to a design without the ability to test: it really makes you think. I also enjoy the exercise of writing as much of the firmware as I can while the hardware is in the mail, then seeing how much actually works when it shows up.In terms of bad decisions... I used builtin gpio pull-up resistors for I2C: it works, but the margin is very tight, it&#x27;s just not worth it (and also means I can&#x27;t put the ESP32 in sleep mode in some cases...). Wifi uses phase to encode information, so having no RF matching will impact its performance beyond the -6dB I mentioned in the README. The inductor&#x2F;capacitor values are much larger than necessary. The routing of the I2C lines taking a huge bite out of the ground plane under the switcher IC is dubious. Using 1.5V alkaline batteries is nice because I don&#x27;t have to worry about burning my house down... but I&#x27;ve gone through 200+ AAA batteries over the last year, and it feels very wasteful.I learned most of what little I know about PCB design from this youtube channel, I can&#x27;t recommend it enough: https:&#x2F;&#x2F;www.youtube.com&#x2F;@PhilsLab reply anigbrowl 19 hours agorootparentprevNext step is a system integrator like m5stack.com, either build a nice unit from their library of components and let them worry about the minor issues (power regulation etc). If you&#x27;re prototyping at home just put them in your own enclosure, if you want to go industrial you can 3d print something that integrates with their stuff (eg user-friendly modules like Core) or use the stamp components.If you have done all the circuitry want to just print&#x2F;assemble your own PCBs, sites like PCB unlimited will make up short runs or Digikey will handle larger scales. reply vinnymac 21 hours agorootparentprevI usually use https:&#x2F;&#x2F;oshpark.com&#x2F; or https:&#x2F;&#x2F;jlcpcb.com&#x2F; with EasyEDA or Kicad depending on what you&#x27;re comfortable with. A good 3D printer wouldn&#x27;t hurt either. reply schappim 21 hours agoparentprevThe error message \"504 Gateway Timeout nginx&#x2F;1.18.0 (Ubuntu)\" suggests that Nginx, running on Ubuntu, is acting as a proxy server and is timing out while trying to connect to the backend server. The SSL cert is on the proxy server. reply declan_roberts 21 hours agorootparentSo the answer isâ€¦ theyâ€™re not! reply harry247 21 hours agoprevOMG!! I didn&#x27;t expect that I am getting that much traffic from HN. I put this besides nginx but still its too much traffic to process for Esp32 S2 chip. Lol reply anigbrowl 19 hours agoparentShoulda spent the extra $2 for S3Just kidding, ESP32 in general is great, but the newer ones do offer a bunch of extra stuff onboard for very little more cost&#x2F;complexity. reply joshxyz 19 hours agoparentprevpoor chip must be fried already lol reply _ache_ 22 hours agoprevHere is the tutorial: https:&#x2F;&#x2F;khalsalabs.com&#x2F;hosting-a-website-on-esp32-webserver-....There is a nginx proxy. Nothing fancy here but kudo for the tutorial. reply harry247 21 hours agoparentthanks !! yes it behind nginx reply robbywashere_ 22 hours agoprevHow the heck did you get a kubernetes cluster to run on a esp32?! reply alexchamberlain 22 hours agoparentWas that a joke? There doesn&#x27;t seem to be any implication this is running k8s? reply kaptainscarlet 22 hours agorootparentIt&#x27;s running k8s with a load balancer for all the HN traffic reply andrewstuart2 22 hours agorootparentSorry, where&#x27;s the assumption it&#x27;s using kubernetes coming from? All I see in the response is nginx which doesn&#x27;t imply anything k8s. The blog linked from the page doesn&#x27;t mention k8s either (nor nginx).That said, you can run an ingress for a service that&#x27;s just an externalname reference with the right annotations depending on your ingress controller and I&#x27;m pretty sure it&#x27;d just work. reply forkerenok 22 hours agorootparentI think both commenters said those things in jest referring to the oft occurring on HN criticism that everything has to run on k8s nowadays, even a simple website. reply LoganDark 21 hours agorootparentprevthe joke is that every website needs k8s even though none of them do reply raxi 21 hours agoprev~20 years ago there have been websites running on 8-bit chips consuming 400 bytes of ROM and 32 bytes of RAM (e.g. http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20050601082859&#x2F;http:&#x2F;&#x2F;www-ccs.cs...., an open-source clone: http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20010109144200&#x2F;http:&#x2F;&#x2F;www.chat.ru...)A 32-bit CPU is very rich reply strags 21 hours agoparentI remember that PIC project. I don&#x27;t know if source was ever released, but I recall a lot of folks being very dubious about the claims made.Quote: The PIC has 1024 words (12-bits) of program ROM, ~256 bytes contain a hand-crafted RFC1122-compliant implementation of TCP&#x2F;IP including.HTTP&#x2F;1.0 and i2c eeprom Filesystem, using 3 to 99 instructions. TCP and UDP protocol stack, using 70 to 99 instructions. ICMP [supports upto ping -s 11], using upto 14 instructions. IP - Internet Protocol, v4, using 68 to 77 instructions. SLIP - Serial Line IP packetisation, about 76 inst Fully buffered UART, upto 115200 bps, using 38 to 56 instructions. Operating system: RTOS with Rate Monotonic Analysis, using 3 to 15 instructions. reply russdill 21 hours agoparentprevThere&#x27;s a big difference between http and https reply hermannj314 21 hours agorootparentI&#x27;m not really sure I&#x27;ve drunk the kool-aid yet on why my static blog needs to be served securely over HTTPS.Edit: wow. Some people are super sensitive about what ports I serve text on. Good luck to you all in life. reply rany_ 20 hours agorootparentEverything should be served securely these days. Prior to HTTPS being absolutely king, ISPs here used to inject EXEs with malware and do all sorts of nasty stuff. With HTTPS dominating they don&#x27;t do that sort of thing anymore as the share of HTTP traffic is so low making ROI very low.Anyway I&#x27;ll give you one reason based on the above on why you should serve your content over HTTPS, it shields you from potentially having your visitors be victims of something like this and in all likelihood they will blame you for whatever malware their ISP sent their way... they did get infected from your website, after all. reply chrisfosterelli 20 hours agorootparentAnd further, while edge cases around MitM do exist, the reality is really that it&#x27;d almost certainly just fine if someone&#x27;s personal blog was just http in 99.99% of cases. But most of the web traffic isn&#x27;t someone&#x27;s blog and it really should be encrypted, and it&#x27;s simple enough to set up for free nowadays, so it&#x27;s going to be far easier to get most of the web to be encrypted if we increasingly work to phase out http.Yes, small blogs are a &#x27;casualty&#x27; of this progression towards expecting HTTPS in that they have to put a tiny bit more work in, but if we didn&#x27;t do this we&#x27;d be back in the days of nitpicking about every single &#x27;acceptable&#x27; case of http while vendors use the fact that it doesn&#x27;t have widespread adoption to leave session cookies in plaintext requests for tools like fire sheep to grab. reply ndriscoll 19 hours agorootparentprevPeople can be really tedious when it comes to this subject. Like, for the authenticity use-case, the server could present its certificate followed by a signed but unencrypted page, in a standard way so the browser could check the signature. Then the signature for static resources can be cached on the server (or middle boxes) and no key exchange or encryption is needed, greatly reducing computational needs to serve a page while still keeping it essentially secure. There&#x27;s also fewer hops in this scenario (so better user experience), and it&#x27;s easier to do things like filtering with a simpler proxy without needing to install CAs. But no one wants to have a productive conversation about actual trade-offs here.Edit: in fact, if we used client certs for user identity[0], signed requests could also be used for form submission for e.g. public forums or youtube uploads where you might not care about privacy of the submission itself.[0] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38129870 reply Isthatablackgsd 21 hours agorootparentprevOne of the reasons is to prevent ISP and other to intercept the page and interject the codes before it arrives to the users. It a common method to put the payload in the http. I believe it called middle-in-the-man method. With https, it reduced a lot of attacks.There was a news about Comcast interjected a Steam storefront page with a data cap warning on it to a Comcast subscriber. And this happened inside Steam app which was using http at the time. reply LoganDark 21 hours agorootparentprevHTTPS doesn&#x27;t really have to do with whether the page content is sensitive. It&#x27;s more about protecting visitors from MitM attacks, traffic analysis, and their browser screaming at them and refusing to load your site because it&#x27;s \"\"insecure\"\" reply JTyQZSnP3cQGa8B 21 hours agorootparentprevSome ISPs modify the content and inject ads in HTTP web sites. reply hermannj314 21 hours agorootparentI get that. I understand why viewing http is insecure, I dont understand why serving it is insecure.Apparently this rubs people the wrong way. I get it, run Lets Encrypt and certbot blah blah, but if I am hosting an ESP32 in my house for a hobby project, I running HTTP on the LAN. reply bee_rider 21 hours agorootparentHTTP in your house, over your LAN, seems like a fine thing to do. I think people are assuming that your blog would be on the internet. reply Volundr 20 hours agorootparentprev> I get that. I understand why viewing http is insecure, I dont understand why serving it is insecure.Presumably you are serving that content so it can be consumed no? It&#x27;s not like your consumers can consume https if you only serve http. But yeah I suppose if you are serving read-only content and don&#x27;t give a shit about what happens client side, there&#x27;s a lot less reason for https. reply xboxnolifes 18 hours agorootparentprev> I understand why viewing http is insecure, I dont understand why serving it is insecure.People are assuming you want others to be able to see want you are serving. In such case, the server is the only one who can secure the transmission to prevent MITM. The viewer cannot reach over and add in https into the request to prevent their ISP from injecting ads (or other kinds of MITM changes). reply bmicraft 17 hours agorootparentWeelll they could use a vpn reply IshKebab 19 hours agorootparentprevServing data via http is insecure because that data can be intercepted, read and modified.If it&#x27;s entirely public data then there&#x27;s no security risk to the server. The security benefit is for the clients, so unless you hate your users you should use encryption even for totally public static data. reply Filligree 21 hours agorootparentprevMine tries, and breaks the page in the process. I effectively cannot view any pure HTTP website. reply Isthatablackgsd 21 hours agorootparentYou can view pure HTTP website through VPN. It basically encrypted tunnel between you and VPN server through your ISP. So your ISP couldn&#x27;t try to interject the encrypted connection.However, your browser might prevent you from connecting to http due to strict https only policy. My browser will stop any connection to http page and throw up a warning. reply VBprogrammer 21 hours agorootparentprevIf your ISP is this shitty you should probably take your money elsewhere. reply smugma 20 hours agorootparentISPâ€™s are often a monopoly or duopoly of equally shitty options. reply BrandoElFollito 20 hours agorootparentNot everywhere though. The idea of an ISP in France injecting content is insane (and wild be completely illegal as well) reply northwest65 20 hours agorootparentprevThen leave whatever tech backwater you live in and move to the first world. reply VBprogrammer 29 minutes agorootparentMy understanding is that this tech backwater is most likely the good ol&#x27; US of A. replyJleagle 21 hours agorootparentprevhttps:&#x2F;&#x2F;doesmysiteneedhttps.com&#x2F; reply tlavoie 19 hours agorootparentprevHaving HTTPS as the only option for a site is an excellent default, both for protecting the confidentiality and integrity content, as well as validating the identity of the site for the client. Maybe a good way to put it is that the vast majority of the site&#x27;s uses and data needs no protection, but protecting all of it well is probably much easier to do correctly than just selectively encrypting the important parts.My humble little personal site has largely unauthenticated, static blog stuff. It also has personal apps that nobody else uses, but I want to protect the authentication bits. reply groestl 19 hours agorootparentprevYou can serve to yourself whatever you want, but please don&#x27;t make other people leak what you&#x27;ve served them. reply ben0x539 19 hours agorootparentprevYou obviously don&#x27;t need to serve your static blog over HTTPS, but I think it would be nice of you to serve it over HTTPS anyway! reply spiderice 19 hours agorootparentprev> Some people are super sensitive about what ports I serve text onIf I thought the key difference between http and https was which port it is served on, Iâ€™d probably be confused by peopleâ€™s reaction too. reply echelon 21 hours agorootparentprevYou&#x27;re being downvoted, but you&#x27;re right.I&#x27;m fine serving my personal website under http.- If someone is worried they&#x27;ll be found out using my site, then fine, don&#x27;t use it. This advice is just for my site, and it&#x27;s fine to desire security elsewhere and in other contexts.- If an ISP or MITM want to inject some content in my website, then fine. We&#x27;ll all know not to use those providers. I promise I&#x27;m not important enough for this to be a vector someone would want to exploit.None of the information I have to offer you requires HTTPS. I assure you.I think it&#x27;s fine that https is becoming the default, especially for web services. But we shouldn&#x27;t enforce it. It&#x27;s an undue burden to have to support all the certificate machinery just to serve some basic info.We really need to get back to the basic, easy to hack web. Where it took nothing to spin up services on your home machines and serve them as demos to others. That ethos was great. reply jacquesm 21 hours agorootparentThe web was fantastic until money got involved. And the best parts of the web are still where there is no money involved. reply bobmaxup 20 hours agorootparentWhen wasn&#x27;t money involved?Geocities was bought for $3.6 billion dollars by Yahoo in 1999. It lauched in 1994. The web is only three years older than that.I had my first website on Angelfire in 1996 before my 10th birthday. WhoWhere purchased Angelfire a year later, and then they were bought by Lycos a year after that for $133 million.Also, I don&#x27;t remeber it being fantastic. To me, even with all faults considered, things are much nicer today. reply stevenhuang 20 hours agorootparentprevThe point is not your safety, but the safety of all your viewers.The more ubiquitous http is for the average internet user, the more worth the squeeze MITM becomes for the targeted user. reply echelon 17 hours agorootparentThat&#x27;s bullshit when you&#x27;re accessing my website, where I have some photos of some old science projects and that&#x27;s it.A much better middle ground would have been for websites to advertise certain features (login, user accounts) and for browsers to warn when not using SSL. Or to do it based on some heuristic, such as cookie use on a given domain.The current implementation keeps everyone non-technical from using http, which is a loss for everyone.Google unilaterally got to make this decision for everyone. Small websites don&#x27;t matter to their bottom line anymore. They&#x27;ve already scraped and indexed the content, pulled the value away onto walled gardens, and left that web to rot. reply bobmaxup 10 hours agorootparentI don&#x27;t remember that being the reasoning:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cBhZ6S0PFCYGoogle I&#x2F;O 2014 - HTTPS Everywherehttps:&#x2F;&#x2F;developers.google.com&#x2F;search&#x2F;blog&#x2F;2014&#x2F;08&#x2F;https-as-r... reply stevenhuang 13 hours agorootparentprevyou&#x27;re still not getting itit doesn&#x27;t matter what content is being servedthe point is if your site is on HTTP a third party may silently inject malicious code into the response.any visitor that views your site now becomes subject to this threat vector.you may argue nothing will come of it, sure, but then you should make that your argument. replyHelixEndeavor 21 hours agoparentprevWell, given that the modern web has a lot more requirements for security to even permit most browsers to view a site, it makes sense that the base hardware needs have increased noticeably. reply raxi 21 hours agorootparentExactly those PIC18 devices, still in production and on sale, w&#x2F;o any changes during the years: http:&#x2F;&#x2F;utronix.se&#x2F;Of course, no https, but.. it is not a platform limitation, just an undemanded feature: how would you get a https cert for 192.168.0.1 or a similar intranet address where those device suppose to work? They are just not for cloud datacenters reply Filligree 21 hours agorootparentYou can make an HTTPs certificate with that in the SAN section, and it should work fine. You can&#x27;t get one from a publicly trusted provider, of course, but that&#x27;s fine; you don&#x27;t own the IP.In other words, make your own certificate authority for your own machines. It isn&#x27;t that hard. reply raxi 20 hours agorootparentThe problems here is not that hardness, and not even yearly certificate updates, or bothering with new certs on every IP address change, but (as the commentator above rightly pointed out)...1. Planned obsolescence built into HTTPS: no HTTPS-aware server device from year 1999 would work with 2023 browsers. Just because \"too old crypto\". Plain HTTP works.Being on a buy side I am against HTTPS in such devices, but I understand the sell side&#x27;s position. reply Kevcmk 21 hours agoparentprevYou&#x27;d be a bad parent reply SilverBirch 20 hours agoprevI personally think it&#x27;s pretty mean to hear that a website is hosted on an ESP32 and then post it on HN. Guess we&#x27;re going to test this one to destruction... reply Aissen 22 hours agoprevLooks like there&#x27;s a (non-caching?) nginx reverse proxy in front to do the TLS. I remember trying to do TLS on an ESP8266 and there was a hardcoded limit on the SSL buffer size, limiting the maximum cert chain that could be served. I wonder if there is a similar reason here. reply LoganDark 21 hours agoprevHosting a website on ESP32 is one thing, but running Python on it?! Are you insane? No wonder it gets hugged to death if more than 2 people visit it at once. reply YiraldyGuber 16 hours agoparentThank god someone said it. That poor ESP32 needs to be rescued from this sadist. reply declan_roberts 20 hours agoprevWebsite hosted on ESP ($3 chip) and served from $15,000 CDN server. reply harry247 19 hours agoparentnope, no CDN. It has NGINX reverse proxy reply 00deadbeef 22 hours agoprevItâ€™s down now. If the typical hackernews traffic could all be queued up, how long would it take the ESP32 to finish processing it all? reply notRobot 22 hours agoparentDoes anyone have recent information on how much traffic being on the HN frontpage gets you? Would be helpful to know. reply mobilio 22 hours agorootparent15-20k but can be easy solved with CloudFlare full page cache rule reply yjftsjthsd-h 22 hours agorootparentWell yeah, anything is easy if your server isn&#x27;t doing the work. reply mikercampbell 22 hours agorootparentâ€œThis website is run on an ESP32 chip that only cost $3 as for my proxy, Iâ€™m using distributed cluster of rack servers and relaysâ€ reply snvzz 22 hours agorootparentprevLikely possible to handle HN load just fine... if the page was static.edit: page loaded. It&#x27;s apparently Microdot, a micro web framework running on micropython. reply LoganDark 21 hours agorootparentThat explains why it instantly fell over reply waveBidder 22 hours agorootparentprevit&#x27;s currently dead reply osigurdson 22 hours agorootparentprevMy website runs on baling twine and duct tape (...and CloudFlare). reply harry247 21 hours agoparentprevI think it will drop most of the traffic, as I posted in load test results. The traffic coming to this is crazy. I can see nginx access logs and requests are not stopping!! reply korhojoa 19 hours agorootparentWhy don&#x27;t you configure the nginx that&#x27;s in front to cache? I mean, it&#x27;s still hosted on the device, it&#x27;s just got a caching proxy in front, like all the big boys do. reply harry247 18 hours agorootparentJust did it !! Enabling 10m caching on nginx now (It is good atleast now the link will open from HN homepage). Thanks for suggesting this, I should have done it before reply harry247 19 hours agorootparentprevIts good idea, I never tried or probably didn&#x27;t know much about nginx caching. Will Nginx cache webpage and serve directly without hitting much the esp32 server. I am going to read on it now reply harry247 21 hours agoprevI did some load tests before postings, simulate 100 users at a time. It was serving only 47 of them: https:&#x2F;&#x2F;bashify.io&#x2F;images&#x2F;tNg4SB https:&#x2F;&#x2F;bashify.io&#x2F;images&#x2F;7OtLGf reply oytis 8 hours agoprevCool, but is it news? MCUs have been capable of running simple webpages for at least 20 years. What&#x27;s unusual is exposing it to public internet, where it can be quickly hugged to death. reply shrubble 22 hours agoprevEven on the much lower end ESP8266 it was usual to use a web browser to let the user configure the application etc. It was fast enough, for one user at a time... reply c22 21 hours agoprevI put a website into 95% of my esp32 projects, mostly to make configuration easier. reply calibas 21 hours agoparentOut of curiosity, what do you use for the server?I&#x27;ve had good experiences with the HTTP Server component built into ESP-IDF. I see the example in this post uses Microdot. reply c22 18 hours agorootparentI just open a socket and write text to it. reply acbmkt 21 hours agoprevhttps:&#x2F;&#x2F;acbmkt.com&#x2F;intelligent-cameras-in-the-service-of-hea... reply simlevesque 23 hours agoprevWebsite doesn&#x27;t load. reply harry247 21 hours agoparentit was too much traffic to handle for this little poor chip reply flycatcha 22 hours agoprevThe tutorial talks about how to do a local webserver where you manually navigate to the IP address. How is this being served over the internet? reply panki27 22 hours agoparentDNS and port forwarding. reply grishka 22 hours agorootparentAnd probably buying a static IP from your ISP. reply byteknight 22 hours agorootparentJust DDNS reply grishka 22 hours agorootparentNot all home ISPs give you an entire IPv4 address. Some have fewer IPs than subscribers and thus use a NAT. Sometimes a very restrictive one. reply roywashere 21 hours agorootparentIn Europe, most ISPs use carrier graden NAT nowadays reply panki27 5 hours agorootparentYou can get around the issues of CG-NAT by setting up a reverse tunnel to an external server you control. replyLoganDark 21 hours agorootparentprevnginx reverse proxy reply harry247 18 hours agoparentprevyes, I have a static IP and port forwarded. It IP can be mapped to website (dns name) via namecheap and godaddy like services. This is enough to put it on internetIn this setup I added a nginx in between (doesn&#x27;t enable cache yet) for load balancing. reply 0172 22 hours agoprevIt is feasible to run a small static HTML&#x2F;CSS&#x2F;JS page from this microcontroller with a Nginx, Varnish, etc cache frontend. reply imhoguy 21 hours agoprevPut that on OpenWRT router (usually more powerful than ESP) and you call it \"hosted on edge\" reply amelius 19 hours agoprevI want to see a website hosted on a computer built entirely from discrete transistors. reply jdjdjdhhd 20 hours agoprevI used to host a website on ESP8266 for my personal use... An even weaker and cheaper chip reply kaptainscarlet 22 hours agoprevI got a gateway error. Perhaps it \"works on your machine\" only. :-D reply gotbeans 22 hours agoprevHn&#x27;s Hug of death reply mikercampbell 22 hours agoparentCame to do my part reply nektro 20 hours agoprevwe didn&#x27;t stop hosting websites on hardware like this because it&#x27;s impossible. we stopped because spammers made it impractical. reply amelius 22 hours agoprevPerhaps it would work better on a cluster of these. reply spudlyo 22 hours agoparentRight, like a Beowulf cluster!? reply system2 21 hours agoprevWebsite I tried to host on ESP32* reply harry247 21 hours agoparentLol! this is true after posting on HN reply rabbits_2002 21 hours agoprevwell not sure what i expected but it seems to be down lol reply m3kw9 21 hours agoprevLooks burnt out now reply Poudlardo 21 hours agoprev...but not for long reply jtth 22 hours agoprevSure seems like it! reply ulrischa 23 hours agoprevIf you look for something similar for the raspberry pico w try phew! from pimoroni: https:&#x2F;&#x2F;github.com&#x2F;pimoroni&#x2F;phew Also I think microdot will be running on the pico too. reply kleton 19 hours agoprev [â€“] Sat sri akal reply harry247 18 hours agoparent [â€“] Sat Sri Akaal replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The webpage, created on November 1st, 2023, is situated on ESP32, a series of low-cost, low-power system-on-a-chip microcontrollers.",
      "It offers a tutorial on how to host a website on ESP32, providing valuable information to individuals interested in utilizing this system for their sites.",
      "The establishment of this tutorial webpage demonstrates a practical application of ESP32, revealing its potential for hosting websites."
    ],
    "commentSummary": [
      "The webpage, established on November 1st, 2023, is hosted on ESP32, a series of low-cost, low-power system on a chip microcontrollers with integrated Wi-Fi and dual-mode Bluetooth.",
      "This site features a tutorial demonstrating how to host a website on an ESP32, offering users a guide to leveraging this technology.",
      "It offers hands-on tech learning opportunities for those seeking to understand web hosting on low-power microcontroller systems like the ESP32."
    ],
    "points": 196,
    "commentCount": 160,
    "retryCount": 0,
    "time": 1699209084
  },
  {
    "id": 38158309,
    "title": "milliFORTH: Claimed the Smallest Real Programming Language",
    "originLink": "https://github.com/fuzzballcat/milliForth",
    "originBody": "milliForth A FORTH in 422 bytes â€” the smallest real programming language ever, as of yet. The code in the above gif, that of hello_world.FORTH, is a modified version of the hello world program used by sectorFORTH (see below) bytes? Yes, bytes. This is a FORTH so small it fits in a 512-byte boot sector. This isn't new â€” sectorFORTH1 successfully fit a FORTH within the boot sector. However, milliFORTH appears to be the smallest \"real\" programming language implementation ever, beating out sectorLISP2, a mind-blowing 436 byte implementation of LISP, by 14 bytes. (\"real\" excludes esolangs and other non-production languages - for example, the sectorLISP author's implementation of BF is just 99 bytes, and their binary lambda calculus implementation is 383 bytes, but neither language is used to any serious capacity.) Language sectorFORTH1 was an extensive guide throughout the process of implementing milliFORTH, and milliFORTH's design actually converged on sectorFORTH unintentionally in a few areas. That said, the language implemented is intentionally very similar, being the 'minimal FORTH'. FORTH itself will not be explained here (prior understanding assumed). Being so small, milliFORTH contains just a handful of words: Word Signature Function @ ( addr -- value ) Get a value at an address ! ( value addr -- ) Store a value at an address sp@ ( -- sp ) Get pointer to top of the data stack rp@ ( -- rp ) Get pointer to top of the return stack 0= ( value -- flag ) Check if a value equals zero (-1 = TRUE, 0 = FALSE) + ( a b -- a+b ) Sum two numbers nand ( a b -- aNANDb ) NAND two numbers exit ( r:addr -- ) Pop from the return stack, resume execution at the popped address key ( -- key ) Read a keystroke emit ( char -- ) Print out an ASCII character state ( -- state ) The state of the interpreter (0 = compile words, 1 = execute words) >in ( -- >in ) The current offset into the terminal input buffer here ( -- here ) The pointer to the next available space in the dictionary latest ( -- latest ) The pointer to the most recent dictionary space milliFORTH is effectively the same FORTH as implemented by sectorFORTH, with a few modifications: Words don't get hidden while you are defining them. This doesn't really hinder your actual ability to write programs, but rather makes it possible to hang the interpreter if you do something wrong in this respect. There's no tib (terminal input buffer) word, because tib always starts at 0x0000, so you can just use >in and don't need to add anything to it. In the small (production) version, the delete key doesn't work. I think this is fair since sectorLISP doesn't handle backspace either; even if you add it back, milliFORTH is still smaller by a few bytes. Error handling is even sparser. Successful input results in nothing (no familiar ok.). Erroneous input prints an extra blank line between the previous input and the next prompt. Use sector.bin is an assembled binary of sector.asm. You can run it using qemu-system-i386 -fda sector.bin (as found in the makefile), or by using any emulator of your choice. Alternatively, make will reassemble sector.asm, then run the above qemu emulator. Included in this repo is a pyautogui script which can be run to automatically type in the hello_world.FORTH file into your qemu emulator. A very useful tool. It is self-explaining, but usage involves simply starting the QEMU emulator, running the python script, and putting your cursor into the QEMU emulator again. make sizecheck is a utility which assembles sector.asm and then lists files including size. This is useful for checking binary size. Note that it will always return 512, as bootloaders must have a fixed size of 512 bytes; to view the true size, the following two lines must be commented at the end of sector.asm: ; times 510-($-$$) db 0 ; db 0x55, 0xaa References Footnotes The immensely inspirational sectorForth, to which much credit is due: https://github.com/cesarblum/sectorforth/. â†© â†©2 Mind-blowing sectorLISP: https://justine.lol/sectorlisp2/, https://github.com/jart/sectorlisp. â†©",
    "commentLink": "https://news.ycombinator.com/item?id=38158309",
    "commentBody": "milliForthHacker NewspastloginmilliForth (github.com/fuzzballcat) 187 points by binarycrusader 14 hours ago| hidepastfavorite47 comments tromp 9 hours ago> However, milliFORTH appears to be the smallest programming language implementation ever, beating out sectorLISP2, a mind-blowing 436 byte implementation of LISP, by 14 bytes.The sectorlambda implementation of Binary Lambda Calculus is shorter yet at 383 bytes [1].And the BLC self-interpreter is only 29 bytes [2].> A FORTH in 422 bytes â€” the smallest real programming language ever, as of yet.That may still be true, as BLC is an esoteric programming language.[1] https:&#x2F;&#x2F;justine.lol&#x2F;lambda&#x2F;[2] https:&#x2F;&#x2F;ioccc.org&#x2F;2012&#x2F;tromp&#x2F;hint.html reply RodgerTheGreat 2 hours agoparentThe main advantage of a miniature Forth like this over BLC, Lisp, Brainfuck, etc, is that Forth grants low-level access to the hardware; you can easily bootstrap from it to something indistinguishable from a feature-rich Forth with a full suite of metaprogramming capabilities, or implement an entire operating system on top of it. reply jlokier 1 hour agorootparentMiniature Lisps are similar to Forth in this way. Having worked with both I&#x27;m inclined to favour Lisps for providing cleaner structure and abstractions for code and data to build up from, with low overhead despite the superfical differences. (See SectorLisp2 (436 bytes) vs SectorForth (491 bytes) size: https:&#x2F;&#x2F;justine.lol&#x2F;sectorlisp2&#x2F;).Lisps are given low-level hardware access primitives (peek&#x2F;poke etc) when they are designed for bootstrapping an OS, and low-level OS primitives (such as system calls) when they are designed for bootstrapping a rich environment on a different OS. Basically the same primitives as Forth for the same purposes. reply mepian 1 hour agorootparentprevNothing is stopping Lisp from having low-level access to hardware. reply tromp 1 hour agorootparentprevIs low-level hardware access part of the language definition [1] ?[1] https:&#x2F;&#x2F;forth-standard.org&#x2F;standard&#x2F;words ? reply eichin 9 hours agoparentprevHmm, I&#x27;d be a little surprised if this was smaller than some of the 8-bit era forths (the TIL book had a 50ish-byte \"inner interpreter\" for Z80 but that didn&#x27;t include any of the baseline \"words\" that this does.) I&#x27;m sure it wins for 32-bit systems though. reply PaulHoule 2 hours agoprevWhen I was in high school I wrote a nice FORTH for the TRS-80 Color Computer using the OS-9 operating system which was a Unix-like multitasking OS that would fit on a 6809 microcomputer.I think it was around 2000 lines of assembly code to implement most of the FORTH-83 standard although mine was unusual in that it did not support the block-based I&#x2F;O that was common on â€œlanguage systemâ€ FORTHs but instead it had handle-based API for accessing files similar to Unix, C and MS-DOS in version 2 and up.The programming environment was a lot like Linux overall in that Iâ€™d use an ed or vi clone to edit files, then run something like an assembler or C compiler. Iâ€™d run my FORTH binary and it would present an interactive environment like most FORTHs. reply anotherhue 13 hours agoprevI love projects like these, reminds me of the magic within the machine, as opposed to the normal cacophony of the world that comes via the machine. reply digitalsankhara 8 hours agoparentI do like this reasoning. Magic within the machine for me was programming forth as part of my post grad (controlling and processing proton precession magnetometers) and the device was a Triangle Digital Services (now defunct as a viable company I think) TDS2020 forth SBC.Almost a zen like experience - you, the machine and your focus. No world. reply alexisread 2 hours agoprevLooking at the code, this looks remarkably similar to sectorforth? Thing is, with sectorforth and this, 2 of the primitives are not required so reducing the VM to 6ops, so you can probably go smaller. Looks as though Cesar was happy with fitting into a sector: https:&#x2F;&#x2F;github.com&#x2F;cesarblum&#x2F;sectorforth&#x2F;issues reply mikewarot 9 hours agoprevI&#x27;ve always wondered how to boot a virtual machine from a single sector with no other OS... now I know, which is cool. reply badcppdev 8 hours agoprevDoes anyone in the Forth community know if Charles Moore is still with us? reply thesuperbigfrog 6 hours agoparentHe has recently done some amazing low power work with GreenArrays:\"GreenArrays is shipping its 144-core asynchronous chip that needs little energy (7 pJ&#x2F;inst). Idle cores use no power (100 nW). Active ones (4 mW) run fast (666 Mips), then wait for communication (idle).Tight coding to minimize instructions executed will minimize power. The programmer can also reduce instruction fetches, transistor switching and duty cycle.\"https:&#x2F;&#x2F;youtu.be&#x2F;0PclgBd6_ZsIt is like the elegance of Forth in low power, multi-core hardware. reply badcppdev 5 hours agorootparentI think you&#x27;re right that he&#x27;s still at Green Arrays. Still a director there apparently.I do have to note that the linked video is from 2013 so I assume they&#x27;ve moved on from that. reply LeonenTheDK 1 hour agorootparentI have heard nothing new out of Green Arrays but the site still seems to be selling the boards. If I had free cash available I&#x27;d try ordering one just to see. It&#x27;s a really interesting concept and I&#x27;d love to see it developed. reply t-3 3 hours agoparentprevHe is. SVFIG&#x27;s annual \"Forth Day\" meeting is on November 18th and he&#x27;s likely to be there giving the traditional \"Fireside Chat\". reply badcppdev 26 minutes agorootparentThat&#x27;s awesome to know. I remember reading his blog years ago when he was setting up Green Arrays reply weinzierl 5 hours agoparentprevYes, and so is Elizabeth Rather. reply Stratoscope 8 hours agoparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Charles_H._Moore reply tromp 7 hours agorootparentStill going strong at 85 years old... reply lynx23 10 hours agoprevPorting JonesFORTH to x86-64 was one of the most rewarding fun-project experiences lately. Forth is fun to play with. reply LordShredda 12 hours agoprev [â€“] Does the benefit of it being embeddable on a QR code outweigh the lack of quality of life features like subtraction? Nevertheless, truly an impressive feat that shows how simple computers can be without all these modern API layers. reply gavinhoward 12 hours agoparentFor one specific purpose, the benefit could be worth it: bootstrapping.Guix bootstraps from a tiny audited binary, and milliForth could be used for the same purpose.Imagine bootstrapping a full Linux distro from a milliForth binary and source code for everything. Everything would be fully auditable, no Trusting Trust problem, and a full Software Bill of Materials. reply eternityforest 10 hours agorootparentI always thought of FORTH as the #1 hardest to think of uses for, out of all the non-esoteric and non-obsolete languages. That&#x27;s a really neat application! reply defrost 10 hours agorootparentBooting from a firmware Forth loader was how the early Sun SunOS (BSD) workstations did their thing - you could hotkey to stop the default OS load and boot from an image on an alternative drive, across a network, or modify the Forth loader toreply AlotOfReading 10 hours agorootparentThat forth system is also where Device Trees originated. Parts of the Linux kernel driver interface show that heritage in the naming conventions, OF_* == OpenFirmware. reply nine_k 10 hours agorootparentprevBeside bootstrapping, Forth works well on tiny MCUs. reply adastra22 9 hours agorootparentprevBitcoin&#x27;s smart contracting language is a Forth variant, and arguably this was a very astute and smart choice by Satoshi.When designing a multi-party contract, the essential problem is that two people specify the terms they each want to apply to any case in which the funds must be spent, then these two sets of requirements must be merged into a single program. This is, in general, difficult to do securely. We can establish conventions that are relatively easy to follow, but it would be a lot nicer and more powerful if we could syntactically enforce that both sets of requirements are enforced in the combined program.Concatenative languages like Forth meet this requirement nicely. If you represent the spend requirements as a program, then combining the two programs together in such a way that they both are equally enforce is as simple as literally concatenating the two programs together.For example, suppose Alice&#x27;s requirement is that Alice signs the transaction with her key, and Bob&#x27;s requirement is that Bob signs with his key, and the spending transaction is after some specified time T. Expressed in bitcoin script:Alice:CHECKSIGVERIFYBob:CHECKLOCKTIMEVERIFY DROPCHECKSIGVERIFYThe combined script that meets both these sets of requirements is as simple as putting Alice&#x27;s script, then Bob&#x27;s, unaltered:Alice&Bob:CHECKSIGVERIFYCHECKLOCKTIMEVERIFY DROPCHECKSIGVERIFY 1(The `1` at the end is a quirk of bitcoin that it has to finish with a non-zero value on the stack. This combined script could also be simplified in a couple of ways. Also there&#x27;s a couple of ways in which this can fail in practice. Alice&#x27;s script could contain OP_RETURN, for example, which causes the entire script to become unspendable. Or a mismatched IF&#x2F;ELSE. A better designed and strongly typed Forth dialect would fix these issues.)Bitcoin&#x27;s Forth is not type checked, but suppose that it were. And furthermore, suppose that it had a powerful dependently typed system that captured various key signing and stack requirements at the type level. It could be used to track not just what a program does, but also the properties of a program. Alice could put a constraint in her program that says \"lock time can be no later than April 2024,\" and this becomes part of both the input and output type requirements of her program. Then when Bob&#x27;s program is specified with T=15 May 2024, then his program no longer type checks when concatenated to the end of Alice&#x27;s.No one has yet written a system like this, but it would be really powerful if it did exist. Alice writes here smart contract conditions all by lonesome self, and Bob writes his. Then they literally concatenate one program to the other, and if it type checks then Alice and Bob can be certain that both sets of conditions are satisfied. reply tromp 8 hours agorootparentBitcoin script is a large source of complexity, and it turns out to be mostly redundant. Schnorr signatures allow the use of so-called scriptless scripts [1] [2], which can do most of the things than Bitcoin script is used for, including multisig, absolute and relative timelocks, payment channels, discrete log contracts, and atomic swaps (using adaptor signatures). All without the need for a scripting language. The example you gave of a multisig output that can be spent by either Alice, or Bob after some time, is easily handled too.[1] https:&#x2F;&#x2F;github.com&#x2F;BlockstreamResearch&#x2F;scriptless-scripts[2] https:&#x2F;&#x2F;tlu.tarilabs.com&#x2F;cryptography&#x2F;introduction-to-script... reply adastra22 7 hours agorootparent> absolute and relative timelocksThat&#x27;s a stretch. Replacing timelocks with hashcash (or equivalent) is hardly the same thing.But generally speaking \"scriptless scripts\" suffer from the need for interactive construction. There&#x27;s a long way to go until scriptless scripts is a full replacement for script, if it is even possible. reply tromp 7 hours agorootparentWho said anything about replacing timelocks with hashcash? I&#x27;m talking about actual relative timelocks, that put a minimum block distance between 2 particular transactions.> suffer from the need for interactive construction.Interactive construction is already widely in use in relative timelocks&#x27; prime application of payment channels. reply adastra22 7 hours agorootparentThe scope of potential smart contract applications is much broader, and vastly more interesting than payment channels (yawn). And payment channels are pretty much the only application (definitionally) in which you can get away with \"all parties are online, or this contract will be cancelled anyway\" assumptions.Do you have a citation for a block-based relative timelock? The only such things I&#x27;ve seen are verifiable delay functions, which are a lot more like hashcash.What about absolute lock times? Those are far more interesting in applications of business logic, like logistics contracts. Most of which require non-interactive state updates. reply tromp 6 hours agorootparent> payment channels (yawn)Don&#x27;t you find 2nd layers like Lightning interesting, as a way to scale far beyond the limited L1 capacity?> Do you have a citation for a block-based relative timelock?See the thread starting with https:&#x2F;&#x2F;lists.launchpad.net&#x2F;mimblewimble&#x2F;msg00546.html> What about absolute lock times?Those are trivially supported in any Mimblewimble chain. reply adastra22 26 minutes agorootparentNot really, no. Lightning is still payments, multi-party and multi-hop. If the entire realm of Blockchain and smart contract technology was restricted to just payments, well I suppose that would be something. But far short of the total revolution of societal structure that was promised. I personally wouldnâ€™t care to work on it if that was the case. Lightning is an implementation of distributed payments. Smart contracts are an implementation of decentralized law.Your link to the mailing list thread describes essentially the same approach that bitcoin has for dealing with lock times. I would not normally describe this as â€œscriptless scripts,â€ which I have understood to me using the signing operation itself to achieve some goal that would otherwise be done in script. The locktime here is still enforced by special cased code of the consensus algorithm. If the number of things you want to do are fixed and enumerable then I suppose this is a valid approach. But it does not allow for general, arbitrary, future-defined constraints.To give a concrete, simple example: show me how to do a return peg validation on scriptless scripts. If it canâ€™t even do that, it is not a replacement for scripts. reply tromp 7 minutes agorootparent> far short of the total revolution of societal structure that was promised.Satoshi promised no such thing. That sounds more like an Ethereum promise (to which I don&#x27;t subscribe).> show me how to do a return peg validation on scriptless scriptsHow does one do return peg validation in bitcoin script?eternityforest 9 hours agorootparentprevForth doesn&#x27;t seem to do much to stop you from making mistakes, unless Bitcoin has added extra stuff.I don&#x27;t use or study anything crypto related, so I&#x27;m just guessing, but wouldn&#x27;t something like Prolog work for describing contracts? reply lifthrasiir 10 hours agorootparentprevBut that doesn&#x27;t directly relate to the verifiability of milliForth itself. An extremely shortened code can be harder to verify, for example it may work as intended unless a very specific input is used to break out of its sandbox (so to say). Bootstrapping needs a short and readable enough seed for that reason, and I can&#x27;t be entirely sure that it is indeed the case for milliForth. reply alexisread 2 hours agorootparentTechnically I guess what you&#x27;d do is hand type in binary code to create a hex editor, then bootstrap this forth off that, by hand-typing it in.If you include formal verification tools as part of the stack then you can verify the tools by eye, type them in, and use them to verify the function of the stack. Admittedly Forth is tricky to do here, but something like lisp&#x2F;scheme&#x2F;wat can actually be proven out with say microkanren.From there we can trust the software stack. reply taneq 4 hours agorootparentprevThe sneakiest possible program representable in n bytes is never less sneaky than the sneakiest possible program representable in n-1 bytes, assuming you can pad a program out with nops. reply lifthrasiir 3 hours agorootparentYou don&#x27;t need a sneaky program, you only need a program that misbehaves on sneaky inputs. reply Avshalom 9 hours agoparentprevsubtraction is trivial to add in userspace once you&#x27;ve loaded it though : - sp@ @ nand 1 + + ;or as it appears in the hello_world.FORTH file : dup sp@ @ ; : invert dup nand ; : negate invert 1 + ; : - negate + ;There is of course no benefit to this thing at all other than reclaiming the crown from those deviant lispers... well I suppose if you&#x27;re making a your own computer from scratch like https:&#x2F;&#x2F;www.homebrewcpuring.org&#x2F; it might be a useful starting point. reply benj111 8 hours agoparentprev [â€“] : - not 1 + + ;Disclaimer: I&#x27;ve never written forth, so this may not be valid for any particular forth implementation. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "milliFORTH, a FORTH programming language, is the smallest real programming language to date, taking up only 422 bytes, making it small enough to fit in a 512-byte boot sector.",
      "Although sectorFORTH previously achieved this, milliFORTH is even smaller, out-sizing sectorLISP2 by 14 bytes. Despite its minimalistic nature, it doesn't compromise programming efficiency.",
      "Modifications in milliFORTH include the removal of 'tib', trimmed error handling, a non-functional delete key, an assembled binary of sector.asm that's compatible with emulators, and a utility for checking binary size."
    ],
    "commentSummary": [
      "MilliFORTH, being only 422 bytes in size, claims to be the smallest existing real programming language, which is even small enough to fit within a 512-byte boot sector.",
      "FORTH, the language milliFORTH is based on, is a minimalist language containing just a few words for functionality yet managing to maintain programming efficiency.",
      "The new milliFORTH has some modifications compared to its predecessor, sectorFORTH, including an absence of tib, limited error handling, and a non-operating delete key. It also includes an assembled binary of sector.asm and a utility to show the binary size."
    ],
    "points": 187,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1699239803
  },
  {
    "id": 38155324,
    "title": "Fermented Grain Attracts and Intoxicates Grizzly Bears Leading to Train Fatalities in Montana",
    "originLink": "https://cowboystatedaily.com/2023/11/04/63-grizzlies-some-drunk-on-fermented-grain-killed-by-trains-in-montana/",
    "originBody": "Getting â€œdrunkâ€ on fermented grain has killed dozens of grizzly bears along a treacherous stretch of rail line near Glacier National Park, Montana. Since 1980, 63 bears have been killed along a stretch of rail line that goes over Marias Pass and the Great Bear Wilderness, according to reports from state and federal wildlife agencies. The worst year on record was 2019, when eight grizzlies were killed by trains, and three have been killed so far this year. Many of them likely died during a â€œdrunken attemptâ€ to outrun the trains, a Wyoming bear expert said. Grain spilled from railcars along the tracks can be a tempting treat for bears, retired federal ecologist Chuck Neal of Cody told Cowboy State Daily. With enough moisture from snow and rain, â€œthe spilled grain actually ferments in place and becomes a de facto brewery,â€ he said. Two Bear Populations There are two major populations of grizzlies in the Lower 48, each of them estimated at more than 1,000 bears. Wyomingites are most familiar with Greater Yellowstone Ecosystem grizzles, which occupy expansive territory in Wyoming, Montana and Idaho. The grizzlies getting killed by trains near Glacier Park belong to the Northern Continental Divide grizzly population. Itâ€™s centered in the rugged mountains for northwest Montana, but some of those bears also have been pushing out into Montanaâ€™s prairies. So far, the two populations have remained distinct. However, they are about only 60 miles apart in some places and could be close to comingling, which conservationists say could be pivotal to the long-term vitality of the species in the Lower 48. Yellowstone Bears Hit By Vehicles, Not Trains Trains can be a threat to grizzlies in some areas, such as near Glacier Park or in parts of Canada, biologist Frank van Manen told Cowboy State Daily. He heads the U.S. Geological Surveyâ€™s Interagency Grizzly Bear Study Team in the GYE. Luckily for GYE grizzlies, getting hit by trains hasnâ€™t been a problem yet, he said. â€œWe are fortunate to not have that issue as a mortality source in the GYE,â€ he said. â€œMortalities from train collisions are also a major issue in several Canadian parks, sometimes representing the main source of mortality.â€ Vehicles have taken a toll on grizzlies the GYE. So far this year, six grizzlies have been killed by vehicles, including one each on the North Fork and South Fork highways near Cody, and one in the Grayling Creek area of Yellowstone National Park, according to USGS reports. One of Wyomingâ€™s most famous bears, Grizzly 610, was struck by a vehicle on U.S. Highway 89 on Oct. 9 and languished by the roadside for several hours. Much to the relief of her worldwide fanbase, she appeared to recover fully and reunited with her three yearling cubs. Grizzly bears getting hit by trains isn't a big problem in Wyoming, but is still a concern for wildlife officials. (Getty Images) Asleep On The Tracks As for the Glacier Park bears, not enough is being done to protect them from getting hit by trains, Neal said. And being intoxicated on fermented grains just makes things all the worse for the grizzlies. Bears that are attracted to the fermented grain â€œmight fall asleep right on site if they get drunk first. They can, and have, fallen asleep in a drunken stupor right on the tracks,â€ Neal said. â€œOther times they loiter on the tracks until a train approaches, at high speed, then drunkenly attempt to outrun the train â€” no can do â€” and are smashed,â€ he added. â€˜Bears Will Continue To Dieâ€™ Neal said he thinks the Burlington Northern Santa Fe (BNSF) rail company isnâ€™t doing enough to protect the bears from such a terrible fate. â€œBNSF have been stalling doing anything for some years believing that the grizzly is going to be delisted (from endangered species status), another reason I am against delisting, and they will not have to come up with some type of plan that will give them an â€˜incidental takeâ€™ permit from USFWS (U.S. Fish and Wildlife Service),â€ Neal said. â€œAs for what can be done, one idea that has been tossed around is to have some type of noise maker triggered as trains approach known â€˜kill zones,â€™â€ he added. â€œOf course, if the bears are drunk, it is questionable how effective that would be. â€œAnother idea is to not load the train cars so full, an idea that BNSF does not like. Another idea is not run the trains under certain weather conditions when derailment possibilities increase, an idea that BNSF also does not like. So right now, the last word that I have is that not much has been done at all and the bears continue to die.â€ Mark Heinz can be reached at mark@cowboystatedaily.com. IN CASE YOU MISSED IT Some Wyoming Ranchers Not A Fan Of â€˜All-You-Can Killâ€™ Permits To Reduce Elk Mark Heinz 9 min read Hunting Wyoming: Johnson County Busting At Seams With Elk, But Not Much Deer Mark Heinz 5 min read",
    "commentLink": "https://news.ycombinator.com/item?id=38155324",
    "commentBody": "Drunk grizzlies keep getting hit by trains in MontanaHacker NewspastloginDrunk grizzlies keep getting hit by trains in Montana (cowboystatedaily.com) 184 points by LinuxBender 16 hours ago| hidepastfavorite141 comments hellotheretoday 14 hours agoI worked in a place that had several apple trees in the parking lot that led to a similar issue. The apples would ferment then drunk squirrels would chase people around sometimes. It was pretty funny unless it happened to you reply usrusr 6 hours agoparentI regularly see a squirrel (of the European red kind, far more reclusive and shy than their American relatives) frantically licking the varnish of a chair that lives on my balcony. Looks very much like a drug habit, I hope it&#x27;s not too unhealthy. reply permo-w 6 hours agorootparentgive it a try reply doublerabbit 4 hours agorootparentI can confirm that my office chairs are not editable. reply giardini 1 hour agorootparentSeems to work pretty fast! reply e40 9 hours agoparentprevConfirms my belief that squirrels are assholes. We have some that urinate on our deck. They also pick and take a single bite out of our tomatoes and lemons, spoiling an entire harvest just before we pick it. reply sonofhans 9 hours agorootparentOMG yes. I feed crows on my balcony, giving them a few dozen peanuts every day. I mind less that the squirrels steal them and much more that they fucking pee on the deck railing every time. Iâ€™ve taken to leaving a window open and shooting them with a water pistol; that blows their minds. reply chasd00 5 hours agorootparentI learned right on HN that you can dust the peanuts with cayenne pepper. The squirrels wonâ€™t like it but the crows canâ€™t taste it. reply gosub100 4 hours agorootparentSupposedly that&#x27;s how peppers evolved their capsaicin irritant. Discouraged land-bound consumers but encouraged avians who would carry the seeds further. reply bluedino 6 hours agorootparentprevCheck your states nuisance animal laws, you may be able to switch from a water pistol to a pellet gun or bow. reply e40 16 minutes agorootparentThought about it. My neighbors are so close that I worry about using anything other than a water gun, which was pretty ineffective, though they seem to really, really hate water, for some reason. reply Tao3300 4 hours agorootparentprevWell now you&#x27;re just swapping one inconvenient squirrel fluid for another. reply doubled112 3 hours agorootparentThat squirrel will only leave that fluid once though. reply giardini 1 hour agorootparentprevSquirrels are large enough to make a meal.And you can sell squirrel caps to the neighbors&#x27; kids. reply capableweb 4 hours agorootparentprevPellet gun or a bow sounds like it could seriously hurt the animal, why would you prefer that rather than harmless water? They&#x27;ll learn after a while to stay away with just water. reply e40 15 minutes agorootparentThe water has not been a deterrent, in my experience. reply stcroixx 4 hours agorootparentprevThey&#x27;re good to eat, taste similar to rabbit. Might shoot some today. reply PH95VuimJjqBqy 3 hours agorootparentI find this reply humorous as it points out the flaw in the above posters thinking.Having said that, I tend to lean on the side of the other poster in that harming animals should be avoided unless there&#x27;s a specific reason (such as eating them)At some point the world isn&#x27;t kumbaya and the animals will learn when they tend to die or be physically hurt when approaching that activity. reply capableweb 3 hours agorootparentprevYeah, not everyone have a local butcher and need to hunt food themselves, not common to come across folks like you here :) reply zdragnar 3 hours agorootparentYou really don&#x27;t want to take squirrels to your local butcher. If they don&#x27;t laugh you out of the shop, there&#x27;s not nearly enough meat on them to pay them for their time.Get yourself a good, sharp paring knife or pocket knife and some kitchen shears and you&#x27;re good to go. reply JKCalhoun 6 hours agorootparentprevSounds like they may not want to. reply nickpeterson 5 hours agorootparentItâ€™s not impossible, I used to bullseye womp rats in my T-16. reply e40 17 minutes agorootparentprevEXACTLY THIS! OMG. reply sandworm101 3 hours agorootparentprevGet a cat. The presence&#x2F;smell of a cat will do even if they never directly harm a squirrel. The week after my cat passed away, I was shocked at how brazen the local squirrels became. Suddenly they were everywhere around the house. reply moron4hire 3 hours agorootparentBut then you have a cat reply rollcat 2 hours agorootparentI say that&#x27;s a win-win.(inb4 beetlejuicing.) reply bbarnett 7 hours agorootparentprevThey&#x27;re probably marking their territory against other squirrels, and who wouldn&#x27;t, with those wondrous peanuts.(Not that I blame you for squinting them, I am only providing reason why, perhaps) reply 2023throwawayy 3 hours agorootparentprevDonâ€™t feed the wildlife.It is often illegal. reply jjgreen 6 hours agorootparentprevMy Dad called them \"rats with PR\", I rather like them ... reply giardini 50 minutes agorootparentI once thought squirrels were cute. Then I became a condo association president.I removed squirrel carcasses from all rooftop A&#x2F;C units, along with the harvested nutshells that they had left behind. I patched the holes they opened into the walls and crevices of the condo apartments.Finally I helped remove the 40-year-old pecan tree adjacent to one building&#x27;s slab and which had hosted these pests for decades. The tree was lifting the slab and likely would have broken the foundation.I have resolved that buildings should never coexist with either large trees (nut-bearing or otherwise) or with rodents.Two years ago some jackass planted two pecan trees in the condo front yard. This year the pecans are falling and the squirrels have already moved back. The homeowners&#x27; association is clueless. The cycle continues. reply nullserver 4 hours agorootparentprevI worked at a pet store in college. This mother came in with her daughter and they wanted to get a Pet. They werenâ€™t sure what but definitely not a rat, and definitely not a male anything.I worked with them for about three hours, and at the end sent them home with a male rat. Both delighted as can be.He was a very social and playful one that I had been planning on taking home. reply j-a-a-p 5 hours agorootparentprevMaybe the mouse keeper will help, a device I created when I was student. It is a motion detector duct taped to a vacuum cleaner. Works very good on mice, perhaps also on squirrels. reply zabzonk 4 hours agorootparentprevget a powerful water gun and zap them with it - works well, and is extremely satisfying (for you, not the squirrel) reply INTPenis 8 hours agoparentprevWhere I used to live there was a pear tree in a dog park, and a huge rabbit problem in the whole city. So every autumn your dog might end up chasing a rabbit around the dog park in the middle of the night. reply newZWhoDis 2 hours agoparentprevAh, the old â€œdrunk or rabiesâ€ game reply fblp 13 hours agoprevVarious states have pretty strict (including criminal laws) re littering and securing loads on vehicles. Montana does not.Linked are some examples that mostly apply to vehicles on highways but some apply to trains. Montana doesn&#x27;t seem to have rules for trains, and even for trucks carrying agricultural produce and fertilizer seem to be exempt from littering laws.There are also environmental protection laws that may apply.So this seems like an issue that could be prevented by introducing littering laws &#x2F; secure loads that apply in other states.https:&#x2F;&#x2F;www.pulltarps.com&#x2F;wp-content&#x2F;uploads&#x2F;2019&#x2F;08&#x2F;WhitePa... reply gosub100 4 hours agoparentThe railroad owns the property on which they operate their trains. I don&#x27;t think that spilling grain on your own property constitutes littering. reply malfist 4 hours agorootparentI don&#x27;t believe they do. The article mentions the trains are running through national parks. reply gosub100 4 hours agorootparentThat&#x27;s called an easement. I&#x27;d rather talk about what it effectively means, rather than the semantics of the word \"own\". reply PH95VuimJjqBqy 3 hours agorootparentis it an easement or do they own the land? It&#x27;s not clear to me which is correct although I WOULD expect it to be an easement.I wonder if it&#x27;s both depending on where it&#x27;s at. reply dboreham 3 hours agoparentprevThese trains come from other states and are going to other states so I doubt MT regulations are the root cause. reply labrador 14 hours agoprevGrain hoppers are typically covered, so why are they running uncovered grain cars on this run? reply hristov 13 hours agoparentIf you read to the last paragraph, the article suggests but does not outright state that the grain is spilled as the result of derailments. So there is some kind of derailment, grain cars tumble over spill all of their grain, and the railroad decides it is not worthwhile to collect it.This results in piles of grain by the track side which gets wet and starts fermenting, and the grizzlys eat it, get drunk and decide to race trains. reply 9991 11 hours agorootparentâ€¦ causing a train derailment, and completing the cycle.Is this some new form of life? reply pfannkuchen 10 hours agorootparentThis is the yeast&#x2F;bear version of toxoplasmosis cat&#x2F;rat? reply jeffrallen 10 hours agorootparentI come to HN for quality speculation like this.Keep up the good work! reply badcppdev 6 hours agorootparentprevMaybe if the bears learn to put logs on the tracks... or they develop fire and burn the railway sleepers.https:&#x2F;&#x2F;www.lightspeedmagazine.com&#x2F;fiction&#x2F;bears-discover-fi... reply Tao3300 4 hours agorootparentHeyyyy Boo-boo! Here comes that iron pic-a-nic basket! reply sethammons 6 hours agorootparentprevHitting a bear will not derail a train reply COGlory 2 hours agorootparentNo, but moose can cause derailments.https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S13506... reply looping8 10 hours agorootparentprevBut how often could that happen? I thought derailments are very rare in modern trains. reply m3047 42 minutes agorootparentI can&#x27;t say how the railroads treat cargo, but I can tell you how they treat humans. I got tired of flying and rode Amtrack between Tacoma and Oakland about a half dozen times between 2010-2020 (so a dozen one way rides), until my truck got stolen from their parking lot after they turned off the security cameras and patrols without telling the customers.On the way back one time we were sitting for hours because a freight had come apart in two places on the only through track. I watched the crews hump couplers brought up from (presumably) Crater Lake past the bar car. Just a day working on the railroad.They put the train together wrong once, without the crew car between the engines and first class. This meant that first class looked out over the engines. (They duct taped around the door to keep the snow and rain from pouring in.) About a half dozen of us stood there taking turns sharing bottles of wine and looking out the window in the dark as the train went over the Cascades en route to Crater Lake. Very cool.On the way down one time, shortly after \"lights out\", there was an excited PA announcement \"STAY IN YOUR ROOMS AND DON&#x27;T TOUCH ANYTHING METAL\" and as I came to I noticed that the lights and ventilation were off and the train seemed to be coasting to a stop. The second announcement that anyone who saw \"lights\" outside the train should speak to a conductor was no more reassuring. Upshot was the dining car had suffered an electrical fire (coach apparently filled with smoke, but we were spared that in first class). They diverted to some huge rail yard somewhere in the vicinity of Crater Lake where they disassembled the train and put the wretched dining car at the end of the train; they catered in breakfast in Sacramento.Dunno why all the best stuff happens just north of Crater Lake.Northbound run looking out of the antique bar car: http:&#x2F;&#x2F;athena.m3047.net&#x2F;train.jpgThere was a theater downstairs, I kid you not! http:&#x2F;&#x2F;athena.m3047.net&#x2F;train-theater.jpg reply usrusr 10 hours agorootparentprevTrue, but we are talking about American trains.Edit: https:&#x2F;&#x2F;youtu.be&#x2F;7Ao-24274JwThis example is not in Montana, but apparently minor derailings are a thing. And the cars that are not derailed don&#x27;t really look much better. reply dboreham 3 hours agorootparentI live in Montana, near a railroad (not the line with the Grizzlies). There are derailments every couple of years on the ~50 miles of track I have visibility for. Two of those were corn&#x2F;grain that ended up in big piles on the track. reply jacquesm 7 hours agorootparentprevThose tracks are an insult to any rail engineer. Unbelievable. reply capableweb 4 hours agorootparentThinking you were overreacting, I gave the video a look.I&#x27;m not sure if that&#x27;s a example from reality, it cannot be real that people are allowed and do drive trains over those \"rails\"? reply dncornholio 3 hours agorootparentprevI&#x27;m curious why you brought that video up. Because that doesn&#x27;t look like a representative example if you ask me. reply hnbad 5 hours agorootparentprevIf you look at the shape of those tracks it&#x27;s more surprising that this only turned into a \"minor\" derailing, although of course the initial video is sped up quite a bit.John Oliver had a segment on train derailments a while back (can&#x27;t find the YT video) and it seems that a lot of US rail infrastructure including the trains are just in very bad shape and there don&#x27;t seem to be any federal regulations forcing them to invest whereas the environmental damage often ends up being swept under the rug.Not that German rail is anything to brag about but I can&#x27;t imagine we&#x27;d be allowed to let a train go anywhere near a track that looked like that. reply lostlogin 9 hours agorootparentprevQuite a lot.I searched and found this database.I donâ€™t know how it compares to other countries but itâ€™s a long list.https:&#x2F;&#x2F;data.transportation.gov&#x2F;Railroads&#x2F;Rail-Equipment-Acc... reply usrusr 7 hours agorootparentGood news, everybody, apparently the bureaucracy part is still operational.Scrolling down to the web data viewer and then right until you can order by date, descending, is ... impressive. reply t-3 4 hours agorootparentprevThey are less common than before, but over 1000 trains still derail every year in the US. reply dmix 14 hours agoparentprevMoney falls out of armoured bank trucks all over highways when itâ€™s supposed to be secured in the back sometimes. Stuff happens. Humans etc reply passwordoops 4 hours agorootparentYeah, yeah. My hand just happened to fall into that man&#x27;s back pocket. It&#x27;s not my fault his wallet stuck to my palm reply dmix 2 hours agorootparentI know people are joking but money has fallen out of trucks at least 8 times in recent history:https:&#x2F;&#x2F;www.charlotteobserver.com&#x2F;news&#x2F;local&#x2F;article27933093... reply Swizec 14 hours agorootparentprev> Money falls out of armoured bank trucks all over highways when itâ€™s supposed to be secured in the back sometimesI had a few classmates back in high school whose uncles could get you cheap Nike, Adidas, Calvin Klein etc that fell out of trucks. Happens all the time. Stuff sometimes just gets lost on the way. reply msrenee 13 hours agorootparentI&#x27;m pretty sure saying goods \"fell off the truck\" is just a euphemism for them being stolen. That&#x27;s why they&#x27;re sold cheap. reply johndunne 7 hours agorootparentHappens so often, the evolved euphemism is \"fell out the back of a low flying aircraft.\" Us Dubliners are fond of this one. reply TeMPOraL 10 hours agorootparentprev*whoosh* is the sound the truck just made, passing over your head, as jokes fall of it :) reply brvsft 4 hours agorootparentLeave this particular commentary on Reddit, please. reply adolph 13 hours agorootparentprevWhy would they be sold cheaply because they are stolen? If the item is the same, then why wouldnâ€™t the price be nearly the same? reply zztop44 13 hours agorootparentIf the price were nearly the same, why would anyone choose to deal with criminals and expose themselves to some level of legal risk when they could get the same product (with a warranty) in a store? reply wjnc 11 hours agorootparentprevYou get downvoted but are describing a nice trend. In the old days all you had were stores and poachers. Who was who was easy to tell and the poachers had to suffer a loss. In the modern days, where say novelty sneakers can be bought via innumerable websites the price difference between stores and poachers must have come down. A smart poachers sells stuff that fell of the wagon at retail via pop-up websites. Anything Amazon sells will have fallen of a wagon here or there. reply transcriptase 13 hours agorootparentprevA discount moves the stolen items out of your possession much faster, reducing risk of being caught with them and need for storage. reply psunavy03 12 hours agorootparentprevThat wasn&#x27;t what \"fell off the truck\" actually meant. reply jahnu 5 hours agorootparentprevThe ole 5 finger discount. reply notatoad 12 hours agoparentprevmy understanding was that grain comes out of the opening on the bottom used to empty the car, rather than flying out the top.whatever the mechanism is, grain on the tracks is definitly not exclusive to this run, it&#x27;s a problem anywhere there&#x27;s railways and bears. reply Tao3300 14 hours agoprevAveraging 1.5 bears a year since 1980. Is that a lot? reply tomjakubowski 14 hours agoparentThe state grizzly population is about 2,100. Seems like a lot, compared to per capita human-train deaths reply steve_adams_86 11 hours agorootparentLooks like that would be over 780 people hit by trains per year at that rate. Iâ€™m guessing thatâ€™s a couple orders of magnitude higher, haha. These bears have a problem on their hands. reply tomjakubowski 2 hours agorootparent780 people hit per year in Montana&#x27;s population yeah? At national population (1.5&#x2F;2100&#x2F;*3.5e6) the Montana train-bear fatality rate would correspond to a rate of 214k people hit per year. reply tomalbrc 10 hours agorootparentprevTurns out humans get hit by trains much more often than you would think reply tomjakubowski 2 hours agorootparentOn the order of 1000 train fatalities per year in the United Statesthe Montana grizzly bear rate is >200x the American person rate! replydottedmag 9 hours agoprevIsn&#x27;t it nice that the article has a summary with all the facts upfront for a change? reply generic92034 3 hours agoprevAre the proportions in the first large image of the article off? Somehow the bear looks too small to me, compared to the rails. reply dboreham 3 hours agoparentPerhaps it was on the Great Western track when it was broad gauge? reply Aardwolf 6 hours agoprevIf it&#x27;s that easy for things to ferment, makes me wonder if drunkenness existed (in animals) before humanity reply jahnu 6 hours agoparentDefinitely.https:&#x2F;&#x2F;www.nationalgeographic.com&#x2F;animals&#x2F;article&#x2F;151121-an...https:&#x2F;&#x2F;www.sciencedaily.com&#x2F;releases&#x2F;2022&#x2F;04&#x2F;220401141345.h... reply v3ss0n 13 hours agoprevSo , cocaine bear isn&#x27;t far fetched reply dreamcompiler 13 hours agoparentBears are weird. Cocaine bear actually happened, although it was a very small black bear and it died of an overdose before it could hurt anybody [0].I&#x27;ve had bears in my yard fairly often. They used to come at night and tear up my porch trying to get to the bird feeders. We finally wised up and stopped feeding the birds.One time a bear tore up my windshield wipers because I made the mistake of leaving a candy bar in my truck overnight.Bears are weird, but you learn their weirdnesses when you live in the country.[0] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Cocaine_Bear_(bear) reply TACD 7 hours agorootparent> According to the bear&#x27;s owners, the Cocaine Bear has the authority to officiate legally binding weddings in the mall where it is kept due to Kentucky&#x27;s marriage laws. This claim is only partly true; the bear does not have the authority to solemnize weddings, but the state of Kentucky cannot invalidate marriages performed by unqualified persons if the parties believe that the person marrying them has the authority to do so. As such, it is a belief in the Cocaine Bear&#x27;s authority that allows it to officiate legally binding weddings in Kentucky.*Wiping away a tear* God bless America. reply arcbyte 5 hours agorootparentI&#x27;d like to see the divorce proceedings of a marriage solemnized by this bear. Almost certainly your prenuptial is not going to be binding because you cannot reasonably believe you were actually married by a bear. Any good lawyer is gonna rip this one apart. reply giardini 24 minutes agorootparentThat depends: if you hire Cocaine Lawyer Bear to defend you, the opposing party will likely be unable to find counsel to argue their case. reply mauvehaus 4 hours agorootparentprevWhat do you the cardinality of the intersection of the sets of \"couples who have prenups\" and \"couples who get married by a bear\" is? reply pbhjpbhj 6 hours agorootparentprevEven when the party officiating is very clearly not a person? reply kQq9oHeAz6wLLS 12 hours agorootparentprevMany years ago at Lake Shasta, a couple a few spots down the campground from us had the canopy on their truck destroyed when a bear tried to get the cooler they had stored in there.Unfortunately for them, they were also sleeping back there. They weren&#x27;t hurt, just badly shaken, and from listening to the woman, I have a feeling she never camped again. reply shepherdjerred 13 hours agoparentprevCocaine bear was real, although less exciting than the adaptations.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cocaine_Bear_(bear) reply tjpnz 13 hours agoparentprevIt&#x27;s based on a true story. reply jccalhoun 6 hours agoprevHow much grain is spilled? It seems that given the size of the bears that it would have to be a whole lot to actually get the bears drunk. reply 2OEH8eoCRo0 5 hours agoparentI think animals get drunk much easier than humans because we have an adaptation that allows us to process alcohol. reply MichaelMoser123 4 hours agorootparentFor Humans this adaptation is very localized, see https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ALDH2Lots of people have less of that ALDH2 gene, so a much smaller dose will be enough to get them drunk. reply jb1991 6 hours agoprevAs a drunk grizzly myself, I resent the implication that life has somehow taken us down the wrong track. reply daoboy 14 hours agoprevI understand that moose are responsible for quite a few train derailments, too.I presume they are sober, though. reply dboreham 1 hour agoparentUmmm. No. Train locomotive weighs 100 tons. Moose weighs 0.5 ton max.Moose are bad news vs a car though. reply zoky 10 hours agoparentprevMoose donâ€™t give a fuck. A moose would derail a train just â€˜cause. reply pbhjpbhj 6 hours agoparentprevRIP Tycho Brahe&#x27;s moose. reply AnimalMuppet 4 hours agoparentprevSource? I would have figured that a moose didn&#x27;t have enough mass. reply COGlory 2 hours agorootparenthttps:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S13506... reply daoboy 1 hour agorootparentprevNo source.I&#x27;m just spreading moosinformation reply cwmma 2 hours agorootparentprevMoose are way bigger then you think. reply AnimalMuppet 40 minutes agorootparentI know how big a moose is. I&#x27;ve been uncomfortably close to them more than once.But I also know how big a train is, and I still don&#x27;t see it. I know that in terms of mass, a train is to a car as a car is to an empty beer can. A car can derail a train, but won&#x27;t very often. A moose might weigh as much as a small car, but has less stiff metal in the frame. It still might derail a train... but not very often. reply giardini 20 minutes agorootparentBear and moose have plenty of fat et al. The derailment isn&#x27;t a simple metal-on-metal action, it&#x27;s a metal-on-moose-on-metal interaction. The moose is skin-protein-fat-bone-fat-protein-skin. Do the math. reply AnimalMuppet 0 minutes agorootparentI&#x27;m not sure what math you think I&#x27;m supposed to do, but I know that a railroad wheel has ten tons of weight on a strip a couple of inches wide. If a moose is thin enough to not get knocked aside by the snowplow[1], it&#x27;s probably not thick enough to not get cut through by the wheel.Still waiting for some evidence for the claim that \"moose are responsible for quite a few train derailments\".-----[1] Yes, I know that not all railroad engines have them. Most that don&#x27;t have a flat plate that still gets down pretty close to the track.black_puppydog 9 hours agoprevI \"love\" how the title makes it seem like it&#x27;s the grizzlies&#x27; fault. As if they&#x27;re the actors here. reply isametry 9 hours agoparentThe grizzlies are mentioned in passive voice, which by definition makes a subject acted on (rather than the acting one) in a sentence.While I see what youâ€™re getting at from a human reader standpoint, in reality the reason for this phrasing (instead of the grammatically equivalent â€œTrains keep hitting drunk grizzliesâ€) is that starting with â€œDrunk grizzliesâ€ just makes for an unmistakably better headline. reply black_puppydog 8 hours agorootparentRight, the cause for the outcome might be (also messed-up) media industry incentives, but the outcome is still that the more accurate \"we keep getting grizzlies drunk then running them over\" stance gets replaced by something that reads more like it&#x27;s their fault. reply isametry 6 hours agorootparentThe messed up incentives ofâ€¦ Seeking as much attention as possible, which is as old as media itself and arguably the very purpose of its existence. Yeah, I guess youâ€™re right. But then again, Iâ€™m afraid thereâ€™s bigger evils to fight in this world.Plus in this particular case, apart from the feelings it might generate in more engaged readers such as you, I donâ€™t think the title has further real-life implications.People conscious enough to actually have an impact on the matter (e.g. those to potentially reach out to organizations, or appeal to railroad companies directly to secure their cargo better) will surely read the article and realize the bears are 100% the victims.Whereas the people who will think â€œhaha, stoopid wasted bearsâ€ and move on, wouldnâ€™t have actively done anything about the situation to begin with. They might still share the article though, which in turn increases its reach and improves its chance of finding someone who cares.If thereâ€™s a fallacy here please call me out, but at this moment I donâ€™t think the â€œsensationalizedâ€ element in this title is a bad thing. At worst itâ€™s neutral, apart from earning Cowboy State Daily some (un)deserved bucks. reply CapsAdmin 11 hours agoprevMakes me wonder what a train can do other than to try and slow down and honk to make animals step aside and not run along the track.I would assume spraying water have been tried? reply Symbiote 10 hours agoparentYou could read the final paragraphs of the article, which has some suggestions. reply lostlogin 9 hours agorootparentYeah, but they would inconvenience the rail company. It needs to be cost neutral and effortless. reply gnicholas 11 hours agoparentprevYou&#x27;d have to be spraying the water pretty hard&#x2F;fast for it to give the bear enough warning to get out of the way. I&#x27;d think that could injure the bear (though I suppose that&#x27;s better than hitting it?). reply paulcole 4 hours agoprev> Since 1980, 63 grizzlies have been hit by trains and killed along a section of railroad near Glacier National ParkThey could at least try to hide the fact that the headline is misleading rather than putting it directly below itâ€¦ reply JCharante 14 hours agoprevCould they try putting a very very strong and focused spotlight on the front of the train? The light would be visible through their eyelids and wake them up. reply mattigames 14 hours agoparentAnd they could stick a couple of yoga balls on the front of the train so the bears have a higher chance of survival. reply lettergram 14 hours agoprevThis article has no real facts in itâ€¦How often do train cars carrying fermented grain come through?How many bears are actually hit in this manner? (there is just someone saying this is what happens, but no autopsy or anything)I canâ€™t imagine fermented grain leaks are all that common. Iâ€™ve been next to a lot of rail road tracks and never seen leaks. Imagine if you had a leak and it remained for 1000 or 2000 miles.This story doesnâ€™t make senseSeems more reasonable a bear doesnâ€™t know what a train is, has gone def or is asleep and just gets hit. reply sitharus 14 hours agoparentI don&#x27;t think the railcars are carrying fermented grain, they carry grain which spills and ferments... However getting grain to ferment is somewhat tricky compared to fruit, you need the right amount of water at the right time to get it to malt and then ferment so I&#x27;m surprised there&#x27;s enough of it. Just surprising, not doubting it happens.I assume the spills are from hoppers with worn out chutes, because I would not expect open hoppers for grain transport. reply mindwok 14 hours agoparentprevYeah, agree. The article takes \"bears killed by trains each year\" and then \"one guy said sometimes they are drunk on spilled grain\" and then interpolated for hyperbole. reply AnimalMuppet 3 hours agorootparentNo, some of them are objectively drunk on spilled grain. It happens. Railroad maintenance-of-way workers run into them sometimes.I don&#x27;t know what fraction of bears killed by trains are drunk, though. I don&#x27;t know if anyone knows. At least, I have not seen any sources. reply mattigames 14 hours agoparentprevThey don&#x27;t need to know what a train is to run from one, they are taller and louder than them so instincts will immediately kick in telling them to run. reply HankB99 13 hours agorootparentIn evolutionary terms I don&#x27;t think that there was anything that represented a danger to grizzly bears (save adult males killing cubs) so they may not have evolved behavior patterns to run from big loud things. reply usrusr 5 hours agorootparentI think it&#x27;s safe to assume that those instincts habe been dialed in long before the current state of relative danger fell into place. Fear of wiry bipeds that aren&#x27;t actually that big but might carry sparkling stuff would be a far more specific, younger and thus variable fear.But then on the other hand train speed is dangerously difficult to assess even for us humans who in theory know full well what a train is. I believe that this is because much of the perception of movement is in terms of \"size-units over time\", which is why a fly at walking speed appears fast whereas big ship at 25 knots appears almost stationary unless you are too close for your own good. reply dclowd9901 12 hours agoprevSorry, but why is this on the front page? 63 deaths in 43 years. Im actually surprised only that many have been hit.I love curiosities as much as the next person, but Iâ€™m getting the sense someoneâ€™s trying to make a mountain out of a molehill here. reply aendruk 11 hours agoparentThe issue of perverse behavior of self-preserving economic entities is interesting and touches upon broader concerns.> BNSF have been stalling doing anything for some years believing that the grizzly is going to be delisted (from endangered species status) reply myrmidon 4 hours agoparentprevI agree that this is not very front page relevant, but its still a lot of animals dying-- since the local bear population is 300M)-- if railways were similarly lethal for us, the would be over 200000 deaths per year. reply Dalewyn 12 hours agoparentprevJournalist: \"Look, just bear with me. I need to pump out another piece about global war-- I mean climate ch-- I mean extreme weath-- uhh, the environment.\" reply anon23432343 5 hours agoprevCowboy news?HN has become a second reddit... reply LinuxBender 3 hours agoparentThe article could be worded differently and submitted to Nature to make it more HN friendly but I figured people might find the content something curious to discuss either way and perhaps give a small taste of the Mountain West to keep things interesting. reply jadbox 4 hours agoparentprevLet&#x27;s be honest, hn has always been populated with cowboys...https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Cowboy_coding reply anon23432343 4 hours agorootparentYou mean Coding Ninjas? Coding Rockstars? Coding Superstars? Coding ? reply foggedb0nk 14 hours agoprev [â€“] I wonder where all the grain comes from? Is this grain that spills out of grain cars on the trains or? reply COGlory 14 hours agoparentSpills out of the cars and ferments naturally. reply tekla 14 hours agoparentprev [â€“] Its written in the title subsection at the very top> Many died because they got drunk on fermented grain spilled from railcars replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A significant number of grizzly bears have perished since the 1980s along a railway line close to Glacier National Park, Montana, primarily caused by intoxication from fermented grain spillage from the railcars.",
      "The blend of spilled grain and moisture initiates fermentation, attracting the bears, with already three reported deaths in 2021 due to train hits.",
      "Although mitigations like reducing grain quantity in railcars and setting noise triggers in known \"kill zones\" are proposed, limited action has been taken to resolve this issue so far."
    ],
    "commentSummary": [
      "Dozens of grizzly bears have tragically died since the 1980s near Glacier National Park in Montana due to intoxication from fermented grain leaked from railcars.",
      "So far in 2021, three bears have died after being hit by trains, suggesting the issue persists.",
      "Possible solutions include cutting down the amount of grain in railcars and deploying noise triggers in recognized \"kill zones\", but little action has been taken to mitigate the death rate."
    ],
    "points": 184,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1699216148
  },
  {
    "id": 38157466,
    "title": "Using RTL-SDR Dongle to Receive NRSC-5 Digital Radio Stations: A Comprehensive Guide",
    "originLink": "https://github.com/theori-io/nrsc5",
    "originBody": "nrsc5 This program receives NRSC-5 digital radio stations using an RTL-SDR dongle. It offers a command-line interface as well as an API upon which other applications can be built. Before using it, you'll first need to compile the program using the build instructions below. Building on Ubuntu, Debian or Raspbian $ sudo apt install git build-essential cmake autoconf libtool libao-dev libfftw3-dev librtlsdr-dev $ git clone https://github.com/theori-io/nrsc5.git $ cd nrsc5 $ mkdir build $ cd build $ cmake [options] .. $ make $ sudo make install $ sudo ldconfig Available build options: -DUSE_NEON=ON Use NEON instructions. [ARM, default=OFF] -DUSE_SSE=ON Use SSSE3 instructions. [x86, default=OFF] -DUSE_FAAD2=ON AAC decoding with FAAD2. [default=ON] -DLIBRARY_DEBUG_LEVEL=1 Debug logging level for libnrsc5. [default=5] -DBUILD_DOC=ON Generate html API documentation [default=OFF] You can test the program using the included sample capture: $ xz -d/tmp/nrsc5.rb $ brew install --HEAD -s /tmp/nrsc5.rb Building for Windows To build the program for Windows, you can either use MSYS2 on Windows, or else use a cross-compiler on an Ubuntu, Debian or macOS machine. Scripts are provided to help with both cases. Building on Windows with MSYS2 Install MSYS2. Open a terminal using the \"MSYS2 MinGW 32-bit\" shortcut. (Or use the 64-bit shortcut if you prefer a 64-bit build.) $ pacman -Syu If this is the first time running pacman, you will be told to close the terminal window. After doing so, reopen using the same shortcut as before. $ pacman -Su $ pacman -S git $ git clone https://github.com/theori-io/nrsc5.git $ nrsc5/support/msys2-build You can test your installation using the included sample file: $ cd ~/nrsc5/support $ xz -d sample.xz $ nrsc5.exe -r sample 0 If the sample file does not work, make sure you followed all of the instructions. If it still doesn't work, file an issue with the error message. Please put \"[Windows]\" in the title of the issue. Once everything is built, you can run nrsc5 independently of MSYS2. Copy the following files from your MSYS2/mingw32 directory (e.g. C:\\msys64\\mingw32\\bin): libnrsc5.dll nrsc5.exe Cross-compiling for Windows from Ubuntu / Debian $ sudo apt install mingw-w64 $ support/win-cross-compile 32 Replace 32 with 64 if you want a 64-bit build. Once the build is complete, copy *.dll and nrsc5.exe from the build-win32/bin (or build-win64/bin) folder to your Windows machine. Cross-compiling for Windows from macOS $ brew install mingw-w64 $ support/win-cross-compile 32 Replace 32 with 64 if you want a 64-bit build. Once the build is complete, copy *.dll and nrsc5.exe from the build-win32/bin (or build-win64/bin) folder to your Windows machine. Usage Command-line options: frequency center frequency in MHz or Hz(do not provide frequency when reading from file) program audio program to decode(0, 1, 2, or 3) -g gain gain(example: 49.6)(automatic gain selection if not specified) -d device-index rtl-sdr device -p ppm-error rtl-sdr ppm error -H rtltcp-host rtl_tcp host with optional port(example: localhost:1234) -r iq-input read IQ samples from input file -w iq-output write IQ samples to output file -o audio-output write audio to output file -t audio-type type of audio output (wav or raw)(default is wav. used in conjunction with -o) -q disable log output -l log-level set log level(1 = DEBUG, 2 = INFO, 3 = WARN) -v print the version number and exit --am receive AM signals(default is FM) -T enable bias-T -D direct-sampling-mode enable direct sampling(1 = I-ADC input, 2 = Q-ADC input) --dump-aas-files dir-name dump AAS files(WARNING: insecure) --dump-hdc file-name dump HDC packets Examples: Tune to 107.1 MHz and play audio program 0: $ nrsc5 107.1 0 Tune to 107.1 MHz and play audio program 0. Manually set gain to 49.0 dB and save raw IQ samples to a file: $ nrsc5 -g 49.0 -w samples1071 107.1 0 Read raw IQ samples from a file and play back audio program 0: $ nrsc5 -r samples1071 0 Tune to 90.5 MHz and convert audio program 0 to WAV format for playback in an external media player: $ nrsc5 -o - 90.5 0mplayer - Keyboard commands: To switch between audio programs at runtime, press 0 through 7. To quit, press Q. RTL-SDR drivers on Windows If you get errors trying to access your RTL-SDR device, then you may need to use Zadig to change the USB driver. Once you download and run Zadig, select your RTL-SDR device, ensure the driver is set to WinUSB, and then click \"Replace Driver\". If your device is not listed, enable \"Options\" -> \"List All Devices\".",
    "commentLink": "https://news.ycombinator.com/item?id=38157466",
    "commentBody": "Nrsc5: Receive NRSC-5 digital radio stations using an RTL-SDR dongleHacker NewspastloginNrsc5: Receive NRSC-5 digital radio stations using an RTL-SDR dongle (github.com/theori-io) 176 points by greesil 17 hours ago| hidepastfavorite66 comments patja 51 minutes agoI have a little hobby project where I record an FM radio music station using a SDR and then remove all the non-music portions for offline listening. I like the music selections the DJs pick, but I prefer not to listen to the DJ commentary and the advertisements.I evaluated three methods of recording: analog capture from a standalone FM receiver, using this nrsc5 library to record the \"HD\" radio stream, and using an AirSpy SDR with this library: https:&#x2F;&#x2F;github.com&#x2F;jj1bdx&#x2F;airspy-fmradionRecording the \"HD\" (what a misnomer) radio was nice in that there was no hiss or multipath effects, but in comparison to the other methods the digital compression artifacts became impossible to un-hear. It seems to top out at about 96 kbpsThe airspy-fmradion library has some nice stuff in it to address multipath, resulting in the best audio quality of the three methods I tested.I use https:&#x2F;&#x2F;github.com&#x2F;ina-foss&#x2F;inaSpeechSegmenter to identify which segments of the recordings are speech vs. music. reply zdw 16 hours agoprevA GUI built on top of this: https:&#x2F;&#x2F;github.com&#x2F;markjfine&#x2F;nrsc5-dui reply westurner 15 hours agoparentFrom https:&#x2F;&#x2F;github.com&#x2F;markjfine&#x2F;nrsc5-dui#maps :> Maps: When listening to radio stations operated by iHeartMedia, you may view live traffic maps and weather radar. The images are typically sent every few minutes and will fill the tab area once received, processed, and loaded. Clicking the Map Viewer button on the toolbar will open a larger window to view the maps at full size. The weather radar information from the last 12 hours will be stored and can be played back by selecting the Animate Radar option. The delay between frames (in seconds) can be adjusted by changing the Animation Speed value. Other stations provide Navteq&#x2F;HERE navigation information... it&#x27;s on the TODO &#x27;like to have&#x27; list.Is this an easier way to get weather info without Internet than e.g. Raspberry-NOAA and a large antenna?https:&#x2F;&#x2F;www.google.com&#x2F;search?q=weather+satellite+antenna+ha... https:&#x2F;&#x2F;github.com&#x2F;jekhokie&#x2F;raspberry-noaa-v2#raspberry-noaa... :> NOAA and Meteor-M 2 satellite imagery capture setup for the regular 64 bit Debian Bullseye computers and Raspberry Pi! reply ac29 2 hours agorootparent> Is this an easier way to get weather info without Internet than e.g. Raspberry-NOAA and a large antenna?If you&#x27;re OK with audio only, you cant beat NOAA weather radio: https:&#x2F;&#x2F;www.weather.gov&#x2F;nwr&#x2F;You can listen with a SDR, or any number of cheap radios. reply argilo 14 hours agorootparentprevIf you live within the coverage area of an FM radio station that&#x27;s sending weather radar, it will probably be easier to receive than NOAA satellites. reply argilo 15 hours agoprevI&#x27;m a co-maintainer of this project. If anyone has questions, I&#x27;d be happy to answer them. reply fooblaster 13 minutes agoparentWhat documentation did you use to decode hd radio protocol? From what I recall, the formats were all closed in some capacity. Did you reverse engineer it? reply argilo 0 minutes agorootparentMost of the details are available in the specification, which is available here: https:&#x2F;&#x2F;www.nrscstandards.org&#x2F;standards-and-guidelines&#x2F;docum...The missing parts are the audio codec (HDC) and the formats of the various data streams. These have required reverse engineering. reply geerlingguy 14 hours agoparentprevNo questions, just thanks so much for maintaining it!Back in 2019 the project enabled me to listen to the local baseball team&#x27;s broadcasts [1] without a traditional radio since their online streams were restricted due to MLB&#x27;s dumb blackout rules.[1] https:&#x2F;&#x2F;www.jeffgeerling.com&#x2F;blog&#x2F;2019&#x2F;hospital-stay-and-mlb... reply argilo 14 hours agorootparentI&#x27;m happy to hear you found it useful! reply myself248 2 hours agoparentprevHave you ever found interesting&#x2F;unexplained data in the packet streams? I know there&#x27;s provision for traffic stuff in there, and a few other defined types, but what else is hiding?Reason I ask, is analog FM had all sorts of sideband streams for the longest time, like SCA audio and DirectBand digital, that weren&#x27;t well known at the time. And while internet access is certainly easier to get these days, there might still be a justification for one-way data broadcasts in certain applications, so I wonder if someone&#x27;s out there leasing subchannels or sub-packet-IDs or what-not, and shuffling encrypted data around in them. reply patja 49 minutes agorootparentThere was the somewhat recent issue in Seattle where a station included an image file with no file extension, and it bricked the radios in Mazdas: https:&#x2F;&#x2F;arstechnica.com&#x2F;cars&#x2F;2022&#x2F;02&#x2F;radio-station-snafu-in-... reply argilo 2 hours agorootparentprevSome stations have \"HD TMC\" (traffic message channel) streams. The others that I&#x27;ve encountered are \"HERE Images\" (traffic & weather radar images, discussed and mostly reverse engineered in https:&#x2F;&#x2F;github.com&#x2F;theori-io&#x2F;nrsc5&#x2F;pull&#x2F;308), \"HERE TPEG\", \"TTN TPEG\", \"TTN STM\", \"NavteqAdmin\" and \"NavteqPacketData1.\"At present, nrsc5 makes these streams available through its API, but doesn&#x27;t otherwise do anything useful with them. I don&#x27;t believe any of the GUIs process them either. Some reverse engineering work is needed. reply argilo 1 hour agorootparentAlso, all of those stream types seem to be used across multiple stations. I haven&#x27;t seen any weird one-off streams. reply anonymousiam 1 hour agoparentprevAwesome project! I hope it doesn&#x27;t get taken down by DTS!Here&#x27;s my question: Was there a reason for not using the GNU Radio framework for this project?https:&#x2F;&#x2F;www.gnuradio.orgThanks! reply argilo 1 hour agorootparentThat decision was made by the original author (Andrew Wesie), but I think it makes sense because nrsc5 only needs a tiny fraction of the functionality that GNU Radio provides. Implementing the functionality directly in nrsc5 avoids the need for a large and complex dependency.One downside is that we don&#x27;t get the broad hardware support that GNU Radio provides, but maybe we&#x27;ll someday take another crack at integrating SoapySDR. (See https:&#x2F;&#x2F;github.com&#x2F;theori-io&#x2F;nrsc5&#x2F;pull&#x2F;131 for an earlier experiment with that.) reply westurner 11 hours agoparentprevWould it be feasible to do something similar with OpenWRT opkg packages to support capturing weather radar (and weather forecasts and alerts?) data from digital FM radio with a USB RTL-SDR radio?Python apps require a bunch of disk space, which is at a premium on low-wattage always-on routers.OpenWRT&#x27;s luci-app-statistics application supports rrdtool and collectd for archived stats over time (optionally on a USB stick or an SSD instead of the flash ROM of the router, which has a max lifetime in terms of number of writes) https:&#x2F;&#x2F;github.com&#x2F;openwrt&#x2F;luci&#x2F;tree&#x2F;master&#x2F;applications&#x2F;luc...From https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38138230 :> LuCI is the OpenWRT web UI which is written in Lua; which is now implemented mostly as a JSON-RPC API instead of with server-side HTML templates for usability and performance on embedded devices. [...] Notes on how to write a LuCI app in Lua: reply argilo 11 hours agorootparentIt might be possible, but I&#x27;m not sure whether a typical router would have enough CPU horsepower to do the processing required to demodulate the signal. reply westurner 9 hours agorootparentWhat models of Raspberry Pi are sufficient, or how many Mhz and RAM are necessary to demodulate an HD radio stream?(Pi Pico, Pi Zero, and Pi A+&#x2F;B+&#x2F;2&#x2F;3&#x2F;4 have 2x20 pin headers for HATs. Orange Pi 5 Plus has hardware H.265 encoding with hw-enc and gstreamer fwiu.) reply argilo 5 hours agorootparentI haven&#x27;t investigated the CPU and RAM requirements in depth, but I have used nrsc5 on a Pi 3B without issue.I suspect a Pi Pico would be too small. reply argilo 3 hours agorootparentprevAlso, I should mention that Python is not a requirement. The GUIs that people have created so far were written in Python, but the nrsc5 project itself is built entirely in C, and it exposes a C API. reply wg0 12 hours agoparentprevWhat dongles and antennas are recommended. Asking as a noob always fascinated by SDR.Also, is SDR computationally expensive? reply argilo 12 hours agorootparentMy preferred dongle is the RTL-SDR Blog V3: https:&#x2F;&#x2F;www.rtl-sdr.com&#x2F;buy-rtl-sdr-dvb-t-dongles&#x2F;They also have a new V4 version, but it requires updated drivers that will take a bit of time to percolate out to Linux distributions and the like.NooElec also has a line of RTL-SDR dongles that I would recommend: https:&#x2F;&#x2F;www.nooelec.com&#x2F;store&#x2F;sdr&#x2F;sdr-receivers.htmlThe best antenna will depend on what you&#x27;re trying to receive. Different antennas are optimized for different frequency ranges, and may be either omnidirectional (receiving equally well from many directions) or favour reception in one particular direction. To get started, you could buy a dongle that comes with an antenna included, but you may eventually want to switch to something targeted to the particular application you have in mind.As for the computational cost, it depends a lot on what you&#x27;re trying to do. As a general rule, the wider the bandwidth of the signal, the more processing power it will take to demodulate. HD Radio is relatively narrow-band (400 kHz), so it doesn&#x27;t require much CPU power to receive it. reply cf100clunk 2 hours agorootparentFM Band and TV antennae are easy to DIY, and you&#x27;ll find loads of great leading and bleeding edge designs and plans online at forums like this:https:&#x2F;&#x2F;www.digitalhome.ca&#x2F;forums&#x2F;antenna-research-developme... reply cf100clunk 2 hours agoprevGiven the recent discussion about integrated FM radio receivers in mobile phones, I wonder if those tuners could be used internally in place of a dongle? A bit of discussion here:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37778149This would be outside my capabilities. reply argilo 1 hour agoparentIt would only be possible if the tuner exposed raw RF samples, the way RTL-SDR dongles do. I&#x27;m not sure whether any of the integrated FM receivers do that. reply shane350 12 hours agoprevThis is actually extremely useful when hooked up to a laptop for traveling, because of the embedded traffic information and maps in the sideband data. reply ghotli 12 hours agoparentIf you&#x27;re willing to share more on this subject, color me interested. reply argilo 12 hours agorootparentSome stations send out traffic and weather images (as well as album art and station logos). The files can be dumped to disk using nrsc5&#x27;s \"--dump-aas-files\" option. A few people have built GUIs that display the information in a more convenient way:https:&#x2F;&#x2F;github.com&#x2F;cmnybo&#x2F;nrsc5-gui https:&#x2F;&#x2F;github.com&#x2F;markjfine&#x2F;nrsc5-dui https:&#x2F;&#x2F;github.com&#x2F;KYDronePilot&#x2F;hdfm reply argilo 12 hours agorootparentIn addition to files, some stations also send out \"stream\" and \"packet\" data. There is ongoing work to reverse engineer the formats. See the discussion here for details: https:&#x2F;&#x2F;github.com&#x2F;theori-io&#x2F;nrsc5&#x2F;pull&#x2F;308 reply westurner 12 hours agorootparentAre there yet Clock, Weather Forecast, or Emergency Alert text data channels in digital FM radio?FWIU there are also DVB data streams? reply argilo 11 hours agorootparentTime information is broadcast, but in my experience it&#x27;s often inaccurate.There&#x27;s also a special stream for emergency alerts, but I haven&#x27;t seen it in use.There are various data streams, but not DVB.A lot of the details are described in the standard: https:&#x2F;&#x2F;www.nrscstandards.org&#x2F;standards-and-guidelines&#x2F;docum... reply westurner 11 hours agorootparentDVB-T could technically carry clock, weather forecasts, and alerts as text data feeds.What needs to be done to link WEA Wireless Emergency Alerts with HD radio data streams? WX radio could possibly embed a data channel? If it doesn&#x27;t already for e.g. accessible captioning?DVB-T: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DVB-T :> This system transmits compressed digital audio, digital video and other data in an MPEG transport stream, using coded orthogonal frequency-division multiplexing (COFDM or OFDM) modulation.From https:&#x2F;&#x2F;www.rtl-sdr.com&#x2F;about-rtl-sdr&#x2F; :> The origins of RTL-SDR stem from mass produced DVB-T TV tuner dongles that were based on the RTL2832U chipset. [...]> Over the years since its discovery RTL-SDR has become extremely popular and has democratized access to the radio spectrum. Now anyone including hobbyists on a budget can access the radio spectrum. It&#x27;s worth noting that this sort of SDR capability would have cost hundreds or even thousands of dollars just a few years ago. The RTL-SDR is also sometimes referred to as RTL2832U, DVB-T SDR, DVB-T dongle, RTL dongle, or the \"cheap software defined radio\"From https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;RTLSDR&#x2F;comments&#x2F;6nsnqy&#x2F;comment&#x2F;dkbv... :> [You need an upconverter to receive the time from the WWV shortwave clock station on 2.5, 5, 10, 15, and 20 MHz] http:&#x2F;&#x2F;www.nooelec.com&#x2F;store&#x2F;ham-it-up.html From https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37712506 :> TIL there&#x27;s a regular heartbeat in the quantum foam; [...] https:&#x2F;&#x2F;journals.aps.org&#x2F;prresearch&#x2F;abstract&#x2F;10.1103&#x2F;PhysRev... replycf100clunk 15 hours agoprevPreviously discussed at HN:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14564279 reply greesil 14 hours agoparentOops. It was new to me. reply cf100clunk 2 hours agorootparentNothing personal, not a dig, I&#x27;m a data archive exhumer by nature. I and others add such links if there has already been a bit of commentary on the topic. As others have said, reposts can often be valuable. Here are a couple more:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25192522https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27317802 reply carbocation 14 hours agorootparentprevPreviously discussed is not bad--especially since the last discussion was 5+ years ago! reply cosmojg 12 hours agorootparentprevHN encourages good reposts. You did good. reply wkat4242 6 hours agoprevIs this like DAB&#x2F;DAB+? I&#x27;ve never heard of NRSC-5. reply cf100clunk 2 hours agoparentIt is commercially known as the iBiquity In-Band-On-Channel (IBOC) system a.k.a. HD Radio, now owned by DTS. It has been implemented in the North American FM radio band and features multiplexing of 2 or more subchannels per station. It has been introduced in the AM band as well but a variety of technical issues have tended to be too problematic. Unlike DAB, which has its own dedicated bandwith, HD Radio is a system of sharing an analogue FM radio station&#x27;s existing bandwidth with a slice of digital payload. HD Radio-capable receivers pick up the analogue signal as usual, and if the digital streams are viable will switch to reproducing them for the listener. HD Radio can be broadcast digital-only, but I&#x27;m not personally up-to-date on any presently doing that, given all the legacy analogue FM receivers out there. reply psim1 1 hour agorootparent> if the digital streams are viable will switch to reproducing them for the listener.This produces a weird effect when listening in the car and moving in-and-out of good digital SNR range. When the radio switches between analog and digital, unless the station has them perfectly synced (never), there&#x27;s always a little echo or stutter. It seems like it would be nothing but it&#x27;s quite annoying. reply cf100clunk 1 hour agorootparentParticularly annoying while driving in very hilly areas, that&#x27;s for sure. There are some traits of signal propogation that are very difficult to work around, even with buffering and error correction remedies. reply argilo 1 hour agorootparentprevI&#x27;m not aware of any digital-only FM stations either. But there are a handful of digital-only AM stations, like WWFD. reply argilo 6 hours agoparentprevYes, it is similar. It is used primarily in North America, while DAB is used primarily in Europe. reply donatj 15 hours agoprevOooh nice. I&#x27;ve been wanting to listen to HD radio with my SDR for years. Tried a couple libraries that claimed to work but never had much success. reply homero 14 hours agoprevWhat&#x27;s surprising is HD doesn&#x27;t mean high definition. It doesn&#x27;t mean anything they just named it HD Radio reply hunter2_ 13 hours agoparentHybrid digital, which I believe refers to having both analog and digital over a single carrier. reply argilo 13 hours agorootparentI&#x27;ve heard that term as well, but the official site denies it: https:&#x2F;&#x2F;hdradio.com&#x2F;trademark&#x2F; reply superkuh 16 hours agoprevBack in 2015 this title would&#x27;ve read, \"Listen to HD radio with a $8 RTL SDR dongle\".First we ran out of E4000 tuners for the rtl-sdr dongles... but there were R820T. Then then R820T ran out and R820T2 replaced them. Then a proliferation of slightly less task specific R828D variants. The price steadily increased. Then the pandemic hit and it spiked even more.At $8 a pop you could afford to use many of them mod them, etc. But at $30 you pretty much treat it like any other SDR. reply greesil 14 hours agoparentI was originally going to have $20 in the headline and then I did a price check on Amazon. Man, the times have changed. reply londons_explore 8 hours agorootparentI&#x27;m pretty sure the price has gone up because Amazon charges a huge markup&#x2F;fees.If you buy it from the source, it&#x27;s now $9.https:&#x2F;&#x2F;www.aliexpress.com&#x2F;item&#x2F;1005003302259707.html reply argilo 5 hours agorootparentModels targeting the SDR market (e.g. from RTL-SDR Blog or NooElec) do have some improvements compared to models sold as TV tuners. Most importantly, they often include a more accurate crystal oscillator, which is useful when receiving narrow-band signals. reply superkuh 2 hours agorootparentprevI was ready to say you&#x27;ve made my day but unfortunately those rtlsdr dongles use the mostly unwanted Fitipower FC0012 tuner and not any of the better Raphael tuners (R820T2&#x2F;etc) that are most common and expected for rtl-sdr dongles. The frequency range supported and the rtl-sdr software ecosystem for FC0012 are minimal. Elonics E4000 (E4K) 54 - 2200 MHz (1100 MHz-1250 MHz gap) Rafael Micro R820T 24 - 1766 MHz (>1500 MHz is bad w&#x2F;o tuner cooling) Rafael Micro R820T2 24 - 1766 MHz (>1500 MHz is bad w&#x2F;o tuner cooling) Rafael Micro R820T2 13 - 1864 MHz (mutability&#x27;s driver) Fitipower FC0012 22 - 948 MHz Fitipower FC0013 22 - 1100 MHz (FC0013B&#x2F;C, FC0013G has seperate L band input) FCI FC2580 146 - 308 MHz and 438 - 924 MHzLooking around on aliexpress most at that price at FC0012 tuner based (even if they say R820T). There are a couple actual R828D dongles but the price shown seems to be one time thing \"welcome deal\"&#x2F;75% off and the actual price is more like $25. Dang. Still, I&#x27;ll keep my eyes on aliexpress now as well as ebay, so thanks. reply H8crilA 15 hours agoparentprevYou&#x27;re all correct, except there are not that many $30 SDRs out there. Most of them start at around $200 and end in very very high thousands. Also, most SDRs have clearly superior capabilities to the RTL-SDR, though often those capabilities are not required.BTW, for simple AM&#x2F;FM demodulation I cannot recommend enough the handy https:&#x2F;&#x2F;github.com&#x2F;charlie-foxtrot&#x2F;RTLSDR-Airband. Most of the LiveATC.net feeds run on it, for example. It can be used for any AM&#x2F;FM audio, for any number of simultaneous channels, even on a weak machine like a raspberry pi. reply londons_explore 8 hours agoprevHow strong a signal do you need for this to work?I have an RTL-SDR (R828D tuner) and I can just about receive regular FM stations, but the quality is pretty bad - voice is just about intelligible through the fuzz.I don&#x27;t know if my rtlsdr is bad, my antenna is bad, the signal strength is weak, I have interference, or this is just the quality expected from a $10 &#x27;TV tuner&#x27;. reply argilo 5 hours agoparentRTL-SDR tuners usually have very good reception between 50 and 1000 MHz, which includes the FM broadcast band. It is easy to overload the analog-to-digital converter, so it&#x27;s important to make sure the gain is not set too high. Setting the gain too low could also prevent reception if you&#x27;re far from the transmitter. If you&#x27;re receiving from indoors, interference from electronics in your house can cause trouble, and walls attenuate radio signals. You could try moving the antenna, or check whether it works better outdoors. reply ronsor 15 hours agoprev [â€“] If only there were a good library for using DRM (digital radio mondiale) with SDRs. reply Johnythree 15 hours agoparent [â€“] Are you referring to AM&#x2F;SW DRM, or VHF DRM?For AM&#x2F;SW DRM, the \"Dream DRM decoder\" works fine for me.The trouble is that most of the Broadcasters have abandoned AM DRM.I just now did a quick scan. Of the 17 MW&#x2F;SW channels I have programmed, there isn&#x27;t one on air with DRM. reply ronsor 11 hours agorootparentI&#x27;ve used Dream before, and it works fine, but I wish there was a more modular option in the form of a library. That would be easier to plug into SDR tools.I would write one when I get the time, but I&#x27;ve never been able to find the actual specification anywhere. reply argilo 5 hours agorootparentThe documents look to be available here: https:&#x2F;&#x2F;www.drm.org&#x2F;specification&#x2F; reply argilo 14 hours agorootparentprev [â€“] DRM broadcasts are definitely hard to find. A schedule is available here: https:&#x2F;&#x2F;www.drm.org&#x2F;what-can-i-hear&#x2F;broadcast-schedule-2&#x2F; replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The program nrsc5 utilizes an RTL-SDR dongle to receive NRSC-5 digital radio stations and offers both a command-line interface and an API.",
      "The user is required to compile the program with the guide given, which includes explicit instructions for various system configurations, as well as sample file testing and troubleshooting.",
      "Additional commands are provided for audio program transitioning and exiting. For smooth operation on Windows, users are advised to make adjustments using Zadig on the USB driver."
    ],
    "commentSummary": [
      "The nrsc5 is a program that utilizes an RTL-SDR dongle to receive NRSC-5 digital radio stations, providing a command-line interface and an API for the development of other applications.",
      "The program comes with comprehensive instructions for building it on various systems like Ubuntu, Debian, Raspbian, and Windows using different build options, along with advice for troubleshooting and testing with a sample capture file.",
      "Users are advised to use Zadig to prevent errors by altering the USB driver, particularly for Windows systems, and provided with keyboard commands for audio program switching, quitting, and command-line options."
    ],
    "points": 176,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1699231668
  }
]
